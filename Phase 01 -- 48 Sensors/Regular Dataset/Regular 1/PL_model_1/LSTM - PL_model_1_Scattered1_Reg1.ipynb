{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered1_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.343526</td>\n",
       "      <td>132.086973</td>\n",
       "      <td>81.738024</td>\n",
       "      <td>120.226650</td>\n",
       "      <td>127.621983</td>\n",
       "      <td>141.734180</td>\n",
       "      <td>104.808368</td>\n",
       "      <td>135.708107</td>\n",
       "      <td>100.574201</td>\n",
       "      <td>121.172680</td>\n",
       "      <td>...</td>\n",
       "      <td>100.001333</td>\n",
       "      <td>93.030358</td>\n",
       "      <td>108.610318</td>\n",
       "      <td>80.971633</td>\n",
       "      <td>127.068253</td>\n",
       "      <td>113.273672</td>\n",
       "      <td>102.118525</td>\n",
       "      <td>120.868728</td>\n",
       "      <td>81.395424</td>\n",
       "      <td>119.242231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.733610</td>\n",
       "      <td>136.739848</td>\n",
       "      <td>85.595980</td>\n",
       "      <td>121.531174</td>\n",
       "      <td>128.150768</td>\n",
       "      <td>146.754292</td>\n",
       "      <td>112.456378</td>\n",
       "      <td>142.308630</td>\n",
       "      <td>99.220561</td>\n",
       "      <td>113.623552</td>\n",
       "      <td>...</td>\n",
       "      <td>94.944468</td>\n",
       "      <td>92.532748</td>\n",
       "      <td>102.178885</td>\n",
       "      <td>85.745827</td>\n",
       "      <td>127.498364</td>\n",
       "      <td>101.891695</td>\n",
       "      <td>100.754638</td>\n",
       "      <td>132.995837</td>\n",
       "      <td>81.702073</td>\n",
       "      <td>109.877760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.503893</td>\n",
       "      <td>122.812742</td>\n",
       "      <td>79.306588</td>\n",
       "      <td>116.294374</td>\n",
       "      <td>121.246788</td>\n",
       "      <td>147.866563</td>\n",
       "      <td>114.673974</td>\n",
       "      <td>145.754454</td>\n",
       "      <td>98.140959</td>\n",
       "      <td>113.898725</td>\n",
       "      <td>...</td>\n",
       "      <td>106.442714</td>\n",
       "      <td>104.297185</td>\n",
       "      <td>109.932826</td>\n",
       "      <td>83.496819</td>\n",
       "      <td>130.000814</td>\n",
       "      <td>106.665402</td>\n",
       "      <td>100.613200</td>\n",
       "      <td>124.160411</td>\n",
       "      <td>71.817836</td>\n",
       "      <td>119.550575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.439048</td>\n",
       "      <td>128.212611</td>\n",
       "      <td>91.820230</td>\n",
       "      <td>108.778849</td>\n",
       "      <td>126.667009</td>\n",
       "      <td>154.660799</td>\n",
       "      <td>114.044773</td>\n",
       "      <td>138.616554</td>\n",
       "      <td>103.117246</td>\n",
       "      <td>119.596471</td>\n",
       "      <td>...</td>\n",
       "      <td>102.668226</td>\n",
       "      <td>97.500489</td>\n",
       "      <td>103.068480</td>\n",
       "      <td>85.886335</td>\n",
       "      <td>135.283285</td>\n",
       "      <td>109.376512</td>\n",
       "      <td>105.742410</td>\n",
       "      <td>129.627503</td>\n",
       "      <td>75.726365</td>\n",
       "      <td>114.868672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.124270</td>\n",
       "      <td>136.003305</td>\n",
       "      <td>84.028696</td>\n",
       "      <td>124.663682</td>\n",
       "      <td>135.153193</td>\n",
       "      <td>143.579315</td>\n",
       "      <td>110.567758</td>\n",
       "      <td>136.614045</td>\n",
       "      <td>99.838684</td>\n",
       "      <td>120.974244</td>\n",
       "      <td>...</td>\n",
       "      <td>105.042089</td>\n",
       "      <td>105.829707</td>\n",
       "      <td>104.026800</td>\n",
       "      <td>91.996143</td>\n",
       "      <td>128.574882</td>\n",
       "      <td>114.729913</td>\n",
       "      <td>107.554774</td>\n",
       "      <td>130.510348</td>\n",
       "      <td>72.869732</td>\n",
       "      <td>117.584639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>123.531786</td>\n",
       "      <td>111.827755</td>\n",
       "      <td>107.779643</td>\n",
       "      <td>82.168949</td>\n",
       "      <td>160.639754</td>\n",
       "      <td>141.197845</td>\n",
       "      <td>140.466814</td>\n",
       "      <td>118.054020</td>\n",
       "      <td>115.311765</td>\n",
       "      <td>108.883964</td>\n",
       "      <td>...</td>\n",
       "      <td>105.092582</td>\n",
       "      <td>116.499181</td>\n",
       "      <td>108.017102</td>\n",
       "      <td>63.983262</td>\n",
       "      <td>136.902008</td>\n",
       "      <td>119.899001</td>\n",
       "      <td>129.392320</td>\n",
       "      <td>115.266232</td>\n",
       "      <td>108.386809</td>\n",
       "      <td>75.319480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>123.851078</td>\n",
       "      <td>116.655550</td>\n",
       "      <td>108.100194</td>\n",
       "      <td>74.078485</td>\n",
       "      <td>145.023257</td>\n",
       "      <td>137.365691</td>\n",
       "      <td>132.357551</td>\n",
       "      <td>125.922760</td>\n",
       "      <td>113.876599</td>\n",
       "      <td>105.546221</td>\n",
       "      <td>...</td>\n",
       "      <td>115.201369</td>\n",
       "      <td>120.309415</td>\n",
       "      <td>107.178216</td>\n",
       "      <td>66.857463</td>\n",
       "      <td>129.040700</td>\n",
       "      <td>112.293239</td>\n",
       "      <td>126.342438</td>\n",
       "      <td>119.304489</td>\n",
       "      <td>102.924841</td>\n",
       "      <td>80.198171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>124.844037</td>\n",
       "      <td>107.987896</td>\n",
       "      <td>112.489683</td>\n",
       "      <td>79.497709</td>\n",
       "      <td>144.453616</td>\n",
       "      <td>139.741209</td>\n",
       "      <td>131.974434</td>\n",
       "      <td>112.734939</td>\n",
       "      <td>115.106630</td>\n",
       "      <td>101.798861</td>\n",
       "      <td>...</td>\n",
       "      <td>107.010916</td>\n",
       "      <td>123.180474</td>\n",
       "      <td>115.516488</td>\n",
       "      <td>68.961718</td>\n",
       "      <td>138.727408</td>\n",
       "      <td>117.477414</td>\n",
       "      <td>132.911279</td>\n",
       "      <td>115.787103</td>\n",
       "      <td>112.693310</td>\n",
       "      <td>92.181445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>121.997334</td>\n",
       "      <td>111.025481</td>\n",
       "      <td>110.218214</td>\n",
       "      <td>80.971138</td>\n",
       "      <td>153.382936</td>\n",
       "      <td>137.157752</td>\n",
       "      <td>145.942473</td>\n",
       "      <td>103.728702</td>\n",
       "      <td>115.493008</td>\n",
       "      <td>109.567823</td>\n",
       "      <td>...</td>\n",
       "      <td>105.686902</td>\n",
       "      <td>111.360597</td>\n",
       "      <td>108.686738</td>\n",
       "      <td>72.850986</td>\n",
       "      <td>137.977188</td>\n",
       "      <td>115.350605</td>\n",
       "      <td>134.436456</td>\n",
       "      <td>117.627324</td>\n",
       "      <td>112.260333</td>\n",
       "      <td>82.313619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>135.537949</td>\n",
       "      <td>113.579156</td>\n",
       "      <td>101.360531</td>\n",
       "      <td>74.057339</td>\n",
       "      <td>147.118505</td>\n",
       "      <td>139.976279</td>\n",
       "      <td>136.489963</td>\n",
       "      <td>116.466831</td>\n",
       "      <td>118.252801</td>\n",
       "      <td>103.166842</td>\n",
       "      <td>...</td>\n",
       "      <td>105.027692</td>\n",
       "      <td>119.419711</td>\n",
       "      <td>106.443638</td>\n",
       "      <td>69.086185</td>\n",
       "      <td>140.365965</td>\n",
       "      <td>119.729570</td>\n",
       "      <td>139.456786</td>\n",
       "      <td>116.487862</td>\n",
       "      <td>108.080218</td>\n",
       "      <td>84.064615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0      99.343526  132.086973   81.738024  120.226650  127.621983  141.734180   \n",
       "1     104.733610  136.739848   85.595980  121.531174  128.150768  146.754292   \n",
       "2     102.503893  122.812742   79.306588  116.294374  121.246788  147.866563   \n",
       "3     105.439048  128.212611   91.820230  108.778849  126.667009  154.660799   \n",
       "4     104.124270  136.003305   84.028696  124.663682  135.153193  143.579315   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  123.531786  111.827755  107.779643   82.168949  160.639754  141.197845   \n",
       "2439  123.851078  116.655550  108.100194   74.078485  145.023257  137.365691   \n",
       "2440  124.844037  107.987896  112.489683   79.497709  144.453616  139.741209   \n",
       "2441  121.997334  111.025481  110.218214   80.971138  153.382936  137.157752   \n",
       "2442  135.537949  113.579156  101.360531   74.057339  147.118505  139.976279   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     104.808368  135.708107  100.574201  121.172680  ...  100.001333   \n",
       "1     112.456378  142.308630   99.220561  113.623552  ...   94.944468   \n",
       "2     114.673974  145.754454   98.140959  113.898725  ...  106.442714   \n",
       "3     114.044773  138.616554  103.117246  119.596471  ...  102.668226   \n",
       "4     110.567758  136.614045   99.838684  120.974244  ...  105.042089   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  140.466814  118.054020  115.311765  108.883964  ...  105.092582   \n",
       "2439  132.357551  125.922760  113.876599  105.546221  ...  115.201369   \n",
       "2440  131.974434  112.734939  115.106630  101.798861  ...  107.010916   \n",
       "2441  145.942473  103.728702  115.493008  109.567823  ...  105.686902   \n",
       "2442  136.489963  116.466831  118.252801  103.166842  ...  105.027692   \n",
       "\n",
       "              39          40         41          42          43          44  \\\n",
       "0      93.030358  108.610318  80.971633  127.068253  113.273672  102.118525   \n",
       "1      92.532748  102.178885  85.745827  127.498364  101.891695  100.754638   \n",
       "2     104.297185  109.932826  83.496819  130.000814  106.665402  100.613200   \n",
       "3      97.500489  103.068480  85.886335  135.283285  109.376512  105.742410   \n",
       "4     105.829707  104.026800  91.996143  128.574882  114.729913  107.554774   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  116.499181  108.017102  63.983262  136.902008  119.899001  129.392320   \n",
       "2439  120.309415  107.178216  66.857463  129.040700  112.293239  126.342438   \n",
       "2440  123.180474  115.516488  68.961718  138.727408  117.477414  132.911279   \n",
       "2441  111.360597  108.686738  72.850986  137.977188  115.350605  134.436456   \n",
       "2442  119.419711  106.443638  69.086185  140.365965  119.729570  139.456786   \n",
       "\n",
       "              45          46          47  \n",
       "0     120.868728   81.395424  119.242231  \n",
       "1     132.995837   81.702073  109.877760  \n",
       "2     124.160411   71.817836  119.550575  \n",
       "3     129.627503   75.726365  114.868672  \n",
       "4     130.510348   72.869732  117.584639  \n",
       "...          ...         ...         ...  \n",
       "2438  115.266232  108.386809   75.319480  \n",
       "2439  119.304489  102.924841   80.198171  \n",
       "2440  115.787103  112.693310   92.181445  \n",
       "2441  117.627324  112.260333   82.313619  \n",
       "2442  116.487862  108.080218   84.064615  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.343526</td>\n",
       "      <td>132.086973</td>\n",
       "      <td>81.738024</td>\n",
       "      <td>120.226650</td>\n",
       "      <td>127.621983</td>\n",
       "      <td>141.734180</td>\n",
       "      <td>104.808368</td>\n",
       "      <td>135.708107</td>\n",
       "      <td>100.574201</td>\n",
       "      <td>121.172680</td>\n",
       "      <td>...</td>\n",
       "      <td>100.001333</td>\n",
       "      <td>93.030358</td>\n",
       "      <td>108.610318</td>\n",
       "      <td>80.971633</td>\n",
       "      <td>127.068253</td>\n",
       "      <td>113.273672</td>\n",
       "      <td>102.118525</td>\n",
       "      <td>120.868728</td>\n",
       "      <td>81.395424</td>\n",
       "      <td>119.242231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.733610</td>\n",
       "      <td>136.739848</td>\n",
       "      <td>85.595980</td>\n",
       "      <td>121.531174</td>\n",
       "      <td>128.150768</td>\n",
       "      <td>146.754292</td>\n",
       "      <td>112.456378</td>\n",
       "      <td>142.308630</td>\n",
       "      <td>99.220561</td>\n",
       "      <td>113.623552</td>\n",
       "      <td>...</td>\n",
       "      <td>94.944468</td>\n",
       "      <td>92.532748</td>\n",
       "      <td>102.178885</td>\n",
       "      <td>85.745827</td>\n",
       "      <td>127.498364</td>\n",
       "      <td>101.891695</td>\n",
       "      <td>100.754638</td>\n",
       "      <td>132.995837</td>\n",
       "      <td>81.702073</td>\n",
       "      <td>109.877760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.503893</td>\n",
       "      <td>122.812742</td>\n",
       "      <td>79.306588</td>\n",
       "      <td>116.294374</td>\n",
       "      <td>121.246788</td>\n",
       "      <td>147.866563</td>\n",
       "      <td>114.673974</td>\n",
       "      <td>145.754454</td>\n",
       "      <td>98.140959</td>\n",
       "      <td>113.898725</td>\n",
       "      <td>...</td>\n",
       "      <td>106.442714</td>\n",
       "      <td>104.297185</td>\n",
       "      <td>109.932826</td>\n",
       "      <td>83.496819</td>\n",
       "      <td>130.000814</td>\n",
       "      <td>106.665402</td>\n",
       "      <td>100.613200</td>\n",
       "      <td>124.160411</td>\n",
       "      <td>71.817836</td>\n",
       "      <td>119.550575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.439048</td>\n",
       "      <td>128.212611</td>\n",
       "      <td>91.820230</td>\n",
       "      <td>108.778849</td>\n",
       "      <td>126.667009</td>\n",
       "      <td>154.660799</td>\n",
       "      <td>114.044773</td>\n",
       "      <td>138.616554</td>\n",
       "      <td>103.117246</td>\n",
       "      <td>119.596471</td>\n",
       "      <td>...</td>\n",
       "      <td>102.668226</td>\n",
       "      <td>97.500489</td>\n",
       "      <td>103.068480</td>\n",
       "      <td>85.886335</td>\n",
       "      <td>135.283285</td>\n",
       "      <td>109.376512</td>\n",
       "      <td>105.742410</td>\n",
       "      <td>129.627503</td>\n",
       "      <td>75.726365</td>\n",
       "      <td>114.868672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.124270</td>\n",
       "      <td>136.003305</td>\n",
       "      <td>84.028696</td>\n",
       "      <td>124.663682</td>\n",
       "      <td>135.153193</td>\n",
       "      <td>143.579315</td>\n",
       "      <td>110.567758</td>\n",
       "      <td>136.614045</td>\n",
       "      <td>99.838684</td>\n",
       "      <td>120.974244</td>\n",
       "      <td>...</td>\n",
       "      <td>105.042089</td>\n",
       "      <td>105.829707</td>\n",
       "      <td>104.026800</td>\n",
       "      <td>91.996143</td>\n",
       "      <td>128.574882</td>\n",
       "      <td>114.729913</td>\n",
       "      <td>107.554774</td>\n",
       "      <td>130.510348</td>\n",
       "      <td>72.869732</td>\n",
       "      <td>117.584639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>123.531786</td>\n",
       "      <td>111.827755</td>\n",
       "      <td>107.779643</td>\n",
       "      <td>82.168949</td>\n",
       "      <td>160.639754</td>\n",
       "      <td>141.197845</td>\n",
       "      <td>140.466814</td>\n",
       "      <td>118.054020</td>\n",
       "      <td>115.311765</td>\n",
       "      <td>108.883964</td>\n",
       "      <td>...</td>\n",
       "      <td>105.092582</td>\n",
       "      <td>116.499181</td>\n",
       "      <td>108.017102</td>\n",
       "      <td>63.983262</td>\n",
       "      <td>136.902008</td>\n",
       "      <td>119.899001</td>\n",
       "      <td>129.392320</td>\n",
       "      <td>115.266232</td>\n",
       "      <td>108.386809</td>\n",
       "      <td>75.319480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>123.851078</td>\n",
       "      <td>116.655550</td>\n",
       "      <td>108.100194</td>\n",
       "      <td>74.078485</td>\n",
       "      <td>145.023257</td>\n",
       "      <td>137.365691</td>\n",
       "      <td>132.357551</td>\n",
       "      <td>125.922760</td>\n",
       "      <td>113.876599</td>\n",
       "      <td>105.546221</td>\n",
       "      <td>...</td>\n",
       "      <td>115.201369</td>\n",
       "      <td>120.309415</td>\n",
       "      <td>107.178216</td>\n",
       "      <td>66.857463</td>\n",
       "      <td>129.040700</td>\n",
       "      <td>112.293239</td>\n",
       "      <td>126.342438</td>\n",
       "      <td>119.304489</td>\n",
       "      <td>102.924841</td>\n",
       "      <td>80.198171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>124.844037</td>\n",
       "      <td>107.987896</td>\n",
       "      <td>112.489683</td>\n",
       "      <td>79.497709</td>\n",
       "      <td>144.453616</td>\n",
       "      <td>139.741209</td>\n",
       "      <td>131.974434</td>\n",
       "      <td>112.734939</td>\n",
       "      <td>115.106630</td>\n",
       "      <td>101.798861</td>\n",
       "      <td>...</td>\n",
       "      <td>107.010916</td>\n",
       "      <td>123.180474</td>\n",
       "      <td>115.516488</td>\n",
       "      <td>68.961718</td>\n",
       "      <td>138.727408</td>\n",
       "      <td>117.477414</td>\n",
       "      <td>132.911279</td>\n",
       "      <td>115.787103</td>\n",
       "      <td>112.693310</td>\n",
       "      <td>92.181445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>121.997334</td>\n",
       "      <td>111.025481</td>\n",
       "      <td>110.218214</td>\n",
       "      <td>80.971138</td>\n",
       "      <td>153.382936</td>\n",
       "      <td>137.157752</td>\n",
       "      <td>145.942473</td>\n",
       "      <td>103.728702</td>\n",
       "      <td>115.493008</td>\n",
       "      <td>109.567823</td>\n",
       "      <td>...</td>\n",
       "      <td>105.686902</td>\n",
       "      <td>111.360597</td>\n",
       "      <td>108.686738</td>\n",
       "      <td>72.850986</td>\n",
       "      <td>137.977188</td>\n",
       "      <td>115.350605</td>\n",
       "      <td>134.436456</td>\n",
       "      <td>117.627324</td>\n",
       "      <td>112.260333</td>\n",
       "      <td>82.313619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>135.537949</td>\n",
       "      <td>113.579156</td>\n",
       "      <td>101.360531</td>\n",
       "      <td>74.057339</td>\n",
       "      <td>147.118505</td>\n",
       "      <td>139.976279</td>\n",
       "      <td>136.489963</td>\n",
       "      <td>116.466831</td>\n",
       "      <td>118.252801</td>\n",
       "      <td>103.166842</td>\n",
       "      <td>...</td>\n",
       "      <td>105.027692</td>\n",
       "      <td>119.419711</td>\n",
       "      <td>106.443638</td>\n",
       "      <td>69.086185</td>\n",
       "      <td>140.365965</td>\n",
       "      <td>119.729570</td>\n",
       "      <td>139.456786</td>\n",
       "      <td>116.487862</td>\n",
       "      <td>108.080218</td>\n",
       "      <td>84.064615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0      99.343526  132.086973   81.738024  120.226650  127.621983  141.734180   \n",
       "1     104.733610  136.739848   85.595980  121.531174  128.150768  146.754292   \n",
       "2     102.503893  122.812742   79.306588  116.294374  121.246788  147.866563   \n",
       "3     105.439048  128.212611   91.820230  108.778849  126.667009  154.660799   \n",
       "4     104.124270  136.003305   84.028696  124.663682  135.153193  143.579315   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  123.531786  111.827755  107.779643   82.168949  160.639754  141.197845   \n",
       "2439  123.851078  116.655550  108.100194   74.078485  145.023257  137.365691   \n",
       "2440  124.844037  107.987896  112.489683   79.497709  144.453616  139.741209   \n",
       "2441  121.997334  111.025481  110.218214   80.971138  153.382936  137.157752   \n",
       "2442  135.537949  113.579156  101.360531   74.057339  147.118505  139.976279   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     104.808368  135.708107  100.574201  121.172680  ...  100.001333   \n",
       "1     112.456378  142.308630   99.220561  113.623552  ...   94.944468   \n",
       "2     114.673974  145.754454   98.140959  113.898725  ...  106.442714   \n",
       "3     114.044773  138.616554  103.117246  119.596471  ...  102.668226   \n",
       "4     110.567758  136.614045   99.838684  120.974244  ...  105.042089   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  140.466814  118.054020  115.311765  108.883964  ...  105.092582   \n",
       "2439  132.357551  125.922760  113.876599  105.546221  ...  115.201369   \n",
       "2440  131.974434  112.734939  115.106630  101.798861  ...  107.010916   \n",
       "2441  145.942473  103.728702  115.493008  109.567823  ...  105.686902   \n",
       "2442  136.489963  116.466831  118.252801  103.166842  ...  105.027692   \n",
       "\n",
       "        sensor40    sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      93.030358  108.610318  80.971633  127.068253  113.273672  102.118525   \n",
       "1      92.532748  102.178885  85.745827  127.498364  101.891695  100.754638   \n",
       "2     104.297185  109.932826  83.496819  130.000814  106.665402  100.613200   \n",
       "3      97.500489  103.068480  85.886335  135.283285  109.376512  105.742410   \n",
       "4     105.829707  104.026800  91.996143  128.574882  114.729913  107.554774   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  116.499181  108.017102  63.983262  136.902008  119.899001  129.392320   \n",
       "2439  120.309415  107.178216  66.857463  129.040700  112.293239  126.342438   \n",
       "2440  123.180474  115.516488  68.961718  138.727408  117.477414  132.911279   \n",
       "2441  111.360597  108.686738  72.850986  137.977188  115.350605  134.436456   \n",
       "2442  119.419711  106.443638  69.086185  140.365965  119.729570  139.456786   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     120.868728   81.395424  119.242231  \n",
       "1     132.995837   81.702073  109.877760  \n",
       "2     124.160411   71.817836  119.550575  \n",
       "3     129.627503   75.726365  114.868672  \n",
       "4     130.510348   72.869732  117.584639  \n",
       "...          ...         ...         ...  \n",
       "2438  115.266232  108.386809   75.319480  \n",
       "2439  119.304489  102.924841   80.198171  \n",
       "2440  115.787103  112.693310   92.181445  \n",
       "2441  117.627324  112.260333   82.313619  \n",
       "2442  116.487862  108.080218   84.064615  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 21ms/step - loss: 1390.1849 - val_loss: 1328.1595\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1219.3806 - val_loss: 1219.3280\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1128.7484 - val_loss: 1139.7385\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1062.3944 - val_loss: 1080.1826\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1013.4371 - val_loss: 1035.4844\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 977.9450 - val_loss: 1002.9455\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 953.0475 - val_loss: 980.1722\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 936.3830 - val_loss: 964.4529\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 925.8470 - val_loss: 954.5584\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 919.7126 - val_loss: 948.3938\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5307 - val_loss: 945.1021\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.0169 - val_loss: 943.2477\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.3844 - val_loss: 942.2045\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.1595 - val_loss: 941.6183\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0469 - val_loss: 941.4892\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0446 - val_loss: 941.3408\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0282 - val_loss: 941.2856\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0035 - val_loss: 941.3784\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0476 - val_loss: 941.2933\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0517 - val_loss: 941.3440\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0460 - val_loss: 941.1435\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0002 - val_loss: 941.1702\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0551 - val_loss: 941.1980\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0372 - val_loss: 941.2360\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0169 - val_loss: 941.3528\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.0121 - val_loss: 941.1898\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0070 - val_loss: 942.3691\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0230 - val_loss: 941.9718\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.0528 - val_loss: 941.6980\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 914.0394 - val_loss: 941.6045\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 903.3950 - val_loss: 895.3060\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 858.2704 - val_loss: 872.8333\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 842.5267 - val_loss: 861.1328\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 828.3555 - val_loss: 845.0976\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 801.9680 - val_loss: 809.6802\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 763.6009 - val_loss: 774.4266\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 719.5598 - val_loss: 725.0301\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 664.2103 - val_loss: 663.2909\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 604.7574 - val_loss: 599.7057\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 545.1226 - val_loss: 547.2289\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 496.8500 - val_loss: 498.8002\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 452.7393 - val_loss: 458.3050\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 413.8156 - val_loss: 417.3873\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 376.3008 - val_loss: 381.3181\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 341.2915 - val_loss: 342.8107\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 306.6942 - val_loss: 311.5640\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 277.2203 - val_loss: 282.6075\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 250.2773 - val_loss: 255.2623\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 225.6799 - val_loss: 229.1854\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 200.7426 - val_loss: 212.1701\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 180.5217 - val_loss: 184.2801\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 160.0993 - val_loss: 164.8002\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 141.1163 - val_loss: 143.6354\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 122.6371 - val_loss: 123.8607\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 109.7229 - val_loss: 108.7979\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 92.3372 - val_loss: 94.4886\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 80.3616 - val_loss: 82.4219\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 69.6668 - val_loss: 75.2794\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 62.6477 - val_loss: 66.8264\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.5386 - val_loss: 59.0513\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 48.2741 - val_loss: 52.8740\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.9098 - val_loss: 49.0581\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.8773 - val_loss: 40.8991\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.9071 - val_loss: 37.7803\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.3443 - val_loss: 31.9657\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.2339 - val_loss: 31.5705\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.6846 - val_loss: 27.2771\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4307 - val_loss: 24.8024\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.8252 - val_loss: 22.8683\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.0026 - val_loss: 20.0411\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.1325 - val_loss: 19.2953\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.0027 - val_loss: 19.4121\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.8878 - val_loss: 18.9381\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.0170 - val_loss: 16.0243\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8399 - val_loss: 15.1263\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1132 - val_loss: 13.8598\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.3497 - val_loss: 13.8931\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 10.8404 - val_loss: 12.8768\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.2160 - val_loss: 12.6934\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3735 - val_loss: 11.8292\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.2009 - val_loss: 12.0278\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7789 - val_loss: 10.4242\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7099 - val_loss: 18.1687\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4779 - val_loss: 10.0239\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.8447 - val_loss: 9.4218\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6158 - val_loss: 11.4704\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.8801 - val_loss: 8.8254\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6789 - val_loss: 9.9247\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4823 - val_loss: 10.1001\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7986 - val_loss: 8.7205\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5811 - val_loss: 8.0359\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0659 - val_loss: 9.0714\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2292 - val_loss: 8.3407\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8955 - val_loss: 7.1371\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4872 - val_loss: 8.4143\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8491 - val_loss: 7.9887\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7100 - val_loss: 8.0152\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1926 - val_loss: 7.2660\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2926 - val_loss: 9.1239\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5885 - val_loss: 7.8860\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0548 - val_loss: 7.5957\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3368 - val_loss: 7.3241\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6847 - val_loss: 7.3646\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.4547 - val_loss: 9.2360\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.6876 - val_loss: 5.8832\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.5916 - val_loss: 6.2928\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.7859 - val_loss: 6.7617\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5637 - val_loss: 7.4488\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1096 - val_loss: 10.6876\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7569 - val_loss: 6.9608\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2821 - val_loss: 6.7527\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0584 - val_loss: 7.3620\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7008 - val_loss: 7.6823\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3741 - val_loss: 6.5915\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7648 - val_loss: 6.6483\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8914 - val_loss: 7.2573\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0188 - val_loss: 6.3816\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5514 - val_loss: 16.1190\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4510 - val_loss: 6.4932\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3887 - val_loss: 6.3578\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7373 - val_loss: 6.5330\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0469 - val_loss: 6.1417\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5397 - val_loss: 7.0682\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0528 - val_loss: 6.3376\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.7448 - val_loss: 6.0819\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3846 - val_loss: 6.0320\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6380 - val_loss: 8.1338\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3350 - val_loss: 5.8825\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7885 - val_loss: 7.3127\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2539 - val_loss: 7.8556\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0090 - val_loss: 6.0903\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8778 - val_loss: 8.8229\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1457 - val_loss: 6.7400\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8149 - val_loss: 6.4244\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0513 - val_loss: 6.3489\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1458 - val_loss: 6.6136\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3076 - val_loss: 5.9482\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7585 - val_loss: 6.4973\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6764 - val_loss: 6.1283\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7714 - val_loss: 6.3552\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7739 - val_loss: 5.8821\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.6877 - val_loss: 5.6177\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4708 - val_loss: 6.1070\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4033 - val_loss: 7.0753\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7134 - val_loss: 6.0402\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1046 - val_loss: 6.5755\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4017 - val_loss: 5.6046\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6291 - val_loss: 6.7835\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1705 - val_loss: 29.4124\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1808 - val_loss: 6.0719\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0290 - val_loss: 5.4957\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9821 - val_loss: 5.3458\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4070 - val_loss: 6.0198\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6190 - val_loss: 5.8753\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1088 - val_loss: 5.7676\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1572 - val_loss: 5.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2045 - val_loss: 6.1845\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9639 - val_loss: 7.4291\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1116 - val_loss: 5.6244\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9495 - val_loss: 6.8217\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8923 - val_loss: 6.4719\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.4828 - val_loss: 7.3655\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1295 - val_loss: 5.6302\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9701 - val_loss: 5.3142\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.8042 - val_loss: 6.0470\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0465 - val_loss: 6.1443\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9712 - val_loss: 5.9885\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9876 - val_loss: 6.5661\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7790 - val_loss: 6.0926\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3592 - val_loss: 5.5327\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7087 - val_loss: 6.0443\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6172 - val_loss: 5.8047\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0603 - val_loss: 7.0703\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0241 - val_loss: 8.5038\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1676 - val_loss: 5.6973\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1596 - val_loss: 5.4910\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5789 - val_loss: 6.2058\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7885 - val_loss: 6.9569\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2461 - val_loss: 7.9834\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4145 - val_loss: 6.2527\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4724 - val_loss: 5.5260\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.5679 - val_loss: 6.7048\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0960 - val_loss: 7.6003\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7628 - val_loss: 5.7940\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3975 - val_loss: 5.7759\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3927 - val_loss: 6.1928\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3643 - val_loss: 5.5316\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6111 - val_loss: 5.9175\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9913 - val_loss: 6.0191\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7808 - val_loss: 5.7940\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5134 - val_loss: 5.9309\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3938 - val_loss: 5.8821\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3494 - val_loss: 14.5200\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6100 - val_loss: 5.6396\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1972 - val_loss: 5.6384\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2073 - val_loss: 5.6313\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2006 - val_loss: 5.6139\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2908 - val_loss: 5.9744\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3337 - val_loss: 5.7923\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7775 - val_loss: 6.0553\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 6.055262329147472\n",
      "Mean Absolute Error (MAE): 1.6706360875630077\n",
      "Root Mean Squared Error (RMSE): 2.460744263256032\n",
      "Time taken: 1254.8404185771942\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 20ms/step - loss: 1387.7184 - val_loss: 1269.2842\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1225.5197 - val_loss: 1165.1213\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1135.9152 - val_loss: 1088.2305\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1069.8518 - val_loss: 1031.2197\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1021.2286 - val_loss: 989.5848\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 985.8899 - val_loss: 959.7570\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 960.9485 - val_loss: 939.2227\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 944.3048 - val_loss: 925.9265\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 933.7226 - val_loss: 917.7479\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.5248 - val_loss: 913.1992\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.3366 - val_loss: 911.0135\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.7831 - val_loss: 909.9832\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.1473 - val_loss: 909.5569\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9030 - val_loss: 909.4741\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.8281 - val_loss: 909.4376\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7990 - val_loss: 909.4584\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.7963 - val_loss: 909.5203\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.8040 - val_loss: 909.4118\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 921.8173 - val_loss: 909.3626\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 921.7838 - val_loss: 909.3713\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7958 - val_loss: 909.4910\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7708 - val_loss: 909.4058\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7943 - val_loss: 909.4560\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7921 - val_loss: 909.4664\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8273 - val_loss: 909.4579\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7582 - val_loss: 909.5044\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7831 - val_loss: 909.4396\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7979 - val_loss: 909.4641\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7738 - val_loss: 909.4285\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7952 - val_loss: 909.3984\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.7973 - val_loss: 909.4609\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7849 - val_loss: 909.4074\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8116 - val_loss: 909.4675\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9276 - val_loss: 912.2086\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.5217 - val_loss: 908.9761\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 890.3036 - val_loss: 859.7822\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 858.8913 - val_loss: 841.1810\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 844.4346 - val_loss: 826.9745\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 834.0650 - val_loss: 816.3505\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 818.2834 - val_loss: 793.6619\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 778.1735 - val_loss: 748.6357\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 734.3450 - val_loss: 694.8957\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 665.5513 - val_loss: 628.4628\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 610.4603 - val_loss: 581.7076\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 565.0159 - val_loss: 537.4134\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 519.8949 - val_loss: 492.8163\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 479.9875 - val_loss: 452.7800\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 440.0500 - val_loss: 418.2692\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 405.7500 - val_loss: 385.3818\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 370.8091 - val_loss: 349.3881\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 339.2964 - val_loss: 323.5812\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 308.2952 - val_loss: 290.5268\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 281.0047 - val_loss: 261.0962\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 254.6696 - val_loss: 237.8243\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 229.5383 - val_loss: 216.6767\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 206.8695 - val_loss: 197.4987\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 187.6914 - val_loss: 176.3745\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 168.2521 - val_loss: 158.4016\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 151.1104 - val_loss: 140.6016\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 134.7531 - val_loss: 128.1190\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 119.6301 - val_loss: 113.0643\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 107.1774 - val_loss: 102.2759\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 96.0620 - val_loss: 91.1808\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 83.4330 - val_loss: 78.3767\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.6624 - val_loss: 70.8955\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 68.0196 - val_loss: 64.0241\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.5170 - val_loss: 59.5629\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.2624 - val_loss: 52.5042\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.7638 - val_loss: 45.5481\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 42.3011 - val_loss: 43.4428\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 40.0597 - val_loss: 38.1804\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.6362 - val_loss: 36.5948\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.0383 - val_loss: 32.3448\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.2997 - val_loss: 30.4043\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1913 - val_loss: 29.1458\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.5762 - val_loss: 23.8811\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.6259 - val_loss: 24.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.3501 - val_loss: 22.8844\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.2908 - val_loss: 19.8963\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 18.1333 - val_loss: 20.8259\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.2842 - val_loss: 18.6780\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.7909 - val_loss: 17.0897\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.6452 - val_loss: 26.9220\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.1716 - val_loss: 18.1007\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8408 - val_loss: 13.9559\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.4420 - val_loss: 14.7451\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.3243 - val_loss: 13.8522\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1028 - val_loss: 14.3781\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3853 - val_loss: 15.5974\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.7697 - val_loss: 12.9449\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.2808 - val_loss: 16.5409\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3293 - val_loss: 14.1979\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5337 - val_loss: 15.2198\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1457 - val_loss: 12.8360\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.5893 - val_loss: 17.2622\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.8158 - val_loss: 12.1802\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.4193 - val_loss: 12.0380\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 8.1473 - val_loss: 10.4826\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5148 - val_loss: 11.5204\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.6606 - val_loss: 10.5056\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6827 - val_loss: 12.0754\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.7971 - val_loss: 13.0015\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1746 - val_loss: 9.4160\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0887 - val_loss: 12.3551\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0559 - val_loss: 11.6820\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9646 - val_loss: 10.4784\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5432 - val_loss: 9.5008\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2457 - val_loss: 9.6818\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1755 - val_loss: 9.8741\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1149 - val_loss: 9.1814\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2366 - val_loss: 11.0020\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9681 - val_loss: 8.6515\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6486 - val_loss: 10.1898\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4659 - val_loss: 8.5908\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8419 - val_loss: 9.5019\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3364 - val_loss: 11.0353\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4499 - val_loss: 13.5370\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2107 - val_loss: 10.6896\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.7275 - val_loss: 8.3292\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1975 - val_loss: 8.3826\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2052 - val_loss: 10.1830\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3627 - val_loss: 8.8236\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3918 - val_loss: 8.2766\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8376 - val_loss: 9.0185\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7750 - val_loss: 9.0458\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4300 - val_loss: 8.0296\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9790 - val_loss: 9.0280\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5898 - val_loss: 18.8646\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5457 - val_loss: 8.9415\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4786 - val_loss: 8.5727\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0330 - val_loss: 9.4428\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0449 - val_loss: 7.5464\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3199 - val_loss: 7.6889\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.2846 - val_loss: 7.8833\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5121 - val_loss: 7.8448\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.7053 - val_loss: 9.6069\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3511 - val_loss: 8.7358\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.8810 - val_loss: 8.3428\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1192 - val_loss: 7.4358\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1438 - val_loss: 7.4640\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1732 - val_loss: 9.3254\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9007 - val_loss: 6.9010\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9807 - val_loss: 8.5535\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6682 - val_loss: 7.1732\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4103 - val_loss: 7.1057\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8764 - val_loss: 8.2116\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9096 - val_loss: 10.1745\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6855 - val_loss: 8.6978\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7407 - val_loss: 11.9618\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1686 - val_loss: 7.3983\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3383 - val_loss: 7.8920\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4470 - val_loss: 7.7311\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4134 - val_loss: 7.7634\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6829 - val_loss: 7.5851\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1632 - val_loss: 8.1476\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 3.3824 - val_loss: 8.8728\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1681 - val_loss: 7.6824\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9253 - val_loss: 7.0921\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3071 - val_loss: 9.0446\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9587 - val_loss: 7.2747\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2089 - val_loss: 8.9514\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0792 - val_loss: 8.4268\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3176 - val_loss: 6.7029\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7296 - val_loss: 7.2588\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8122 - val_loss: 7.3185\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0653 - val_loss: 7.7145\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2108 - val_loss: 7.6838\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0627 - val_loss: 6.8307\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8577 - val_loss: 6.7109\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3831 - val_loss: 8.1593\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4075 - val_loss: 7.0104\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2200 - val_loss: 8.8900\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.5983 - val_loss: 7.5406\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5397 - val_loss: 7.1575\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8835 - val_loss: 7.6485\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5388 - val_loss: 10.0230\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.2234 - val_loss: 6.9650\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2429 - val_loss: 7.0833\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9459 - val_loss: 6.9653\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4933 - val_loss: 6.6928\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9129 - val_loss: 6.9836\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4511 - val_loss: 7.3531\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5231 - val_loss: 7.7346\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5216 - val_loss: 8.4032\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2342 - val_loss: 8.3675\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0612 - val_loss: 7.2491\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4542 - val_loss: 8.4686\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8497 - val_loss: 6.9263\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3635 - val_loss: 8.9097\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1103 - val_loss: 7.2936\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0186 - val_loss: 7.3647\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4317 - val_loss: 9.0857\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2763 - val_loss: 7.4719\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2913 - val_loss: 7.3328\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1679 - val_loss: 7.8436\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.2913 - val_loss: 32.0607\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0825 - val_loss: 6.5741\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9574 - val_loss: 6.6695\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9260 - val_loss: 6.7741\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7953 - val_loss: 7.0308\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 7.030807851716641\n",
      "Mean Absolute Error (MAE): 1.7763431662010045\n",
      "Root Mean Squared Error (RMSE): 2.6515670558589766\n",
      "Time taken: 1248.0879929065704\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1362.9313 - val_loss: 1282.4344\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1194.2786 - val_loss: 1177.8071\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1107.3242 - val_loss: 1105.7267\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1044.6895 - val_loss: 1053.1848\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 999.2571 - val_loss: 1015.6690\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 966.9453 - val_loss: 989.2082\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 944.7692 - val_loss: 971.7739\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 930.2216 - val_loss: 960.5250\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.3202 - val_loss: 954.2057\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.3258 - val_loss: 950.7870\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 913.8187 - val_loss: 949.3344\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 912.7126 - val_loss: 948.6860\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1935 - val_loss: 948.4556\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.0461 - val_loss: 948.3922\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 911.9683 - val_loss: 948.4047\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.9768 - val_loss: 948.4148\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 911.9651 - val_loss: 948.3938\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.0010 - val_loss: 948.4032\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 911.9941 - val_loss: 948.4001\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 911.9647 - val_loss: 948.4300\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0126 - val_loss: 948.4156\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.9720 - val_loss: 948.4152\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0055 - val_loss: 948.3979\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.9929 - val_loss: 948.4240\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.9509 - val_loss: 948.4150\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.9825 - val_loss: 948.4023\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0729 - val_loss: 948.4024\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0822 - val_loss: 948.5624\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.2545 - val_loss: 948.4391\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 912.0754 - val_loss: 948.3964\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.0439 - val_loss: 948.3525\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 910.0067 - val_loss: 936.8958\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 862.7949 - val_loss: 875.0543\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 794.6359 - val_loss: 777.9075\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 721.1407 - val_loss: 723.2458\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 669.4722 - val_loss: 668.4158\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 622.8500 - val_loss: 618.0089\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 573.8948 - val_loss: 575.2229\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 528.9180 - val_loss: 529.1609\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 486.6736 - val_loss: 488.8647\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 446.4736 - val_loss: 444.7509\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 409.4972 - val_loss: 409.7530\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 375.8617 - val_loss: 377.4754\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 342.4893 - val_loss: 343.1302\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 312.0321 - val_loss: 313.3815\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 283.2764 - val_loss: 279.7014\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 257.9516 - val_loss: 258.0301\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 233.1410 - val_loss: 229.8325\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 211.1953 - val_loss: 212.1134\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 190.8555 - val_loss: 186.4039\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 171.1138 - val_loss: 168.5238\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 152.3120 - val_loss: 149.5742\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 135.6014 - val_loss: 135.5160\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 120.6447 - val_loss: 118.7670\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 106.6160 - val_loss: 111.0674\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 96.2218 - val_loss: 95.5134\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 84.6260 - val_loss: 82.8668\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 75.1595 - val_loss: 72.0815\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.5221 - val_loss: 64.3059\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.9911 - val_loss: 58.5639\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.3834 - val_loss: 53.9717\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.6954 - val_loss: 45.4078\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.2734 - val_loss: 41.4882\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.4062 - val_loss: 36.9004\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.8945 - val_loss: 32.7117\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.4284 - val_loss: 29.1441\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.4618 - val_loss: 27.3255\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.3753 - val_loss: 22.1336\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.9652 - val_loss: 23.0938\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.2138 - val_loss: 21.0992\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.1579 - val_loss: 17.0563\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 16.9559 - val_loss: 17.6756\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.1084 - val_loss: 15.1108\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.9506 - val_loss: 15.5785\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3649 - val_loss: 15.6867\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.1627 - val_loss: 14.6970\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.1364 - val_loss: 14.8032\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6821 - val_loss: 11.5194\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5393 - val_loss: 10.6698\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.4914 - val_loss: 15.0399\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.4749 - val_loss: 13.1088\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.4701 - val_loss: 9.7413\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2003 - val_loss: 11.7460\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3722 - val_loss: 9.2366\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2593 - val_loss: 8.2879\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0008 - val_loss: 10.3917\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7634 - val_loss: 9.9196\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2869 - val_loss: 14.2293\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9115 - val_loss: 8.4807\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4001 - val_loss: 14.5567\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3501 - val_loss: 8.5306\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.7555 - val_loss: 11.9476\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8352 - val_loss: 7.6553\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9976 - val_loss: 8.7181\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4865 - val_loss: 9.6604\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3376 - val_loss: 9.5702\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1978 - val_loss: 20.9505\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9585 - val_loss: 6.7585\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1066 - val_loss: 7.2128\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4792 - val_loss: 7.9782\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4837 - val_loss: 8.5457\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5556 - val_loss: 6.8319\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8317 - val_loss: 7.6939\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5113 - val_loss: 7.7495\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6608 - val_loss: 8.6567\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1579 - val_loss: 10.3364\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9683 - val_loss: 8.0229\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.7347 - val_loss: 7.6436\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.4415 - val_loss: 7.0971\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.3183 - val_loss: 6.5432\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.3090 - val_loss: 8.4418\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1575 - val_loss: 8.4152\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3163 - val_loss: 12.2926\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6251 - val_loss: 10.1293\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7837 - val_loss: 7.2458\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5665 - val_loss: 7.5042\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8844 - val_loss: 6.6980\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7231 - val_loss: 7.9500\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8672 - val_loss: 6.2623\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1534 - val_loss: 12.6631\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5918 - val_loss: 6.2812\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3369 - val_loss: 6.4771\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3594 - val_loss: 6.5273\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5353 - val_loss: 6.4413\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3967 - val_loss: 6.5606\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8411 - val_loss: 8.1049\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.4679 - val_loss: 6.7021\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0232 - val_loss: 6.0043\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0876 - val_loss: 6.2562\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.5184 - val_loss: 6.4031\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7403 - val_loss: 6.9916\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2536 - val_loss: 6.7330\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2150 - val_loss: 11.2123\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6464 - val_loss: 8.1162\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9864 - val_loss: 6.0374\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6271 - val_loss: 6.2977\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0380 - val_loss: 6.2053\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9202 - val_loss: 6.8936\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1775 - val_loss: 7.0276\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5345 - val_loss: 6.7303\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8187 - val_loss: 6.0016\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6690 - val_loss: 6.1937\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4215 - val_loss: 6.2796\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6094 - val_loss: 5.8093\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7391 - val_loss: 6.0207\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1485 - val_loss: 11.3824\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.3584 - val_loss: 6.3511\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7259 - val_loss: 6.7811\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.5028 - val_loss: 6.3330\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5049 - val_loss: 5.9981\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5968 - val_loss: 6.2326\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6082 - val_loss: 6.1893\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0011 - val_loss: 6.7756\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4938 - val_loss: 5.8363\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3664 - val_loss: 6.0335\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3339 - val_loss: 5.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3731 - val_loss: 7.0355\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6228 - val_loss: 7.2703\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5920 - val_loss: 6.7012\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7512 - val_loss: 6.6666\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3138 - val_loss: 6.0252\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3615 - val_loss: 5.9968\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2871 - val_loss: 7.1590\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9768 - val_loss: 6.2814\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.1274 - val_loss: 6.1722\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1629 - val_loss: 6.3720\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.0132 - val_loss: 6.1194\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1094 - val_loss: 6.2224\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.1890 - val_loss: 6.6931\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5467 - val_loss: 6.5077\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0419 - val_loss: 6.2354\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8158 - val_loss: 6.4312\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6363 - val_loss: 5.5954\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9790 - val_loss: 6.0546\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8724 - val_loss: 5.7916\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1149 - val_loss: 7.0217\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1252 - val_loss: 6.7995\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9684 - val_loss: 6.2476\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1926 - val_loss: 7.6503\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3318 - val_loss: 6.0789\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6222 - val_loss: 6.0488\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8173 - val_loss: 6.5065\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9387 - val_loss: 6.7463\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7042 - val_loss: 5.6755\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0997 - val_loss: 7.3753\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 6.8835 - val_loss: 6.9599\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.0587 - val_loss: 6.0262\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.7320 - val_loss: 5.5201\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5820 - val_loss: 6.2368\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5246 - val_loss: 6.1360\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5876 - val_loss: 5.8008\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5541 - val_loss: 6.9810\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7594 - val_loss: 6.2501\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7540 - val_loss: 6.2145\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1036 - val_loss: 8.6154\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4142 - val_loss: 5.8458\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5055 - val_loss: 6.0754\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7634 - val_loss: 6.9216\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1933 - val_loss: 8.4967\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9967 - val_loss: 7.7188\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 7.718790815889023\n",
      "Mean Absolute Error (MAE): 1.8260916839273291\n",
      "Root Mean Squared Error (RMSE): 2.7782711919265592\n",
      "Time taken: 1246.179702758789\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1385.7690 - val_loss: 1236.3788\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1223.0741 - val_loss: 1140.7985\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1136.7708 - val_loss: 1070.7482\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1072.0078 - val_loss: 1018.5489\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1023.2744 - val_loss: 980.0402\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 987.4152 - val_loss: 953.0543\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 962.1061 - val_loss: 934.9953\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 944.7054 - val_loss: 923.3367\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 933.6113 - val_loss: 916.9088\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.2000 - val_loss: 913.8395\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.7978 - val_loss: 912.6507\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.1660 - val_loss: 912.3865\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.4379 - val_loss: 912.6279\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.1752 - val_loss: 912.8110\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0323 - val_loss: 913.0204\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0433 - val_loss: 913.1221\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9871 - val_loss: 913.2097\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0428 - val_loss: 913.1176\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9929 - val_loss: 913.2631\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0233 - val_loss: 913.1896\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0192 - val_loss: 913.2335\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0280 - val_loss: 913.2068\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.0299 - val_loss: 913.2628\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.0301 - val_loss: 913.2596\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9958 - val_loss: 913.2245\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0132 - val_loss: 913.1807\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0344 - val_loss: 913.1562\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0660 - val_loss: 913.2245\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.0181 - val_loss: 913.2043\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.0069 - val_loss: 913.1812\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9889 - val_loss: 913.1975\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.1868 - val_loss: 913.1192\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 906.8435 - val_loss: 866.2425\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 861.4974 - val_loss: 843.0227\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 838.6910 - val_loss: 822.3939\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 819.7570 - val_loss: 804.3962\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 800.3375 - val_loss: 785.8810\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 781.1650 - val_loss: 765.8744\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 748.0247 - val_loss: 707.5853\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 678.0161 - val_loss: 641.3311\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 627.7867 - val_loss: 598.2909\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 587.5603 - val_loss: 561.3820\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 550.2109 - val_loss: 524.6320\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 513.3770 - val_loss: 489.9507\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 479.4118 - val_loss: 457.6618\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 446.7083 - val_loss: 425.6837\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 415.8204 - val_loss: 397.5982\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 386.2058 - val_loss: 369.5596\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 358.8456 - val_loss: 342.4319\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 332.3798 - val_loss: 317.8660\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 306.6343 - val_loss: 293.8014\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 283.1302 - val_loss: 272.0538\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 261.1906 - val_loss: 248.6110\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 239.5224 - val_loss: 230.7362\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 220.0966 - val_loss: 209.5574\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 201.4973 - val_loss: 196.4148\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 184.5090 - val_loss: 176.8572\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 168.8141 - val_loss: 161.4347\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 153.8676 - val_loss: 149.4080\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 140.5604 - val_loss: 132.9321\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 126.9032 - val_loss: 125.5152\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 113.2667 - val_loss: 110.6515\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 101.6474 - val_loss: 98.6332\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 91.6333 - val_loss: 91.7241\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 83.3707 - val_loss: 79.2123\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 75.1314 - val_loss: 72.5120\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 67.4838 - val_loss: 64.0289\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.4287 - val_loss: 56.7365\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.4791 - val_loss: 52.4829\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.6752 - val_loss: 48.0124\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.7570 - val_loss: 45.9233\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.1588 - val_loss: 38.2327\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.6037 - val_loss: 36.3985\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.7945 - val_loss: 35.4813\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.9920 - val_loss: 30.0848\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.4453 - val_loss: 28.2607\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.1498 - val_loss: 24.0245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.3674 - val_loss: 25.5398\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.8802 - val_loss: 23.9325\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.3804 - val_loss: 19.3281\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.1038 - val_loss: 19.6132\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.7990 - val_loss: 19.5462\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.5978 - val_loss: 17.1671\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.7389 - val_loss: 15.9083\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.0244 - val_loss: 15.5357\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.9458 - val_loss: 15.7052\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.9560 - val_loss: 14.0399\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3665 - val_loss: 13.7407\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2522 - val_loss: 13.0018\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5022 - val_loss: 13.9074\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5966 - val_loss: 12.1193\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3399 - val_loss: 11.3929\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2270 - val_loss: 11.9545\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.6296 - val_loss: 11.7300\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4674 - val_loss: 11.4130\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4200 - val_loss: 10.9926\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8995 - val_loss: 11.8107\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3339 - val_loss: 9.7065\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5412 - val_loss: 10.9142\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3750 - val_loss: 9.4499\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6211 - val_loss: 14.9107\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0192 - val_loss: 10.3105\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8010 - val_loss: 9.1109\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2040 - val_loss: 11.1041\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2827 - val_loss: 10.2850\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8745 - val_loss: 10.1983\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8513 - val_loss: 9.7038\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2008 - val_loss: 10.0937\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1170 - val_loss: 8.8907\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6026 - val_loss: 9.1465\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8578 - val_loss: 9.2060\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6121 - val_loss: 8.0370\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9375 - val_loss: 8.3473\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9418 - val_loss: 8.5411\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.2900 - val_loss: 8.5213\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7306 - val_loss: 8.8605\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0130 - val_loss: 8.2730\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7177 - val_loss: 9.0607\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4401 - val_loss: 7.8932\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3126 - val_loss: 8.9702\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9953 - val_loss: 8.1837\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4096 - val_loss: 7.4052\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6568 - val_loss: 7.2403\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7545 - val_loss: 9.4147\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1114 - val_loss: 8.9470\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3824 - val_loss: 8.4477\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9227 - val_loss: 9.0840\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5914 - val_loss: 10.2123\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0968 - val_loss: 8.8416\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0026 - val_loss: 7.9707\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9892 - val_loss: 7.4385\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2645 - val_loss: 7.5653\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4856 - val_loss: 7.3529\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7874 - val_loss: 7.5417\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7661 - val_loss: 7.0405\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2061 - val_loss: 8.5027\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2371 - val_loss: 7.7785\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4778 - val_loss: 7.0507\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2261 - val_loss: 8.3494\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3461 - val_loss: 7.0996\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1603 - val_loss: 6.5438\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1728 - val_loss: 8.0312\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8356 - val_loss: 7.0945\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9992 - val_loss: 7.3280\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5635 - val_loss: 7.0211\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7266 - val_loss: 7.8667\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8803 - val_loss: 7.9422\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8799 - val_loss: 7.5138\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4532 - val_loss: 7.2377\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8553 - val_loss: 7.2034\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9009 - val_loss: 6.4538\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5922 - val_loss: 7.1704\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5496 - val_loss: 7.6089\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7257 - val_loss: 7.2388\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6790 - val_loss: 6.5200\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8947 - val_loss: 7.2161\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7532 - val_loss: 13.5678\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4288 - val_loss: 7.6458\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2568 - val_loss: 7.5294\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4818 - val_loss: 6.9459\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1319 - val_loss: 6.5413\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1170 - val_loss: 6.7084\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1832 - val_loss: 8.0035\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4627 - val_loss: 7.0194\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9959 - val_loss: 6.5209\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0818 - val_loss: 7.5533\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2600 - val_loss: 6.8677\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9715 - val_loss: 6.4512\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5652 - val_loss: 7.7820\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8340 - val_loss: 7.4935\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0832 - val_loss: 7.2302\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2138 - val_loss: 6.7616\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0255 - val_loss: 6.6207\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9176 - val_loss: 6.8182\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0005 - val_loss: 6.5883\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0257 - val_loss: 6.9439\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1268 - val_loss: 6.6882\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8413 - val_loss: 7.1423\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6936 - val_loss: 6.8738\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9365 - val_loss: 7.4816\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8349 - val_loss: 6.7322\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6656 - val_loss: 7.2081\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5931 - val_loss: 6.9255\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5944 - val_loss: 6.8351\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5622 - val_loss: 6.9571\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4863 - val_loss: 6.7193\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7609 - val_loss: 6.9004\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5124 - val_loss: 7.7666\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4317 - val_loss: 6.3568\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4559 - val_loss: 6.8538\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2678 - val_loss: 6.6164\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4002 - val_loss: 6.9944\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2918 - val_loss: 6.3328\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4293 - val_loss: 6.1787\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2404 - val_loss: 7.5736\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5102 - val_loss: 7.5391\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2260 - val_loss: 6.6903\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2343 - val_loss: 6.4060\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9052 - val_loss: 6.4065\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2122 - val_loss: 7.8745\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 7.8741065900772815\n",
      "Mean Absolute Error (MAE): 1.8914597522529364\n",
      "Root Mean Squared Error (RMSE): 2.806083853001774\n",
      "Time taken: 1198.8834209442139\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1407.2239 - val_loss: 1263.7032\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1244.0293 - val_loss: 1157.5608\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1154.1897 - val_loss: 1079.7115\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1087.1991 - val_loss: 1021.1765\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1036.5791 - val_loss: 976.6902\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 998.3614 - val_loss: 944.1462\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 971.2988 - val_loss: 921.7474\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 952.9265 - val_loss: 907.0305\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 941.1482 - val_loss: 897.7497\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 934.1519 - val_loss: 892.6679\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.3179 - val_loss: 889.9739\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.4536 - val_loss: 888.8216\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.5862 - val_loss: 888.3100\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.2903 - val_loss: 888.1447\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.3732 - val_loss: 888.2710\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.2936 - val_loss: 888.1279\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.5305 - val_loss: 888.1762\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.1962 - val_loss: 888.1474\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.2338 - val_loss: 887.9800\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.1759 - val_loss: 888.0406\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.1852 - val_loss: 887.8898\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.0980 - val_loss: 886.7332\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 893.5421 - val_loss: 836.7700\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 864.4739 - val_loss: 822.2044\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 850.4818 - val_loss: 810.4409\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 838.4379 - val_loss: 798.8399\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 825.5687 - val_loss: 788.4807\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 809.5635 - val_loss: 768.2309\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 770.2352 - val_loss: 719.0773\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 714.9784 - val_loss: 656.6320\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 657.4277 - val_loss: 603.3950\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 610.1666 - val_loss: 559.5880\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 567.6234 - val_loss: 518.4928\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 526.4026 - val_loss: 478.5153\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 486.9200 - val_loss: 441.5302\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 450.7547 - val_loss: 408.3121\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 416.9855 - val_loss: 376.1808\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 384.8177 - val_loss: 347.0906\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 357.1876 - val_loss: 319.0811\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 326.2254 - val_loss: 288.0524\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 296.1956 - val_loss: 262.5673\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 270.0228 - val_loss: 245.9836\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 247.0742 - val_loss: 218.0438\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 224.5688 - val_loss: 199.9879\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 204.9723 - val_loss: 181.2516\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 185.7274 - val_loss: 161.2329\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 168.7632 - val_loss: 148.1486\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 153.3421 - val_loss: 133.3113\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 136.0611 - val_loss: 117.6284\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 121.5543 - val_loss: 104.8631\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 108.8230 - val_loss: 96.8727\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 95.9327 - val_loss: 81.2280\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 86.1731 - val_loss: 73.5932\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 78.3069 - val_loss: 65.9600\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.0709 - val_loss: 58.8752\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.7054 - val_loss: 53.5045\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.6463 - val_loss: 46.9022\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.6239 - val_loss: 42.7307\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.5271 - val_loss: 39.8516\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.5664 - val_loss: 32.2963\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.2976 - val_loss: 30.4025\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.9347 - val_loss: 28.2289\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.0326 - val_loss: 24.0562\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1828 - val_loss: 23.4796\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.7513 - val_loss: 25.9386\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.7776 - val_loss: 23.2914\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.8471 - val_loss: 19.6958\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.9795 - val_loss: 20.4148\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.5202 - val_loss: 14.7922\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.5950 - val_loss: 17.9505\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.5103 - val_loss: 18.4610\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.3876 - val_loss: 14.8244\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2566 - val_loss: 14.8263\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.2325 - val_loss: 12.3207\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8525 - val_loss: 13.9757\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.1638 - val_loss: 11.9276\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5984 - val_loss: 13.6091\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 11.4262 - val_loss: 14.0008\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0032 - val_loss: 13.0779\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2739 - val_loss: 12.7542\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5627 - val_loss: 12.6507\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.9271 - val_loss: 10.5775\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6789 - val_loss: 10.0048\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2819 - val_loss: 8.8929\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9179 - val_loss: 10.4980\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3698 - val_loss: 9.0018\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.5009 - val_loss: 10.6342\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.7078 - val_loss: 9.7705\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2887 - val_loss: 14.0243\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0951 - val_loss: 11.9923\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5678 - val_loss: 9.8454\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0624 - val_loss: 11.1834\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4012 - val_loss: 9.9430\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9674 - val_loss: 9.5227\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7460 - val_loss: 8.6666\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8592 - val_loss: 9.3114\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3249 - val_loss: 10.3607\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3973 - val_loss: 8.0716\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9928 - val_loss: 10.6836\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2639 - val_loss: 8.4917\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2362 - val_loss: 8.4633\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1679 - val_loss: 9.1999\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9561 - val_loss: 7.8924\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9702 - val_loss: 7.7691\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8585 - val_loss: 8.1645\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7303 - val_loss: 8.9388\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5521 - val_loss: 8.4741\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5824 - val_loss: 9.3889\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4425 - val_loss: 8.9158\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8536 - val_loss: 7.9196\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3012 - val_loss: 7.4071\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9667 - val_loss: 9.5695\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6360 - val_loss: 8.0315\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1287 - val_loss: 7.4135\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4115 - val_loss: 7.1498\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4361 - val_loss: 11.1935\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7509 - val_loss: 6.6017\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7579 - val_loss: 6.6713\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1793 - val_loss: 7.5863\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4376 - val_loss: 7.1023\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6514 - val_loss: 8.1694\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1401 - val_loss: 6.6373\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5111 - val_loss: 6.6332\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4694 - val_loss: 6.7246\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7120 - val_loss: 7.7765\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2935 - val_loss: 7.1530\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0821 - val_loss: 6.9101\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3274 - val_loss: 6.4128\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3169 - val_loss: 6.9207\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5525 - val_loss: 8.2411\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2163 - val_loss: 7.4443\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2615 - val_loss: 9.0430\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8994 - val_loss: 7.1157\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0461 - val_loss: 6.3638\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8798 - val_loss: 6.5703\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5240 - val_loss: 6.5032\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5902 - val_loss: 6.2450\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5377 - val_loss: 7.3730\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9369 - val_loss: 8.0904\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5273 - val_loss: 6.4729\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5219 - val_loss: 7.5633\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9951 - val_loss: 7.2575\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4809 - val_loss: 7.1237\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3942 - val_loss: 6.6713\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5950 - val_loss: 7.2317\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3503 - val_loss: 6.3091\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7981 - val_loss: 6.0740\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3536 - val_loss: 6.4259\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4203 - val_loss: 8.6835\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5867 - val_loss: 7.5402\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3654 - val_loss: 7.5922\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2511 - val_loss: 6.4234\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5019 - val_loss: 8.7989\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1924 - val_loss: 6.5943\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9793 - val_loss: 14.0340\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2701 - val_loss: 6.4587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6732 - val_loss: 7.4777\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9769 - val_loss: 7.9226\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4664 - val_loss: 5.9312\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9195 - val_loss: 6.0722\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7255 - val_loss: 6.7982\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7683 - val_loss: 6.4592\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7027 - val_loss: 6.0530\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0653 - val_loss: 8.0795\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8209 - val_loss: 6.7796\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6823 - val_loss: 7.1283\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8021 - val_loss: 6.3166\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2295 - val_loss: 8.8930\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2239 - val_loss: 7.4413\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7379 - val_loss: 5.8018\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6526 - val_loss: 6.4312\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6263 - val_loss: 6.1180\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3779 - val_loss: 6.0419\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6114 - val_loss: 6.1628\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8013 - val_loss: 6.4523\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5462 - val_loss: 6.6374\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5060 - val_loss: 6.9613\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6598 - val_loss: 6.6115\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2520 - val_loss: 6.1949\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2966 - val_loss: 6.8381\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2485 - val_loss: 6.3191\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3374 - val_loss: 6.5585\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3731 - val_loss: 6.2594\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4071 - val_loss: 6.3635\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5241 - val_loss: 18.8761\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2099 - val_loss: 5.6313\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0599 - val_loss: 5.8184\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0290 - val_loss: 7.1610\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3886 - val_loss: 5.6690\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2415 - val_loss: 6.0959\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0701 - val_loss: 7.4633\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4324 - val_loss: 7.2283\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5487 - val_loss: 6.1990\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9682 - val_loss: 5.8129\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1713 - val_loss: 7.0357\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1024 - val_loss: 6.4428\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2518 - val_loss: 6.9671\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6066 - val_loss: 7.9360\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1421 - val_loss: 6.0056\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7979 - val_loss: 5.9647\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 5.964574457327568\n",
      "Mean Absolute Error (MAE): 1.658140562114248\n",
      "Root Mean Squared Error (RMSE): 2.4422478288100837\n",
      "Time taken: 1204.037768125534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_5824\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  6.055262  1.670636  2.460744  1254.840419\n",
      "1        2  7.030808  1.776343  2.651567  1248.087993\n",
      "2        3  7.718791  1.826092  2.778271  1246.179703\n",
      "3        4  7.874107  1.891460  2.806084  1198.883421\n",
      "4        5  5.964574  1.658141  2.442248  1204.037768\n",
      "5  Average  6.928708  1.764534  2.627783  1230.405861\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered1_Reg1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered1_Reg1.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered1_Reg1.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAADArklEQVR4nOzdeXgUVdo28LuqOwvZISGbCZCEhE0QBEEUEQVlUQcV10FRB3V0QAf38VX5BLdxGcd1XMYR1NFXx3lFcWNRUVSQVZCdEAIkZIEQ0iEhW3fV90fTlTRJIMmTdFd17t91odXVle5z7qpO8qTqnFJ0XddBREREREQkoPq7AUREREREZH0sLIiIiIiISIyFBRERERERibGwICIiIiIiMRYWREREREQkxsKCiIiIiIjEWFgQEREREZEYCwsiIiIiIhJjYUFERERERGIsLIiIiIiISIyFBRFRJzR//nwoioK1a9f6uyktsmHDBlx33XVITU1FSEgIunXrhnHjxmHevHlwuVz+bh4REQGw+7sBREREJ/LWW2/htttuQ0JCAq6//npkZmbiyJEj+PbbbzF9+nQUFhbif/7nf/zdTCKiTo+FBRERmdYvv/yC2267DSNHjsRXX32FyMhI47lZs2Zh7dq12Lx5c7u8V2VlJcLDw9vltYiIOiNeCkVERM369ddfMXHiRERFRSEiIgJjx47FL7/84rVNXV0d5syZg8zMTISGhiI2NhajRo3C0qVLjW2Kiopw0003ISUlBSEhIUhKSsLkyZOxZ8+eE77/nDlzoCgK3n//fa+iwmPYsGG48cYbAQDff/89FEXB999/77XNnj17oCgK5s+fb6y78cYbERERgZycHEyaNAmRkZGYOnUqZs6ciYiICBw9erTRe1177bVITEz0uvTq66+/xjnnnIPw8HBERkbioosuwpYtW07YJyKiQMXCgoiImrRlyxacc8452LhxI+6//3488sgjyM3NxZgxY7Bq1Spju0cffRRz5szBeeedh1deeQUPPfQQevTogfXr1xvbTJkyBQsWLMBNN92Ef/zjH7jzzjtx5MgR7Nu3r9n3P3r0KL799luMHj0aPXr0aPf+OZ1OjB8/HvHx8XjuuecwZcoUXH311aisrMSXX37ZqC2ff/45rrjiCthsNgDAe++9h4suuggRERF4+umn8cgjj2Dr1q0YNWrUSQsmIqJAxEuhiIioSQ8//DDq6urw008/IT09HQAwbdo09OnTB/fffz9++OEHAMCXX36JSZMm4c0332zydcrKyrBixQo8++yzuPfee431Dz744Anff9euXairq8PAgQPbqUfeampqcOWVV+Kpp54y1um6jlNOOQUfffQRrrzySmP9l19+icrKSlx99dUAgIqKCtx55524+eabvfp9ww03oE+fPnjyySebzYOIKFDxjAURETXicrmwZMkSXHrppUZRAQBJSUn4/e9/j59++gnl5eUAgJiYGGzZsgXZ2dlNvlaXLl0QHByM77//HocPH25xGzyv39QlUO3l9ttv93qsKAquvPJKfPXVV6ioqDDWf/TRRzjllFMwatQoAMDSpUtRVlaGa6+9FiUlJcY/m82GESNGYNmyZR3WZiIis2JhQUREjRw8eBBHjx5Fnz59Gj3Xr18/aJqGvLw8AMDcuXNRVlaGrKwsDBw4EPfddx9+++03Y/uQkBA8/fTT+Prrr5GQkIDRo0fjmWeeQVFR0QnbEBUVBQA4cuRIO/asnt1uR0pKSqP1V199NaqqqrBw4UIA7rMTX331Fa688kooigIARhF1/vnno3v37l7/lixZggMHDnRIm4mIzIyFBRERiYwePRo5OTl4++23ceqpp+Ktt97C6aefjrfeesvYZtasWdi5cyeeeuophIaG4pFHHkG/fv3w66+/Nvu6vXv3ht1ux6ZNm1rUDs8v/cdr7j4XISEhUNXGPwbPPPNM9OrVC//5z38AAJ9//jmqqqqMy6AAQNM0AO5xFkuXLm3077PPPmtRm4mIAgkLCyIiaqR79+4ICwvDjh07Gj23fft2qKqK1NRUY123bt1w00034X//93+Rl5eHQYMG4dFHH/X6uoyMDNxzzz1YsmQJNm/ejNraWvztb39rtg1hYWE4//zzsXz5cuPsyIl07doVgHtMR0N79+496dce76qrrsKiRYtQXl6Ojz76CL169cKZZ57p1RcAiI+Px7hx4xr9GzNmTKvfk4jI6lhYEBFRIzabDRdeeCE+++wzrxmOiouL8cEHH2DUqFHGpUqHDh3y+tqIiAj07t0bNTU1ANwzKlVXV3ttk5GRgcjISGOb5vy///f/oOs6rr/+eq8xDx7r1q3DO++8AwDo2bMnbDYbli9f7rXNP/7xj5Z1uoGrr74aNTU1eOedd7Bo0SJcddVVXs+PHz8eUVFRePLJJ1FXV9fo6w8ePNjq9yQisjrOCkVE1Im9/fbbWLRoUaP1f/7zn/H4449j6dKlGDVqFP70pz/BbrfjjTfeQE1NDZ555hlj2/79+2PMmDEYOnQounXrhrVr1+K///0vZs6cCQDYuXMnxo4di6uuugr9+/eH3W7HggULUFxcjGuuueaE7TvrrLPw6quv4k9/+hP69u3rdeft77//HgsXLsTjjz8OAIiOjsaVV16Jl19+GYqiICMjA1988UWbxjucfvrp6N27Nx566CHU1NR4XQYFuMd/vPbaa7j++utx+umn45prrkH37t2xb98+fPnllzj77LPxyiuvtPp9iYgsTSciok5n3rx5OoBm/+Xl5em6ruvr16/Xx48fr0dEROhhYWH6eeedp69YscLrtR5//HF9+PDhekxMjN6lSxe9b9+++hNPPKHX1tbquq7rJSUl+owZM/S+ffvq4eHhenR0tD5ixAj9P//5T4vbu27dOv33v/+9npycrAcFBeldu3bVx44dq7/zzju6y+Uytjt48KA+ZcoUPSwsTO/atav+xz/+Ud+8ebMOQJ83b56x3Q033KCHh4ef8D0feughHYDeu3fvZrdZtmyZPn78eD06OloPDQ3VMzIy9BtvvFFfu3Zti/tGRBQoFF3Xdb9VNUREREREFBA4xoKIiIiIiMRYWBARERERkRgLCyIiIiIiEmNhQUREREREYiwsiIiIiIhIjIUFERERERGJ8QZ5LaBpGgoKChAZGQlFUfzdHCIiIiIin9B1HUeOHEFycjJU9cTnJFhYtEBBQQFSU1P93QwiIiIiIr/Iy8tDSkrKCbdhYdECkZGRANyBRkVF+fz9XS4XcnJykJGRAZvN5vP3DwTMUI4ZyjA/OWYow/zkmKEcM5TxR37l5eVITU01fh8+ERYWLeC5/CkqKspvhUVERASioqL4IWwjZijHDGWYnxwzlGF+csxQjhnK+DO/lgwH4OBtIiIiIiISY2FhEScbLEMnxwzlmKEM85NjhjLMT44ZyjFDGTPnp+i6rvu7EWZXXl6O6OhoOBwOv1wKRURERETkD635PZhjLCxA13VUVlYiPDyc0922ETOUY4YyzE+OGcowPzl/Z6hpGmpra33+vu1J13UcPXoUYWFhPA7boCPyCwoKarfxGiwsLEDTNOTn5yMzM5MDndqIGcoxQxnmJ8cMZZifnD8zrK2tRW5uLjRN8+n7tjdd1+F0OmG321lYtEFH5RcTE4PExETxa7KwICIiIjIxXddRWFgIm82G1NRUU19jfzK6rqOmpgYhISEsLNqgvfPznAE5cOAAACApKUn0eiwsiIiIiEzM6XTi6NGjSE5ORlhYmL+bI+IZ2hsaGsrCog06Ir8uXboAAA4cOID4+HjR2TjrlrydiKIoCA4O5gdQgBnKMUMZ5ifHDGWYn5y/MnS5XACA4OBgn75vR7HyGRcz6Ij8PAVrXV2d6HV4xsICVFVFenq6v5thacxQjhnKMD85ZijD/OT8nWEgFIWKoiAkJMTfzbCsjsqvvY4tlowWoOs6ysrKwJmB244ZyjFDGeYnxwxlmJ8cM5TzDD5mhm1j9vxYWFiApmkoKiqy/EwQ/sQM5ZihDPOTY4YyzE+OGbYPyeU2vXr1wgsvvNDi7b///nsoioKysrI2v6fZSC9X6kgsLIiIiIioXSmK0uQ/VVURFhaGRx99tE2vu2bNGtx6660t3v6ss85CYWEhoqOj2/R+LRWIBUxbcIwFEREREbWrwsJCY/mjjz7C7NmzsWPHDui6jurqasTFxRnP67oOl8sFu/3kv5Z27969Ve0IDg5GYmJiq76G2o5nLCxAURTeKVWIGcoxQxnmJ8cMZZifHDNsucTERONfdHQ0FEUxHu/atQtRUVH4+uuvMXToUISEhOCnn35CTk4OJk+ejISEBEREROCMM87AN9984/W6x18KpSgK3nrrLVx22WUICwtDZmYmFi5caDx//JmE+fPnIyYmBosXL0a/fv0QERGBCRMmeBVCTqcTd955J2JiYhAbG4sHHngAN9xwAy699NI253H48GFMmzYNXbt2RVhYGCZOnIjs7Gzj+b179+KSSy5B165dER4ejgEDBuCrr74yvnbq1Kno3r07wsLCMHDgQMybN6/NbelILCwsQFVVy98Qx9+YoRwzlGF+csxQhvnJMUM5RVEQFBQEAPjLX/6Cv/71r9i2bRsGDRqEiooKTJo0Cd9++y1+/fVXTJgwAZdccgn27dt3wtecM2cOrrrqKvz222+YNGkSpk6ditLS0ma3P3r0KJ577jm89957WL58Ofbt24d7773XeP7pp5/G+++/j3nz5uHnn39GeXk5Pv30U1G/b7zxRqxduxYLFy7EypUroes6Jk2aZIyXmDFjBmpqarB8+XJs2rQJTz/9NCIiIgAAjzzyCLZu3Yqvv/4a27Ztw+uvv97qMze+wkuhLEDTNJSWlqJbt278ZtZGzFCOGcowPzlmKMP85MyU4SUv/4SDR2p8/r7dI0Pw+R2j2vz1nlmNAGDu3Lm44IILjOe6deuG0047zXj82GOPYcGCBVi4cCFmzpzZ7GveeOONuPbaawEATz75JF566SWsXr0aEyZMaHL7uro6vP7668jIyAAAzJw5E3PnzjWef/nll/Hggw/isssuAwC88sorxtmDtsjOzsbChQvx888/46yzzgIAvP/++0hNTcWnn36KK6+8Evv27cOUKVMwcOBAAPCa1njfvn0YMmQIhg0bBl3Xccopp7TosjF/MGeryIuu6ygpKUHXrl393RTLYoZyzFCG+ckxQxnmJ2emDA8eqUFRebW/m9Emnhv+DRs2zGt9RUUFHn30UXz55ZcoLCyE0+lEVVXVSc9YDBo0yFgODw9HVFQUDhw40Oz2YWFhRlEBAElJScb2DocDxcXFGD58uPG8zWbD0KFD2zwb2LZt22C32zFixAhjXWxsLPr06YNt27YBAO68807cfvvtWLJkCcaNG4cpU6YY/br99tsxZcoUrF+/HhdccAEmTZqEMWPGtKktHY2FBREREZHFdI/0z03m2vN9w8PDvR7fe++9WLp0KZ577jn07t0bXbp0wRVXXIHa2toTvo7n0ioPRVFOWAQ0tb2/7wtx8803Y/z48fjyyy+xZMkSPPXUU/jb3/6GO+64AxMnTsTevXvx1VdfYenSpZg0aRL+9Kc/4W9/+5tf29wUvxYWy5cvx7PPPot169ahsLAQCxYsaHZgzG233YY33ngDf//73zFr1ixjfWlpKe644w58/vnnUFUVU6ZMwYsvvmhclwYAv/32G2bMmIE1a9age/fuuOOOO3D//fd3cO/az+GjtSgor4O9pBK9E6L83RwiIiLyM8nlSGb1888/48YbbzQuQaqoqMCePXt82obo6GgkJCRgzZo1GD16NAD3GZb169dj8ODBbXrNfv36wel0YtWqVcalUIcOHcKOHTvQv39/Y7vU1FTcdtttuO222/Dggw/in//8J+644w4A7tmwbrjhBkybNg0jRozAQw89xMLieJWVlTjttNPwhz/8AZdffnmz2y1YsAC//PILkpOTGz03depUFBYWYunSpairq8NNN92EW2+9FR988AEAoLy8HBdeeCHGjRuH119/HZs2bcIf/vAHxMTEtGoeZH86++nvUePU0CfxMBbPGu3v5liSoijGrBTUNsxQhvnJMUMZ5ifHDNtHc+NTMjMz8cknn+CSSy6Boih45JFH/HIzwjvuuANPPfUUevfujb59++Lll1/G4cOHW7TfN23ahMjISOOxoig47bTTMHnyZNxyyy144403EBkZib/85S845ZRTMHnyZADArFmzMHHiRGRlZeHw4cNYtmwZ+vXrBwCYPXs2hg4digEDBqC6uhqLFi0ynjMbvxYWEydOxMSJE0+4zf79+3HHHXdg8eLFuOiii7ye27ZtGxYtWoQ1a9YY1+m9/PLLmDRpEp577jkkJyfj/fffR21tLd5++20EBwdjwIAB2LBhA55//nnLFBaRoUGoqahBRbXT302xLFVVkZSU5O9mWBozlGF+csxQhvnJMUO5hrNCHe/555/HH/7wB5x11lmIi4vDAw88gPLych+3EHjggQdQVFSEadOmwWaz4dZbb8X48eNhs9lO+rWesxweNpsNTqcT8+bNw5///GdcfPHFqK2txejRo/HVV18ZWbhcLsyYMQP5+fmIiorChAkT8Pe//x2A+14cDz74IPbs2YMuXbrgnHPOwYcfftj+HW8Hiu7vi8qOURSl0aVQmqZh3LhxmDx5Mv785z+jV69emDVrlnEp1Ntvv4177rkHhw8fNr7G6XQiNDQUH3/8MS677DJMmzat0TRhy5Ytw/nnn4/S0tIWDcAqLy9HdHQ0HA4HoqJ8fynS+c99j90llYgIsWPznPE+f/9AoGkaiouLkZCQ4PeZPKyKGcowPzlmKMP85PyVYXV1NXJzc5GWlobQ0FCfvW9H0HUddXV1CAoKssyZH03T0K9fP1x11VV47LHH/NqWjsrvRMdYa34PNvXg7aeffhp2ux133nlnk88XFRUhPj7ea53dbke3bt1QVFRkbJOWlua1TUJCgvFcU4VFTU0Namrqp3DzVMsul8uYycBzW3pN07wG/DS3XlVVYzBRU+s9r9twPeA+mCNC3BVyZa0TLpcGVW08KMlms0HXda/1nrY0t76lbe+IPrVkfXv2yeVy4fDhw4iNjYXNZguIPvl6P+m6jrKyMiPDQOiTL/eT5xiMi4sLmD6dbH1798npdHp9jgOhT77cT5qmweFwIC4uLmD65Ov95Pkcx8fH+7RPDdvb1N+D22PwcXOv0d7rj7/LdmtfpzXa2sa9e/diyZIlOPfcc1FTU4NXX30Vubm5uPbaaxvtq/bIprWauku5tC2efQOg0THZmjabtrBYt24dXnzxRaxfv97nFe1TTz2FOXPmNFqfk5NjDAqPjo5GUlISiouL4XA4jG3i4uIQFxeH/fv3o7Ky0lifmJiImJgY7Nmzx2t2g5SUFERERCAnJ8frG1FaWhrsdjuys7Nh19w3T9F1oLyqFmFBCnJzc41tVVVFVlYWKisrkZ+fb6wPDg5Geno6HA6HUWgB7lkYUlNTUVpaipKSEmO9L/vUUGZmJpxOZ4f26cCBAygtLcWuXbugqmpA9MnX+yk9PR0ul8vIMBD65Mv95Jn/vrS0FAkJCQHRJ1/vp5ycHONzbLfbA6JPvtxPnj+kFRQUoKqqKiD65Ov9pGmacZWEL/vU8Be92tpar7YHBwfDZrOhpqbG6xfAkJAQKIqC6mrvKWlDQ0Oh67rXH1AVRUFoaCg0TfPKS1VVhISEwOVyGTdyA9zFYnBwMJxOp3FPiobr6+rqvIohu92OoKAgY73na4KCgmC3203Xp7q6OsybNw/33XcfdF3Hqaeeiq+//hppaWnGex/fJ4+O7lNISEij9e2xn2pqaoz2Hv95CgsLQ0uZ9lKoF154AXfffbfXqUaXy2Xc9XLPnj0ddilUU2csPN8UPKeAfPnXk5kf/IqvtxQDAH68/zykdO0S0H8R6og+1dXVITs7G7179+YZizb2Sdd1ZGdnIyMjg2cs2njGYteuXcjMzERQUFBA9Olk69u7T3V1ddi1a5fxOQ6EPvn6jEVOTg4yMjK8frZauU/+OGOxa9cu9OnTx3hfX/Spuroa+/btQ1paGkJCGk/3arUzFjU1NcYv1GY8Y+Hr9a1VXV1t5NdebfFcCpWeno7g4GCv5yoqKhATE2PtS6Guv/56jBs3zmvd+PHjcf311+Omm24CAIwcORJlZWVYt24dhg4dCgD47rvvoGmacROSkSNH4qGHHjKuRwOApUuXok+fPs2OrwgJCWnyg+v5QdZQw2/OkvXNDQiy2WyIDgs2Hh+pdkJRlCa3b+369mp7W/rU0vXt1SebzYb4+HjY7fZGP1CbYoU++Xo/aZqG7t27N8oQsG6fTrS+vfukKAri4+ONrw2EPknXt7ZPdru90efY6n3y5X5SFAVxcXGw2WxNfo0V+9TW9W3tk+dz7PmF2Fd9avh6zV3F0R5Xd7T2tdu63m63Gxm25XVaw1d9kq5vKV3XG+XXHm1p+HpNfe9oKb8WFhUVFdi1a5fxODc3Fxs2bEC3bt3Qo0cPxMbGem0fFBSExMRE9OnTB4B7XuAJEybglltuweuvv466ujrMnDkT11xzjTE17e9//3vMmTMH06dPxwMPPIDNmzfjxRdfNEbaW0FUl/rZE45U151gS2qOqrovf6K2Y4YyzE+OGcowPzlmKKcozc8KRSdn9vz8Oi3E2rVrMWTIEAwZMgQAcPfdd2PIkCGYPXt2i1/j/fffR9++fTF27FhMmjQJo0aNwptvvmk8Hx0djSVLliA3NxdDhw7FPffcg9mzZ1tmqlkAiAyp/0tFOaecbRNN05CXl9foNDe1HDOUYX5yzFCG+ckxQzld11FbW9sulwR1RmbPz69nLMaMGdOqYJq6+2K3bt2Mm+E1Z9CgQfjxxx9b2zzTiAip3008Y9E2uq6jsrLStB9EK2CGMsxPjhnKMD85Ztg+XC6Xqf/qbnZmzo8TWVtAw0uhyqtYWBARERGR+bCwsIBIrzMWvBSKiIiIiMyHhYUFNJwVqpyXQrWJqqpITExsdvYNOjlmKMP85JihDPOTY4btozWX8YwZMwazZs0yHvfq1QsvvPDCCb9GURSv2wy0VXu9Tnsz62VQAAsLS/CeFYpnLNpCURTExMS0y9R1nRUzlGF+csxQhvnJMcOWu+SSSzBhwoRG6xVFwcqVK6GqKn777bdWv+6aNWvafQKeRx99FIMHD260vrCwEBMnTmzX9zre/PnzERMT0+LtFUUxpps1IxYWFhDhNSsUz1i0haZp2L17N2fyEGCGMsxPjhnKMD85Zthy06dPx9KlS73uTg64B8C/9dZbGDZsGAYNGtTq1+3evXur7gQtkZiY2OR9zfzJc4NBs04gwMLCAhpON8szFm1j9unZrIAZyjA/OWYow/zkmGHLXXzxxejevTvmz5/vtb6iogKffPIJ/vCHP+DQoUO49tprccoppyAsLAwDBw7E//7v/57wdY+/FCo7OxujR49GaGgo+vfvj6VLlzb6mgceeABZWVkICwtDeno6HnnkEdTVuf9QO3/+fMyZMwcbN240bhLnafPxl0Jt2rQJ559/Prp06YLY2FjceuutqKioMJ6/8cYbcemll+K5555DUlISYmNjMWPGDOO92mLfvn2YPHkyIiIiEBUVhauvvhqFhYXG8xs3bsR5552HyMhIREVFYejQoVi7di0AYO/evbjkkkvQtWtXhIeHY8CAAfjqq6/a3JaWMO2dt6leeLAdCgAdnBWKiIiIzM9ut2PatGmYP38+HnroIePSnY8//hgulwvXXnstKisrMXToUDzwwAOIiorCl19+ieuvvx4ZGRkYPnz4Sd9D0zRcfvnlSEhIwKpVq+BwOLzGY3hERkZi/vz5SE5OxqZNm3DLLbcgMjIS999/P66++mps3rwZixYtwjfffAPAfQ+041VWVmL8+PEYOXIk1qxZgwMHDuDmm2/GzJkzvYqnZcuWISkpCcuWLcOuXbtw9dVXY/DgwbjllltanaGmaUZR8cMPP8DpdGLGjBmYNm0afvjhBwDA1KlTMWTIELz22muw2WzYsGGDMQZjxowZqK2txfLlyxEeHo6tW7ciIiKi1e1oDRYWFqCqCsKCVFTWaTxjQURERMAb5wIVB3z/vhHxwB9/aNGmf/jDH/Dss8/ihx9+wJgxYwC4zxBceumliI6ORkxMDO69915j+zvuuAOLFy/Gf/7znxYVFt988w22b9+OxYsXIzk5GQDw5JNPNhoX8fDDDxvLvXr1wr333osPP/wQ999/P7p06YKIiAjY7XYkJiY2+14ffPABqqur8e677yI8PBwA8Morr+CSSy7B008/jYSEBABA165d8corr8Bms6Fv37646KKL8O2337apsPj222+xadMm5ObmIjU1FQDwzjvv4NRTT8WaNWswfPhw7Nu3D/fddx/69u0LAMjMzDS+ft++fZgyZQoGDhwIAEhPT291G1qLhYUFqKqK6LAgVDpqOMaijVRVRUpKCmfyEGCGMsxPjhnKMD85U2VYcQA4UuDvVpxQ3759cdZZZ+Htt9/GmDFjsGvXLvz444/GmQGXy4Unn3wS//nPf7B//37U1taipqamxWMotm3bhtTUVKOoAICRI0c22u6jjz7CSy+9hJycHFRUVMDpdCIqKqpVfdm2bRtOO+00o6gAgLPPPhuapmHHjh1GYTFgwADYbPWXsCclJWHTpk2teq+G75mammoUFQDQv39/xMTEYNu2bRg+fDjuvvtu3HzzzXjvvfcwbtw4XHnllcjIyAAA3Hnnnbj99tuxZMkSjBs3DlOmTGnTuJbWMMEng05GURREdXFPOVvOMxZtoigKIiIiTDuLghUwQxnmJ8cMZZifnKkyjIgHIpN9/y8ivlXNnD59Ov7v//4PR44cwbx585CRkYHzzz8fiqLg2WefxYsvvogHHngAy5Ytw4YNGzB+/HjU1ta2W0wrV67E1KlTMWnSJHzxxRf49ddf8dBDD7XrezR0/FSwiqK062B/z7Hn+f+jjz6KLVu24KKLLsJ3332H/v37Y8GCBQCAm2++Gbt378b111+PTZs2YdiwYXj55ZfbrS1N4RkLC3C5XAiCu6CodWqornMhNMh2kq+ihlwuF3JycpCRkeH1lwRqOWYow/zkmKEM85MzVYYtvBzJ36666ir8+c9/xgcffIB3330Xt912G2pqahASEoKff/4ZkydPxnXXXQfAPaZg586d6N+/f4teu1+/fsjLy0NhYSGSkpIAAL/88ovXNitWrEDPnj3x0EMPGev27t3rtU1wcDBcLtdJ32v+/PmorKw0zlr8/PPPUFUVffr0aVF7W8vTv7y8POOsxZYtW1BWVoZ+/foZ22VlZSErKwt33XUXrr32WsybNw+XXXYZACA1NRW33XYbbrvtNjz44IP45z//iTvuuKND2gvwjIVlhAXV/3WE4yzahtMDyjFDGeYnxwxlmJ8cM2ydiIgIXH311XjwwQdRWFiIG2+80ZhVKzMzE0uXLsWKFSuwbds2/PGPf0RxcXGLX3vcuHHIysrCDTfcgI0bN+LHH3/0KiA877Fv3z58+OGHyMnJwUsvvWT8Rd+jV69eyM3NxYYNG1BSUoKamppG7zV16lSEhobihhtuwObNm7Fs2TLccccduP76643LoNrK5XJhw4YNXv+2bduGcePGYeDAgZg6dSrWr1+P1atX44YbbsA555yDYcOGoaqqCjNnzsT333+PvXv34ueff8aaNWuMomPWrFlYvHgxcnNzsX79eixbtsyrIOkILCwsIiK4fldxnAURERFZxfTp03H48GGMHz/eazzEww8/jNNPPx3jx4/HmDFjkJiYiEsvvbTFr6uqKhYsWICqqioMHz4cN998M5544gmvbX73u9/hrrvuwsyZMzF48GCsWLECjzzyiNc2U6ZMwYQJE3Deeeehe/fuTU55GxYWhsWLF6O0tBRnnHEGrrjiCowdOxavvPJK68JoQkVFBYYMGeL175JLLoGiKPjss8/QtWtXjB49GuPGjUN6ejreffddAIDNZsOhQ4cwbdo0ZGVl4aqrrsLEiRMxZ84cAO6CZcaMGejXrx8mTJiArKws/OMf/xC390QUnZMxn1R5eTmio6PhcDhaPdinPbhcLtz13gos3F4OAPh0xtkYnBrj83ZYmcvlQnZ2NjIzM/1/+tqimKEM85NjhjLMT85fGVZXVyM3NxdpaWkIDQ312ft2BF3XUV1djdDQUHOMVbGYjsrvRMdYa34P5hkLC1BVFafExxqPeS+L1lNVFWlpaeaYycOimKEM85NjhjLMT44Ztg+z3c3aasycHz8ZFhETHmwsc4xF29jtnKtAihnKMD85ZijD/OSYoRzPVMiYOT8WFhagaRqOOkqNxxxj0XqapiE7O5uD7gSYoQzzk2OGMsxPjhm2j+rqan83wdLMnB8LC4toOHj7CAsLIiIiIjIZFhYWEd5wVqgqXgpFRERERObCwsIiwnnGgoiIqFPjRJ7UUdrr8j6OQLIAVVUxIDMdwH4AQDkHb7eaqqrIzMzkTB4CzFCG+ckxQxnmJ+evDIOCgqAoCg4ePIju3bubevDuyXiKo+rqakv3w1/aOz9d11FbW4uDBw9CVVUEBwef/ItOgIWFRYTZG955m2cs2sLpdIo/MJ0dM5RhfnLMUIb5yfkjQ5vNhpSUFOTn52PPnj0+fe+OoOs6iwqBjsgvLCwMPXr0EBfNLCwsQNM0lBTlG485xqL1NE1Dbm4ubwwlwAxlmJ8cM5RhfnL+zDAiIgKZmZmoq7P2HxddLhf27t2LHj168Dhsg47Iz2azwW63t0uxwsLCIoJtCkLsKmqcGqebJSIi6oRsNpvlfxl3uVxQVRWhoaGW74s/mD0/XmhpIZGh7jqQN8gjIiIiIrNhYWERqqoiKjQIAG+Q11YcsCjHDGWYnxwzlGF+csxQjhnKmDk/RefcZSdVXl6O6OhoOBwOREVF+a0dk1/9GRvzyqAoQM4Tk6CqHPhERERERB2nNb8Hm7fkIYOu66ioqEDUsUuhdB2oqOXlUK3hyZB1dNsxQxnmJ8cMZZifHDOUY4YyZs+PhYUFaJqG/Px8RIbUj7Uvr+LlUK3hybC9bgDTGTFDGeYnxwxlmJ8cM5RjhjJmz4+FhYV4Bm8DHMBNRERERObCwsJCGhYWPGNBRERERGbC+1hYgKIoCA4ORnSX+rMUPGPROp4MeafPtmOGMsxPjhnKMD85ZijHDGXMnh8LCwtQVRXp6emIKtpjrOOUs63jyZDajhnKMD85ZijD/OSYoRwzlDF7frwUygJ0XUdZWRnHWAh4MjTrLApWwAxlmJ8cM5RhfnLMUI4Zypg9PxYWFqBpGoqKihARXH/rdo6xaB1PhmadRcEKmKEM85NjhjLMT44ZyjFDGbPnx8LCQqK6NDhjUcMzFkRERERkHhxjYXaaBuSvQUT+RnQJiwMQCoBnLIiIiIjIXFhYmJ2iQH3nYqRodaiN6w/gYQAcY9FaiqIgPDzctLMoWAEzlGF+csxQhvnJMUM5Zihj9vxYWJidokCJSADK82GvKjFWc1ao1lFVFampqf5uhqUxQxnmJ8cMZZifHDOUY4YyZs+PYywsQI+IBwAoR0tgU9yDdcp5xqJVNE1DSUmJaQc7WQEzlGF+csxQhvnJMUM5Zihj9vxYWFhB+LHCQteQGnIUAHCEYyxaRdd1lJSUmHZ6NitghjLMT44ZyjA/OWYoxwxlzJ4fCwsL8JyxAIBewRUAeMaCiIiIiMyFhYUVhHc3Fk8JKgfAMRZEREREZC4sLKwgMsFYTLa7C4tap4bqOpe/WmQ5iqIgOjratLMoWAEzlGF+csxQhvnJMUM5Zihj9vw4K5QFqJGJxnKCWm4sO6rqEBpka+pL6DiqqiIpKcnfzbA0ZijD/OSYoQzzk2OGcsxQxuz58YyFBWhh9ZdCJdnqC4vi8mp/NMeSNE1DYWGhaWdRsAJmKMP85JihDPOTY4ZyzFDG7PmxsLAAPbx+8HZ3pcxYLnSwsGgpXdfhcDhMO4uCFTBDGeYnxwxlmJ8cM5RjhjJmz4+FhRVE1J+xiNEOG8tFLCyIiIiIyCRYWFhBcAQ0excAQERdqbGaZyyIiIiIyCxYWFiAoijQj005G1JzyFhf6KjyV5MsR1EUxMXFmXYWBStghjLMT44ZyjA/OWYoxwxlzJ4fCwsLUFUVtqhkAICtpgzBcN/DgmcsWk5VVcTFxUFVeci3FTOUYX5yzFCG+ckxQzlmKGP2/MzZKvKiaRqO2iKNx2mhlQA4xqI1NE1DXl6eaWdRsAJmKMP85JihDPOTY4ZyzFDG7PmxsLAAXddRY48yHveJcF8CVeSoNu2sAGaj6zoqKyuZlwAzlGF+csxQhvnJMUM5Zihj9vxYWFiEMzTWWE7vUgEAqHVpKK2s9VeTiIiIiIgMfi0sli9fjksuuQTJyclQFAWffvqp8VxdXR0eeOABDBw4EOHh4UhOTsa0adNQUFDg9RqlpaWYOnUqoqKiEBMTg+nTp6OiosJrm99++w3nnHMOQkNDkZqaimeeecYX3WtXzi71hUWP4Pr+cZwFEREREZmBXwuLyspKnHbaaXj11VcbPXf06FGsX78ejzzyCNavX49PPvkEO3bswO9+9zuv7aZOnYotW7Zg6dKl+OKLL7B8+XLceuutxvPl5eW48MIL0bNnT6xbtw7PPvssHn30Ubz55psd3r/2oqoqIpN6G4+TbEeMZRYWLaOqKhITE0072MkKmKEM85NjhjLMT44ZyjFDGbPnp+gmuUhLURQsWLAAl156abPbrFmzBsOHD8fevXvRo0cPbNu2Df3798eaNWswbNgwAMCiRYswadIk5OfnIzk5Ga+99hoeeughFBUVITg4GADwl7/8BZ9++im2b9/eoraVl5cjOjoaDocDUVFRJ/+CjrB/HfDP8wEAu3peg3E73AXWY5MH4PqRvfzTJiIiIiIKaK35Pdic5U4zHA4HFEVBTEwMAGDlypWIiYkxigoAGDduHFRVxapVq4xtRo8ebRQVADB+/Hjs2LEDhw8fhhVomoa9pTXG44Z33+YZi5bRNA27d+827SwKVsAMZZifHDOUYX5yzFCOGcqYPT+7vxvQUtXV1XjggQdw7bXXGtVSUVER4uPjvbaz2+3o1q0bioqKjG3S0tK8tklISDCe69q1a6P3qqmpQU1N/S/y5eXlAACXywWXywXAfYZFVVVomuY1Mr+59aqqQlGUZtd7XrfhesB9ALlcLhxFmPFceF39TfIKyqrgcrlgs9mg67rXgeZpS3PrW9r2juhTS9a3Z59cLheqq6vhdDphs9kCok++3k+6rqOmpsbIMBD65Mv95DkGXS5XwPTpZOvbu09Op9PrcxwIffLlftI0DbW1tXC5XAHTJ1/vJ8/nWNf1gOmTh6/2U8PPcVBQUED0yZf7CUCjn8Ud3afWXNxkicKirq4OV111FXRdx2uvvdbh7/fUU09hzpw5jdbn5OQgIiICABAdHY2kpCQUFxfD4XAY28TFxSEuLg779+9HZWWlsT4xMRExMTHYs2cPamvrZ3JKSUlBREQEcnJyvA6GtLQ02O12ZGdnQ9M0lDoq4AqOhK32CIKrDhrb5RaVIicnB1lZWaisrER+fr7xXHBwMNLT0+FwOIxCCwDCw8ORmpqK0tJSlJSUGOt92aeGMjMz4XQ6kZuba6xTVbVd+3TgwAGUlpZi165dxs1lrN4nX++n9PR0uFwuI8NA6JMv95OmaSgtLUVpaSkSEhICok++3k85OTnG59hutwdEn3y5nzx/SCsoKEBVVVVA9MnX+0nTNONqh0DpE+Db/XTkyBHjc5ycnBwQffLlfsrIyEBdXZ3Xz+KO7lNYWP0ft0/G9GMsPEXF7t278d133yE2tn52pLfffhv33HOP1yVNTqcToaGh+Pjjj3HZZZdh2rRpKC8v95pxatmyZTj//PNRWlra4jMWnh3jOVvi6zMWu3btQr9vb4ByaCf0oHAMrJ2HihonesaG4bu7RwdkVd6efaqrq0N2djZ69+7NMxZt7JOu68jOzkZGRgbPWLTxjMWuXbuQmZmJoKCggOjTyda3d588P0w9n+NA6JOvz1jk5OQgIyPDeH+r98kfZyx27dqFPn36GO9r9T55+PKMhedzzDMWbTtjsXPnTq+fxR3dp4qKCsTExLRojIWpz1h4iors7GwsW7bMq6gAgJEjR6KsrAzr1q3D0KFDAQDfffcdNE3DiBEjjG0eeugh1NXVISgoCACwdOlS9OnTp8miAgBCQkIQEhLSaL3nB1lDDb85S9Yf/7oN16uqih49egCRCcChnVDqKpEWpWPTQfdN8jyvqShKk6/T3Pr2antb+tTS9e3VJ7vdjh49ehjfxE62vRX65Ov9pOs6UlNTG2UIWLdPJ1rf3n3yfI7tdnuLtpe0vbn1Vt9PQUFBjT7HVu+TL/eTqqpISUmB3W5v9Bk+0euYuU9tXd/WPnk+x55fEgOhTw35ok9NfY6t3qfWrJf2qS0/i6Vtb+r7RXP8Oni7oqICGzZswIYNGwAAubm52LBhA/bt24e6ujpcccUVWLt2Ld5//324XC4UFRWhqKjIOLXUr18/TJgwAbfccgtWr16Nn3/+GTNnzsQ111yD5ORkAMDvf/97BAcHY/r06diyZQs++ugjvPjii7j77rv91e1WUxQFERERUCISjHVZ4UcBADVODWVH6/zVNMswMmzFh4O8MUMZ5ifHDGWYnxwzlGOGMmbPz6+Fxdq1azFkyBAMGTIEAHD33XdjyJAhmD17Nvbv34+FCxciPz8fgwcPRlJSkvFvxYoVxmu8//776Nu3L8aOHYtJkyZh1KhRXveoiI6OxpIlS5Cbm4uhQ4finnvuwezZs73udWF2LpcLO3fuhBYeZ6zL6FJ/7Rtnhjo5T4bHn4KklmOGMsxPjhnKMD85ZijHDGXMnp9fL4UaM2bMCUeat2T4R7du3fDBBx+ccJtBgwbhxx9/bHX7zETTNCC8/oxFanAFgG4AgKLyKvRP9tP9NSykqesUqXWYoQzzk2OGMsxPjhnKMUMZM+dnqftYdHoR9VPrJtnKjWWesSAiIiIif2NhYSF6g8IiTikzlgvLWFgQERERkX+ZelYoclNVFWlpaVBL6+dDjnHx7tutYWTYzAwIdHLMUIb5yTFDGeYnxwzlmKGM2fMzZ6uoEbvd7nUpVMO7bxeVVzX1JXQczzSf1HbMUIb5yTFDGeYnxwzlmKGMmfNjYWEBmqa578Ad2g1Q3LvMXnUQYcHuuYp5xuLkjAxNPODJ7JihDPOTY4YyzE+OGcoxQxmz58fCwkpUGxDmnnJWqTiIxOhQAO6b5JnkBupERERE1EmxsLAaz03yKoqRFOW+O/jRWhfKq51+bBQRERERdXYsLKzGM85Cq0NGeP1g7kIHx1kQERERkf+wsLAAVVWRmZnpngEg+hRjfWZombHMKWdPzCtDahNmKMP85JihDPOTY4ZyzFDG7PmZs1XUiNN57FKnmB7GunR7/cxQew9V+rpJlmNkSG3GDGWYnxwzlGF+csxQjhnKmDk/FhYWoGkacnNz3TMAxPQy1qeqB43l3BIWFifilSG1CTOUYX5yzFCG+ckxQzlmKGP2/FhYWE2DMxbdnUXG8m4WFkRERETkRywsrKZrT2MxtHI/IkPcN0nZw0uhiIiIiMiPWFhYhDFIJzwesLmnmVXK9iKtezgAIP9wFWqcLn81zxLMOtDJSpihDPOTY4YyzE+OGcoxQxkz56fovLPaSZWXlyM6OhoOhwNRUVH+bg7w8jDgUDYQFIY7077Ewt8KAQBL7xqNzIRIPzeOiIiIiAJFa34PNm/JQwZd11FRUVF/d23POIu6o+gfXX8vC46zaF6jDKnVmKEM85NjhjLMT44ZyjFDGbPnx8LCAjRNQ35+fv0MAA3GWfQLO2ws72Fh0axGGVKrMUMZ5ifHDGWYnxwzlGOGMmbPj4WFFTWYGaqXWmIsc8pZIiIiIvIXFhZWFFN/xiJBP2As81IoIiIiIvIXFhYWoCgKgoODoSiKe0WDwiL0SB7iIoIB8IzFiTTKkFqNGcowPzlmKMP85JihHDOUMXt+nBWqBUw3K1RlCfBshns5YyyurLwXa/a4x1psnjMeEcfubUFEREREJMFZoQKMrusoKyurnwEgLBYICnMvl+1FWly4sS0HcDetUYbUasxQhvnJMUMZ5ifHDOWYoYzZ82NhYQGapqGoqKh+BgBFqb8cqiwPabFhxrYcZ9G0RhlSqzFDGeYnxwxlmJ8cM5RjhjJmz4+FhVV5ZoZy1aBvxFFjde5BFhZERERE5HssLKyqwb0s0oNKjeU9h1hYEBEREZHvsbCwAEVREB4e7j0DQIN7WSTpxfA8xUuhmtZkhtQqzFCG+ckxQxnmJ8cM5ZihjNnzY2FhAaqqIjU1FaraYHc1mHI2+EgekqO7AAByD5r3Nu/+1GSG1CrMUIb5yTFDGeYnxwzlmKGM2fMzZ6vIi6ZpKCkp8R6o0+CMBQ7vRXp398xQ5dVOlFbW+riF5tdkhtQqzFCG+ckxQxnmJ8cM5ZihjNnzY2FhAbquo6SkxPtMRIMxFijbh16xDaac5TiLRprMkFqFGcowPzlmKMP85JihHDOUMXt+LCysKjQGCDl2k5Lj7mWxmzNDEREREZGPsbCwqob3snDkIy021Hhq18EKPzWKiIiIiDorFhYWoCgKoqOjG88A4BlnoTnRP6K+mNhaUO7D1llDsxlSizFDGeYnxwxlmJ8cM5RjhjJmz8/u7wbQyamqiqSkpMZPNBhnEe8sQmx4MA5V1mJLQTl0XTftQecPzWZILcYMZZifHDOUYX5yzFCOGcqYPT+esbAATdNQWFjYeAaArmnGolKagwGnRAMASitrUVRe7csmml6zGVKLMUMZ5ifHDGWYnxwzlGOGMmbPj4WFBei6DofD0XgGgO596pcPbMOpyVHGw837eTlUQ81mSC3GDGWYnxwzlGF+csxQjhnKmD0/FhZWFt+vfvnANpx67IwFAGze7/BDg4iIiIios2JhYWXh3YGwWPfywe04Nbm+sNhSwMKCiIiIiHyHhYUFKIqCuLi4xoOxFQXofuysRUUxUrtUIzLUPR6fl0J5azZDajFmKMP85JihDPOTY4ZyzFDG7PmxsLAAVVURFxcHVW1id8X3NRaVBmctisqrcfBIja+aaHonzJBahBnKMD85ZijD/OSYoRwzlDF7fuZsFXnRNA15eXlNzwDQvb6wcI+zqB/Azcuh6p0wQ2oRZijD/OSYoQzzk2OGcsxQxuz5sbCwAF3XUVlZ2fQMACcYwL2FN8oznDBDahFmKMP85JihDPOTY4ZyzFDG7PmxsLC67g0Ki4PbMSCZM0MRERERke+xsLC68FggPN69fGAb0uLCERZsAwBs5qVQREREROQjLCwsQFVVJCYmNj9QxzOA+2gJbFWH0D/JPc4ir7QKjqN1PmqluZ00QzopZijD/OSYoQzzk2OGcsxQxuz5mbNV5EVRFMTExDQ/tVj3E42z4FkLoAUZ0kkxQxnmJ8cMZZifHDOUY4YyZs+PhYUFaJqG3bt3Nz8DwHEDuAck188Mxcuh3E6aIZ0UM5RhfnLMUIb5yTFDOWYoY/b8WFhYgK7rqK2tbX4GgIaFxUHvMxabeKM8AC3IkE6KGcowPzlmKMP85JihHDOUMXt+LCwCgde9LLajd3wEQoPcu3bdnlLTHnxEREREFDhYWASCLjFAZJJ7+eA2BKkKzujVDQBQ4KjG3kNH/dc2IiIiIuoUWFhYgKqqSElJOfEMAJ6zFlWHgYoDGJkRazy1IudQB7fQ/FqUIZ0QM5RhfnLMUIb5yTFDOWYoY/b8zNkq8qIoCiIiIk48A0B8//rlg9twVkac8fDnnJIObJ01tChDOiFmKMP85JihDPOTY4ZyzFDG7PmxsLAAl8uFnTt3wuVyNb9RfMNxFttwanIUIkPsAIBfcg5B0zr3OIsWZUgnxAxlmJ8cM5RhfnLMUI4Zypg9PxYWFnHSacXiB9QvF2yA3aZiRLp7nMWhylrsPHCkA1tnDWadms1KmKEM85NjhjLMT44ZyjFDGTPnx8IiUCSeCtiC3cv5awDA63KoFbs4zoKIiIiIOg4Li0BhDwGSBruXS3OAykM4qzcHcBMRERGRb/i1sFi+fDkuueQSJCcnQ1EUfPrpp17P67qO2bNnIykpCV26dMG4ceOQnZ3ttU1paSmmTp2KqKgoxMTEYPr06aioqPDa5rfffsM555yD0NBQpKam4plnnunorrUrVVWRlpZ28hkAUofXL+evQVZ8JGLD3WcxVu0+BKfLvKfOOlqLM6RmMUMZ5ifHDGWYnxwzlGOGMmbPz6+tqqysxGmnnYZXX321yeefeeYZvPTSS3j99dexatUqhIeHY/z48aiurja2mTp1KrZs2YKlS5fiiy++wPLly3Hrrbcaz5eXl+PCCy9Ez549sW7dOjz77LN49NFH8eabb3Z4/9qT3W4/+UYpZ9Qv56+Gqio489i0s0dqnNhc0Lnvwt2iDOmEmKEM85NjhjLMT44ZyjFDGTPn59fCYuLEiXj88cdx2WWXNXpO13W88MILePjhhzF58mQMGjQI7777LgoKCowzG9u2bcOiRYvw1ltvYcSIERg1ahRefvllfPjhhygoKAAAvP/++6itrcXbb7+NAQMG4JprrsGdd96J559/3pddFdE0DdnZ2ScfrNOwsMhbDQA4y+t+Fp132tkWZ0jNYoYyzE+OGcowPzlmKMcMZcyen2lLntzcXBQVFWHcuHHGuujoaIwYMQIrV67ENddcg5UrVyImJgbDhg0zthk3bhxUVcWqVatw2WWXYeXKlRg9ejSCg4ONbcaPH4+nn34ahw8fRteuXRu9d01NDWpqaozH5eXuv/S7XC5jei9FUaCqKjRNg67XT+Xa3HpVVaEoSrPrj582zHOKS9M0uFwu4/8N1zdks9mgRyUDUclQygug718P3Vl73ADuEvzxnLRWt70j+tSS9TabDbque633tKW59SdquyfDQOqTL/eTruvQdb3R9lbuky/3k+dzrGkabDZbQPTpZOvbu08NvxcGSp98uZ88X9tUW6zaJ1/vJ88xCCBg+uThq/10/O80gdAnX+4nAI1+Fnd0nxoun4xpC4uioiIAQEJCgtf6hIQE47mioiLEx8d7PW+329GtWzevbdLS0hq9hue5pgqLp556CnPmzGm0PicnBxEREQDcRU5SUhKKi4vhcDiMbeLi4hAXF4f9+/ejsrLSWJ+YmIiYmBjs2bMHtbW1xvqUlBREREQgJyfH62BIS0uD3W43qtLS0lLs2rULffr0gdPpRG5urrGtqqrIyspCZWUltOh+iCovgFJXif0bvkWvoROQEBmM4iO1WJ1big1bdiC+ayRSU1NRWlqKkpL6sxi+7FNDmZmZJ+xTfn6+sT44OBjp6elwOBzGPgaA8PDwE/bpwIEDRoaqqgZEn3y9n9LT0+FyuYwMA6FPvtxPns9xaWkpEhISAqJPvt5POTk5xufYbrcHRJ98uZ88P+8KCgpQVVUVEH3y9X7SNA2HDx8GgIDpE+Db/XTkyBHjc5ycnBwQffLlfsrIyEBdXZ3Xz+KO7lNYWBhaStFbU4Z0IEVRsGDBAlx66aUAgBUrVuDss89GQUEBkpKSjO2uuuoqKIqCjz76CE8++STeeecd7Nixw+u14uPjMWfOHNx+++248MILkZaWhjfeeMN4fuvWrRgwYAC2bt2Kfv36NWpLU2csPDsmKirKaK8vz1js2rULvXv3RlBQkLG+IU9Vrq94BerSh93bTHoO6vBb8Minm/HeL3sBAE9ffiquHJZqiaq8Pf/SUFdXh+zsbPTu3Rs2my0g+uSPMxbZ2dnIyMiAzWYLiD75+ozFrl27kJmZiaCgoIDo08nWt3efPD9MPZ/jQOiTr89Y5OTkICMjw3h/q/fJH2csPH/k87yv1fvk4av95HQ6vX6nCYQ++fqMxc6dO71+Fnd0nyoqKhATEwOHw2H8Htwc056xSExMBAAUFxd7FRbFxcUYPHiwsc2BAwe8vs7pdKK0tNT4+sTERBQXF3tt43ns2eZ4ISEhCAkJabTe84OsoYbfnCXrj3/dhutVVTW+iSmK0uz2iqJA6XFm/XvlrwWG34LLTz/FKCwWbCjA1cN7tmvb29Knlq5XFKVV65tri91ub5Thiba3Qp98vZ90XUdWVlajDAHr9ulE69u7Tw0/xy3ZXtL25tZbfT8FBQU1+hxbvU++3E+qqiIzM7PJz/CJXsfMfWrr+rb26fifx4HQp4Z80aemPsdW71Nr1kv71JafxdK2N/X9ojnmnKsK7lNDiYmJ+Pbbb4115eXlWLVqFUaOHAkAGDlyJMrKyrBu3Tpjm++++w6apmHEiBHGNsuXL0ddXZ2xzdKlS9GnT58mL4MyK6fT2bINkwY1ulHe4NQYpMWFAwB+2V2K/MNHO6KJptfiDKlZzFCG+ckxQxnmJ8cM5ZihjJnz82thUVFRgQ0bNmDDhg0A3AO2N2zYgH379kFRFMyaNQuPP/44Fi5ciE2bNmHatGlITk42Lpfq168fJkyYgFtuuQWrV6/Gzz//jJkzZ+Kaa65BcnIyAOD3v/89goODMX36dGzZsgUfffQRXnzxRdx9991+6nXraZqG3NzcJk+HNdLEjfIURcHlQ04xNvlsQ0HHNNTEWpUhNYkZyjA/OWYow/zkmKEcM5Qxe35+LSzWrl2LIUOGYMiQIQCAu+++G0OGDMHs2bMBAPfffz/uuOMO3HrrrTjjjDNQUVGBRYsWITQ01HiN999/H3379sXYsWMxadIkjBo1yuseFdHR0ViyZAlyc3MxdOhQ3HPPPZg9e7bXvS4Cjtf9LNxnLS5tUFh8sj6/VSP8iYiIiIhOxq9jLMaMGXPCX3AVRcHcuXMxd+7cZrfp1q0bPvjggxO+z6BBg/Djjz+2uZ2Wk3oG8Mux5fzVQJ8JSO0WhuFp3bA6txQ5Byuxab8Dg1Ji/NlKIiIiIgogph1jQd6aG2DTpJTh9ct7VxqLl3udtdjfHs2ylFZlSE1ihjLMT44ZyjA/OWYoxwxlzJyfaaabNbPy8nJER0e3aJot03jpdPcYC8UG3LcLCOuG8uo6DHv8G9Q6NXQLD8aq/xmLIJt5D04iIiIi8q/W/B7M3yotQNd1VFRUtG5cRJ+Jx77YBWQvBQBEhQbhgv7umwOWVtbi47X5zX11wGlThuSFGcowPzlmKMP85JihHDOUMXt+LCwsQNM05Ofnt24GgD6T6pd3fGUsTh9VfxfyF77ZiaO15p2yrD21KUPywgxlmJ8cM5RhfnLMUI4Zypg9PxYWgSp1BBAa417e9S3gdN9W/vQeXTF+gPusxYEjNZj38x7/tI+IiIiIAgoLi0BlswNZ493LtUeAvT8ZT903vi/UYzdRfP37HByurPVDA4mIiIgokLCwsABFURAcHNyqW6oDqB9nAQA7vjYWe8dH4KphqQCAIzVOvLpsV3s009TanCEZmKEM85NjhjLMT44ZyjFDGbPnx1mhWsCSs0IBQHU58Ew6oNUB0T2AWb8Bxw7EIkc1zn12GWqcGoJtKpbePRo9Y8P93GAiIiIiMhPOChVgdF1HWVlZ62cACI0Ceo1yLzv2AcVbjKcSo0Pxh2MDuWtdGv743jpU1ATuQO42Z0gGZijD/OSYoQzzk2OGcsxQxuz5sbCwAE3TUFRU1LYZALxmh/ra66nbx2QgLc59lmJ70RH8+X9/hUsz54EqJcqQADBDKeYnxwxlmJ8cM5RjhjJmz4+FRaDrM6F+eceXXk9FhQbhXzcMQ1SoHQDw7fYD+OvX23zZOiIiIiIKECwsAl1MDyBhoHu54FegaJPX0+ndI/DadUNhOzZN1D9/zMVzi3fA6TJnJUxERERE5sTCwgIURUF4eHjbZwA4fVr98i+vNXr67N5xmDt5gPH4lWW7cMXrK7GnpLJt72dC4gyJGQoxPzlmKMP85JihHDOUMXt+nBWqBSw7K5RHTQXw9/5AtQOwBQOzNgORCY02e+OHHDyzeIcxziIs2IZbzknHVWek4pSYLr5uNRERERH5GWeFCjCapqGkpKTtA3VCIoDTb3Avu2qBtW83udkfz83Af28biV6xYQCAo7UuvPhtNkY9/R2mvb0a7/2yF+v2HsbRWuvNHiXOkJihEPOTY4YyzE+OGcoxQxmz52f3dwPo5HRdR0lJCbp27dr2FxnxR2Dlq4DuAta8BYy6CwgKbbTZkB5d8eWd5+DxL7fiozV50HRA14HlOw9i+c6DANy3wkiMCkVcRAjiIoIRExaMELuKELsKu02FrgOarkPXdWjHlt2voxvLqgKEBtkQYlcRbFehQPHcYgPGyT1FMZYVBfA8UhQgOaYLLh2cDLutZbVxu2TYyTFDGeYnxwxlmJ8cM5RjhjJmz4+FRWcRnQL0nwxs+QQ4WgJs+hg4/fomNw0PseOpywfhjvMz8d91+fhoTR72l1UZz+s6UOioRqGj2letb1LZ0VrcfE66X9tARERERG68FKozGTmjfvmXf7grhBNIjumCO8dm4sf7z8MnfzoLj00egGvOSMWglGjERYRA9fO4oc83Fvi3AURERERk4BkLC1AUBdHR0fIZAFKGASnDgfzVwIGtwK//bvasRUOqquD0Hl1xeg/v024uTUfZ0Vo4qupQ69JQU6ehzqVBURSoCqAqClTFfYmTqihQVc86wKUBNU4Xapwaap2aUePo0Bssw7izpG78B3j8y63IOViJjfkOHDxSg+6RISftQ7tl2IkxQxnmJ8cMZZifHDOUY4YyZs+Ps0K1gOVnhWpo1zfAv6e4l0OigNtXADGp/m1TKz2zaDv+8X2Oe/mKQbhqmLXaT0RERGQVnBUqwGiahsLCwvaZAaD3OGDwde7lmnJg4R0nvSTKbMb2izeWl20/0KKvadcMOylmKMP85JihDPOTY4ZyzFDG7PmxsLAAXdfhcDjQbieXxj8BRJ3iXt69DFg3v31e10cGp3ZF17AgAMCP2SWodZ78w9XuGXZCzFCG+ckxQxnmJ8cM5ZihjNnzY2HRGXWJAX73Uv3jJQ8DB7b7rTmtZVMVjOnjPmtRUePEmj2lfm4REREREbGw6Kx6j6u/aV5tBTBvIlDwq3/b1Arn9a2/HOq7Fl4ORUREREQdh4WFBSiKgri4uPafAWD8E0DSYPdyVSnwzu+AvSva9z06yLmZ3WE7Nt9tSwqLDsuwE2GGMsxPjhnKMD85ZijHDGXMnh8LCwtQVRVxcXFQ1XbeXSGRwA0LgR5nuR/XlAPvXQaseAWoqzrx1/pZdFgQhvZ0T3+bW1KJ3JLKE27fYRl2IsxQhvnJMUMZ5ifHDOWYoYzZ8zNnq8iLpmnIy8vrmBkAQqOB6/4P6H2B+7GzGljyEPDiYGD1P4GaI+3/nu3k/FZcDtWhGXYSzFCG+ckxQxnmJ8cM5ZihjNnzY2FhAbquo7KysuNmAAgOA6754NiYi2On1iqKgK/uBZ7uBcybBCx/Fti5BCjeAlQ7OqYdrTS2QWHxzdbiE27b4Rl2AsxQhvnJMUMZ5ifHDOWYoYzZ8+Odt8nNHuyeKWrEH4FlTwLbv3Cv15zA3p/d/xqyBbsvpQqOAILCAEUBFBWAUr/ste7YYwBG8aIoJ1g+RrUBqt39LzQaiE5x/0s6Db2TT0ev2DDsOXQUq3IPtfgu3ERERETU/lhYkLeEAcA177tniFr/nvs+F6W7G2/nqgWOHnL/8xPl8rdw0aCBeHVZDjQd+HpzIaaN7OW39hARERF1ZiwsLEBVVSQmJvp2oE7yEPc/ACjNdZ+xOLwXcOQD5fnA0cNA7RH3GIy6agC6+w7euub+B8+y7l7uCNu/wMXnjMery3IAAF9sbL6w8EuGAYYZyjA/OWYow/zkmKEcM5Qxe34sLCxAURTExMT4rwHd0tz/JIxBRseKjIYFR6PlY9vpOqC7AM3lviTr6CHAkQd8eB3grAIKN6BvYiQyuocj52Al1uwtRZGjGonRoY3e3u8ZBgBmKMP85JihDPOTY4ZyzFDG7PmZs9whL5qmYffu3aadAaBFVPXYP5v7n80O2ILc/+zBgD3E/S8o9Ni/Lu5B5SGR7juFh8cB3fu4b+yXOND9mof3QKkuw8WDkgG465AvNxU2+fYBkaGfMUMZ5ifHDGWYnxwzlGOGMmbPj4WFBei6jtraWtPOAOBzyYPrlwt/w8WDkoyHX/xW0OSXMEM5ZijD/OSYoQzzk2OGcsxQxuz5sbAg60k6rX65cAMyEyLRJyESAPDrvjLkHz7qp4YRERERdV4sLMh6kgbXLxduBACvsxZf/tb05VBERERE1HFYWFiAqqpISUkx7QwAPte9L2A/NkC7YAMA4OLTko2nP9tQAE3zPkXIDOWYoQzzk2OGMsxPjhnKMUMZs+dnzlaRF0VREBERAaXhjeM6M5vdfb8NACjNAaodSIsLx6mnRAEAthaW492Ve7y+hBnKMUMZ5ifHDGWYnxwzlGOGMmbPj4WFBbhcLuzcuRMul8vfTTGPhpdDFW0CANw/vq+x6smvt2Nn8RHjMTOUY4YyzE+OGcowPzlmKMcMZcyeHwsLizDrtGJ+03AA97HLoUZndceNZ/UCANQ6Ndz5v7+ixln/wWOGcsxQhvnJMUMZ5ifHDOWYoYyZ82NhQdbkNeXsBmPxLxP7IishAgCwvegI/rZkp2/bRURERNRJsbAga+reD7AFu5ePzQwFAKFBNrx4zRAE29yH9pvLd2PJliJ/tJCIiIioU2FhYQGqqiItLc20MwD4hT0YiO/vXi7JBmrqx1P0S4rC/RP6GI/v+mgDdh6oYIZCPA5lmJ8cM5RhfnLMUI4Zypg9P3O2ihqx2+3+boL5GJdD6cYAbo/po9JwybEpaCtrXbj13XUorzHvNYlWweNQhvnJMUMZ5ifHDOWYoYyZ82NhYQGapiE7O9vUg3X8ookb5XkoioJnpgzCwFOiAQB5h6sw/e0VqKlz+rCBgYXHoQzzk2OGMsxPjhnKMUMZs+fHwoKsq+HMUPvXN3q6S7ANb04biriIEADAb0XVeGbxDl+1joiIiKhTYWFB1pUwoP4O3FsWAEWbG22SFN0Fb1w/FME2941k3v55L77aVOjLVhIRERF1CiwsyLrsIcCZf3Iva3XAp7cBztpGmw3t2RUPTepnPL7/v79h98EKX7WSiIiIqFNQdF3X/d0IsysvL0d0dDQcDgeioqJ8/v66rkPTNKiqatpbuPuNswZ4cwxwYKv78ej7gfMfarSZpmmY9dEGLNzoPlvRNzESC/50NroE23zYWGvjcSjD/OSYoQzzk2OGcsxQxh/5teb3YJ6xsAink4OOm2QPAS59DVCPzZDw49+A/esabaYoCh69qA96x9ffPO+Jr7b6sqUBgcehDPOTY4YyzE+OGcoxQxkz58fCwgI0TUNubq5pZwDwu+TB7jMVAKC7gE9uBarKvDbRNA0HCvLw6rWD0SXIfZbi/VX78Ou+w75tq4XxOJRhfnLMUIb5yTFDOWYoY/b8WFhQYDjn7vrpZw/tAj6+AXDVNdqsd3wE7rkwCwCg68BDCzbD6TLnh5OIiIjISlhYUGCwBQFXzge6dHM/3v098NV97urhODee1Qv9ktzXCG4tLMc7K/f6rp1EREREAYqFhUWY9dbtptItDbjmA8AW7H68bh7wy2vG054M7TYVT1x2Kjxjnp5fsgOFjipft9aSeBzKMD85ZijD/OSYoRwzlDFzfuZtGQCXy4VHHnkEaWlp6NKlCzIyMvDYY4+h4URWuq5j9uzZSEpKQpcuXTBu3DhkZ2d7vU5paSmmTp2KqKgoxMTEYPr06aiosM50ozabDVlZWbDZOIPRSfUcCfzulfrHi/8H2LGoUYan9+iKa4f3AABU1rrw5Ffb/dFaS+FxKMP85JihDPOTY4ZyzFDG7PmZurB4+umn8dprr+GVV17Btm3b8PTTT+OZZ57Byy+/bGzzzDPP4KWXXsLrr7+OVatWITw8HOPHj0d1dbWxzdSpU7FlyxYsXboUX3zxBZYvX45bb73VH11qE13XUVFRAc4M3EKnXQ2Mvu/YAx34v+nQizY1yvCB8X3RLdx9dmPxliJU17n80Fjr4HEow/zkmKEM85NjhnLMUMbs+Zm6sFixYgUmT56Miy66CL169cIVV1yBCy+8EKtXrwbgDveFF17Aww8/jMmTJ2PQoEF49913UVBQgE8//RQAsG3bNixatAhvvfUWRowYgVGjRuHll1/Ghx9+iIKCAj/2ruU0TUN+fr5pZwAwpTH/A/S/1L1cWwF8cDUKszd6ZRgdFoSxfePdmzg1rOcMUSfE41CG+ckxQxnmJ8cM5ZihjNnzs/u7ASdy1lln4c0338TOnTuRlZWFjRs34qeffsLzzz8PAMjNzUVRURHGjRtnfE10dDRGjBiBlStX4pprrsHKlSsRExODYcOGGduMGzcOqqpi1apVuOyyyxq9b01NDWpqaozH5eXlANyXZrlc7r9qK4oCVVWhaZpX1djces+NTJpb73ndhusB9wHkcrmM/zdc35DNZjNumnJ8W5pb39K2d0SfWrJe3KffvQq1LA9KwToo5fuR8uO9cGV9DYRGGtufmd4NH6/LBwCs3FWCkemx5u5TG9a3137SdR26rjfa3sp98uV+8nyONU2DzWYLiD6dbH1796nh98JA6ZMv95Pna5tqi1X75Ov95DkGAQRMnzx8tZ+O/50mEPrky/0EoNHP4o7uU2vOjpi6sPjLX/6C8vJy9O3bFzabDS6XC0888QSmTp0KACgqKgIAJCQkeH1dQkKC8VxRURHi4+O9nrfb7ejWrZuxzfGeeuopzJkzp9H6nJwcRES4b7AWHR2NpKQkFBcXw+FwGNvExcUhLi4O+/fvR2VlpbE+MTERMTEx2LNnD2pra431KSkpiIiIQE5OjtfBkJaWBrvdjuzsbGiahtLSUuzatQt9+vSB0+lEbm6usa2qqsjKykJlZSXy8/ON9cHBwUhPT4fD4fDqa3h4OFJTU1FaWoqSkhJjvS/71FBmZmaH9anr2BeQ8Nm1QHk+upRuRcV7V2P/Oc8iNiEZcXFx6Nmlvt3fb92P6SPM3yd/7af09HS4XC7s2rXL+IZn9T75cj95PselpaVISEgIiD75ej/l5OQY3wvtdntA9MmX+6lr164AgIKCAlRV1U9YYeU++Xo/aZqGw4fdZ7cDpU+Ab/fTkSNHjM9xcnJyQPTJl/spIyMDdXV1Xj+LO7pPYWFhaClFN+tFWgA+/PBD3HfffXj22WcxYMAAbNiwAbNmzcLzzz+PG264AStWrMDZZ5+NgoICJCUlGV931VVXQVEUfPTRR3jyySfxzjvvYMeOHV6vHR8fjzlz5uD2229v9L5NnbHw7BjPrcx9WcFqmoa9e/eiZ8+esNvtxvqGArEqb7c+HdgK/e3xUGrdA/b1PhdDv3IeVHswNE3DmOe+x77SKgTbFGz8fxeiS7Dd/H3yw34CgD179qBHjx7GNlbvky/3k+dz3KtXL9jtzR9jVurTyda3d5+cTqfxvVBV1YDoky/3k67r2LdvH3r06AHFMy2exfvk6/3k+Rynp6cbr2/1Pnn48oxFw99pAqFPvtxPiqIgNzfX62dxR/epoqICMTExcDgcxu/BzTF1YZGamoq//OUvmDFjhrHu8ccfx7///W9s374du3fvRkZGBn799VcMHjzY2Obcc8/F4MGD8eKLL+Ltt9/GPffcY/yFAXD/cAoNDcXHH3/c5KVQxysvL0d0dHSLAiWT2rsSeO8ywHnsr3QDrwIuex1QbXjgv7/ho7V5AIAPbh6Bs3rH+bGhRERERObRmt+DTT14++jRo15/GQVgXJsMuE8fJSYm4ttvvzWeLy8vx6pVqzBy5EgAwMiRI1FWVoZ169YZ23z33XfQNA0jRozwQS/kdF1HWVlZq65xI296jzNRcck/oXvucbHpP8CPfwMAnJnRzdjul92H/NE8S+BxKMP85JihDPOTY4ZyzFDG7PmZurC45JJL8MQTT+DLL7/Enj17sGDBAjz//PPGWQZFUTBr1iw8/vjjWLhwITZt2oRp06YhOTkZl156KQCgX79+mDBhAm655RasXr0aP//8M2bOnIlrrrkGycnJfuxdy2mahqKioiYvTaGW0TQN+SGZ0KbMA5Rjcz///BJQdRhnpsca2/2yu9RPLTQ/HocyzE+OGcowPzlmKMcMZcyen6kLi5dffhlXXHEF/vSnP6Ffv36499578cc//hGPPfaYsc3999+PO+64A7feeivOOOMMVFRUYNGiRQgNDTW2ef/999G3b1+MHTsWkyZNwqhRo/Dmm2/6o0vkb30mAqdf716uPQKsegNJ0V3QK9Y9MOnXvMOoquX9LIiIiIhay9SzQkVGRuKFF17ACy+80Ow2iqJg7ty5mDt3brPbdOvWDR988EEHtJAsadRdwPr3AN0F/PIP4Mw/4cz0WOw5dBR1Lh3r9x3G2RxnQURERNQqpj5jQW6KoiA8PNxrFg9qHa8Mu/YCTrvG/US1A1jzz+Muh+I4i6bwOJRhfnLMUIb5yTFDOWYoY/b8TD0rlFlwVqgAVLILePUMQNeAsFgU/2ENRjz3CwBgWM+u+O/tZ/m5gURERET+FzCzQpGbpmkoKSkx7UAdK2iUYVxvYMDl7uWjh5Cw83+RFhcOANiYX8ZxFk3gcSjD/OSYoQzzk2OGcsxQxuz5sbCwAF3XUVJSYtqpxaygyQxH31u/vOYtnNHLfVfaOpeO7UXlPm6h+fE4lGF+csxQhvnJMUM5Zihj9vxYWFDnFd8PSD3TvXw4F4Nj689SbC864qdGEREREVkTCwvq3E453VgcbN9rLO9gYUFERETUKiwsLEBRFERHR5t2BgAraDbDpMHGYlrtTmN5WyEvhToej0MZ5ifHDGWYnxwzlGOGMmbPj4WFBaiqiqSkJKgqd1dbNZth8hBjsUvJJiREhQBwXwpl1usX/YXHoQzzk2OGMsxPjhnKMUMZs+dnzlaRF03TUFhYaNoZAKyg2QxjewPBEe7lgg3om+ieRs1RVYei8moft9LceBzKMD85ZijD/OSYoRwzlDF7fiwsLEDXdTgcDv4FXaDZDFUVSDrNvezIw5A4p/EUB3B743Eow/zkmKEM85NjhnLMUMbs+bGwIGowzmJ48D5jeXshCwsiIiKilmJhQdRgnEVv1y5jmfeyICIiImo5FhYWoCgK4uLiTDsDgBWcMMPkwcZiXPlW2FX3Njxj4Y3HoQzzk2OGMsxPjhnKMUMZs+fXpsIiLy8P+fn5xuPVq1dj1qxZePPNN9utYVRPVVXExcWZdgYAKzhhht0ygOBI93aFG9E73j2YO+dgBWqd5hwc5Q88DmWYnxwzlGF+csxQjhnKmD2/NrXq97//PZYtWwYAKCoqwgUXXIDVq1fjoYcewty5c9u1geSeASAvL8+0MwBYwQkzbDiAuzwfw44N4HZqOnIOVviwlebG41CG+ckxQxnmJ8cM5ZihjNnza1NhsXnzZgwfPhwA8J///AennnoqVqxYgffffx/z589vz/YR3DMAVFZWmnYGACs4aYYNLoca2aX+bBzHWdTjcSjD/OSYoQzzk2OGcsxQxuz5tamwqKurQ0iI+0Zi33zzDX73u98BAPr27YvCwsL2ax2RrzQYwN0PDQZwc5wFERERUYu0qbAYMGAAXn/9dfz4449YunQpJkyYAAAoKChAbGxsuzaQyCcaFBbJR3cYy7yXBREREVHLtKmwePrpp/HGG29gzJgxuPbaa3Haae7r0xcuXGhcIkXtR1VVJCYmmnagjhWcNMOuaUCI+67bIQd+Q0xYEABeCtUQj0MZ5ifHDGWYnxwzlGOGMmbPT9HbeJGWy+VCeXk5unbtaqzbs2cPwsLCEB8f324NNIPy8nJER0fD4XAgKirK382hjjL/YmDPjwCAWxM+xJK97oFR6x+5AN3Cg/3ZMiIiIiK/aM3vwW0qd6qqqlBTU2MUFXv37sULL7yAHTt2BFxRYQaapmH37t2mnQHAClqUYcKpxuJZUSXGMs9auPE4lGF+csxQhvnJMUM5Zihj9vzaVFhMnjwZ7777LgCgrKwMI0aMwN/+9jdceumleO2119q1geSeAaC2tta0MwBYQYsyjMs0FgcEFxvLOznOAgCPQynmJ8cMZZifHDOUY4YyZs+vTYXF+vXrcc455wAA/vvf/yIhIQF79+7Fu+++i5deeqldG0jkM937GIs99TxjeUcx72VBREREdDJtKiyOHj2KyEj3nYqXLFmCyy+/HKqq4swzz8TevXvbtYFEPhOXZSx2q6o/jncW84wFERER0cm0qbDo3bs3Pv30U+Tl5WHx4sW48MILAQAHDhzg4OYOoKoqUlJSTDsDgBW0KMPw7kBoDADAfigbp8R0AeC+FMqspxx9icehDPOTY4YyzE+OGcoxQxmz59emVs2ePRv33nsvevXqheHDh2PkyJEA3GcvhgwZcpKvptZSFAURERFQFMXfTbGsFmWoKPVnLcrzMSjeDgA4UuNEoaPaB600Nx6HMsxPjhnKMD85ZijHDGXMnl+bCosrrrgC+/btw9q1a7F48WJj/dixY/H3v/+93RpHbi6XCzt37oTL5fJ3UyyrxRl2r78canjUIWN5By+H4nEoxPzkmKEM85NjhnLMUMbs+dnb+oWJiYlITExEfn4+ACAlJYU3x+tAZp1WzEpalGGDcRanBhcDSAXgvhzqvD6cSpnHoQzzk2OGMsxPjhnKMUMZM+fXpjMWmqZh7ty5iI6ORs+ePdGzZ0/ExMTgscceM3VniU4qrn5mqF56vrHMMxZEREREJ9amMxYPPfQQ/vWvf+Gvf/0rzj77bADATz/9hEcffRTV1dV44okn2rWRRD7T4F4W3ar2QFVGQtM5MxQRERHRySh6G6a7SU5Oxuuvv47f/e53Xus/++wz/OlPf8L+/fvbrYFm0JpbmXcEz81QgoODTTtYx+xanKHmAp5IBFy1QPe+OL/qaewuqUSIXcXWuRNgUztv/jwOZZifHDOUYX5yzFCOGcr4I7/W/B7cpkuhSktL0bdv30br+/bti9LS0ra8JJ2E3d7m4TB0TIsyVG1AbG/38qEc9I0PAwDUODXsKz3aga2zBh6HMsxPjhnKMD85ZijHDGXMnF+bCovTTjsNr7zySqP1r7zyCgYNGiRuFHnTNA3Z2dkcvyLQqgw9A7i1OgyLcRirdxR17suheBzKMD85ZijD/OSYoRwzlDF7fm0qeZ555hlcdNFF+Oabb4x7WKxcuRJ5eXn46quv2rWBRD7XYGaogcEHAMQAcI+zmHBqon/aRERERGRybTpjce6552Lnzp247LLLUFZWhrKyMlx++eXYsmUL3nvvvfZuI5Fvda+fGSoNecYyZ4YiIiIial6bL9JKTk5uNPvTxo0b8a9//QtvvvmmuGFEfuM1M9ReBNtOQ61Lw85OfikUERER0Ym06YwF+ZaqqsjMzISqcne1VasyjK0vLNSSnUjvHg4AyC2pRI3TnHe69AUehzLMT44ZyjA/OWYoxwxlzJ6fOVtFjTidTn83wfJanGFwGBDdw71cko0+CRHur9d05JZUdlDrrIHHoQzzk2OGMsxPjhnKMUMZM+fHwsICNE1Dbm6uaWcAsIJWZ9j92ADuGgcGd60xVnfmmaF4HMowPzlmKMP85JihHDOUMXt+rRpjcfnll5/w+bKyMklbiMwjLgvY9Q0AYFBIMYAgAJ27sCAiIiI6kVYVFtHR0Sd9ftq0aaIGEZlCg5mhMpAHIB0AsGm/o5kvICIiIurcWlVYzJs3r6PaQSdh1kE6VtKqDBMGGovRju2Ii+iHkooa/JbvgK7rUBSlA1pofjwOZZifHDOUYX5yzFCOGcqYOT9F13Xd340wu/LyckRHR8PhcCAqKsrfzSFfqD0KPHUKoGtA0mmYHvI3fLv9AADg+3vHoFdcuJ8bSERERNTxWvN7sHlLHjLouo6KigqwBmy7VmcYHAbE9nYvH9iOIadEGE9tzC9r/wZaAI9DGeYnxwxlmJ8cM5RjhjJmz4+FhQVomob8/HzTzgBgBW3KMPHY5VCuGoyIOmSs3pjXOcdZ8DiUYX5yzFCG+ckxQzlmKGP2/FhYEDUn4VRjsZ+6z1jurGcsiIiIiE6EhQVRcxIHGYsRh7eiR7cwAMCWAgfqXOb8SwERERGRv7CwsABFURAcHNxpZyJqD23KMLH+jAWKNmFQinu65eo6DTuLO9/9LHgcyjA/OWYow/zkmKEcM5Qxe34sLCxAVVWkp6ebenoxs2tThhEJQHh393LRZgxOqb+Py2/5nW+cBY9DGeYnxwxlmJ8cM5RjhjJmz8+crSIvuq6jrKzMtDMAWEGbMlSU+nEWR0swNLbWeGpjXln7NtACeBzKMD85ZijD/OSYoRwzlDF7fiwsLEDTNBQVFZl2BgAraHOGifU3yuun7oV67Mzjhk5YWPA4lGF+csxQhvnJMUM5Zihj9vxYWBCdSIPCIrRkC7ISIgEA2QcqcLTW6a9WEREREZkOCwuiE2lQWKBoM05LiQEAuDQdWwrK/dMmIiIiIhNiYWEBiqIgPDzctDMAWEGbM4zNBGwh7uWiTRiUWj+Au7ONs+BxKMP85JihDPOTY4ZyzFDG7PmZvrDYv38/rrvuOsTGxqJLly4YOHAg1q5dazyv6zpmz56NpKQkdOnSBePGjUN2drbXa5SWlmLq1KmIiopCTEwMpk+fjoqKCl93pc1UVUVqaqppZwCwgjZnaLMD8f3cy4d2YXBCsPHUxk42MxSPQxnmJ8cMZZifHDOUY4YyZs/PnK065vDhwzj77LMRFBSEr7/+Glu3bsXf/vY3dO3a1djmmWeewUsvvYTXX38dq1atQnh4OMaPH4/q6mpjm6lTp2LLli1YunQpvvjiCyxfvhy33nqrP7rUJpqmoaSkxLQDdaxAlKFxPwsdfdQ8hNjdH5v1ew+3XwMtgMehDPOTY4YyzE+OGcoxQxmz52fqwuLpp59Gamoq5s2bh+HDhyMtLQ0XXnghMjIyALjPVrzwwgt4+OGHMXnyZAwaNAjvvvsuCgoK8OmnnwIAtm3bhkWLFuGtt97CiBEjMGrUKLz88sv48MMPUVBQ4MfetZyu6ygpKTHt1GJWIMqwwR247Qc2Y3BqDABgf1kVCh1V7dRC8+NxKMP85JihDPOTY4ZyzFDG7PmZurBYuHAhhg0bhiuvvBLx8fEYMmQI/vnPfxrP5+bmoqioCOPGjTPWRUdHY8SIEVi5ciUAYOXKlYiJicGwYcOMbcaNGwdVVbFq1SrfdYasK6HBHbgLN+KMXt2Mh2v2dK6zFkRERETNsfu7ASeye/duvPbaa7j77rvxP//zP1izZg3uvPNOBAcH44YbbkBRUREAICEhwevrEhISjOeKiooQHx/v9bzdbke3bt2MbY5XU1ODmpoa43F5uXv2H5fLBZfLBcA9eEZVVWia5lU1NrdeVVUoitLses/rNlwPuE95uVwu4/8N1zdks9mg67rXek9bmlvf0rZ3RJ9asr69++TJsNV9SjgVqmKDorug563G0PMfNLZfk1uKSwYl+a1PvtxPuq5D1/VG21u5T77cT57PsaZpsNlsAdGnk61v7z41/F4YKH3y5X7yfG1TbbFqn3y9nzzHIICA6ZOHr/bT8b/TBEKffLmfADT6WdzRfWrN2RFTFxaapmHYsGF48sknAQBDhgzB5s2b8frrr+OGG27osPd96qmnMGfOnEbrc3JyEBERAcB9ZiQpKQnFxcVwOOoH8cbFxSEuLg779+9HZWWlsT4xMRExMTHYs2cPamvr7+CckpKCiIgI5OTkeB0MaWlpsNvtyM7Ohq7rqKysRE5ODrKysuB0OpGbm2tsq6oqsrKyUFlZifz8fGN9cHAw0tPT4XA4vIqo8PBwpKamorS0FCUlJcZ6X/apoczMzA7v08GDB40MFUVpdZ8y4/vDVrwJOLANMZX7oCqApgOrcw9B0zS/9MnX+ykjIwPh4eFGhoHQJ1/uJ8/n+PDhw4iPjw+IPvl6P+3evdv4HNtstoDoky/3U7du3RAdHY2CggJUVdVfxmnlPvl6P+m6jqqqKiiKEjB9Any7nyoqKozPcVJSUkD0yZf7qXfv3ggNDfX6WdzRfQoLC0NLKbpZL9IC0LNnT1xwwQV46623jHWvvfYaHn/8cezfvx+7d+9GRkYGfv31VwwePNjY5txzz8XgwYPx4osv4u2338Y999yDw4frL1lxOp0IDQ3Fxx9/jMsuu6zR+zZ1xsKzY6KiogCYr4INxKrcVH1a9ACUNe7L8Fy//y9+93UothSWQ1GADY9cgIgQm/X6FIj7iX1in9gn9ol9Yp/Yp3btU0VFBWJiYuBwOIzfg5tj6jMWZ599Nnbs2OG1bufOnejZsycAd5WXmJiIb7/91igsysvLsWrVKtx+++0AgJEjR6KsrAzr1q3D0KFDAQDfffcdNE3DiBEjmnzfkJAQhISENFpvs9lgszX9C+TxWrv++NdtuF7TNBQXFyMhIcGoTpvaXlGUVq1vr7a3pU8tXd9efQKAAwcOICEhwWubFvcpdQRwrLCw7V+LM9J+hy2F5dB1YP2+MpzXN76JVwms/dTwODz+tazapxOtb+8+NcyvJdtL2t7ceqvvJ0VRGh2DVu+TL/eTpmkoLCxEQkJCq17HzH1q6/q29un474OB0KeGfLGfmvqdxup9as16aZ/a8rNY2nbPfmoJUw/evuuuu/DLL7/gySefxK5du/DBBx/gzTffxIwZMwC4Ozpr1iw8/vjjWLhwITZt2oRp06YhOTkZl156KQCgX79+mDBhAm655RasXr0aP//8M2bOnIlrrrkGycnJfuxdy+m6blxKQW0jzjB1eP1y/moM61U/5fGaPaXC1lkDj0MZ5ifHDGWYnxwzlGOGMmbPz9RnLM444wwsWLAADz74IObOnYu0tDS88MILmDp1qrHN/fffj8rKStx6660oKyvDqFGjsGjRIoSGhhrbvP/++5g5cybGjh0LVVUxZcoUvPTSS/7oEllVTA8gIgGoKAby1+KMS6KNp9ZyZigiIiIicxcWAHDxxRfj4osvbvZ5RVEwd+5czJ07t9ltunXrhg8++KAjmkedhaK4z1ps+xyoKUdCzR706BaGfaVHsSG/DDVOF0LsTZ/GJCIiIuoMTH0pFLl5ZjFqzTVu5K1dMkxtMCYnb5VxP4tap4ZN+Y5mvihw8DiUYX5yzFCG+ckxQzlmKGP2/FhYWICqqoiLizvhwGQ6sXbJ0KuwWIMzvMZZBP7lUDwOZZifHDOUYX5yzFCOGcqYPT9ztoq8aJqGvLy8RlOOUcu1S4ZJpwG2YPdy3ioM87oDd+AP4OZxKMP85JihDPOTY4ZyzFDG7PmxsLAAz421zDoDgBW0S4b2ECBpsHu5NAcZYVXoFu4uNNbuKYWmBfb+4XEow/zkmKEM85NjhnLMUMbs+bGwIGqNBtPOKvlrMKyn+3Ko8monsg9U+KtVRERERH7HwoKoNY67n8XwtPrLoVZ3gsuhiIiIiJrDwsICVFVFYmKiaQfqWEG7ZZjSoLDY5z3OYm2AFxY8DmWYnxwzlGF+csxQjhnKmD0/c7aKvCiKgpiYGNNOLWYF7ZZhVBLQtZd7ef86DIgPRpcg9/0r1uQGdmHB41CG+ckxQxnmJ8cM5ZihjNnzY2FhAZqmYffu3aadAcAK2jXDnme7/++qQVDhrxjSIwYAUOCoxv6yKvnrmxSPQxnmJ8cMZZifHDOUY4YyZs+PhYUF6LqO2tpa084AYAXtmmHPs+qX967oNJdD8TiUYX5yzFCG+ckxQzlmKGP2/FhYELWW54wFAOz9CcMbFBarA/xyKCIiIqLmsLAgaq2uvYDIZPdy3moMPiUcNtV9rePaTnAHbiIiIqKmsLCwAFVVkZKSYtoZAKygXTNUlPrLoeqOIqJ0C/onRQEAdhQfgeNonfw9TIjHoQzzk2OGMsxPjhnKMUMZs+dnzlaRF0VREBERYdoZAKyg3TPs1eByqD0/4YyG4yz2BublUDwOZZifHDOUYX5yzFCOGcqYPT8WFhbgcrmwc+dOuFwufzfFsto9Q69xFitwRq+uxsM1AXo5FI9DGeYnxwxlmJ8cM5RjhjJmz4+FhUWYdVoxK2nXDOOygLA49/K+XzCsR7Tx1JoAnhmKx6EM85NjhjLMT44ZyjFDGTPnx8KCqC0ajrOocaD70WykxYUDAH7LL0N1nTn/kkBERETUUVhYELXVcZdDDevpvhyqzqVjQ16Zf9pERERE5CcsLCxAVVWkpaWZdgYAK+iQDI8bwD0iPdZ4uGp34F0OxeNQhvnJMUMZ5ifHDOWYoYzZ8zNnq6gRu93u7yZYXrtnGN8fCD02tmLvCoxoMID7l92H2ve9TILHoQzzk2OGMsxPjhnKMUMZM+fHwsICNE1Ddna2qQfrmF2HZKjagB7HxllUlSLVlYeUrl0AAOv3HQ64cRY8DmWYnxwzlGF+csxQjhnKmD0/FhZEEp4B3ACw9yeceexyqBqnho0cZ0FERESdCAsLIonjBnCf2WCcxS8BOM6CiIiIqDksLIgkkk4DgtzTzHaWcRZERERETWFhYQGqqiIzM9O0MwBYQYdlaLMDPUa4l48UIhVFATvOgsehDPOTY4YyzE+OGcoxQxmz52fOVlEjTqfT302wvA7LsJnLoQJxnAWPQxnmJ8cMZZifHDOUY4YyZs6PhYUFaJqG3Nxc084AYAUdmuEJxlmsDKDLoXgcyjA/OWYow/zkmKEcM5Qxe34sLIikTjkdsIe6l/f+hDPTuxlPcZwFERERdRYsLIik7CFAyhnu5bJ9SFEOIbWbZ5xFWUCNsyAiIiJqDgsLizDrIB0r6dAMve5nsQJnprkvh6p1atgQQOMseBzKMD85ZijD/OSYoRwzlDFzfuZtGRlsNhuysrJgs9n83RTL6vAMvcZZ/Ow1zmLFrpKOeU8f43Eow/zkmKEM85NjhnLMUMbs+bGwsABd11FRUQFd1/3dFMvq8AxTzgBUu3t57wqc3TvOeOrHACkseBzKMD85ZijD/OSYoRwzlDF7fiwsLEDTNOTn55t2BgAr6PAMg8OA5NPdy4eykag6kBkfAQDYmFcGx9G6jnlfH+JxKMP85JihDPOTY4ZyzFDG7PmxsCBqL70aXA6150eck9kdAKDpwMrdgXHWgoiIiKg5LCyI2kvaufXLu5fhnKz6y6GWZ7OwICIiosDGwsICFEVBcHAwFEXxd1MsyycZ9hhZfz+LnO8xoldXBNvcH7HlOw+a9nrIluJxKMP85JihDPOTY4ZyzFDG7PkputV/2/GB8vJyREdHw+FwICoqyt/NITN7dzKw+3v38sx1+P2CEqzIcd8k7/t7x6BXXLj/2kZERETUSq35PZhnLCxA13WUlZVZ/i/e/uSzDNPPq1/evcwYZwEAP2Yf7Nj37mA8DmWYnxwzlGF+csxQjhnKmD0/FhYWoGkaioqKTDsDgBX4LMP0MfXLu7/HOZmBM86Cx6EM85NjhjLMT44ZyjFDGbPnx8KCqD0lDgK6dHMv5/6I/glhiA0PBgCszDmEOpc5vxEQERERSbGwIGpPqgqkH5sdqsYBtWgjRh07a1FR48SGvDL/tY2IiIioA7GwsABFURAeHm7aGQCswKcZNrwcKsd7nMXyndYdZ8HjUIb5yTFDGeYnxwzlmKGM2fNjYWEBqqoiNTUVqsrd1VY+zdBrALf3OIsfLFxY8DiUYX5yzFCG+ckxQzlmKGP2/MzZKvKiaRpKSkpMO1DHCnyaYdeeQNc093LeKiSEONEvyT0922/5Dhw4Ut3xbegAPA5lmJ8cM5RhfnLMUI4Zypg9PxYWFqDrOkpKSkw7tZgV+DzDjGNnLbQ6YO8KnN+3/nKo73dY86wFj0MZ5ifHDGWYnxwzlGOGMmbPj4UFUUdoOM5i1zc4v2+88XDZ9gO+bw8RERFRB2NhQdQR0s8D1CD38o6vMTglBl3D3I9/zC5BrdOcpzCJiIiI2oqFhQUoioLo6GjTzgBgBT7PMDQK6DXKvezYB1vJNpyb5b4cqqLGibV7Sn3TjnbE41CG+ckxQxnmJ8cM5ZihjNnzY2FhAaqqIikpybQzAFiBXzLsM7F+ecdXOK/h5VA7rHc5FI9DGeYnxwxlmJ8cM5RjhjJmz8+crSIvmqahsLDQtDMAWIFfMsyaUL+8YxHOzeoO9dgfGL6z4DgLHocyzE+OGcowPzlmKMcMZcyeHwsLC9B1HQ6Hw7QzAFiBXzLs2hOIH+Be3r8WMa7DGNqzKwAg52Al9h066ru2tAMehzLMT44ZyjA/OWYoxwxlzJ4fCwuijtTwcqjsxV6XQ323vdgPDSIiIiLqGCwsiDqS1ziLRV7Tzn5n0ftZEBERETWFhYUFKIqCuLg4084AYAV+yzD5dCD8WDGR8x36dLMjOToUAPBLziFU1Dh92x4BHocyzE+OGcowPzlmKMcMZcyen6UKi7/+9a9QFAWzZs0y1lVXV2PGjBmIjY1FREQEpkyZguJi70tM9u3bh4suughhYWGIj4/HfffdB6fTOr/QqaqKuLg4084AYAV+y1BVgazx7mVnFZQ9P2JsvwQAQK1Lw/Kd1jlrweNQhvnJMUMZ5ifHDOWYoYzZ8zNnq5qwZs0avPHGGxg0aJDX+rvuuguff/45Pv74Y/zwww8oKCjA5Zdfbjzvcrlw0UUXoba2FitWrMA777yD+fPnY/bs2b7uQptpmoa8vDzTzgBgBX7N8LhpZy/on2A8/GardcZZ8DiUYX5yzFCG+ckxQzlmKGP2/CxRWFRUVGDq1Kn45z//ia5duxrrHQ4H/vWvf+H555/H+eefj6FDh2LevHlYsWIFfvnlFwDAkiVLsHXrVvz73//G4MGDMXHiRDz22GN49dVXUVtb668utYqu66isrDTtDABW4NcM08cAdvflT9i5GGemdUNEiB0A8N2OA3C6zPnN4Xg8DmWYnxwzlGF+csxQjhnKmD0/SxQWM2bMwEUXXYRx48Z5rV+3bh3q6uq81vft2xc9evTAypUrAQArV67EwIEDkZBQ/1fi8ePHo7y8HFu2bPFNB6hzCw4H0s51Lx8pRPDB33BuH/dduMuO1mHt3sN+bBwRERFR+7D7uwEn8+GHH2L9+vVYs2ZNo+eKiooQHByMmJgYr/UJCQkoKioytmlYVHie9zzXlJqaGtTU1BiPy8vLAbgvq3K5XADcg2dUVYWmaV5VY3PrVVWFoijNrve8bsP1gPuUl8vlMv7fcH1DNpsNuq57rfe0pbn1LW17R/SpJevbu0+eDP3RJyVzPNTsxQAAfftXGNd3Gr78rRAAsGRLEc5MjzX9ftJ1HbquN9qex17L2u75HGuaBpvNFhB9Otn69u5Tw++FgdInX+4nz9c21Rar9snX+8lzDAIImD55+Go/Hf87TSD0yZf7CUCjn8Ud3afWnB0xdWGRl5eHP//5z1i6dClCQ0N99r5PPfUU5syZ02h9Tk4OIiIiAADR0dFISkpCcXExHA6HsU1cXBzi4uKwf/9+VFZWGusTExMRExODPXv2eF2ClZKSgoiICOTk5HgdDGlpabDb7cjOzoau66itrUVOTg6ysrLgdDqRm5trbKuqKrKyslBZWYn8/HxjfXBwMNLT0+FwOLyKqPDwcKSmpqK0tBQlJSXGel/2qaHMzMwO71NJSYmRoWdGBV/2yW7PRO9jz2nbv0LK2ZdCVQBNB77+bT8eubi/6fdT7969ERsba2TYEfspEI89T588n+OysjJ07949IPrk6/20e/du43Nss9kCok++3E+xsbFITExEYWEhjh6tv0Gnlfvk6/2k6zqcTidUVQ2YPgG+3U8VFRXG5zgpKSkg+uTL/ZSZmYmYmBivn8Ud3aewsDC0lKKb9SItAJ9++ikuu+wy2Gw2Y53L5TIqqsWLF2PcuHE4fPiw11mLnj17YtasWbjrrrswe/ZsLFy4EBs2bDCez83NRXp6OtavX48hQ4Y0et+mzlh4dkxUVBQA81WwgViVB1qf1LfOh1K4AQDguvM3XPffAqzcXQoAWHLXaGTGR1iuT0Dg7Sf2iX1in9gn9ol9Yp/q11dUVCAmJgYOh8P4Pbg5pj5jMXbsWGzatMlr3U033YS+ffvigQceQGpqKoKCgvDtt99iypQpAIAdO3Zg3759GDlyJABg5MiReOKJJ3DgwAHEx7vvJ7B06VJERUWhf//+Tb5vSEgIQkJCGq232WxeRQ5Qv+OP19r1x79uw/WapmHPnj3o1auXUZ02tb2iKK1a315tb0ufWrq+vfoEAHv37kWvXr28tvFpn/pMAo4VFracpbhwwFijsFi6tRhZCZGm3k8Nj8PjX4vH3snb2DC/lmwvaXtz662+nxRFaXQMWr1PvtxPmqZh9+7d6NWrV6tex8x9auv6tvbp+O+DgdCnhnyxn5r6ncbqfWrNemmf2vKzWNp2z35qCVMP3o6MjMSpp57q9S88PByxsbE49dRTER0djenTp+Puu+/GsmXLsG7dOtx0000YOXIkzjzzTADAhRdeiP79++P666/Hxo0bsXjxYjz88MOYMWNGk8WDGXkuoTDxySXTM0WGfSbUL+/4GuP61Y/9WWqBaWdNkaGFMT85ZijD/OSYoRwzlDF7fqYuLFri73//Oy6++GJMmTIFo0ePRmJiIj755BPjeZvNhi+++AI2mw0jR47Eddddh2nTpmHu3Ll+bDV1SomDgKhT3Mu5y5EarqFvYiQAYENeGQodVX5sHBEREZGMqS+Fasr333/v9Tg0NBSvvvoqXn311Wa/pmfPnvjqq686uGVEJ6EoQNYEYO2/AFctsHsZJpzaB9uLjgAAPt9YgFtHZ/i5kURERERtY/kzFp2BqqpISUk54fgBOjHTZOh1F+6vMXnwKcbDT38t8EODWs40GVoU85NjhjLMT44ZyjFDGbPnZ85WkRdFURAREdGqwTPkzTQZ9joHCAp3L+/4GmldQ3BaagwAYGthOXYWH/Ff207CNBlaFPOTY4YyzE+OGcoxQxmz58fCwgJcLhd27tzZaIoyajnTZBgUCmRe4F6uKgVyf8Blg5ONpz/9db+fGnZypsnQopifHDOUYX5yzFCOGcqYPT8WFhZx/DzG1HqmyfDUy+uXt3yCi09Lhk11/+Xhsw0F0DRzzvQAmChDi2J+csxQhvnJMUM5Zihj5vxYWBD5WuaFQLD7Du7Y9jniQhWckxkHANhfVoW1ew/7sXFEREREbcPCgsjXgrrUD+KudgC7l+GyIfWDuBeY+HIoIiIiouawsLAAVVWRlpZm2hkArMB0GQ5ocDnU5k9wQf8EhAW775b55W8FqHGa79pJ02VoMcxPjhnKMD85ZijHDGXMnp85W0WN2O2Wu+WI6Zgqw95jgZBo9/L2LxGmODF+QCIAoLzaiR92HPRj45pnqgwtiPnJMUMZ5ifHDOWYoYyZ82NhYQGapiE7O9vUg3XMznQZ2kOAvhe5l2uPALu+we9Oq58datHmIj81rHmmy9BimJ8cM5RhfnLMUI4Zypg9PxYWRP5y3OxQZ/WORWSI+68QS7cVo9Zpzm8aRERERE1hYUHkL+ljgC5d3cs7FiFEq8HYfvEAgCPVTqzIKfFf24iIiIhaiYUFkb/YgoB+l7iX6yqBnV9jwqlJxtNmvByKiIiIqDmKruvmvRuXSZSXlyM6OhoOhwNRUVE+f39d16FpGlRVNe0t3M3OtBnmLgfeOVZc9JmEqin/xumPLUVVnQvdwoOx+n/Gwm4zR/1v2gwtgvnJMUMZ5ifHDOWYoYw/8mvN78Hm+I2FTsrpdPq7CZZnygx7ng1EHjtLkb0UXZwOnNe3OwCgtLIWq/eU+rFxjZkyQwthfnLMUIb5yTFDOWYoY+b8WFhYgKZpyM3NNe0MAFZg2gxVG3DqFPeyVgds/cy0l0OZNkOLYH5yzFCG+ckxQzlmKGP2/FhYEPnbwCvrlzf9F+f3jUew3f3RXLS5CJrGqxWJiIjI/FhYEPlb0mlAbKZ7ee/PiKguwujMOADAgSM1+DXvsB8bR0RERNQyLCwswqy3brcS02aoKMCgq4490IHN/+d1OdTXm8xzOZRpM7QI5ifHDGWYnxwzlGOGMmbOj7NCtYC/Z4WiTuBQDvDy6e7lxIEom/Ydhj3+DZyajlNiuuCnB87j7BlERETkc5wVKsDouo6KigqwBmw702cYmwGcMtS9XLQJMRW7MTIjFgCwv6wKm/eX+7FxbqbP0OSYnxwzlGF+csxQjhnKmD0/FhYWoGka8vPzTTsDgBVYIsOGg7i3foYJpyYaD7/eXOiHBnmzRIYmxvzkmKEM85NjhnLMUMbs+bGwIDILz124AWD757iwfyI8Vz8t2lxk2r9OEBEREQEsLIjMIzoFSD42zqJoE7o7C3FGr24AgN0lldhZXOHHxhERERGdGAsLC1AUBcHBwRy8K2CZDBuetdj2BSaa6HIoy2RoUsxPjhnKMD85ZijHDGXMnh9nhWoBzgpFPlOSDbwyzL2ceiYKr/gUI5/6DgDQNzESi2aN9mPjiIiIqLPhrFABRtd1lJWV8Rp7ActkGJcJdO/rXs5bhSS1HINTYwAA24uOILek0m9Ns0yGJsX85JihDPOTY4ZyzFDG7PmxsLAATdNQVFRk2hkArMBSGfa9+NiCDuz40jSXQ1kqQxNifnLMUIb5yTFDOWYoY/b8WFgQmU2/i+uXt32BiQ3uwv3FRv9PO0tERETUFBYWRGaTNBiITnUv5y5Hj7BaDEqJBgBsLSzH1gL/3yyPiIiI6HgsLCxAURSEh4ebdgYAK7BUhopSfzmUVgdkL8GVQ1OMp/+7Lt9PzbJQhibE/OSYoQzzk2OGcsxQxuz5cVaoFuCsUORze34G5k9yL/e+AGWXf4DhT3yLWpeGbuHB+OXBsQi28+8CRERE1LE4K1SA0TQNJSUlph2oYwWWy7DHSCC6h3t51zeIqTuACwYkAABKK2uxbMcBnzfJchmaDPOTY4YyzE+OGcoxQxmz58fCwgJ0XUdJSYlppxazAstlqKrAkOuOPdCBDR94XQ718VrfXw5luQxNhvnJMUMZ5ifHDOWYoYzZ82NhQWRWg38P4Ng1lL++h3N6xyIhKgQAsGzHARw8UuO/thEREREdh4UFkVnFpAIZ57uXy/bBtmc5Lj/dfdbCpen4bMN+PzaOiIiIyBsLCwtQFAXR0dGmnQHACiyb4enT6pd/fa/R5VC+PBVq2QxNgvnJMUMZ5ifHDOWYoYzZ82NhYQGqqiIpKQmqyt3VVpbNsM8kICzWvbztc6SH12Joz64AgB3FR7A6t9RnTbFshibB/OSYoQzzk2OGcsxQxuz5mbNV5EXTNBQWFpp2BgArsGyG9mBg0DXuZVct8Nt/cP2ZPY2n5/28x2dNsWyGJsH85JihDPOTY4ZyzFDG7PmxsLAAXdfhcDhMOwOAFVg6w9Ovr1/+5VVM6huD+Ej3IO4lW4uQV3rUJ82wdIYmwPzkmKEM85NjhnLMUMbs+bGwIDK7+H5A+nnu5bJ9CF71inHWQtOBd1fu8V/biIiIiI5hYUFkBRP+Cqh29/JPz+O6Prpx5+0P1+Shssbpx8YRERERsbCwBEVREBcXZ9oZAKzA8hnG9wVG3OZedlaj609zcOngZADAkWonPlnf8TfMs3yGfsb85JihDPOTY4ZyzFDG7Pkpulkv0jKR8vJyREdHw+FwICoqyt/Noc6quhx4ZRhQUQwA2DfxXYxe4D6Lkd49HN/cdS5U1ZzfaIiIiMiaWvN7MM9YWICmacjLyzPtDABWEBAZhkYBFzxmPOyx+jGMTIsBAOw+WIkfsg926NsHRIZ+xPzkmKEM85NjhnLMUMbs+bGwsABd11FZWWnaGQCsIGAyHHQVkDrCvXwoG7P6HDae6uipZwMmQz9hfnLMUIb5yTFDOWYoY/b8WFgQWYmiAGfcbDw848i3SO3WBQCwfOdB7DpQ4a+WERERUSfHwoLIavpMAoLCAADq1gW4acQpxlPzV+T6q1VERETUybGwsABVVZGYmGja27dbQUBlGBLhLi4AoOowronNRniwDQDwf+v2w3G0rkPeNqAy9APmJ8cMZZifHDOUY4YyZs/PnK0iL4qiICYmxrRTi1lBwGU48EpjMWz7J7hiaAoAoKrOhQ/X7OuQtwy4DH2M+ckxQxnmJ8cM5ZihjNnzY2FhAZqmYffu3aadAcAKAi7D3mOBLt3cy9u/wk1ndDeeenflXjhd7d/PgMvQx5ifHDOUYX5yzFCOGcqYPT8WFhag6zpqa2tNOwOAFQRchrYgYMCl7mVnFXod/B7n940HAOwvq8KiLUXt/pYBl6GPMT85ZijD/OSYoRwzlDF7fiwsiKxq4FX1y5v+gz+cnWY8fOnbbGiaOb/pEBERUWBiYUFkVakjgOhU93LOMpydUIfTe8QAAHYWV+DLTYX+axsRERF1OiwsLEBVVaSkpJh2BgArCMgMVbV+ELfugvLT87j7gj7G0y98sxOudjxrEZAZ+hDzk2OGMsxPjhnKMUMZs+dnzlaRF0VREBERYdoZAKwgYDMccZtxTwusnYezY49geC/3oO6cg5VYuHF/u71VwGboI8xPjhnKMD85ZijHDGXMnp+pC4unnnoKZ5xxBiIjIxEfH49LL70UO3bs8NqmuroaM2bMQGxsLCIiIjBlyhQUFxd7bbNv3z5cdNFFCAsLQ3x8PO677z44nU5fdkXE5XJh586dcLlc/m6KZQVshpEJwMgZ7mWtDsqyJ3DXBVnG0y9+k91uM0QFbIY+wvzkmKEM85NjhnLMUMbs+Zm6sPjhhx8wY8YM/PLLL1i6dCnq6upw4YUXorKy0tjmrrvuwueff46PP/4YP/zwAwoKCnD55Zcbz7tcLlx00UWora3FihUr8M4772D+/PmYPXu2P7rUZmadVsxKAjbDs+6sn3p208cYGZaPszJiAQB7Dh3Fgl/b76xFwGboI8xPjhnKMD85ZijHDGXMnJ+pC4tFixbhxhtvxIABA3Daaadh/vz52LdvH9atWwcAcDgc+Ne//oXnn38e559/PoYOHYp58+ZhxYoV+OWXXwAAS5YswdatW/Hvf/8bgwcPxsSJE/HYY4/h1VdfRW1trT+7R9Q+QqOAc++vf/zNHK+zFi99l426DrivBREREVFDdn83oDUcDgcAoFs3919n161bh7q6OowbN87Ypm/fvujRowdWrlyJM888EytXrsTAgQORkJBgbDN+/Hjcfvvt2LJlC4YMGdLofWpqalBTU2M8Li8vB+A+++E59aQoClRVhaZpXnMJN7deVVUoitLs+uNPaXkG5WiaBpfLZfy/4fqGbDYbdF33Wu9pS3PrW9r2juhTS9a3d588GQZSn4z1Q26A+ss/oJTtA3K+xekjN+Kc3rH4cdch5JVW4b/r8nH1sBRRn3Rdh67rjbbnsdeytns+x5qmwWazBUSfTra+vfvU8HthoPTJl/vJ87VNtcWqffL1fvIcgwACpk8evtpPx/9OEwh98uV+AtDoZ3FH96k198ywTGGhaRpmzZqFs88+G6eeeioAoKioCMHBwYiJifHaNiEhAUVFRcY2DYsKz/Oe55ry1FNPYc6cOY3W5+TkICIiAgAQHR2NpKQkFBcXGwUPAMTFxSEuLg779+/3umQrMTERMTEx2LNnj9eZkpSUFERERCAnJ8frYEhLS4Pdbkd2drZxoOTk5CArKwtOpxO5ubnGtqqqIisrC5WVlcjPzzfWBwcHIz09HQ6Hw6uv4eHhSE1NRWlpKUpKSoz1vuxTQ5mZmR3ep5KSEiNDRVECok/H76dTzvgzIpfeAwCo/epBTBn8Gn7c5X7ule92YUjXWsBVP7aotX3q3bs3TjnlFCNDX/QpkPaT53NcVlaG7t27B0SffL2fPHebzcnJgc1mC4g++XI/xcbGIi0tDYWFhTh69GhA9MnX+8nzC5aqqgHTJ09/fLWfKioqjM9xUlJSQPTJl/spMzMTCQkJXj+LO7pPYWFhaClFN+ut+45z++234+uvv8ZPP/2ElJQUAMAHH3yAm266yevsAgAMHz4c5513Hp5++mnceuut2Lt3LxYvXmw8f/ToUYSHh+Orr77CxIkTG71XU2csPDsmKioKgG8rWM8vJKqqwmazGesbCsSqvD375Dnb5GlbIPSp0XroUN88FyjeDABwTXkbN69NxbIdBwEAc3/XH1NH9Ghznzy5eJZ90qcA2k+er7PZbDxjITxj4fn6QOiTL/dTc6zcJ1/vJ097g4KCGm1v1T55+Go/ef55fqcJhD75cj95fqfxtMEXfaqoqEBMTAwcDofxe3BzLHHGYubMmfjiiy+wfPlyo6gA3FVhbW0tysrKvM5aFBcXIzEx0dhm9erVXq/nmTXKs83xQkJCEBIS0mi95xeChjw7/nitXX/86zZc73K5sHv3bmRmZhoHUVPbe37QtnR9e7W9LX1q6fr26hMAI8OGX2flPjW5ftyjwPtXuN972RO4+9IlRmHxj+9346ozeiA0yPs9Wtp2l8uFXbt2Ncqww/vUhvVm3E8NP8ct2V7S9ubWW30/KYrS6HNs9T75cj+5XC5kZ2c3+Rk+0euYuU9tXd/WPjX8HDf1OwFgvT415Iv9pOt6o99prN6n1qyX9qktP4ulbW/4x8STMfXgbV3XMXPmTCxYsADfffcd0tLSvJ4fOnQogoKC8O233xrrduzYgX379mHkyJEAgJEjR2LTpk04cOCAsc3SpUsRFRWF/v37+6YjRL7SexzQc5R7uTQHA4s/wwX9j136V16N/129z4+NIyIiokBm6sJixowZ+Pe//40PPvgAkZGRKCoqQlFREaqqqgC4rxGbPn067r77bixbtgzr1q3DTTfdhJEjR+LMM88EAFx44YXo378/rr/+emzcuBGLFy/Gww8/jBkzZjR5VoLI0hQFuKDB+KAfnsZd555iPHzlu10or67zQ8OIiIgo0Jm6sHjttdfgcDgwZswYJCUlGf8++ugjY5u///3vuPjiizFlyhSMHj0aiYmJ+OSTT4znbTYbvvjiC9hsNowcORLXXXcdpk2bhrlz5/qjS0QdL2UY0O8S93JFMfrvfR8XDUoCAByqrMVL32Sf4IuJiIiI2sYyg7f9qby8HNHR0S0atNIRGg7ebs11blSv02VYkg28OgLQXUBoDPbftAbnv7wONU4NdlXBolmj0Ts+olUv2ekybGfMT44ZyjA/OWYoxwxl/JFfa34PNvUZC6rndDpPvhGdUKfKMC4TGHSVe7m6DKfs/Df+eG4GAMCp6Xjsi62tmpfao1Nl2AGYnxwzlGF+csxQjhnKmDk/FhYWoGkacnNzG005Ri3XKTM8515AOfYRX/Eybh+ZgOToUADADzsP4rvtB07wxY11ygzbEfOTY4YyzE+OGcoxQxmz58fCgihQxfUGTnVPPYuqUnTZOB8PTupnPP3YF1tRXedq5ouJiIiIWoeFBVEgG30vgGPXYP78Ei7uF43had0AAHsOHcUr3+3yX9uIiIgooLCwsIgT3fiNWqZTZti9DzDgMvfy0RIoa+fh8UtPRZDNXWy8/kMOthWWt/jlOmWG7Yj5yTFDGeYnxwzlmKGMmfPjrFAt4O9ZoYhEircCr7lvGInQaOCPy/H82lq89K172tnTUqLxyZ/Ohk3l7BxERETkjbNCBRhd11FRUdGmWXzIrVNnmNAfGHS1e7naAXx8E2aMTkVG93AAwMZ8B+av2HPSl+nUGbYD5ifHDGWYnxwzlGOGMmbPj4WFBWiahvz8fNPOAGAFnT7DSc8CXdPcywXrEfLdo/jrlEHG088t3oH8w0dP+BKdPkMh5ifHDGWYnxwzlGOGMmbPj4UFUWcQGg1cOR+whbgfr3odZxz9Eded2QMAUFXnwl+/3u6/9hEREZHlsbAg6iySBwMTnqp//NlMPHBWJGLDgwEAX/xWiDV7Sv3TNiIiIrI8FhYWoCgKgoODfXbr9kDEDI8Z9gfg1Cnu5ZpyRC6+G3dfkGk8PffzrdC0pq/bZIYyzE+OGcowPzlmKMcMZcyeH2eFagHOCkUBpaoM+MeZwJFCAIB2yUuY9GMathcdAQA8d+VpuGJoih8bSERERGbBWaECjK7rKCsrM+0MAFbADBvoEgNc8pLxUF3yMB4/L8Z4/Myi7aiscTb6MmYow/zkmKEM85NjhnLMUMbs+bGwsABN01BUVGTaGQCsgBkeJ+tCYPBU93JNOYb99igu7BcPADhwpAYvfLOz0ZcwQxnmJ8cMZZifHDOUY4YyZs+PhQVRZzX+SSAyyb2c8y2eSN+MYJv7W8JbP+VyIDcRERG1CgsLos6qSwxwyYvGw+4r5uKhMd0BALoO3POfjU1eEkVERETUFBYWFqAoCsLDw007A4AVMMNmZI0HBlzuXq4qxfVH3sSwnl0BAPtKj+Kpr7cZmzJDGeYnxwxlmJ8cM5RjhjJmz4+zQrUAZ4WigHakGHj1DKDaAQAomvwhzvtEQVWdCwDwzh+G49ys7v5sIf3/9u48Tory3vf456nee/aF2Rh2RkVZFJcJMTHnKBEIN3EhcQk3olkMisbEJMfoiVuSG73xXs2Jx2Bujtu9etSQlxpPYkwQRaMgIoig4gg4MMBsDLP39Fr13D9qpqGdAQZKprvh9369+jUz1dU9z/Ptp7rr19X1tBBCCJEmMivUMcayLNra2jL2RJ1sIBkeRF45fPFnyT8rXruZW+eMT/79wz+8S2t3RDJ0SPJzTjJ0RvJzTjJ0TjJ0JtPzk8IiC2itaWtry9ipxbKBZHgIp10BY2fZv3fUc3n3w5zTf5SirTfKkv9cTyxhSoYOyBh0TjJ0RvJzTjJ0TjJ0JtPzk8JCCAGGYZ/I7fICoN76Hb+dUU9lgR+Atds7uOfvg6egFUIIIYQYIIWFEMI26kSYe3fyz9wXf8BD8/PxuOwTxB56fTuv7+hNV+uEEEIIkeGksMgCSikKCgoydgaAbCAZDtMZ34QZl9u/x0Oc/Nq13DFnXPLqe9/YQ/3evjQ1LrvJGHROMnRG8nNOMnROMnQm0/OTWaGGQWaFEseVWB/8x2xofR8APf7z/MT9I55+LwTASRV5PHvt2QS8rnS2UgghhBAjQGaFOsZYlkVTU1PGzgCQDSTDw+ANwqX/D3z2k4fa/g/uavseXyzeA8CHzT3867ObMvbEsUwlY9A5ydAZyc85ydA5ydCZTM9PCossoLWmq6tLduQckAwPU8kkWPhHyLFnhjI6d/C72M3M97wNwDPv7OY/32pIZwuzjoxB5yRDZyQ/5yRD5yRDZzI9PykshBBDG1sLV6+EylMBMBJ9/Jv7fiaoJgDufP4DXvmwNX3tE0IIIURGkcJCCHFgBdXwzRfhlIsAcOs4D5U8AWhipsV3/986VmxuSW8bhRBCCJERpLDIAkopSktLM3YGgGwgGTrgCcAFD6ALxwIwsXc9Px+3EYCYabH48XX8/f3mdLYwK8gYdE4ydEbyc04ydE4ydCbT85NZoYZBZoUSAtjyEjyxAAAdKOKWqkd48n176lm3ofjFhVO57Kyx6WyhEEIIIT5lMivUMcayLHbu3JmxMwBkA8nQOWvSuYQmfgkAFe7gfwSe4MJTqwBIWJqfPLOJf312E7GEZDwUGYPOSYbOSH7OSYbOSYbOZHp+UlhkAa01oVAoY2cAyAaSoXNaaxqnX4f2FwBgvLeMe/2/59ufqUyu88SaBi77P6tp7Y6kq5kZS8agc5KhM5Kfc5Khc5KhM5menxQWQohhM/0l6PPvSv5tbHiCnzZ/j6VfKsLrtp9O1jd08tUHV7OzXb6hWwghhDieSGEhhDgsesZlcPHvwRO0FzRvYt4bl/H3eb1UFfgBaGjvY8HSVXzU0pPGlgohhBBiJElhkQUMw6CiogLDkIfrSEmGzqVkOP0S+PYKKJlsXxntYvzyb/P36Ss5YVQAgNaeKJf8bjXvNHSksdWZQ8agc5KhM5Kfc5Khc5KhM5men8wKNQwyK5QQBxDphj8tgc3PJxfFx36Ob/Ys5h9N9pOe12Xwr/OncMWscRk7PZ4QQgghhiazQh1jLMvi448/ztgZALKBZOjckBn68+GS/wtzfgnKBYCn4XUei/6AxVXbAPu7Lm5//n2ueXw9XeF4OpqeEWQMOicZOiP5OScZOicZOpPp+UlhkQW01sRisYydASAbSIbOHTBDpWDWErjyz5BbAYDRt4eftN/K02Ofw0cMgBffb2bur1/jxfeaj8vHQcagc5KhM5Kfc5Khc5KhM5menxQWQohPx7jPwuLXoWZOclFt6x94p+RWLvG/hcKiqSvC4sfX8a3H3pZZo4QQQohjjBQWQohPT+4o+PrTMO8ecPkACIZ28it+zYr8n3GW2gzAyx+28sX7XuWBV7bKF+oJIYQQxwg5eXsY0n3y9sCXoeTk5MjJr0dIMnTusDNs+QBe/AnUv5qy+I/GHG7ru5Q+7KlpJ5fl8vMLpjJrUsnRaHbGkDHonGTojOTnnGTonGToTDryO5z9YCkshiHdhYUQWUtr2LYClt8OLe8lF3d4K7m+9ypet6Yml1182mhumT+F0lxfOloqhBBCiCHIrFDHGNM0+eijjzBNM91NyVqSoXNHlKFSMHk2fPc1++NR/V+qVxRr4nHvL1meewcXG6/hI8Yz7+zm3P+1ksff3IFpHXvvd8gYdE4ydEbyc04ydE4ydCbT85PCIktk6rRi2UQydO6IMzRcUHs1XPMGjDs7ubgm8RH3eh9kjf86/rfnt5wT+we/eu5NPv8/X+a+5R+xq+PYOsFbxqBzkqEzkp9zkqFzkqEzmZyfO90NEEIcR4onwqI/w8an4M3fQvMmAArpZYHrdRa4XieuXTwcmss9Ky7lNy9v4fM1o7jszDHMnlKO1y3vhQghhBCZSgoLIcTIMgw49esw43LY+Ras/T3U/RVivQB4lMl33X/hdGML18Wu57WP4LWP9lCS4+XimaO59MyxTC7LTXMnhBBCCPFJcvL2MKT75O2BL0Pxer0yg8IRkgydO6oZJqLQsNouMNY+BJb9Dd2d5HNv/CLWWzXU6bHE+98LOXN8ERedVs2pYwqpKc/F48r8IxkyBp2TDJ2R/JyTDJ2TDJ1JR34yK9SnLBMKC8uyMAxDNsIjJBk6N2IZ7loHyxZB186UxVHtYbMey7vWRDZak3hbn8AOXYHXZTClMo/50yu5eGZ1xs4qJWPQOcnQGcnPOcnQOcnQmXTkJ4XFpyzdhYVpmmzZsoWamhpcLteI//9jgWTo3Ihm2NcOz34Xtvz9oKutNk/mMfN8llunY+LCbShmTynnMxMKOMXVwMRoHYWjKnFNOAeCxUe3zYcgY9A5ydAZyc85ydA5ydCZdOR3OPvBco6FECLzBIvh63+A3evs8zAa10PjO7B3a8pqs1wfMMv1AXt0Po26lB4dwPrI4NSt28hX+2aUslA0Bk+iq+oLeGdeyrgTTpUTwYUQQohPmRQWQojMpBRUn2FfBoQ7oeld2LUW3n0K9m4BYJTqZpTqPuBdGWiq+zZTvXUzbH2Qd61JvJdTy+hcxWhfmKKAm8BpXyPn5PMP3B7Lstskh+6FECK79LRAzih78hBxVElhIYTIHoFCmPgF+/K5G6F+Jbz1e9jxBkS6AfuTnRFvMdtzT2WjOgm6djEt9g5TjH3nbMwwtjEjvA3C+933lj/wuprJH4oXU5Xv5rOx1ZzUs4r8aAsesw9Xog/ty0ed/BV7Rquxn5UXKSGEyGRawws/tmcfrDkfLnsSXLLrezTJORbDkO5zLOREJ+ckQ+cyPkPLsqesTUTsd6b2a2NXOM6WrVtIbFxGdcPzVEe3HuSOhqfTVUzYyEEBWhm0BWvYW1ZLrPqz5HuhMLSN/J5toAy6iqbSnn8KCX8hNd4OKqIfY3Rsh1gI4n12m335kFsGueXgywOXt//itn8aHvDmQH7V0TtqEumCTX+EnmaomAqjT4f80RlzlCbjx2CGk/ycO+wMI92w7hHIrYBpX5M3Ixjhcbj2P+AvP9z39+d/BOfdenT/51EmJ28fAzKhsJCp2ZyRDJ07pjJs3Uy0pY7tIQ913R7CO99l9u4HKbHaBq3aqIvp1QH68DNJNZKnwkPc4fBEtRufSjhpOVFPId3F04iWzSBQPJqCohLcwUK7mCqoxgyUYCVieDrroa3OLhaKxkNJDeRVQOcO2FMHe7eB22ffzl8AdS/Ahv9Mfp9IUm65XWCMngkV0+2Po3XUQ2cDeAJQOBYKx9nreYPgyQFfLgRLwOXZdz9aQzwMPU39l2a7mBpzJgSK7HXCnbD9dejYDqNOsj8GFyjc7y72G4MDCz+NsWiZEO0BTxDc3uHdJhG125tbNrw2dO2GDU9AbyvMvAIqpztq8pE4prbhNDmsDJs2wrIroX2b/fcJc+GiB/eN9+PUiI3DhjXw6Pzk9OU2Bd94Biade/T+71Em081mkAceeIB77rmH5uZmZsyYwf33389ZZ511yNulu7CQGRSckwydO+YzjPXBqvux1j9GNG8czVVf5KOic2hIFNPSHaG5O0JHVyeT2//BudEVTFfbMLAA8BPHp+KH+AcjI6rduLBwK2vQdRYKg5F7yo97C7F8BRiJEK5oN4YVG3K9RMmJ4AniankXpVPbrUtPAF8BGo3WEOttx2/2QqQTDDeqcBwUjbOP5BgeMFyAso8CxcMQD0G4w55prG+vXeB4c+yL1uhQK4T27Pu/Lq99XU6ZXYjlVdqTCXhz7YKpp9k+x6fpXTBjdiFVOhlKJkOw1C7S/AXg8YPLZxcddX+1L9rc17FJ58GsayGvCrQFVsJuX28rhFrtPnhz7KNXiSh0N0JPoz1OiydA6Qn2N9m7vIC2+zXwU1t2Pt1N/bcJgS8fy1dAU0eIijETcfly7MLQEwB3wG6vGbcLy2gv9DbbBV7Hdju/QJFdLHpzoH27XbS2f2z3uXKGfSmtsdcJFNntHmjLwIX+vw2PXdQabvu+u3fbhVeo1X6cwu1gJqBkol1gFk3o70+jXZRqDW6/XQS6/fZ9uXz2z4G/DZfdj2iP3X//EEcEjf7CNxGGeMRux8419oQRezaDvxAKxkDBaPt+sd8p3tMVpnTyabiKxtmZNayCHavtPEadANVn2Y/7ip+DGU0d7IVj4b/dB8qwx1Joj92+WMgeT8WToOpUqJhmZ53cmML92XTYY9vt3/f4DTyGhgtaN9sTXTRvtMdQ0Xj7Eiy27yMRsQvpYLH92Ll9dp8/ftX+6Qnaj2PpCft+lky23zSwN0A7V8NtH001PPYbCIZn39EYy7RzMWP2uDbj9s69aV/MeJSd9VsYU1WGy4rZ9+UJ7tefoN0/ZdhvjES67Hw8AXsb9ObZ48iM2RdlpD7u2rKzeuRL9jgGKJ8GLZvs34OlcM0b9vY90N7u3dBeb/evaIK9bXn8+/K3TOjaZb+p0l5vb5Muz77ni7xKyK+0x5fbv+/NhnjYvl1ng91Of749rpSyx2a0x35Me5qht8U+wlUyyd6eyqfa/f2ETJ8V6rgpLJ5++mmuuOIKHnzwQWpra/n1r3/NsmXLqKuro6ys7KC3lcIi+0mGzkmG+5iWpiscJ2FZWBbE4zF6t7+Nsf0fBFrWE8FHi38CjZ5xuHSMseE6qvo+wBPey27PWN6Lj2ZDeBTtVpAIPmLaTb7qY5TqZBRdBFQUDwk8mHhI4CaBV5mU0MU0o57Sg5yo7kSf9vGM+TlWWacwxWhghtrGDGMbBfvNsCWEODy9hSfhC7fgiXYM+zZaGXax1F90qv2L0nTwF9o7wQdrhzL2FbgZxBz7OXbOf4LyF64isONlAHSgGOXNsYudvr2fOKoBoOxiwUrYH1eNhRh2v5RhF0eG2y6Ij5iyiyUGJg1R8NWHMCefL4VFJqitreXMM8/k3//93wGwLIsxY8Zw/fXX85Of/OSgt5XCIvtJhs5Jhs4MlV/CtIgm7EssYRFNmETiFj2ROJ19cTr6YkQTFl63gc9tf562JxxDd+zE1/4Bke69RHo7sMKdjKKTKrWXCvZiKRcNRjUfU027FaQ80cQYvZtS3cEuPYotejQfW5W4lUkJ3RSrHnbrUp41z6ab1HfIFBbjVQsz1DZqjF206zx26jIadDkBolSrPYxReyhUvQSJEFBR8glTrLoppYsCFaKXAN06hy6dwx4KaNbFtOhCqlQ7Zxh1nKK241YWH1pjeMOaSp2uZopqYKaxhZPVDjxq385MRHvoII9OnYuXONWqbVhHi6LaTQd5JHCRQ4QgERSaNgrYowvp0jn4VYwcIuTRxyjVRUANfYQFYJtVyS49irGqhbGqFZc6+Etpiy7kKfNc2nUe33a9wBhjzyHbnOmadRHFdONVad7pPQpMrQ75mH5SQhuDjhQ+mjifXyYWUkI3v/X+G6cZzs/vOlqi2oOBlbK9ZbtmSpgf+QV7KaCIbl7w3UKlak93s47Yji/+nurPLMjowuK4ODU+Fouxbt06br755uQywzCYPXs2q1evHrR+NBolGt13+LK723530DRNTNPe4JRSGIaBZVnsX5sdaPnASTYHWj5wv/svB7sAGrjONM2U5ftzuVzJE3o+2ZYDLR9u249Gn4az/NPu00CGx1KfRvJx0loPuX4292kkH6eBNlmWhcvlwrIsFBq/W+F3uzAMz2H0aQyG8bmD9mmcZfH5T/TJNC3GxxKcHjcJx0wiCYtoQhOKJThNaS73e8jzufG6DWIWhKNxwjGTcNxeNxI3CcctJoTj9EYTGAr8Hhd+jwvDMIjEEnTETZr7142amljCQikwFBhKEYmZ9MZMeqMJ6k2LVQp8OoKyEnRYQfpiJtG4yfuG4lmlcBng6v9pYBGKxPD6/FgaLA3aNCkw95JvdoA27Xd2tUVYe+nrv3RaOfRqHxYK09KYlsbSGo+hKM/3U17gJ8/nprUnSlNXmNaeKG5DUeaJUuXuImj24jJDeOIherWPd62JdJKXzNan4kzy2B/RytW95NOHX8XwkMBLgp16FK9Z00n0v+Q+bs7mS8YaZhnvY6CxR4JBO3m06QLadAEayFERcoiQwEWzLqZJFxPFwwTVzMneZmo87YRjceKmRqOw3ytWaBS9BGjSxbToInoJkEeYfBUinz58xAioGH5iBIjiV3H8REngIqQD9OKnQ+fRoMto0GW063wKVIgi1UMefezWpXysq+jDj4cENWoXpxjbqaSdItVDkeohhygWKnnR/R/C0yjcmHiJ4yVBDwGadAlNuoQWXUQHuXRqO9uJqpHJxm6q1R46dR5NuphmXUwCFz5ieFUCH3F8xPCRwKfi+IjjJY4bkxB+enSAMD7y6T8iqLoIEMWrEniwz3eKaC8RvPQQYKM1iXVWDR/o8fiJUan2UqX24iHR3w9FASGq1F5GqzY8JHhH1/CWdSL1upLJajczjS2cqHbymjWdl62ZADRRwiWx2/jvruWcrHawh0KadRFtuoBeAvRpHxrFScZOpqp6TjZ24COOiYGJQRgfnTqXDp1LGN8nHsMYARXFR5wGXcZGayKbrInEcTFGtTJOtZKrwoTxEtVeNIpC1UOp6iaXMB/qMay2TmG9VYOFYpxqYZJqZJJqYpLRyCTVSBE9dBOkWwcJEcBA4yZhP5bK/unGREP/MVYXMW3/TOAijiu5PK5dRPASxUMUD24sexz29yPQPy4NLLrIoVvn0IefANH+bSKMhUGs//4UJMeBCwurP7NeAtyfuJC9FADQQT6LY9/nXs9SilQPCdzEcdGtgzTocnbockLaz3ijmQmqidFqL1E8hLWPPnw062J26HIadBndOmgfUVYJ8glRoTooUx2MUl3J7cpHnD0UsluXsluXYqHIo4981YeBplcH6CFAjw7SSiF7dCEh7ecEYxenqHqmGDvxE+vfakChaQ+7qO5/fdn/NeFov+YezjGI4+KIRWNjI6NHj2bVqlXMmjUrufxf/uVfePXVV1mzZk3K+nfccQd33nnnoPtZu3Ytubn2u3kFBQVUVlbS1NREV1dXcp3S0lJKS0vZuXMnoVAoubyiooLCwkI+/vhjYrF974JVV1eTm5vLRx99lDIYJkyYgNvtZsuWLSltqKmpIZFIUF9fn1xmGAYnnHACvb297Nq1K7nc6/UyceJEOjs7aW5uTi7PyclhzJgxtLW10da272RV6ZP0SfokfZI+HbhPWmtKRpXjDeayeWs9kWiMXK9B0GMwbuwYcnJyeG9zHaFoApTCpWDi+PH4vB7qP96Gy4CEBb1Rk+LKMXSFojQ2Nib/p9/j4uQTJmLFImzf1Uh3xKQ7amIpF2NGVxINhwn3dFKV56bA7yI3N5fq6mrer29k4/YWogn75dwfCFJUVEh7ewe9fX32qQ1ATm4ueXl5tLXtJRyN9i/X5OUXEAwEad2zh3giYZ8agaagsAif10tLSyum1smdi6KiYpRhsGdPG1b/nWuguKQE07TY296BTi5XlJaW4DMsEn3dBDwGoZhFdwxMTw7doTB9fX1YQMBtML40h1Mnj4ZoiLpdbezuidPam8Dt9hAIBgn39aGsBH6PQcCtCASDaJeX5r1dhCIxu3jR4Pf7cXs89Pb2YloWA3s6/kCAoM9LPNyL361QCiJxjcsfJGZqQr2pkxfk5uaitU4Z1xoIBHOJxeNEIhE8LoXbUHhcBr5gDr19EXr6IgQ9BoV+FyV5fnLz8mhu76G1K0Tc1PjdBvlBH6OK8sl3xSl0JygOumkLJWhPeGjtg6b2LrrDcUIxi7ilcbncKMMgGo1had1/sd80UMogGo/jMcDnNvC5FH6/j4QFoXAE09IkLI1pgeFyYSiFZSZwGYpROW6qC7zMmFiFR5l0dNgf2eoImzT1mrRGFK1dYRKmmRwDShkYLgPTtDAHtmENfGLH1G2Ay1B43C5QBrF4grhpYVkas398TCrL5cxKL7Wj/Vhas7K+lzd2RtjZEeFwKfad3uAyFJUFAcpyPeS47J3wuKmJWRAyXezu6KM74mwyjQGGAo+hQJHcDofdZgWVBX5G+aEo4GZPKMHu7jidkcFHjn7+xSq+cd5pI/5cHgwGGTdunHwUasDhFhZDHbEYM2YM7e3tyUBH8h1WrTV9fX0Eg8HkYa9j6Z3wkXjX2DRNQqEQwWAQpdQx0aeRfpyUUoRCIQKBQMpMFNncp5F8nAa245ycnOQRi2zv06GWf9p9Mk0z+VyolDom+jSSjxNAOBwmEAgMaku29mmkH6eB7TgvL2/Q+tnapwEj9ThZlpWyT3OgPkUTFpZpglLJgsEwDHuKb61Ti4gj3DfqDsfoiyb6z55QuFx2UeZ1G3hdCq1JFkCxhF0UJUyNYdjr+D1uXP1HZAdoFKGoSXe4/2hD/5sMLpcLl6FAWxhKYRgKQ4HP48bjMga1vS9u0RdN0BdLEInbH5MdXxKkMMdHb29vymvx0R57vb29FBYWykehBpSWluJyuWhpaUlZ3tLSQkVFxaD1fT4fPp9v0HKXyzXo82wDG+gnHe7yA31Ozv74gkljYyM1NTXJQTTU+gMvtMNd/mm1/Uj6NNzln1afgGSG+98um/s00o+TaZrs3r17yM91ZmufDrb80+7T/tvxcNZ30vYDLc/2x0kpNWg7zvY+jeTjZJomu3btOuBns7OxT0e6/Ej7tP92PNQ+AWRfn/Y3Eo+T1nrQPs1Q6/s9LvAc3jkEh9ungqCPguDg/b39DUyaHfAOvy0FQYOCoOfQK+7nk23Mc7nI8w++jyN5LXY6xvZ/M/FQjotvavF6vZx++umsWLEiucyyLFasWJFyBEMIIYQQQghxZI6LIxYAN954I4sWLeKMM87grLPO4te//jWhUIirrroq3U0TQgghhBAi6x03hcWll17Knj17uO2222hububUU0/lxRdfpLy8PN1NOySllHxTqkOSoXOSoTOSn3OSoTOSn3OSoXOSoTOZnt9xcfK2U+n+HgshhBBCCCHS4XD2g4+Lcyyyndaazs7Ow5pHWKSSDJ2TDJ2R/JyTDJ2R/JyTDJ2TDJ3J9PyksMgClmXR3Nw8aCoyMXySoXOSoTOSn3OSoTOSn3OSoXOSoTOZnp8UFkIIIYQQQgjHpLAQQgghhBBCOCaFRRZQSpGTk5OxMwBkA8nQOcnQGcnPOcnQGcnPOcnQOcnQmUzPT2aFGgaZFUoIIYQQQhyPZFaoY4xlWbS1tWXsiTrZQDJ0TjJ0RvJzTjJ0RvJzTjJ0TjJ0JtPzk8IiC2itaWtry9ipxbKBZOicZOiM5OecZOiM5OecZOicZOhMpucnhYUQQgghhBDCMSkshBBCCCGEEI5JYZEFlFIUFBRk7AwA2UAydE4ydEbyc04ydEbyc04ydE4ydCbT85NZoYZBZoUSQgghhBDHI5kV6hhjWRZNTU0ZOwNANpAMnZMMnZH8nJMMnZH8nJMMnZMMncn0/KSwyAJaa7q6ujJ2BoBsIBk6Jxk6I/k5Jxk6I/k5Jxk6Jxk6k+n5SWEhhBBCCCGEcMyd7gZkg4GqsLu7Oy3/3zRNent76e7uxuVypaUN2U4ydE4ydEbyc04ydEbyc04ydE4ydCYd+Q3s/w7nKIkUFsPQ09MDwJgxY9LcEiGEEEIIIUZeT08PBQUFB11HZoUaBsuyaGxsJC8vLy3Te3V3dzNmzBh27twps1IdIcnQOcnQGcnPOcnQGcnPOcnQOcnQmXTkp7Wmp6eHqqoqDOPgZ1HIEYthMAyD6urqdDeD/Px82QgdkgydkwydkfyckwydkfyckwydkwydGen8DnWkYoCcvC2EEEIIIYRwTAoLIYQQQgghhGNSWGQBn8/H7bffjs/nS3dTspZk6Jxk6Izk55xk6Izk55xk6Jxk6Eym5ycnbwshhBBCCCEckyMWQgghhBBCCMeksBBCCCGEEEI4JoWFEEIIIYQQwjEpLLLAAw88wPjx4/H7/dTW1vLWW2+lu0kZ6a677uLMM88kLy+PsrIyLrzwQurq6lLW+ad/+ieUUimXxYsXp6nFmeeOO+4YlM9JJ52UvD4SibBkyRJKSkrIzc1lwYIFtLS0pLHFmWf8+PGDMlRKsWTJEkDG4Ce99tprfPnLX6aqqgqlFM8991zK9VprbrvtNiorKwkEAsyePZstW7akrNPe3s7ChQvJz8+nsLCQb33rW/T29o5gL9LrYBnG43Fuuukmpk2bRk5ODlVVVVxxxRU0Njam3MdQ4/buu+8e4Z6kx6HG4JVXXjkom7lz56asI2Pw4BkO9ZyolOKee+5JrnM8j8Hh7L8M5/W3oaGB+fPnEwwGKSsr48c//jGJRGIkuyKFRaZ7+umnufHGG7n99ttZv349M2bMYM6cObS2tqa7aRnn1VdfZcmSJbz55pssX76ceDzO+eefTygUSlnvO9/5Dk1NTcnLr371qzS1ODOdcsopKfm8/vrryet+8IMf8F//9V8sW7aMV199lcbGRi6++OI0tjbzrF27NiW/5cuXA/C1r30tuY6MwX1CoRAzZszggQceGPL6X/3qV/zmN7/hwQcfZM2aNeTk5DBnzhwikUhynYULF/L++++zfPly/vznP/Paa69x9dVXj1QX0u5gGfb19bF+/XpuvfVW1q9fzzPPPENdXR1f+cpXBq37s5/9LGVcXn/99SPR/LQ71BgEmDt3bko2Tz75ZMr1MgYPnuH+2TU1NfHwww+jlGLBggUp6x2vY3A4+y+Hev01TZP58+cTi8VYtWoVjz32GI8++ii33XbbyHZGi4x21lln6SVLliT/Nk1TV1VV6bvuuiuNrcoOra2tGtCvvvpqctkXvvAFfcMNN6SvURnu9ttv1zNmzBjyus7OTu3xePSyZcuSyzZv3qwBvXr16hFqYfa54YYb9KRJk7RlWVprGYMHA+hnn302+bdlWbqiokLfc889yWWdnZ3a5/PpJ598Umut9QcffKABvXbt2uQ6f/3rX7VSSu/evXvE2p4pPpnhUN566y0N6B07diSXjRs3Tt93331Ht3FZYKj8Fi1apC+44IID3kbGYKrhjMELLrhAn3vuuSnLZAzu88n9l+G8/r7wwgvaMAzd3NycXGfp0qU6Pz9fR6PREWu7HLHIYLFYjHXr1jF79uzkMsMwmD17NqtXr05jy7JDV1cXAMXFxSnLn3jiCUpLS5k6dSo333wzfX196WhextqyZQtVVVVMnDiRhQsX0tDQAMC6deuIx+Mp4/Gkk05i7NixMh4PIBaL8fjjj/PNb34TpVRyuYzB4amvr6e5uTllzBUUFFBbW5scc6tXr6awsJAzzjgjuc7s2bMxDIM1a9aMeJuzQVdXF0opCgsLU5bffffdlJSUcNppp3HPPfeM+EcoMtnKlSspKyvjxBNP5JprrmHv3r3J62QMHp6Wlhb+8pe/8K1vfWvQdTIGbZ/cfxnO6+/q1auZNm0a5eXlyXXmzJlDd3c377///oi13T1i/0kctra2NkzTTBkkAOXl5Xz44YdpalV2sCyL73//+5x99tlMnTo1ufzrX/8648aNo6qqio0bN3LTTTdRV1fHM888k8bWZo7a2loeffRRTjzxRJqamrjzzjv5/Oc/z3vvvUdzczNer3fQzkh5eTnNzc3paXCGe+655+js7OTKK69MLpMxOHwD42qo58CB65qbmykrK0u53u12U1xcLONyCJFIhJtuuonLL7+c/Pz85PLvfe97zJw5k+LiYlatWsXNN99MU1MT9957bxpbmxnmzp3LxRdfzIQJE9i2bRu33HIL8+bNY/Xq1bhcLhmDh+mxxx4jLy9v0MdoZQzahtp/Gc7rb3Nz85DPlQPXjRQpLMQxacmSJbz33nsp5wcAKZ95nTZtGpWVlZx33nls27aNSZMmjXQzM868efOSv0+fPp3a2lrGjRvHH/7wBwKBQBpblp0eeugh5s2bR1VVVXKZjEGRLvF4nEsuuQStNUuXLk257sYbb0z+Pn36dLxeL9/97ne56667MvYbfkfKZZddlvx92rRpTJ8+nUmTJrFy5UrOO++8NLYsOz388MMsXLgQv9+fslzGoO1A+y/ZQj4KlcFKS0txuVyDzvpvaWmhoqIiTa3KfNdddx1//vOfeeWVV6iurj7ourW1tQBs3bp1JJqWdQoLCznhhBPYunUrFRUVxGIxOjs7U9aR8Ti0HTt28NJLL/Htb3/7oOvJGDywgXF1sOfAioqKQZNZJBIJ2tvbZVzuZ6Co2LFjB8uXL085WjGU2tpaEokE27dvH5kGZpGJEydSWlqa3GZlDA7fP/7xD+rq6g75vAjH5xg80P7LcF5/KyoqhnyuHLhupEhhkcG8Xi+nn346K1asSC6zLIsVK1Ywa9asNLYsM2mtue6663j22Wd5+eWXmTBhwiFvs2HDBgAqKyuPcuuyU29vL9u2baOyspLTTz8dj8eTMh7r6upoaGiQ8TiERx55hLKyMubPn3/Q9WQMHtiECROoqKhIGXPd3d2sWbMmOeZmzZpFZ2cn69atS67z8ssvY1lWsmg73g0UFVu2bOGll16ipKTkkLfZsGEDhmEM+oiPgF27drF3797kNitjcPgeeughTj/9dGbMmHHIdY+nMXio/ZfhvP7OmjWLTZs2pRS5A28inHzyySPTEZBZoTLdU089pX0+n3700Uf1Bx98oK+++mpdWFiYcta/sF1zzTW6oKBAr1y5Ujc1NSUvfX19Wmutt27dqn/2s5/pt99+W9fX1+s//elPeuLEifqcc85Jc8szxw9/+EO9cuVKXV9fr9944w09e/ZsXVpaqltbW7XWWi9evFiPHTtWv/zyy/rtt9/Ws2bN0rNmzUpzqzOPaZp67Nix+qabbkpZLmNwsJ6eHv3OO+/od955RwP63nvv1e+8805yxqK7775bFxYW6j/96U9648aN+oILLtATJkzQ4XA4eR9z587Vp512ml6zZo1+/fXXdU1Njb788svT1aURd7AMY7GY/spXvqKrq6v1hg0bUp4bB2aKWbVqlb7vvvv0hg0b9LZt2/Tjjz+uR40apa+44oo092xkHCy/np4e/aMf/UivXr1a19fX65deeknPnDlT19TU6EgkkrwPGYMH34611rqrq0sHg0G9dOnSQbc/3sfgofZftD70628ikdBTp07V559/vt6wYYN+8cUX9ahRo/TNN988on2RwiIL3H///Xrs2LHa6/Xqs846S7/55pvpblJGAoa8PPLII1prrRsaGvQ555yji4uLtc/n05MnT9Y//vGPdVdXV3obnkEuvfRSXVlZqb1erx49erS+9NJL9datW5PXh8Nhfe211+qioiIdDAb1RRddpJuamtLY4sz0t7/9TQO6rq4uZbmMwcFeeeWVIbfbRYsWaa3tKWdvvfVWXV5ern0+nz7vvPMG5bp37159+eWX69zcXJ2fn6+vuuoq3dPTk4bepMfBMqyvrz/gc+Mrr7yitdZ63bp1ura2VhcUFGi/36+nTJmif/nLX6bsOB/LDpZfX1+fPv/88/WoUaO0x+PR48aN09/5zncGvbknY/Dg27HWWv/ud7/TgUBAd3Z2Drr98T4GD7X/ovXwXn+3b9+u582bpwOBgC4tLdU//OEPdTweH9G+qP4OCSGEEEIIIcQRk3MshBBCCCGEEI5JYSGEEEIIIYRwTAoLIYQQQgghhGNSWAghhBBCCCEck8JCCCGEEEII4ZgUFkIIIYQQQgjHpLAQQgghhBBCOCaFhRBCCCGEEMIxKSyEEEIck5RSPPfcc+luhhBCHDeksBBCCPGpu/LKK1FKDbrMnTs33U0TQghxlLjT3QAhhBDHprlz5/LII4+kLPP5fGlqjRBCiKNNjlgIIYQ4Knw+HxUVFSmXoqIiwP6Y0tKlS5k3bx6BQICJEyfyxz/+MeX2mzZt4txzzyUQCFBSUsLVV19Nb29vyjoPP/wwp5xyCj6fj8rKSq677rqU69va2rjooosIBoPU1NTw/PPPH91OCyHEcUwKCyGEEGlx6623smDBAt59910WLlzIZZddxubNmwEIhULMmTOHoqIi1q5dy7Jly3jppZdSCoelS5eyZMkSrr76ajZt2sTzzz/P5MmTU/7HnXfeySWXXMLGjRv50pe+xMKFC2lvbx/RfgohxPFCaa11uhshhBDi2HLllVfy+OOP4/f7U5bfcsst3HLLLSilWLx4MUuXLk1e95nPfIaZM2fy29/+lt///vfcdNNN7Ny5k5ycHABeeOEFvvzlL9PY2Eh5eTmjR4/mqquu4he/+MWQbVBK8dOf/pSf//zngF2s5Obm8te//lXO9RBCiKNAzrEQQghxVPzzP/9zSuEAUFxcnPx91qxZKdfNmjWLDRs2ALB582ZmzJiRLCoAzj77bCzLoq6uDqUUjY2NnHfeeQdtw/Tp05O/5+TkkJ+fT2tr65F2SQghxEFIYSGEEOKoyMnJGfTRpE9LIBAY1noejyflb6UUlmUdjSYJIcRxT86xEEIIkRZvvvnmoL+nTJkCwJQpU3j33XcJhULJ69944w0Mw+DEE08kLy+P8ePHs2LFihFtsxBCiAOTIxZCCCGOimg0SnNzc8oyt9tNaWkpAMuWLeOMM87gc5/7HE888QRvvfUWDz30EAALFy7k9ttvZ9GiRdxxxx3s2bOH66+/nm984xuUl5cDcMcdd7B48WLKysqYN28ePT09vPHGG1x//fUj21EhhBCAFBZCCCGOkhdffJHKysqUZSeeeCIffvghYM/Y9NRTT3HttddSWVnJk08+ycknnwxAMBjkb3/7GzfccANnnnkmwWCQBQsWcO+99ybva9GiRUQiEe677z5+9KMfUVpayle/+tWR66AQQogUMiuUEEKIEaeU4tlnn+XCCy9Md1OEEEJ8SuQcCyGEEEIIIYRjUlgIIYQQQgghHJNzLIQQQow4+RSuEEIce+SIhRBCCCGEEMIxKSyEEEIIIYQQjklhIYQQQgghhHBMCgshhBBCCCGEY1JYCCGEEEIIIRyTwkIIIYQQQgjhmBQWQgghhBBCCMeksBBCCCGEEEI4JoWFEEIIIYQQwrH/D8GcWv1Y+TUhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
