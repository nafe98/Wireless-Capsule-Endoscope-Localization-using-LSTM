{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.569633</td>\n",
       "      <td>132.669774</td>\n",
       "      <td>83.130710</td>\n",
       "      <td>119.261204</td>\n",
       "      <td>126.578979</td>\n",
       "      <td>148.830041</td>\n",
       "      <td>111.311166</td>\n",
       "      <td>139.430392</td>\n",
       "      <td>99.615316</td>\n",
       "      <td>118.557826</td>\n",
       "      <td>...</td>\n",
       "      <td>101.044016</td>\n",
       "      <td>97.658267</td>\n",
       "      <td>102.844237</td>\n",
       "      <td>86.402391</td>\n",
       "      <td>128.906224</td>\n",
       "      <td>113.014998</td>\n",
       "      <td>103.942203</td>\n",
       "      <td>128.786522</td>\n",
       "      <td>77.145139</td>\n",
       "      <td>118.042869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.590690</td>\n",
       "      <td>132.661834</td>\n",
       "      <td>83.202932</td>\n",
       "      <td>119.275839</td>\n",
       "      <td>126.462565</td>\n",
       "      <td>148.697946</td>\n",
       "      <td>111.148479</td>\n",
       "      <td>139.223949</td>\n",
       "      <td>99.741752</td>\n",
       "      <td>118.556519</td>\n",
       "      <td>...</td>\n",
       "      <td>101.107786</td>\n",
       "      <td>97.777283</td>\n",
       "      <td>103.059338</td>\n",
       "      <td>86.470380</td>\n",
       "      <td>128.767509</td>\n",
       "      <td>112.867564</td>\n",
       "      <td>103.812490</td>\n",
       "      <td>128.886979</td>\n",
       "      <td>77.151769</td>\n",
       "      <td>117.929142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.614362</td>\n",
       "      <td>132.654377</td>\n",
       "      <td>83.275524</td>\n",
       "      <td>119.290404</td>\n",
       "      <td>126.346475</td>\n",
       "      <td>148.563611</td>\n",
       "      <td>110.985467</td>\n",
       "      <td>139.017736</td>\n",
       "      <td>99.869282</td>\n",
       "      <td>118.556061</td>\n",
       "      <td>...</td>\n",
       "      <td>101.172404</td>\n",
       "      <td>97.896426</td>\n",
       "      <td>103.277594</td>\n",
       "      <td>86.537182</td>\n",
       "      <td>128.629069</td>\n",
       "      <td>112.718192</td>\n",
       "      <td>103.682163</td>\n",
       "      <td>128.986052</td>\n",
       "      <td>77.158113</td>\n",
       "      <td>117.815084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.641009</td>\n",
       "      <td>132.647443</td>\n",
       "      <td>83.348540</td>\n",
       "      <td>119.304798</td>\n",
       "      <td>126.230663</td>\n",
       "      <td>148.427340</td>\n",
       "      <td>110.822296</td>\n",
       "      <td>138.811784</td>\n",
       "      <td>99.997960</td>\n",
       "      <td>118.556686</td>\n",
       "      <td>...</td>\n",
       "      <td>101.238191</td>\n",
       "      <td>98.016139</td>\n",
       "      <td>103.498832</td>\n",
       "      <td>86.603111</td>\n",
       "      <td>128.491033</td>\n",
       "      <td>112.567106</td>\n",
       "      <td>103.551552</td>\n",
       "      <td>129.083881</td>\n",
       "      <td>77.163684</td>\n",
       "      <td>117.700897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.670973</td>\n",
       "      <td>132.641270</td>\n",
       "      <td>83.422108</td>\n",
       "      <td>119.319043</td>\n",
       "      <td>126.115230</td>\n",
       "      <td>148.289453</td>\n",
       "      <td>110.658971</td>\n",
       "      <td>138.605917</td>\n",
       "      <td>100.127885</td>\n",
       "      <td>118.558828</td>\n",
       "      <td>...</td>\n",
       "      <td>101.305368</td>\n",
       "      <td>98.136740</td>\n",
       "      <td>103.722692</td>\n",
       "      <td>86.668617</td>\n",
       "      <td>128.353449</td>\n",
       "      <td>112.414828</td>\n",
       "      <td>103.421125</td>\n",
       "      <td>129.180734</td>\n",
       "      <td>77.168086</td>\n",
       "      <td>117.586815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.182404</td>\n",
       "      <td>113.040767</td>\n",
       "      <td>105.847805</td>\n",
       "      <td>78.217642</td>\n",
       "      <td>150.269492</td>\n",
       "      <td>137.927741</td>\n",
       "      <td>134.314210</td>\n",
       "      <td>115.883798</td>\n",
       "      <td>116.732585</td>\n",
       "      <td>107.086022</td>\n",
       "      <td>...</td>\n",
       "      <td>108.782340</td>\n",
       "      <td>119.546095</td>\n",
       "      <td>108.046566</td>\n",
       "      <td>68.424907</td>\n",
       "      <td>137.006882</td>\n",
       "      <td>114.335066</td>\n",
       "      <td>132.446443</td>\n",
       "      <td>116.008277</td>\n",
       "      <td>108.614011</td>\n",
       "      <td>80.271199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>127.864707</td>\n",
       "      <td>112.881785</td>\n",
       "      <td>105.963448</td>\n",
       "      <td>78.318383</td>\n",
       "      <td>150.043134</td>\n",
       "      <td>137.881055</td>\n",
       "      <td>134.455501</td>\n",
       "      <td>115.928398</td>\n",
       "      <td>116.563981</td>\n",
       "      <td>106.879909</td>\n",
       "      <td>...</td>\n",
       "      <td>108.681141</td>\n",
       "      <td>119.427729</td>\n",
       "      <td>107.891232</td>\n",
       "      <td>68.365363</td>\n",
       "      <td>136.918609</td>\n",
       "      <td>114.504131</td>\n",
       "      <td>132.374344</td>\n",
       "      <td>115.948280</td>\n",
       "      <td>108.700997</td>\n",
       "      <td>80.242280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>127.541664</td>\n",
       "      <td>112.723090</td>\n",
       "      <td>106.081989</td>\n",
       "      <td>78.418798</td>\n",
       "      <td>149.813444</td>\n",
       "      <td>137.835463</td>\n",
       "      <td>134.599156</td>\n",
       "      <td>115.974725</td>\n",
       "      <td>116.396053</td>\n",
       "      <td>106.673908</td>\n",
       "      <td>...</td>\n",
       "      <td>108.578553</td>\n",
       "      <td>119.309889</td>\n",
       "      <td>107.736048</td>\n",
       "      <td>68.304497</td>\n",
       "      <td>136.832191</td>\n",
       "      <td>114.676755</td>\n",
       "      <td>132.301638</td>\n",
       "      <td>115.889961</td>\n",
       "      <td>108.788631</td>\n",
       "      <td>80.215094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>127.213582</td>\n",
       "      <td>112.564486</td>\n",
       "      <td>106.203485</td>\n",
       "      <td>78.518849</td>\n",
       "      <td>149.580484</td>\n",
       "      <td>137.791058</td>\n",
       "      <td>134.745220</td>\n",
       "      <td>116.022311</td>\n",
       "      <td>116.228730</td>\n",
       "      <td>106.467758</td>\n",
       "      <td>...</td>\n",
       "      <td>108.474328</td>\n",
       "      <td>119.192398</td>\n",
       "      <td>107.581271</td>\n",
       "      <td>68.242740</td>\n",
       "      <td>136.747871</td>\n",
       "      <td>114.853115</td>\n",
       "      <td>132.228809</td>\n",
       "      <td>115.833382</td>\n",
       "      <td>108.877154</td>\n",
       "      <td>80.190119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>126.880923</td>\n",
       "      <td>112.405914</td>\n",
       "      <td>106.327742</td>\n",
       "      <td>78.618462</td>\n",
       "      <td>149.344480</td>\n",
       "      <td>137.747874</td>\n",
       "      <td>134.893804</td>\n",
       "      <td>116.070826</td>\n",
       "      <td>116.062017</td>\n",
       "      <td>106.261338</td>\n",
       "      <td>...</td>\n",
       "      <td>108.368265</td>\n",
       "      <td>119.075008</td>\n",
       "      <td>107.426963</td>\n",
       "      <td>68.180452</td>\n",
       "      <td>136.665841</td>\n",
       "      <td>115.033293</td>\n",
       "      <td>132.156320</td>\n",
       "      <td>115.778564</td>\n",
       "      <td>108.966672</td>\n",
       "      <td>80.167640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     102.569633  132.669774   83.130710  119.261204  126.578979  148.830041   \n",
       "1     102.590690  132.661834   83.202932  119.275839  126.462565  148.697946   \n",
       "2     102.614362  132.654377   83.275524  119.290404  126.346475  148.563611   \n",
       "3     102.641009  132.647443   83.348540  119.304798  126.230663  148.427340   \n",
       "4     102.670973  132.641270   83.422108  119.319043  126.115230  148.289453   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.182404  113.040767  105.847805   78.217642  150.269492  137.927741   \n",
       "2439  127.864707  112.881785  105.963448   78.318383  150.043134  137.881055   \n",
       "2440  127.541664  112.723090  106.081989   78.418798  149.813444  137.835463   \n",
       "2441  127.213582  112.564486  106.203485   78.518849  149.580484  137.791058   \n",
       "2442  126.880923  112.405914  106.327742   78.618462  149.344480  137.747874   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     111.311166  139.430392   99.615316  118.557826  ...  101.044016   \n",
       "1     111.148479  139.223949   99.741752  118.556519  ...  101.107786   \n",
       "2     110.985467  139.017736   99.869282  118.556061  ...  101.172404   \n",
       "3     110.822296  138.811784   99.997960  118.556686  ...  101.238191   \n",
       "4     110.658971  138.605917  100.127885  118.558828  ...  101.305368   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  134.314210  115.883798  116.732585  107.086022  ...  108.782340   \n",
       "2439  134.455501  115.928398  116.563981  106.879909  ...  108.681141   \n",
       "2440  134.599156  115.974725  116.396053  106.673908  ...  108.578553   \n",
       "2441  134.745220  116.022311  116.228730  106.467758  ...  108.474328   \n",
       "2442  134.893804  116.070826  116.062017  106.261338  ...  108.368265   \n",
       "\n",
       "              39          40         41          42          43          44  \\\n",
       "0      97.658267  102.844237  86.402391  128.906224  113.014998  103.942203   \n",
       "1      97.777283  103.059338  86.470380  128.767509  112.867564  103.812490   \n",
       "2      97.896426  103.277594  86.537182  128.629069  112.718192  103.682163   \n",
       "3      98.016139  103.498832  86.603111  128.491033  112.567106  103.551552   \n",
       "4      98.136740  103.722692  86.668617  128.353449  112.414828  103.421125   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  119.546095  108.046566  68.424907  137.006882  114.335066  132.446443   \n",
       "2439  119.427729  107.891232  68.365363  136.918609  114.504131  132.374344   \n",
       "2440  119.309889  107.736048  68.304497  136.832191  114.676755  132.301638   \n",
       "2441  119.192398  107.581271  68.242740  136.747871  114.853115  132.228809   \n",
       "2442  119.075008  107.426963  68.180452  136.665841  115.033293  132.156320   \n",
       "\n",
       "              45          46          47  \n",
       "0     128.786522   77.145139  118.042869  \n",
       "1     128.886979   77.151769  117.929142  \n",
       "2     128.986052   77.158113  117.815084  \n",
       "3     129.083881   77.163684  117.700897  \n",
       "4     129.180734   77.168086  117.586815  \n",
       "...          ...         ...         ...  \n",
       "2438  116.008277  108.614011   80.271199  \n",
       "2439  115.948280  108.700997   80.242280  \n",
       "2440  115.889961  108.788631   80.215094  \n",
       "2441  115.833382  108.877154   80.190119  \n",
       "2442  115.778564  108.966672   80.167640  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.569633</td>\n",
       "      <td>132.669774</td>\n",
       "      <td>83.130710</td>\n",
       "      <td>119.261204</td>\n",
       "      <td>126.578979</td>\n",
       "      <td>148.830041</td>\n",
       "      <td>111.311166</td>\n",
       "      <td>139.430392</td>\n",
       "      <td>99.615316</td>\n",
       "      <td>118.557826</td>\n",
       "      <td>...</td>\n",
       "      <td>101.044016</td>\n",
       "      <td>97.658267</td>\n",
       "      <td>102.844237</td>\n",
       "      <td>86.402391</td>\n",
       "      <td>128.906224</td>\n",
       "      <td>113.014998</td>\n",
       "      <td>103.942203</td>\n",
       "      <td>128.786522</td>\n",
       "      <td>77.145139</td>\n",
       "      <td>118.042869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.590690</td>\n",
       "      <td>132.661834</td>\n",
       "      <td>83.202932</td>\n",
       "      <td>119.275839</td>\n",
       "      <td>126.462565</td>\n",
       "      <td>148.697946</td>\n",
       "      <td>111.148479</td>\n",
       "      <td>139.223949</td>\n",
       "      <td>99.741752</td>\n",
       "      <td>118.556519</td>\n",
       "      <td>...</td>\n",
       "      <td>101.107786</td>\n",
       "      <td>97.777283</td>\n",
       "      <td>103.059338</td>\n",
       "      <td>86.470380</td>\n",
       "      <td>128.767509</td>\n",
       "      <td>112.867564</td>\n",
       "      <td>103.812490</td>\n",
       "      <td>128.886979</td>\n",
       "      <td>77.151769</td>\n",
       "      <td>117.929142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.614362</td>\n",
       "      <td>132.654377</td>\n",
       "      <td>83.275524</td>\n",
       "      <td>119.290404</td>\n",
       "      <td>126.346475</td>\n",
       "      <td>148.563611</td>\n",
       "      <td>110.985467</td>\n",
       "      <td>139.017736</td>\n",
       "      <td>99.869282</td>\n",
       "      <td>118.556061</td>\n",
       "      <td>...</td>\n",
       "      <td>101.172404</td>\n",
       "      <td>97.896426</td>\n",
       "      <td>103.277594</td>\n",
       "      <td>86.537182</td>\n",
       "      <td>128.629069</td>\n",
       "      <td>112.718192</td>\n",
       "      <td>103.682163</td>\n",
       "      <td>128.986052</td>\n",
       "      <td>77.158113</td>\n",
       "      <td>117.815084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.641009</td>\n",
       "      <td>132.647443</td>\n",
       "      <td>83.348540</td>\n",
       "      <td>119.304798</td>\n",
       "      <td>126.230663</td>\n",
       "      <td>148.427340</td>\n",
       "      <td>110.822296</td>\n",
       "      <td>138.811784</td>\n",
       "      <td>99.997960</td>\n",
       "      <td>118.556686</td>\n",
       "      <td>...</td>\n",
       "      <td>101.238191</td>\n",
       "      <td>98.016139</td>\n",
       "      <td>103.498832</td>\n",
       "      <td>86.603111</td>\n",
       "      <td>128.491033</td>\n",
       "      <td>112.567106</td>\n",
       "      <td>103.551552</td>\n",
       "      <td>129.083881</td>\n",
       "      <td>77.163684</td>\n",
       "      <td>117.700897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.670973</td>\n",
       "      <td>132.641270</td>\n",
       "      <td>83.422108</td>\n",
       "      <td>119.319043</td>\n",
       "      <td>126.115230</td>\n",
       "      <td>148.289453</td>\n",
       "      <td>110.658971</td>\n",
       "      <td>138.605917</td>\n",
       "      <td>100.127885</td>\n",
       "      <td>118.558828</td>\n",
       "      <td>...</td>\n",
       "      <td>101.305368</td>\n",
       "      <td>98.136740</td>\n",
       "      <td>103.722692</td>\n",
       "      <td>86.668617</td>\n",
       "      <td>128.353449</td>\n",
       "      <td>112.414828</td>\n",
       "      <td>103.421125</td>\n",
       "      <td>129.180734</td>\n",
       "      <td>77.168086</td>\n",
       "      <td>117.586815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.182404</td>\n",
       "      <td>113.040767</td>\n",
       "      <td>105.847805</td>\n",
       "      <td>78.217642</td>\n",
       "      <td>150.269492</td>\n",
       "      <td>137.927741</td>\n",
       "      <td>134.314210</td>\n",
       "      <td>115.883798</td>\n",
       "      <td>116.732585</td>\n",
       "      <td>107.086022</td>\n",
       "      <td>...</td>\n",
       "      <td>108.782340</td>\n",
       "      <td>119.546095</td>\n",
       "      <td>108.046566</td>\n",
       "      <td>68.424907</td>\n",
       "      <td>137.006882</td>\n",
       "      <td>114.335066</td>\n",
       "      <td>132.446443</td>\n",
       "      <td>116.008277</td>\n",
       "      <td>108.614011</td>\n",
       "      <td>80.271199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>127.864707</td>\n",
       "      <td>112.881785</td>\n",
       "      <td>105.963448</td>\n",
       "      <td>78.318383</td>\n",
       "      <td>150.043134</td>\n",
       "      <td>137.881055</td>\n",
       "      <td>134.455501</td>\n",
       "      <td>115.928398</td>\n",
       "      <td>116.563981</td>\n",
       "      <td>106.879909</td>\n",
       "      <td>...</td>\n",
       "      <td>108.681141</td>\n",
       "      <td>119.427729</td>\n",
       "      <td>107.891232</td>\n",
       "      <td>68.365363</td>\n",
       "      <td>136.918609</td>\n",
       "      <td>114.504131</td>\n",
       "      <td>132.374344</td>\n",
       "      <td>115.948280</td>\n",
       "      <td>108.700997</td>\n",
       "      <td>80.242280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>127.541664</td>\n",
       "      <td>112.723090</td>\n",
       "      <td>106.081989</td>\n",
       "      <td>78.418798</td>\n",
       "      <td>149.813444</td>\n",
       "      <td>137.835463</td>\n",
       "      <td>134.599156</td>\n",
       "      <td>115.974725</td>\n",
       "      <td>116.396053</td>\n",
       "      <td>106.673908</td>\n",
       "      <td>...</td>\n",
       "      <td>108.578553</td>\n",
       "      <td>119.309889</td>\n",
       "      <td>107.736048</td>\n",
       "      <td>68.304497</td>\n",
       "      <td>136.832191</td>\n",
       "      <td>114.676755</td>\n",
       "      <td>132.301638</td>\n",
       "      <td>115.889961</td>\n",
       "      <td>108.788631</td>\n",
       "      <td>80.215094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>127.213582</td>\n",
       "      <td>112.564486</td>\n",
       "      <td>106.203485</td>\n",
       "      <td>78.518849</td>\n",
       "      <td>149.580484</td>\n",
       "      <td>137.791058</td>\n",
       "      <td>134.745220</td>\n",
       "      <td>116.022311</td>\n",
       "      <td>116.228730</td>\n",
       "      <td>106.467758</td>\n",
       "      <td>...</td>\n",
       "      <td>108.474328</td>\n",
       "      <td>119.192398</td>\n",
       "      <td>107.581271</td>\n",
       "      <td>68.242740</td>\n",
       "      <td>136.747871</td>\n",
       "      <td>114.853115</td>\n",
       "      <td>132.228809</td>\n",
       "      <td>115.833382</td>\n",
       "      <td>108.877154</td>\n",
       "      <td>80.190119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>126.880923</td>\n",
       "      <td>112.405914</td>\n",
       "      <td>106.327742</td>\n",
       "      <td>78.618462</td>\n",
       "      <td>149.344480</td>\n",
       "      <td>137.747874</td>\n",
       "      <td>134.893804</td>\n",
       "      <td>116.070826</td>\n",
       "      <td>116.062017</td>\n",
       "      <td>106.261338</td>\n",
       "      <td>...</td>\n",
       "      <td>108.368265</td>\n",
       "      <td>119.075008</td>\n",
       "      <td>107.426963</td>\n",
       "      <td>68.180452</td>\n",
       "      <td>136.665841</td>\n",
       "      <td>115.033293</td>\n",
       "      <td>132.156320</td>\n",
       "      <td>115.778564</td>\n",
       "      <td>108.966672</td>\n",
       "      <td>80.167640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     102.569633  132.669774   83.130710  119.261204  126.578979  148.830041   \n",
       "1     102.590690  132.661834   83.202932  119.275839  126.462565  148.697946   \n",
       "2     102.614362  132.654377   83.275524  119.290404  126.346475  148.563611   \n",
       "3     102.641009  132.647443   83.348540  119.304798  126.230663  148.427340   \n",
       "4     102.670973  132.641270   83.422108  119.319043  126.115230  148.289453   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.182404  113.040767  105.847805   78.217642  150.269492  137.927741   \n",
       "2439  127.864707  112.881785  105.963448   78.318383  150.043134  137.881055   \n",
       "2440  127.541664  112.723090  106.081989   78.418798  149.813444  137.835463   \n",
       "2441  127.213582  112.564486  106.203485   78.518849  149.580484  137.791058   \n",
       "2442  126.880923  112.405914  106.327742   78.618462  149.344480  137.747874   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     111.311166  139.430392   99.615316  118.557826  ...  101.044016   \n",
       "1     111.148479  139.223949   99.741752  118.556519  ...  101.107786   \n",
       "2     110.985467  139.017736   99.869282  118.556061  ...  101.172404   \n",
       "3     110.822296  138.811784   99.997960  118.556686  ...  101.238191   \n",
       "4     110.658971  138.605917  100.127885  118.558828  ...  101.305368   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  134.314210  115.883798  116.732585  107.086022  ...  108.782340   \n",
       "2439  134.455501  115.928398  116.563981  106.879909  ...  108.681141   \n",
       "2440  134.599156  115.974725  116.396053  106.673908  ...  108.578553   \n",
       "2441  134.745220  116.022311  116.228730  106.467758  ...  108.474328   \n",
       "2442  134.893804  116.070826  116.062017  106.261338  ...  108.368265   \n",
       "\n",
       "        sensor40    sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      97.658267  102.844237  86.402391  128.906224  113.014998  103.942203   \n",
       "1      97.777283  103.059338  86.470380  128.767509  112.867564  103.812490   \n",
       "2      97.896426  103.277594  86.537182  128.629069  112.718192  103.682163   \n",
       "3      98.016139  103.498832  86.603111  128.491033  112.567106  103.551552   \n",
       "4      98.136740  103.722692  86.668617  128.353449  112.414828  103.421125   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  119.546095  108.046566  68.424907  137.006882  114.335066  132.446443   \n",
       "2439  119.427729  107.891232  68.365363  136.918609  114.504131  132.374344   \n",
       "2440  119.309889  107.736048  68.304497  136.832191  114.676755  132.301638   \n",
       "2441  119.192398  107.581271  68.242740  136.747871  114.853115  132.228809   \n",
       "2442  119.075008  107.426963  68.180452  136.665841  115.033293  132.156320   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     128.786522   77.145139  118.042869  \n",
       "1     128.886979   77.151769  117.929142  \n",
       "2     128.986052   77.158113  117.815084  \n",
       "3     129.083881   77.163684  117.700897  \n",
       "4     129.180734   77.168086  117.586815  \n",
       "...          ...         ...         ...  \n",
       "2438  116.008277  108.614011   80.271199  \n",
       "2439  115.948280  108.700997   80.242280  \n",
       "2440  115.889961  108.788631   80.215094  \n",
       "2441  115.833382  108.877154   80.190119  \n",
       "2442  115.778564  108.966672   80.167640  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 20ms/step - loss: 1371.2108 - val_loss: 1322.9409\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1212.1659 - val_loss: 1214.0658\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1125.6356 - val_loss: 1135.2974\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1061.2123 - val_loss: 1075.5262\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1013.1808 - val_loss: 1031.2130\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 978.2368 - val_loss: 998.6161\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 953.5363 - val_loss: 975.8364\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 936.9163 - val_loss: 960.7675\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 926.4844 - val_loss: 951.1122\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 920.3672 - val_loss: 945.4254\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 917.0815 - val_loss: 942.2264\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 915.5114 - val_loss: 940.6771\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 914.8007 - val_loss: 939.7230\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 912.6235 - val_loss: 934.5458\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 876.4737 - val_loss: 885.1844\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 847.4203 - val_loss: 864.4041\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 831.8412 - val_loss: 849.8747\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 819.7294 - val_loss: 836.5726\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 805.1614 - val_loss: 819.2172\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 787.1022 - val_loss: 798.1191\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 758.9729 - val_loss: 756.3517\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 707.9988 - val_loss: 702.8864\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 654.1269 - val_loss: 642.6975\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 598.5405 - val_loss: 589.1588\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 543.7847 - val_loss: 537.4960\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 496.5960 - val_loss: 492.2332\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 452.8315 - val_loss: 447.7665\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 413.3564 - val_loss: 409.1109\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 374.6483 - val_loss: 370.2386\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 340.3824 - val_loss: 337.4374\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 308.7166 - val_loss: 309.7978\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 281.1735 - val_loss: 276.9023\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 254.1563 - val_loss: 252.4101\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 229.3655 - val_loss: 227.1051\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 207.4890 - val_loss: 204.4435\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 185.3657 - val_loss: 182.8875\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 165.9200 - val_loss: 164.5662\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 148.7018 - val_loss: 148.4159\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 130.0671 - val_loss: 127.5582\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 114.8768 - val_loss: 111.6802\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 99.3977 - val_loss: 100.7188\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 88.4034 - val_loss: 91.7528\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 77.1522 - val_loss: 74.7189\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.0385 - val_loss: 63.5993\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 56.6576 - val_loss: 56.0200\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 49.5011 - val_loss: 48.6453\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 42.6886 - val_loss: 41.7596\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 38.5392 - val_loss: 37.9462\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.6948 - val_loss: 30.3659\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.5349 - val_loss: 26.5045\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.3439 - val_loss: 22.4978\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.3807 - val_loss: 19.1931\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.9486 - val_loss: 24.7360\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.2167 - val_loss: 14.5905\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.3567 - val_loss: 12.2903\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.8647 - val_loss: 10.3110\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3311 - val_loss: 10.9543\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.7390 - val_loss: 7.4310\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.1194 - val_loss: 6.4458\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.6884 - val_loss: 6.2810\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.8419 - val_loss: 4.6015\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.5923 - val_loss: 3.7088\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0988 - val_loss: 4.9173\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.1364 - val_loss: 3.1100\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4330 - val_loss: 2.7583\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9610 - val_loss: 2.2402\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2851 - val_loss: 2.2348\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5999 - val_loss: 1.5069\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.2136 - val_loss: 1.2370\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5484 - val_loss: 2.3404\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1751 - val_loss: 2.1863\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4234 - val_loss: 1.9560\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0464 - val_loss: 0.7616\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1307 - val_loss: 1.1597\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8659 - val_loss: 1.0832\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7977 - val_loss: 1.5593\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2308 - val_loss: 0.9014\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6978 - val_loss: 0.8363\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0302 - val_loss: 1.0286\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7083 - val_loss: 0.7076\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5173 - val_loss: 0.3386\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6263 - val_loss: 0.7074\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8083 - val_loss: 0.4553\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4493 - val_loss: 0.5577\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8603 - val_loss: 0.3927\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4591 - val_loss: 0.5488\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5579 - val_loss: 1.0507\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5412 - val_loss: 0.3192\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1903 - val_loss: 0.1911\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2138 - val_loss: 0.2590\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2508 - val_loss: 0.2257\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4698 - val_loss: 0.3059\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5272 - val_loss: 0.3122\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3079 - val_loss: 0.6952\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5205 - val_loss: 0.2268\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5856 - val_loss: 0.3124\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3193 - val_loss: 0.6135\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2804 - val_loss: 0.3243\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4876 - val_loss: 1.1332\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5456 - val_loss: 0.1341\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2029 - val_loss: 0.2295\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1781 - val_loss: 0.1176\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1649 - val_loss: 0.1905\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3068 - val_loss: 0.1683\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3018 - val_loss: 0.2838\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5548 - val_loss: 0.4113\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3159 - val_loss: 0.2350\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1651 - val_loss: 0.1311\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9969 - val_loss: 0.4023\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2026 - val_loss: 0.1071\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1020 - val_loss: 0.3058\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3737 - val_loss: 0.1314\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3131 - val_loss: 0.7679\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7047 - val_loss: 0.0984\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1308 - val_loss: 0.2579\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2727 - val_loss: 0.1704\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1802 - val_loss: 0.2437\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2512 - val_loss: 0.2098\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5599 - val_loss: 0.2818\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1664 - val_loss: 0.1174\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2087 - val_loss: 0.1836\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4511 - val_loss: 0.2588\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2921 - val_loss: 0.1619\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1837 - val_loss: 0.1644\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2866 - val_loss: 0.1785\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3729 - val_loss: 0.8488\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4654 - val_loss: 0.0890\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0882 - val_loss: 0.1136\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1410 - val_loss: 0.2742\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6257 - val_loss: 0.1336\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0844 - val_loss: 0.1180\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1466 - val_loss: 0.2349\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3256 - val_loss: 0.8574\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3424 - val_loss: 0.1361\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2017 - val_loss: 0.5292\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2636 - val_loss: 0.0928\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1346 - val_loss: 0.2440\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1918 - val_loss: 0.2804\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2702 - val_loss: 0.5760\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2528 - val_loss: 0.1739\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1220 - val_loss: 0.1459\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.0833 - val_loss: 0.0802\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0804 - val_loss: 0.0582\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2340 - val_loss: 0.1736\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1112 - val_loss: 0.4900\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1756 - val_loss: 0.1213\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1277 - val_loss: 0.0993\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3360 - val_loss: 0.4433\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1306 - val_loss: 0.2258\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2122 - val_loss: 0.0972\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1205 - val_loss: 0.1714\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1478 - val_loss: 0.2227\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2772 - val_loss: 0.1343\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1506 - val_loss: 0.2291\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1603 - val_loss: 0.1889\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1591 - val_loss: 0.0942\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2261 - val_loss: 0.3757\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3136 - val_loss: 0.2334\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1441 - val_loss: 0.1062\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2086 - val_loss: 0.2200\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3105 - val_loss: 0.5291\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1979 - val_loss: 0.1215\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1023 - val_loss: 0.0514\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1363 - val_loss: 0.1706\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2818 - val_loss: 0.1292\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1406 - val_loss: 0.1734\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1783 - val_loss: 0.2993\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3908 - val_loss: 0.2033\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1582 - val_loss: 0.1413\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0799 - val_loss: 0.0503\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4025 - val_loss: 0.2586\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0855 - val_loss: 0.0424\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0663 - val_loss: 0.1073\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1033 - val_loss: 0.2628\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5307 - val_loss: 0.1271\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.0610 - val_loss: 0.0570\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.0848 - val_loss: 0.1380\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 10s 24ms/step - loss: 0.4708 - val_loss: 0.9312\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.1575 - val_loss: 0.0425\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0427 - val_loss: 0.0479\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0937 - val_loss: 0.2135\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3104 - val_loss: 0.1271\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0582 - val_loss: 0.0421\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1052 - val_loss: 0.0792\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2302 - val_loss: 0.0717\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1820 - val_loss: 0.5764\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2503 - val_loss: 0.1264\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1092 - val_loss: 0.0533\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0951 - val_loss: 0.0753\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3266 - val_loss: 0.0495\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0523 - val_loss: 0.0664\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2000 - val_loss: 0.1855\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1594 - val_loss: 0.1307\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.0891 - val_loss: 0.0792\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1634 - val_loss: 0.1788\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1292 - val_loss: 0.1829\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3019 - val_loss: 0.1488\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0881 - val_loss: 0.0292\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0968 - val_loss: 0.1652\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1965 - val_loss: 0.4077\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.40769546769919723\n",
      "Mean Absolute Error (MAE): 0.47135768346637946\n",
      "Root Mean Squared Error (RMSE): 0.6385103505027911\n",
      "Time taken: 1269.6549286842346\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 18ms/step - loss: 1386.3582 - val_loss: 1252.8516\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1226.4543 - val_loss: 1151.6658\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1138.6564 - val_loss: 1078.5391\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1072.7920 - val_loss: 1023.4156\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1023.7302 - val_loss: 983.1931\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 987.9082 - val_loss: 954.4064\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 962.5959 - val_loss: 934.7881\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 945.5148 - val_loss: 922.0534\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 934.7231 - val_loss: 914.5798\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.2925 - val_loss: 910.4695\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.8353 - val_loss: 908.6265\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2418 - val_loss: 907.9259\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 922.5693 - val_loss: 907.7076\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.2680 - val_loss: 907.6957\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 922.2065 - val_loss: 907.7557\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.1595 - val_loss: 907.7746\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 922.1282 - val_loss: 907.8284\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 922.1618 - val_loss: 907.8282\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 922.1345 - val_loss: 907.8160\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.1304 - val_loss: 907.7776\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.0956 - val_loss: 907.6747\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.3527 - val_loss: 907.6776\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.0000 - val_loss: 905.3325\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 878.5792 - val_loss: 843.8765\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 852.3233 - val_loss: 827.7753\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 838.9089 - val_loss: 816.1642\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 828.3760 - val_loss: 807.0823\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 813.2362 - val_loss: 790.7024\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 791.7292 - val_loss: 760.7131\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 741.9069 - val_loss: 698.1500\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 678.8315 - val_loss: 642.1241\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 628.0847 - val_loss: 593.9030\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 583.9126 - val_loss: 553.4836\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 542.4741 - val_loss: 513.1224\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 501.1737 - val_loss: 470.9591\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 456.4384 - val_loss: 425.7299\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 414.9985 - val_loss: 385.3763\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 375.6236 - val_loss: 349.8887\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 337.8839 - val_loss: 312.7951\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 302.1779 - val_loss: 278.0960\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 268.8727 - val_loss: 247.8344\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 239.6562 - val_loss: 221.3633\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 213.6014 - val_loss: 197.9664\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 190.4760 - val_loss: 176.7062\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 167.8639 - val_loss: 154.7446\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 147.9029 - val_loss: 135.9978\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 128.1736 - val_loss: 118.1999\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 111.3364 - val_loss: 103.1031\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 97.6009 - val_loss: 99.2666\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 84.7177 - val_loss: 77.1227\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.0851 - val_loss: 67.1135\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.1122 - val_loss: 60.0138\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.4976 - val_loss: 49.0488\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 45.4528 - val_loss: 44.0440\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.3476 - val_loss: 35.9699\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.2967 - val_loss: 31.4406\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.7100 - val_loss: 27.1750\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.7083 - val_loss: 22.1654\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.1901 - val_loss: 18.8785\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.3804 - val_loss: 16.7475\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5387 - val_loss: 14.7717\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.0999 - val_loss: 10.8615\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.1357 - val_loss: 9.1964\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9618 - val_loss: 7.4592\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6493 - val_loss: 6.6956\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6365 - val_loss: 5.0547\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.8489 - val_loss: 4.4129\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8419 - val_loss: 3.7317\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0833 - val_loss: 3.6359\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3946 - val_loss: 3.0825\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4347 - val_loss: 2.3932\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0599 - val_loss: 2.2941\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2550 - val_loss: 4.3970\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9479 - val_loss: 1.6859\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.3203 - val_loss: 1.3603\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6297 - val_loss: 2.2193\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7029 - val_loss: 1.0606\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0199 - val_loss: 0.9087\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1884 - val_loss: 4.4680\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6549 - val_loss: 0.5953\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6228 - val_loss: 0.5352\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7215 - val_loss: 0.6254\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2042 - val_loss: 1.2604\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1679 - val_loss: 0.5952\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7669 - val_loss: 2.8007\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9015 - val_loss: 0.5364\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4109 - val_loss: 0.4254\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0545 - val_loss: 1.2753\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6526 - val_loss: 0.4566\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5385 - val_loss: 0.8419\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3056 - val_loss: 0.5660\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4993 - val_loss: 0.3179\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4066 - val_loss: 0.5409\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.1374 - val_loss: 1.6029\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4305 - val_loss: 0.5685\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8265 - val_loss: 0.3037\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4758 - val_loss: 0.7562\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7456 - val_loss: 0.5149\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4485 - val_loss: 0.5177\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7013 - val_loss: 0.5640\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7017 - val_loss: 0.4269\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2928 - val_loss: 0.5142\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4723 - val_loss: 0.4195\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7546 - val_loss: 0.5928\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3908 - val_loss: 0.8493\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7309 - val_loss: 1.3966\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5671 - val_loss: 0.2348\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2051 - val_loss: 0.2054\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3131 - val_loss: 0.7885\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7521 - val_loss: 0.2081\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1450 - val_loss: 0.1334\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1439 - val_loss: 0.1838\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2222 - val_loss: 0.2419\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6768 - val_loss: 0.6513\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5294 - val_loss: 0.3195\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4224 - val_loss: 0.3751\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2683 - val_loss: 0.1985\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3710 - val_loss: 0.4807\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.4862 - val_loss: 0.6101\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2482 - val_loss: 0.0858\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1171 - val_loss: 0.1430\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3341 - val_loss: 0.9170\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6946 - val_loss: 0.3224\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1992 - val_loss: 0.1295\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2794 - val_loss: 0.1803\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.4839 - val_loss: 0.3980\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4181 - val_loss: 0.2860\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4282 - val_loss: 0.9555\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3368 - val_loss: 0.2131\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9752 - val_loss: 0.9242\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2721 - val_loss: 0.2454\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1813 - val_loss: 0.0671\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1833 - val_loss: 0.8469\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6346 - val_loss: 0.2794\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3349 - val_loss: 0.2412\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2872 - val_loss: 0.2736\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7523 - val_loss: 0.1856\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1507 - val_loss: 0.0852\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1383 - val_loss: 0.1418\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8333 - val_loss: 2.3527\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3630 - val_loss: 0.0783\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0911 - val_loss: 0.0692\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1645 - val_loss: 0.1751\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6446 - val_loss: 0.3947\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1540 - val_loss: 0.0946\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3158 - val_loss: 0.5298\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4136 - val_loss: 0.1520\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1476 - val_loss: 0.0795\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1465 - val_loss: 0.2356\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5579 - val_loss: 0.6889\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4009 - val_loss: 0.1696\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1402 - val_loss: 0.0904\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2183 - val_loss: 0.3236\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3998 - val_loss: 0.2540\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1938 - val_loss: 0.2658\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3459 - val_loss: 0.1837\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4437 - val_loss: 0.2313\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2434 - val_loss: 0.4236\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1403 - val_loss: 0.4266\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5614 - val_loss: 0.1849\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0832 - val_loss: 0.0537\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0476 - val_loss: 0.0540\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0589 - val_loss: 0.0671\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1335 - val_loss: 0.0867\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1608 - val_loss: 0.1451\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2590 - val_loss: 0.1881\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3406 - val_loss: 0.1778\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2152 - val_loss: 0.1918\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2164 - val_loss: 0.6520\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5747 - val_loss: 0.4681\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2208 - val_loss: 0.1175\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1481 - val_loss: 0.0631\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2186 - val_loss: 0.1217\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2685 - val_loss: 0.5088\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2460 - val_loss: 0.0900\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1222 - val_loss: 0.2450\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3325 - val_loss: 0.7326\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3492 - val_loss: 0.1064\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2099 - val_loss: 0.4436\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2936 - val_loss: 0.2944\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1174 - val_loss: 0.1733\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4677 - val_loss: 0.1276\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1187 - val_loss: 0.1450\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5212 - val_loss: 0.2035\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0993 - val_loss: 0.0740\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0591 - val_loss: 0.0857\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0490 - val_loss: 0.0576\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0882 - val_loss: 0.0797\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2794 - val_loss: 1.0058\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2757 - val_loss: 0.0946\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.0864 - val_loss: 0.1161\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5842 - val_loss: 0.3514\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1158 - val_loss: 0.1536\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1607 - val_loss: 0.1257\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1818 - val_loss: 0.2852\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2638 - val_loss: 0.1803\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4559 - val_loss: 0.9658\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3550 - val_loss: 0.0770\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0610 - val_loss: 0.0432\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 0.1032 - val_loss: 0.1882\n",
      "16/16 [==============================] - 2s 9ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.1882021385700129\n",
      "Mean Absolute Error (MAE): 0.3307643739248276\n",
      "Root Mean Squared Error (RMSE): 0.4338227040739257\n",
      "Time taken: 1252.2306668758392\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 14s 24ms/step - loss: 1369.8542 - val_loss: 1256.8606\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1211.6074 - val_loss: 1159.3567\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1123.8318 - val_loss: 1087.3585\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1059.5448 - val_loss: 1034.7915\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 1012.2595 - val_loss: 996.4435\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 978.0448 - val_loss: 969.3073\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 954.1146 - val_loss: 951.1282\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 938.3046 - val_loss: 939.5480\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 928.2625 - val_loss: 932.6681\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 922.5933 - val_loss: 929.2114\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 919.6470 - val_loss: 927.6102\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 918.2903 - val_loss: 927.0320\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 917.6347 - val_loss: 926.9376\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.4725 - val_loss: 926.9948\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 917.3873 - val_loss: 926.9817\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.4031 - val_loss: 927.0204\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.3603 - val_loss: 927.0475\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.4021 - val_loss: 927.0609\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 917.3778 - val_loss: 927.0909\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 917.3403 - val_loss: 927.1099\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.3559 - val_loss: 927.0439\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.3514 - val_loss: 927.0344\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.3472 - val_loss: 927.0679\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 917.3824 - val_loss: 927.0714\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.3485 - val_loss: 927.1270\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.3804 - val_loss: 927.0992\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.3869 - val_loss: 927.1038\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.6872 - val_loss: 927.1091\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.3612 - val_loss: 927.0544\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 892.0941 - val_loss: 868.9606\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 850.1155 - val_loss: 847.7483\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 833.7687 - val_loss: 832.5190\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 817.8759 - val_loss: 813.6860\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 795.5730 - val_loss: 781.1465\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 741.4670 - val_loss: 715.7702\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 685.2762 - val_loss: 662.9145\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 625.8751 - val_loss: 601.0255\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 569.3912 - val_loss: 560.9390\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 522.5866 - val_loss: 503.2241\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 480.0518 - val_loss: 461.5219\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 439.9321 - val_loss: 422.5526\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 401.2924 - val_loss: 387.4002\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 368.0284 - val_loss: 354.1194\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 336.9722 - val_loss: 322.9243\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 307.2208 - val_loss: 293.7071\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 276.5844 - val_loss: 264.7327\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 248.8708 - val_loss: 236.5314\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 225.6392 - val_loss: 219.5598\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 200.6424 - val_loss: 189.1238\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 177.5844 - val_loss: 169.3044\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 155.4106 - val_loss: 151.3335\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 136.4681 - val_loss: 128.0108\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 120.5707 - val_loss: 113.7647\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 105.8419 - val_loss: 98.1987\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 92.9154 - val_loss: 86.2486\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 81.3659 - val_loss: 78.5691\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 72.0370 - val_loss: 67.0937\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 61.1358 - val_loss: 57.8999\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 55.4759 - val_loss: 49.8113\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.4729 - val_loss: 42.9843\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 40.0273 - val_loss: 37.1103\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.1440 - val_loss: 31.7709\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.2000 - val_loss: 32.0449\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.1612 - val_loss: 24.3847\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.6077 - val_loss: 20.3512\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.1009 - val_loss: 19.0150\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6524 - val_loss: 15.5549\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5103 - val_loss: 24.7382\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.4487 - val_loss: 11.2748\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6267 - val_loss: 10.1089\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5583 - val_loss: 8.8837\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2632 - val_loss: 7.5157\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4056 - val_loss: 6.9024\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0658 - val_loss: 5.4951\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3550 - val_loss: 4.9430\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.4838 - val_loss: 4.0786\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0434 - val_loss: 4.0448\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3008 - val_loss: 3.8672\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.2048 - val_loss: 2.5766\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.5166 - val_loss: 5.4739\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.5678 - val_loss: 2.0334\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2616 - val_loss: 2.1675\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1477 - val_loss: 1.7336\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.1443 - val_loss: 1.4086\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.1761 - val_loss: 1.9625\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.5047 - val_loss: 1.1888\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4060 - val_loss: 1.6268\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0626 - val_loss: 0.7472\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9864 - val_loss: 0.8674\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6315 - val_loss: 0.9760\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1395 - val_loss: 1.1261\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1115 - val_loss: 0.9999\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9143 - val_loss: 0.7441\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6524 - val_loss: 2.2128\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4266 - val_loss: 0.6793\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6866 - val_loss: 1.0343\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8162 - val_loss: 0.7662\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1647 - val_loss: 1.1377\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8099 - val_loss: 0.5498\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6133 - val_loss: 0.4927\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6437 - val_loss: 0.6010\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0392 - val_loss: 1.3617\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7582 - val_loss: 1.3711\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4923 - val_loss: 0.4943\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4022 - val_loss: 0.5329\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6955 - val_loss: 0.9459\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5027 - val_loss: 0.4259\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5163 - val_loss: 0.4055\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4964 - val_loss: 1.0533\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4421 - val_loss: 0.6548\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4860 - val_loss: 0.3950\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4066 - val_loss: 0.2260\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2067 - val_loss: 0.5062\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3757 - val_loss: 0.3415\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5203 - val_loss: 0.3834\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4044 - val_loss: 0.5032\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.5058 - val_loss: 1.1182\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4491 - val_loss: 0.5677\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7357 - val_loss: 0.8719\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3730 - val_loss: 0.2841\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4046 - val_loss: 0.4963\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4801 - val_loss: 0.9928\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3508 - val_loss: 0.3977\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7282 - val_loss: 2.4355\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5381 - val_loss: 0.1291\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1628 - val_loss: 0.1724\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5156 - val_loss: 0.4887\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3589 - val_loss: 0.1558\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6169 - val_loss: 0.7550\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3688 - val_loss: 0.3527\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6799 - val_loss: 1.0817\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3862 - val_loss: 0.2956\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3115 - val_loss: 0.1827\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2542 - val_loss: 0.3122\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6258 - val_loss: 0.5324\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2569 - val_loss: 0.3161\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3385 - val_loss: 0.3588\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4597 - val_loss: 0.2387\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3543 - val_loss: 0.2587\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.5445 - val_loss: 1.1056\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1650 - val_loss: 0.1118\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0882 - val_loss: 0.1527\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2374 - val_loss: 0.5061\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2138 - val_loss: 0.2505\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5438 - val_loss: 1.1739\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3969 - val_loss: 0.4449\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1479 - val_loss: 0.1852\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2615 - val_loss: 2.1303\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7004 - val_loss: 0.2348\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1733 - val_loss: 0.2110\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1962 - val_loss: 0.1332\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4081 - val_loss: 0.5868\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3532 - val_loss: 0.1520\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2463 - val_loss: 0.1364\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8417 - val_loss: 0.1282\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1063 - val_loss: 0.0862\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1189 - val_loss: 0.1250\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6016 - val_loss: 0.5413\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2103 - val_loss: 0.1013\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2826 - val_loss: 0.5685\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2825 - val_loss: 0.1563\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3175 - val_loss: 0.2521\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7873 - val_loss: 0.1838\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1366 - val_loss: 0.1166\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0769 - val_loss: 0.0843\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1298 - val_loss: 0.1369\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2501 - val_loss: 0.2258\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2459 - val_loss: 0.1229\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2419 - val_loss: 0.2624\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3358 - val_loss: 0.1580\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2573 - val_loss: 0.2617\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2633 - val_loss: 0.2061\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2971 - val_loss: 0.1111\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2126 - val_loss: 0.4628\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5002 - val_loss: 0.2834\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1565 - val_loss: 0.1690\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2550 - val_loss: 0.2255\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2260 - val_loss: 0.2971\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5043 - val_loss: 0.2654\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1610 - val_loss: 0.1795\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3130 - val_loss: 0.1407\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1348 - val_loss: 0.1654\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2230 - val_loss: 0.5879\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8056 - val_loss: 0.4412\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1252 - val_loss: 0.0519\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0530 - val_loss: 0.0542\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1498 - val_loss: 0.1748\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3351 - val_loss: 0.1896\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1706 - val_loss: 0.1012\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5394 - val_loss: 2.6319\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3872 - val_loss: 0.0568\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0957 - val_loss: 0.1203\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1370 - val_loss: 0.1290\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2695 - val_loss: 0.1753\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1583 - val_loss: 0.6918\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4930 - val_loss: 0.2757\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1681 - val_loss: 0.1616\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1949 - val_loss: 0.3257\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1803 - val_loss: 0.1072\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2630 - val_loss: 0.4762\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.4762115293941996\n",
      "Mean Absolute Error (MAE): 0.5138960802441115\n",
      "Root Mean Squared Error (RMSE): 0.6900808136690946\n",
      "Time taken: 1273.233321905136\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 17ms/step - loss: 1399.9495 - val_loss: 1292.4729\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1234.1862 - val_loss: 1188.3408\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1144.1919 - val_loss: 1112.6873\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1076.5781 - val_loss: 1054.1766\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1025.5579 - val_loss: 1011.0778\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 988.3739 - val_loss: 979.9666\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 961.8057 - val_loss: 957.9476\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 943.6253 - val_loss: 943.4681\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 931.9685 - val_loss: 934.4002\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.9674 - val_loss: 929.1700\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.1343 - val_loss: 926.3925\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2740 - val_loss: 925.1470\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4707 - val_loss: 924.6848\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.1896 - val_loss: 924.4816\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0504 - val_loss: 924.3724\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.0578 - val_loss: 924.3788\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.0164 - val_loss: 924.3887\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0264 - val_loss: 924.3939\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.9957 - val_loss: 924.4498\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0056 - val_loss: 924.4150\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0054 - val_loss: 924.4013\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0077 - val_loss: 924.4537\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0249 - val_loss: 924.4253\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.0746 - val_loss: 924.4258\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.0369 - val_loss: 924.4500\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0126 - val_loss: 924.4211\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0183 - val_loss: 924.4756\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0260 - val_loss: 924.4510\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0242 - val_loss: 924.4355\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.0391 - val_loss: 924.3541\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.1892 - val_loss: 924.1633\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5664 - val_loss: 924.0968\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.1780 - val_loss: 924.1073\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.0295 - val_loss: 924.1589\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.7587 - val_loss: 922.8022\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 877.0922 - val_loss: 859.6917\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 845.8036 - val_loss: 845.6064\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 830.0232 - val_loss: 834.0908\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 810.5853 - val_loss: 823.7492\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 756.9561 - val_loss: 744.3821\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 701.2186 - val_loss: 675.4901\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 624.3801 - val_loss: 601.9667\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 564.6136 - val_loss: 555.6040\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 513.9523 - val_loss: 499.7558\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 467.3635 - val_loss: 453.6492\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 425.5628 - val_loss: 411.4229\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 384.7243 - val_loss: 372.4556\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 346.3499 - val_loss: 341.8730\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 311.9972 - val_loss: 299.6669\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 278.2057 - val_loss: 269.2832\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 248.3874 - val_loss: 239.5506\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 221.5313 - val_loss: 214.2221\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 194.9008 - val_loss: 188.3904\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 173.1316 - val_loss: 166.2838\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 150.7480 - val_loss: 144.6867\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 132.4400 - val_loss: 126.4180\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 116.9569 - val_loss: 115.7186\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 100.1053 - val_loss: 95.6405\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 87.1045 - val_loss: 83.9572\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.2175 - val_loss: 70.4758\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.8871 - val_loss: 61.8707\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.3860 - val_loss: 52.7384\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.9118 - val_loss: 44.1236\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.1716 - val_loss: 38.9894\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 34.8967 - val_loss: 31.7690\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.6792 - val_loss: 28.7927\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.8890 - val_loss: 23.9941\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 22.9516 - val_loss: 20.1881\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 17.4892 - val_loss: 16.6832\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.3102 - val_loss: 15.5408\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.3943 - val_loss: 13.5145\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2055 - val_loss: 10.3293\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0737 - val_loss: 11.1769\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5179 - val_loss: 8.6318\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6393 - val_loss: 7.5016\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8025 - val_loss: 5.7911\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7367 - val_loss: 4.8897\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4833 - val_loss: 5.6269\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2013 - val_loss: 5.5695\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5373 - val_loss: 2.9038\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4194 - val_loss: 3.0294\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0103 - val_loss: 3.2290\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2980 - val_loss: 2.0721\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8225 - val_loss: 2.3470\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7196 - val_loss: 1.4476\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1922 - val_loss: 1.2170\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0292 - val_loss: 1.3328\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0141 - val_loss: 1.8339\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2083 - val_loss: 1.7225\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4419 - val_loss: 1.3612\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9308 - val_loss: 0.8102\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7246 - val_loss: 0.6040\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.0523 - val_loss: 0.8437\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6751 - val_loss: 0.5545\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8374 - val_loss: 0.6893\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2150 - val_loss: 2.1097\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7252 - val_loss: 0.5932\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3990 - val_loss: 0.4214\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8447 - val_loss: 1.0294\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2621 - val_loss: 0.6310\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4587 - val_loss: 0.2431\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2832 - val_loss: 0.4458\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6408 - val_loss: 0.4630\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9281 - val_loss: 1.0436\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5417 - val_loss: 0.5310\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4197 - val_loss: 0.4519\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8547 - val_loss: 1.7293\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6606 - val_loss: 0.1337\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2539 - val_loss: 0.2844\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6916 - val_loss: 0.7376\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0085 - val_loss: 0.2678\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1726 - val_loss: 0.1379\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3544 - val_loss: 1.3404\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3529 - val_loss: 0.2798\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4754 - val_loss: 0.9435\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6380 - val_loss: 0.2567\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2250 - val_loss: 0.2627\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3635 - val_loss: 0.5155\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4343 - val_loss: 0.1392\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1133 - val_loss: 0.0957\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1311 - val_loss: 0.3021\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6653 - val_loss: 0.2890\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2572 - val_loss: 0.1303\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3205 - val_loss: 0.3876\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3615 - val_loss: 0.3515\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5310 - val_loss: 0.4307\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5433 - val_loss: 0.1436\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2706 - val_loss: 0.3426\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3466 - val_loss: 0.3332\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4249 - val_loss: 0.2872\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4600 - val_loss: 0.2132\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3721 - val_loss: 0.2779\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2506 - val_loss: 0.1040\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3887 - val_loss: 1.0971\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7298 - val_loss: 0.1760\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1899 - val_loss: 0.1559\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1591 - val_loss: 0.1572\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8570 - val_loss: 0.4003\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1636 - val_loss: 0.1795\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3606 - val_loss: 0.2482\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2474 - val_loss: 0.1359\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3586 - val_loss: 1.6397\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2786 - val_loss: 0.1791\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3032 - val_loss: 0.2391\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3461 - val_loss: 0.6465\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4289 - val_loss: 0.4289\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3589 - val_loss: 0.1530\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1805 - val_loss: 0.2812\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2523 - val_loss: 0.2821\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4141 - val_loss: 0.0675\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1434 - val_loss: 0.1818\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4983 - val_loss: 0.2567\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2062 - val_loss: 0.4475\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3071 - val_loss: 0.3950\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3190 - val_loss: 0.7287\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1915 - val_loss: 0.2044\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2003 - val_loss: 0.1901\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2199 - val_loss: 0.3893\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7021 - val_loss: 0.1451\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1162 - val_loss: 0.1691\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2096 - val_loss: 0.2966\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1796 - val_loss: 0.4132\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3883 - val_loss: 0.2402\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3600 - val_loss: 0.1827\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1895 - val_loss: 0.1025\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1963 - val_loss: 0.2427\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3956 - val_loss: 0.4609\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2915 - val_loss: 0.3149\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1922 - val_loss: 0.0900\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1545 - val_loss: 0.2478\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3933 - val_loss: 0.5212\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1792 - val_loss: 0.4773\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6777 - val_loss: 0.1393\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0877 - val_loss: 0.0863\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0524 - val_loss: 0.0421\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0611 - val_loss: 0.0778\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1321 - val_loss: 0.0614\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2012 - val_loss: 0.2135\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1685 - val_loss: 0.2100\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2583 - val_loss: 0.1470\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1831 - val_loss: 0.1598\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2671 - val_loss: 0.1948\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3720 - val_loss: 0.4166\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1921 - val_loss: 0.1804\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1973 - val_loss: 0.5445\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3188 - val_loss: 0.1032\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1951 - val_loss: 0.2859\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1829 - val_loss: 0.2082\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1984 - val_loss: 0.1004\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1379 - val_loss: 0.1361\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3650 - val_loss: 0.5594\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3203 - val_loss: 0.0541\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1542 - val_loss: 0.1669\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2515 - val_loss: 0.2046\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2614 - val_loss: 0.2535\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1530 - val_loss: 0.1540\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2309 - val_loss: 0.5517\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1817 - val_loss: 0.0868\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2793 - val_loss: 0.1976\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2138 - val_loss: 0.0880\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.08805028911160191\n",
      "Mean Absolute Error (MAE): 0.22569806254688576\n",
      "Root Mean Squared Error (RMSE): 0.2967326896578837\n",
      "Time taken: 1164.6016499996185\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1387.4783 - val_loss: 1253.6694\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1221.0045 - val_loss: 1149.2645\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1133.4827 - val_loss: 1073.6835\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1068.2161 - val_loss: 1017.3701\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1020.2147 - val_loss: 976.6744\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 985.5757 - val_loss: 947.6070\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 961.3738 - val_loss: 927.7449\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 945.2290 - val_loss: 914.9203\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 935.2177 - val_loss: 907.4403\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 929.4755 - val_loss: 903.2773\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.4466 - val_loss: 901.2157\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.0026 - val_loss: 900.3334\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.4213 - val_loss: 900.0231\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.2151 - val_loss: 899.9062\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 924.1713 - val_loss: 899.8689\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1218 - val_loss: 899.8625\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1403 - val_loss: 899.8404\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1232 - val_loss: 899.8669\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1334 - val_loss: 899.8972\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1373 - val_loss: 899.9222\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1323 - val_loss: 899.9246\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.1382 - val_loss: 899.9148\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1306 - val_loss: 899.9182\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.4873 - val_loss: 900.1677\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1429 - val_loss: 900.0114\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1537 - val_loss: 899.9695\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1266 - val_loss: 899.9012\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1223 - val_loss: 899.9564\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1229 - val_loss: 899.8876\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1278 - val_loss: 899.9025\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.3718 - val_loss: 899.9366\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.6128 - val_loss: 853.0118\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 864.2869 - val_loss: 829.7750\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 844.0419 - val_loss: 812.2321\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 822.2328 - val_loss: 774.6994\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 784.0882 - val_loss: 741.6913\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 752.6770 - val_loss: 712.2065\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 717.7975 - val_loss: 696.4358\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 672.6323 - val_loss: 623.7484\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 606.8643 - val_loss: 559.4697\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 552.9258 - val_loss: 509.2893\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 508.0782 - val_loss: 465.5828\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 464.7426 - val_loss: 422.6961\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 420.6672 - val_loss: 384.7573\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 383.1891 - val_loss: 352.6139\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 346.5555 - val_loss: 315.3445\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 318.4846 - val_loss: 286.4679\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 283.3344 - val_loss: 254.7940\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 254.3601 - val_loss: 229.4278\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 229.3259 - val_loss: 202.4746\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 204.1970 - val_loss: 181.8384\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 181.2898 - val_loss: 160.0573\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 161.2134 - val_loss: 144.4350\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 146.5179 - val_loss: 126.3456\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 126.7675 - val_loss: 111.7715\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 111.7150 - val_loss: 96.2730\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 97.7171 - val_loss: 83.8776\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 86.2799 - val_loss: 75.3058\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.4687 - val_loss: 62.5901\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 64.8550 - val_loss: 58.3455\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 55.6529 - val_loss: 46.6208\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.3556 - val_loss: 44.6841\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.2428 - val_loss: 34.6222\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.5965 - val_loss: 29.1717\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.2581 - val_loss: 25.7676\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.3488 - val_loss: 23.5484\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.5449 - val_loss: 22.0400\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.6879 - val_loss: 15.1691\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.6382 - val_loss: 13.9253\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.6462 - val_loss: 12.4879\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6257 - val_loss: 10.0873\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1663 - val_loss: 7.7141\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0708 - val_loss: 7.0322\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8440 - val_loss: 7.6647\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.9109 - val_loss: 5.6314\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8251 - val_loss: 4.6793\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8527 - val_loss: 3.3658\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3356 - val_loss: 6.8270\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7216 - val_loss: 2.6322\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0456 - val_loss: 3.1703\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5482 - val_loss: 4.0610\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6886 - val_loss: 2.1937\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3175 - val_loss: 1.6781\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8741 - val_loss: 1.4970\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5815 - val_loss: 2.0301\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8474 - val_loss: 2.9654\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4490 - val_loss: 1.4540\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1797 - val_loss: 1.1672\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3752 - val_loss: 1.1906\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9428 - val_loss: 3.0615\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8035 - val_loss: 1.1974\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8117 - val_loss: 0.9513\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7806 - val_loss: 0.7865\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0827 - val_loss: 0.7993\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.3135 - val_loss: 3.2853\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0744 - val_loss: 0.7223\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5759 - val_loss: 0.5935\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5378 - val_loss: 0.4431\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9067 - val_loss: 0.9298\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5845 - val_loss: 0.5338\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4403 - val_loss: 0.5235\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9028 - val_loss: 1.3756\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5392 - val_loss: 0.4410\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4235 - val_loss: 0.5806\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5914 - val_loss: 0.4949\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5018 - val_loss: 1.0830\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5619 - val_loss: 0.3183\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6488 - val_loss: 2.6246\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9674 - val_loss: 1.0818\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1121 - val_loss: 0.8018\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3469 - val_loss: 0.2221\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1942 - val_loss: 2.1646\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5734 - val_loss: 0.2548\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3582 - val_loss: 0.6279\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5225 - val_loss: 0.2922\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4171 - val_loss: 0.2020\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4472 - val_loss: 1.1644\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8053 - val_loss: 0.2899\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2038 - val_loss: 0.2501\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2096 - val_loss: 0.2868\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4063 - val_loss: 0.1992\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6441 - val_loss: 0.5669\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3690 - val_loss: 0.2203\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6514 - val_loss: 0.2836\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9183 - val_loss: 0.9128\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2906 - val_loss: 0.1791\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3163 - val_loss: 0.9124\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4469 - val_loss: 0.2770\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7457 - val_loss: 1.3517\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4669 - val_loss: 0.2657\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2349 - val_loss: 0.1866\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9227 - val_loss: 1.2871\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4607 - val_loss: 0.1613\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3030 - val_loss: 0.1517\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5322 - val_loss: 0.6443\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3206 - val_loss: 0.2252\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2433 - val_loss: 0.3794\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7675 - val_loss: 0.0976\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3017 - val_loss: 0.2889\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3193 - val_loss: 0.1877\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4793 - val_loss: 0.3456\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5177 - val_loss: 0.2536\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2410 - val_loss: 0.3032\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5535 - val_loss: 0.3997\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4551 - val_loss: 0.5312\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2223 - val_loss: 0.2234\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5016 - val_loss: 1.0202\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4923 - val_loss: 0.2802\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2071 - val_loss: 0.1702\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4543 - val_loss: 0.2181\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3381 - val_loss: 0.1438\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2820 - val_loss: 0.3562\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4474 - val_loss: 0.1216\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1452 - val_loss: 0.1747\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2336 - val_loss: 0.2576\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1420 - val_loss: 0.1239\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0819 - val_loss: 0.0706\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1967 - val_loss: 0.3347\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2581 - val_loss: 0.1465\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3034 - val_loss: 0.3331\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5303 - val_loss: 0.1840\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1335 - val_loss: 0.1844\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3634 - val_loss: 0.2827\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5919 - val_loss: 1.7486\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4044 - val_loss: 0.0694\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0943 - val_loss: 0.0808\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1939 - val_loss: 0.3856\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3642 - val_loss: 0.4070\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2612 - val_loss: 0.4569\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4742 - val_loss: 0.3653\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1785 - val_loss: 0.1129\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1456 - val_loss: 0.1612\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2873 - val_loss: 0.5939\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6868 - val_loss: 0.0900\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0846 - val_loss: 0.0718\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1233 - val_loss: 0.1815\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3864 - val_loss: 0.1510\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1749 - val_loss: 0.1003\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4130 - val_loss: 0.4670\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2406 - val_loss: 0.2433\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2813 - val_loss: 0.1812\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2057 - val_loss: 0.1392\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2566 - val_loss: 0.5356\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4245 - val_loss: 0.0727\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1404 - val_loss: 0.4069\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3581 - val_loss: 0.1852\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2683 - val_loss: 0.4094\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2265 - val_loss: 0.5842\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2982 - val_loss: 0.0977\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1345 - val_loss: 0.0979\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2467 - val_loss: 0.4373\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3962 - val_loss: 0.3858\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2378 - val_loss: 0.2787\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1567 - val_loss: 0.1805\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1806 - val_loss: 0.2521\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5537 - val_loss: 0.1614\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1340 - val_loss: 0.0999\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1366 - val_loss: 0.1933\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4351 - val_loss: 0.1170\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1693 - val_loss: 0.3634\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.3633124819884281\n",
      "Mean Absolute Error (MAE): 0.4032082831532975\n",
      "Root Mean Squared Error (RMSE): 0.6027540808558894\n",
      "Time taken: 1174.7868056297302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_6568\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.407695  0.471358  0.638510  1269.654929\n",
      "1        2  0.188202  0.330764  0.433823  1252.230667\n",
      "2        3  0.476212  0.513896  0.690081  1273.233322\n",
      "3        4  0.088050  0.225698  0.296733  1164.601650\n",
      "4        5  0.363312  0.403208  0.602754  1174.786806\n",
      "5  Average  0.304694  0.388985  0.532380  1226.901475\n",
      "Results saved to 'DL_Result_PL_model_1_smoothing2_Reg1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_Reg1.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_smoothing2_Reg1.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
