{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.255262</td>\n",
       "      <td>71.971142</td>\n",
       "      <td>59.297033</td>\n",
       "      <td>70.168324</td>\n",
       "      <td>72.223257</td>\n",
       "      <td>78.156174</td>\n",
       "      <td>70.720922</td>\n",
       "      <td>71.671559</td>\n",
       "      <td>66.034745</td>\n",
       "      <td>68.503224</td>\n",
       "      <td>...</td>\n",
       "      <td>68.551835</td>\n",
       "      <td>66.226435</td>\n",
       "      <td>61.979756</td>\n",
       "      <td>61.124430</td>\n",
       "      <td>73.919407</td>\n",
       "      <td>69.627583</td>\n",
       "      <td>69.312761</td>\n",
       "      <td>70.642658</td>\n",
       "      <td>60.969641</td>\n",
       "      <td>71.643253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.029172</td>\n",
       "      <td>72.095736</td>\n",
       "      <td>59.195081</td>\n",
       "      <td>70.251247</td>\n",
       "      <td>72.172055</td>\n",
       "      <td>78.132133</td>\n",
       "      <td>70.750461</td>\n",
       "      <td>71.889199</td>\n",
       "      <td>66.181062</td>\n",
       "      <td>68.578317</td>\n",
       "      <td>...</td>\n",
       "      <td>68.400324</td>\n",
       "      <td>66.206345</td>\n",
       "      <td>62.317160</td>\n",
       "      <td>61.257023</td>\n",
       "      <td>73.842502</td>\n",
       "      <td>69.644318</td>\n",
       "      <td>69.143555</td>\n",
       "      <td>70.733665</td>\n",
       "      <td>60.837893</td>\n",
       "      <td>71.609484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.807358</td>\n",
       "      <td>72.215594</td>\n",
       "      <td>59.102536</td>\n",
       "      <td>70.334800</td>\n",
       "      <td>72.118766</td>\n",
       "      <td>78.106993</td>\n",
       "      <td>70.778609</td>\n",
       "      <td>72.106459</td>\n",
       "      <td>66.327732</td>\n",
       "      <td>68.653060</td>\n",
       "      <td>...</td>\n",
       "      <td>68.255380</td>\n",
       "      <td>66.185249</td>\n",
       "      <td>62.651930</td>\n",
       "      <td>61.392031</td>\n",
       "      <td>73.768352</td>\n",
       "      <td>69.654955</td>\n",
       "      <td>68.974929</td>\n",
       "      <td>70.827779</td>\n",
       "      <td>60.708759</td>\n",
       "      <td>71.577138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.590422</td>\n",
       "      <td>72.330512</td>\n",
       "      <td>59.019202</td>\n",
       "      <td>70.419252</td>\n",
       "      <td>72.063870</td>\n",
       "      <td>78.081097</td>\n",
       "      <td>70.804871</td>\n",
       "      <td>72.323391</td>\n",
       "      <td>66.474047</td>\n",
       "      <td>68.727520</td>\n",
       "      <td>...</td>\n",
       "      <td>68.117018</td>\n",
       "      <td>66.163235</td>\n",
       "      <td>62.984053</td>\n",
       "      <td>61.529815</td>\n",
       "      <td>73.696280</td>\n",
       "      <td>69.659422</td>\n",
       "      <td>68.807551</td>\n",
       "      <td>70.924403</td>\n",
       "      <td>60.581964</td>\n",
       "      <td>71.546492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.378722</td>\n",
       "      <td>72.440584</td>\n",
       "      <td>58.944518</td>\n",
       "      <td>70.504615</td>\n",
       "      <td>72.007938</td>\n",
       "      <td>78.054497</td>\n",
       "      <td>70.828614</td>\n",
       "      <td>72.539718</td>\n",
       "      <td>66.619117</td>\n",
       "      <td>68.801921</td>\n",
       "      <td>...</td>\n",
       "      <td>67.985182</td>\n",
       "      <td>66.140380</td>\n",
       "      <td>63.313694</td>\n",
       "      <td>61.670413</td>\n",
       "      <td>73.625580</td>\n",
       "      <td>69.657956</td>\n",
       "      <td>68.641912</td>\n",
       "      <td>71.022581</td>\n",
       "      <td>60.456878</td>\n",
       "      <td>71.517763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>73.098106</td>\n",
       "      <td>72.579984</td>\n",
       "      <td>70.393482</td>\n",
       "      <td>57.539933</td>\n",
       "      <td>76.493660</td>\n",
       "      <td>77.698430</td>\n",
       "      <td>73.453426</td>\n",
       "      <td>68.863653</td>\n",
       "      <td>71.662133</td>\n",
       "      <td>68.616475</td>\n",
       "      <td>...</td>\n",
       "      <td>66.113821</td>\n",
       "      <td>69.106403</td>\n",
       "      <td>65.986037</td>\n",
       "      <td>52.906771</td>\n",
       "      <td>72.026893</td>\n",
       "      <td>70.616229</td>\n",
       "      <td>71.559465</td>\n",
       "      <td>75.327743</td>\n",
       "      <td>65.497743</td>\n",
       "      <td>61.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>73.069455</td>\n",
       "      <td>72.686191</td>\n",
       "      <td>70.593724</td>\n",
       "      <td>57.422708</td>\n",
       "      <td>76.310721</td>\n",
       "      <td>77.797643</td>\n",
       "      <td>73.449306</td>\n",
       "      <td>68.746579</td>\n",
       "      <td>71.740129</td>\n",
       "      <td>68.624573</td>\n",
       "      <td>...</td>\n",
       "      <td>65.951813</td>\n",
       "      <td>69.014658</td>\n",
       "      <td>65.824532</td>\n",
       "      <td>52.921059</td>\n",
       "      <td>71.964983</td>\n",
       "      <td>70.654036</td>\n",
       "      <td>71.570827</td>\n",
       "      <td>75.516862</td>\n",
       "      <td>65.328859</td>\n",
       "      <td>61.156410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>73.044731</td>\n",
       "      <td>72.796004</td>\n",
       "      <td>70.796814</td>\n",
       "      <td>57.304143</td>\n",
       "      <td>76.124633</td>\n",
       "      <td>77.899235</td>\n",
       "      <td>73.448736</td>\n",
       "      <td>68.627163</td>\n",
       "      <td>71.822711</td>\n",
       "      <td>68.629821</td>\n",
       "      <td>...</td>\n",
       "      <td>65.790041</td>\n",
       "      <td>68.920597</td>\n",
       "      <td>65.658370</td>\n",
       "      <td>52.938117</td>\n",
       "      <td>71.897161</td>\n",
       "      <td>70.690490</td>\n",
       "      <td>71.585216</td>\n",
       "      <td>75.707701</td>\n",
       "      <td>65.159436</td>\n",
       "      <td>61.274501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>73.024322</td>\n",
       "      <td>72.909059</td>\n",
       "      <td>71.003291</td>\n",
       "      <td>57.183712</td>\n",
       "      <td>75.934742</td>\n",
       "      <td>78.003086</td>\n",
       "      <td>73.451758</td>\n",
       "      <td>68.505253</td>\n",
       "      <td>71.909019</td>\n",
       "      <td>68.631663</td>\n",
       "      <td>...</td>\n",
       "      <td>65.628825</td>\n",
       "      <td>68.823687</td>\n",
       "      <td>65.488250</td>\n",
       "      <td>52.958021</td>\n",
       "      <td>71.823691</td>\n",
       "      <td>70.725859</td>\n",
       "      <td>71.602832</td>\n",
       "      <td>75.900074</td>\n",
       "      <td>64.989387</td>\n",
       "      <td>61.396096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>73.008414</td>\n",
       "      <td>73.024663</td>\n",
       "      <td>71.213720</td>\n",
       "      <td>57.061155</td>\n",
       "      <td>75.740776</td>\n",
       "      <td>78.108850</td>\n",
       "      <td>73.458031</td>\n",
       "      <td>68.380598</td>\n",
       "      <td>71.998276</td>\n",
       "      <td>68.629823</td>\n",
       "      <td>...</td>\n",
       "      <td>65.468769</td>\n",
       "      <td>68.723471</td>\n",
       "      <td>65.315066</td>\n",
       "      <td>52.980600</td>\n",
       "      <td>71.744712</td>\n",
       "      <td>70.760411</td>\n",
       "      <td>71.623714</td>\n",
       "      <td>76.093722</td>\n",
       "      <td>64.818402</td>\n",
       "      <td>61.520882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     72.255262  71.971142  59.297033  70.168324  72.223257  78.156174   \n",
       "1     72.029172  72.095736  59.195081  70.251247  72.172055  78.132133   \n",
       "2     71.807358  72.215594  59.102536  70.334800  72.118766  78.106993   \n",
       "3     71.590422  72.330512  59.019202  70.419252  72.063870  78.081097   \n",
       "4     71.378722  72.440584  58.944518  70.504615  72.007938  78.054497   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.098106  72.579984  70.393482  57.539933  76.493660  77.698430   \n",
       "2439  73.069455  72.686191  70.593724  57.422708  76.310721  77.797643   \n",
       "2440  73.044731  72.796004  70.796814  57.304143  76.124633  77.899235   \n",
       "2441  73.024322  72.909059  71.003291  57.183712  75.934742  78.003086   \n",
       "2442  73.008414  73.024663  71.213720  57.061155  75.740776  78.108850   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     70.720922  71.671559  66.034745  68.503224  ...  68.551835  66.226435   \n",
       "1     70.750461  71.889199  66.181062  68.578317  ...  68.400324  66.206345   \n",
       "2     70.778609  72.106459  66.327732  68.653060  ...  68.255380  66.185249   \n",
       "3     70.804871  72.323391  66.474047  68.727520  ...  68.117018  66.163235   \n",
       "4     70.828614  72.539718  66.619117  68.801921  ...  67.985182  66.140380   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  73.453426  68.863653  71.662133  68.616475  ...  66.113821  69.106403   \n",
       "2439  73.449306  68.746579  71.740129  68.624573  ...  65.951813  69.014658   \n",
       "2440  73.448736  68.627163  71.822711  68.629821  ...  65.790041  68.920597   \n",
       "2441  73.451758  68.505253  71.909019  68.631663  ...  65.628825  68.823687   \n",
       "2442  73.458031  68.380598  71.998276  68.629823  ...  65.468769  68.723471   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     61.979756  61.124430  73.919407  69.627583  69.312761  70.642658   \n",
       "1     62.317160  61.257023  73.842502  69.644318  69.143555  70.733665   \n",
       "2     62.651930  61.392031  73.768352  69.654955  68.974929  70.827779   \n",
       "3     62.984053  61.529815  73.696280  69.659422  68.807551  70.924403   \n",
       "4     63.313694  61.670413  73.625580  69.657956  68.641912  71.022581   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  65.986037  52.906771  72.026893  70.616229  71.559465  75.327743   \n",
       "2439  65.824532  52.921059  71.964983  70.654036  71.570827  75.516862   \n",
       "2440  65.658370  52.938117  71.897161  70.690490  71.585216  75.707701   \n",
       "2441  65.488250  52.958021  71.823691  70.725859  71.602832  75.900074   \n",
       "2442  65.315066  52.980600  71.744712  70.760411  71.623714  76.093722   \n",
       "\n",
       "             46         47  \n",
       "0     60.969641  71.643253  \n",
       "1     60.837893  71.609484  \n",
       "2     60.708759  71.577138  \n",
       "3     60.581964  71.546492  \n",
       "4     60.456878  71.517763  \n",
       "...         ...        ...  \n",
       "2438  65.497743  61.041995  \n",
       "2439  65.328859  61.156410  \n",
       "2440  65.159436  61.274501  \n",
       "2441  64.989387  61.396096  \n",
       "2442  64.818402  61.520882  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.255262</td>\n",
       "      <td>71.971142</td>\n",
       "      <td>59.297033</td>\n",
       "      <td>70.168324</td>\n",
       "      <td>72.223257</td>\n",
       "      <td>78.156174</td>\n",
       "      <td>70.720922</td>\n",
       "      <td>71.671559</td>\n",
       "      <td>66.034745</td>\n",
       "      <td>68.503224</td>\n",
       "      <td>...</td>\n",
       "      <td>68.551835</td>\n",
       "      <td>66.226435</td>\n",
       "      <td>61.979756</td>\n",
       "      <td>61.124430</td>\n",
       "      <td>73.919407</td>\n",
       "      <td>69.627583</td>\n",
       "      <td>69.312761</td>\n",
       "      <td>70.642658</td>\n",
       "      <td>60.969641</td>\n",
       "      <td>71.643253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.029172</td>\n",
       "      <td>72.095736</td>\n",
       "      <td>59.195081</td>\n",
       "      <td>70.251247</td>\n",
       "      <td>72.172055</td>\n",
       "      <td>78.132133</td>\n",
       "      <td>70.750461</td>\n",
       "      <td>71.889199</td>\n",
       "      <td>66.181062</td>\n",
       "      <td>68.578317</td>\n",
       "      <td>...</td>\n",
       "      <td>68.400324</td>\n",
       "      <td>66.206345</td>\n",
       "      <td>62.317160</td>\n",
       "      <td>61.257023</td>\n",
       "      <td>73.842502</td>\n",
       "      <td>69.644318</td>\n",
       "      <td>69.143555</td>\n",
       "      <td>70.733665</td>\n",
       "      <td>60.837893</td>\n",
       "      <td>71.609484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.807358</td>\n",
       "      <td>72.215594</td>\n",
       "      <td>59.102536</td>\n",
       "      <td>70.334800</td>\n",
       "      <td>72.118766</td>\n",
       "      <td>78.106993</td>\n",
       "      <td>70.778609</td>\n",
       "      <td>72.106459</td>\n",
       "      <td>66.327732</td>\n",
       "      <td>68.653060</td>\n",
       "      <td>...</td>\n",
       "      <td>68.255380</td>\n",
       "      <td>66.185249</td>\n",
       "      <td>62.651930</td>\n",
       "      <td>61.392031</td>\n",
       "      <td>73.768352</td>\n",
       "      <td>69.654955</td>\n",
       "      <td>68.974929</td>\n",
       "      <td>70.827779</td>\n",
       "      <td>60.708759</td>\n",
       "      <td>71.577138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.590422</td>\n",
       "      <td>72.330512</td>\n",
       "      <td>59.019202</td>\n",
       "      <td>70.419252</td>\n",
       "      <td>72.063870</td>\n",
       "      <td>78.081097</td>\n",
       "      <td>70.804871</td>\n",
       "      <td>72.323391</td>\n",
       "      <td>66.474047</td>\n",
       "      <td>68.727520</td>\n",
       "      <td>...</td>\n",
       "      <td>68.117018</td>\n",
       "      <td>66.163235</td>\n",
       "      <td>62.984053</td>\n",
       "      <td>61.529815</td>\n",
       "      <td>73.696280</td>\n",
       "      <td>69.659422</td>\n",
       "      <td>68.807551</td>\n",
       "      <td>70.924403</td>\n",
       "      <td>60.581964</td>\n",
       "      <td>71.546492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.378722</td>\n",
       "      <td>72.440584</td>\n",
       "      <td>58.944518</td>\n",
       "      <td>70.504615</td>\n",
       "      <td>72.007938</td>\n",
       "      <td>78.054497</td>\n",
       "      <td>70.828614</td>\n",
       "      <td>72.539718</td>\n",
       "      <td>66.619117</td>\n",
       "      <td>68.801921</td>\n",
       "      <td>...</td>\n",
       "      <td>67.985182</td>\n",
       "      <td>66.140380</td>\n",
       "      <td>63.313694</td>\n",
       "      <td>61.670413</td>\n",
       "      <td>73.625580</td>\n",
       "      <td>69.657956</td>\n",
       "      <td>68.641912</td>\n",
       "      <td>71.022581</td>\n",
       "      <td>60.456878</td>\n",
       "      <td>71.517763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>73.098106</td>\n",
       "      <td>72.579984</td>\n",
       "      <td>70.393482</td>\n",
       "      <td>57.539933</td>\n",
       "      <td>76.493660</td>\n",
       "      <td>77.698430</td>\n",
       "      <td>73.453426</td>\n",
       "      <td>68.863653</td>\n",
       "      <td>71.662133</td>\n",
       "      <td>68.616475</td>\n",
       "      <td>...</td>\n",
       "      <td>66.113821</td>\n",
       "      <td>69.106403</td>\n",
       "      <td>65.986037</td>\n",
       "      <td>52.906771</td>\n",
       "      <td>72.026893</td>\n",
       "      <td>70.616229</td>\n",
       "      <td>71.559465</td>\n",
       "      <td>75.327743</td>\n",
       "      <td>65.497743</td>\n",
       "      <td>61.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>73.069455</td>\n",
       "      <td>72.686191</td>\n",
       "      <td>70.593724</td>\n",
       "      <td>57.422708</td>\n",
       "      <td>76.310721</td>\n",
       "      <td>77.797643</td>\n",
       "      <td>73.449306</td>\n",
       "      <td>68.746579</td>\n",
       "      <td>71.740129</td>\n",
       "      <td>68.624573</td>\n",
       "      <td>...</td>\n",
       "      <td>65.951813</td>\n",
       "      <td>69.014658</td>\n",
       "      <td>65.824532</td>\n",
       "      <td>52.921059</td>\n",
       "      <td>71.964983</td>\n",
       "      <td>70.654036</td>\n",
       "      <td>71.570827</td>\n",
       "      <td>75.516862</td>\n",
       "      <td>65.328859</td>\n",
       "      <td>61.156410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>73.044731</td>\n",
       "      <td>72.796004</td>\n",
       "      <td>70.796814</td>\n",
       "      <td>57.304143</td>\n",
       "      <td>76.124633</td>\n",
       "      <td>77.899235</td>\n",
       "      <td>73.448736</td>\n",
       "      <td>68.627163</td>\n",
       "      <td>71.822711</td>\n",
       "      <td>68.629821</td>\n",
       "      <td>...</td>\n",
       "      <td>65.790041</td>\n",
       "      <td>68.920597</td>\n",
       "      <td>65.658370</td>\n",
       "      <td>52.938117</td>\n",
       "      <td>71.897161</td>\n",
       "      <td>70.690490</td>\n",
       "      <td>71.585216</td>\n",
       "      <td>75.707701</td>\n",
       "      <td>65.159436</td>\n",
       "      <td>61.274501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>73.024322</td>\n",
       "      <td>72.909059</td>\n",
       "      <td>71.003291</td>\n",
       "      <td>57.183712</td>\n",
       "      <td>75.934742</td>\n",
       "      <td>78.003086</td>\n",
       "      <td>73.451758</td>\n",
       "      <td>68.505253</td>\n",
       "      <td>71.909019</td>\n",
       "      <td>68.631663</td>\n",
       "      <td>...</td>\n",
       "      <td>65.628825</td>\n",
       "      <td>68.823687</td>\n",
       "      <td>65.488250</td>\n",
       "      <td>52.958021</td>\n",
       "      <td>71.823691</td>\n",
       "      <td>70.725859</td>\n",
       "      <td>71.602832</td>\n",
       "      <td>75.900074</td>\n",
       "      <td>64.989387</td>\n",
       "      <td>61.396096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>73.008414</td>\n",
       "      <td>73.024663</td>\n",
       "      <td>71.213720</td>\n",
       "      <td>57.061155</td>\n",
       "      <td>75.740776</td>\n",
       "      <td>78.108850</td>\n",
       "      <td>73.458031</td>\n",
       "      <td>68.380598</td>\n",
       "      <td>71.998276</td>\n",
       "      <td>68.629823</td>\n",
       "      <td>...</td>\n",
       "      <td>65.468769</td>\n",
       "      <td>68.723471</td>\n",
       "      <td>65.315066</td>\n",
       "      <td>52.980600</td>\n",
       "      <td>71.744712</td>\n",
       "      <td>70.760411</td>\n",
       "      <td>71.623714</td>\n",
       "      <td>76.093722</td>\n",
       "      <td>64.818402</td>\n",
       "      <td>61.520882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     72.255262  71.971142  59.297033  70.168324  72.223257  78.156174   \n",
       "1     72.029172  72.095736  59.195081  70.251247  72.172055  78.132133   \n",
       "2     71.807358  72.215594  59.102536  70.334800  72.118766  78.106993   \n",
       "3     71.590422  72.330512  59.019202  70.419252  72.063870  78.081097   \n",
       "4     71.378722  72.440584  58.944518  70.504615  72.007938  78.054497   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.098106  72.579984  70.393482  57.539933  76.493660  77.698430   \n",
       "2439  73.069455  72.686191  70.593724  57.422708  76.310721  77.797643   \n",
       "2440  73.044731  72.796004  70.796814  57.304143  76.124633  77.899235   \n",
       "2441  73.024322  72.909059  71.003291  57.183712  75.934742  78.003086   \n",
       "2442  73.008414  73.024663  71.213720  57.061155  75.740776  78.108850   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     70.720922  71.671559  66.034745  68.503224  ...  68.551835  66.226435   \n",
       "1     70.750461  71.889199  66.181062  68.578317  ...  68.400324  66.206345   \n",
       "2     70.778609  72.106459  66.327732  68.653060  ...  68.255380  66.185249   \n",
       "3     70.804871  72.323391  66.474047  68.727520  ...  68.117018  66.163235   \n",
       "4     70.828614  72.539718  66.619117  68.801921  ...  67.985182  66.140380   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  73.453426  68.863653  71.662133  68.616475  ...  66.113821  69.106403   \n",
       "2439  73.449306  68.746579  71.740129  68.624573  ...  65.951813  69.014658   \n",
       "2440  73.448736  68.627163  71.822711  68.629821  ...  65.790041  68.920597   \n",
       "2441  73.451758  68.505253  71.909019  68.631663  ...  65.628825  68.823687   \n",
       "2442  73.458031  68.380598  71.998276  68.629823  ...  65.468769  68.723471   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     61.979756  61.124430  73.919407  69.627583  69.312761  70.642658   \n",
       "1     62.317160  61.257023  73.842502  69.644318  69.143555  70.733665   \n",
       "2     62.651930  61.392031  73.768352  69.654955  68.974929  70.827779   \n",
       "3     62.984053  61.529815  73.696280  69.659422  68.807551  70.924403   \n",
       "4     63.313694  61.670413  73.625580  69.657956  68.641912  71.022581   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  65.986037  52.906771  72.026893  70.616229  71.559465  75.327743   \n",
       "2439  65.824532  52.921059  71.964983  70.654036  71.570827  75.516862   \n",
       "2440  65.658370  52.938117  71.897161  70.690490  71.585216  75.707701   \n",
       "2441  65.488250  52.958021  71.823691  70.725859  71.602832  75.900074   \n",
       "2442  65.315066  52.980600  71.744712  70.760411  71.623714  76.093722   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     60.969641  71.643253  \n",
       "1     60.837893  71.609484  \n",
       "2     60.708759  71.577138  \n",
       "3     60.581964  71.546492  \n",
       "4     60.456878  71.517763  \n",
       "...         ...        ...  \n",
       "2438  65.497743  61.041995  \n",
       "2439  65.328859  61.156410  \n",
       "2440  65.159436  61.274501  \n",
       "2441  64.989387  61.396096  \n",
       "2442  64.818402  61.520882  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1405.8915 - val_loss: 1274.4379\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1241.6520 - val_loss: 1169.3984\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1150.4805 - val_loss: 1092.0927\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1082.6490 - val_loss: 1033.9764\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1031.8917 - val_loss: 990.6860\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 994.4138 - val_loss: 959.3542\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 967.6710 - val_loss: 937.2161\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 949.3055 - val_loss: 922.5534\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 937.3883 - val_loss: 913.3653\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 930.3199 - val_loss: 908.2681\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.4814 - val_loss: 905.5892\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.5771 - val_loss: 904.3436\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.7068 - val_loss: 903.8539\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.3334 - val_loss: 903.6572\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2481 - val_loss: 903.5902\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2383 - val_loss: 903.5900\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2412 - val_loss: 903.5615\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.1805 - val_loss: 903.5746\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2528 - val_loss: 903.6050\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2071 - val_loss: 903.5802\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2046 - val_loss: 903.5978\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.1916 - val_loss: 903.5983\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.1887 - val_loss: 903.5925\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.2000 - val_loss: 903.5959\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.2166 - val_loss: 903.5816\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.2186 - val_loss: 903.6034\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.2164 - val_loss: 903.6043\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.1935 - val_loss: 903.5732\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 923.2066 - val_loss: 903.5967\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 923.1740 - val_loss: 903.6312\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.1981 - val_loss: 903.5813\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.1961 - val_loss: 903.6041\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2120 - val_loss: 903.6205\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2062 - val_loss: 903.5953\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.1887 - val_loss: 903.5968\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 923.1954 - val_loss: 903.6021\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.1904 - val_loss: 903.5976\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.3461 - val_loss: 904.8752\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.7532 - val_loss: 903.6089\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.0061 - val_loss: 902.3355\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 898.8227 - val_loss: 849.6627\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 849.3322 - val_loss: 821.7562\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 820.9562 - val_loss: 794.9075\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 792.8112 - val_loss: 763.9285\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 755.0941 - val_loss: 723.6443\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 717.4556 - val_loss: 692.0745\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 684.0851 - val_loss: 660.4507\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 650.5613 - val_loss: 625.0472\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 615.4331 - val_loss: 588.2917\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 577.9073 - val_loss: 551.6456\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 538.1694 - val_loss: 509.0865\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 501.7121 - val_loss: 473.3056\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 465.1086 - val_loss: 441.0276\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 433.4386 - val_loss: 410.4536\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 403.3869 - val_loss: 380.2324\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 374.5051 - val_loss: 353.8600\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 346.7667 - val_loss: 326.6653\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 321.3536 - val_loss: 301.9083\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 297.0121 - val_loss: 278.9792\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 275.2618 - val_loss: 256.2806\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 253.1714 - val_loss: 239.6090\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 234.4160 - val_loss: 218.9186\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 216.5592 - val_loss: 201.6471\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 199.1822 - val_loss: 203.6042\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 183.3283 - val_loss: 171.8501\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 171.3658 - val_loss: 159.2626\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 157.1101 - val_loss: 149.5228\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 143.9771 - val_loss: 140.1929\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 131.7739 - val_loss: 123.1949\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 123.3298 - val_loss: 115.6365\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 112.6864 - val_loss: 104.2584\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.9092 - val_loss: 95.0696\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 98.1598 - val_loss: 87.4880\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 85.9294 - val_loss: 81.1785\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 78.3790 - val_loss: 74.7384\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 71.5942 - val_loss: 68.8246\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.9010 - val_loss: 64.7764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.5210 - val_loss: 54.6429\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.0452 - val_loss: 48.5854\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.9351 - val_loss: 43.8719\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.5305 - val_loss: 41.5109\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.1336 - val_loss: 37.1174\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.8560 - val_loss: 41.9167\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.3492 - val_loss: 30.1851\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.2900 - val_loss: 25.9949\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.4349 - val_loss: 25.2061\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.8042 - val_loss: 21.4885\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.5907 - val_loss: 22.2065\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.8347 - val_loss: 25.2336\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.7122 - val_loss: 15.8978\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 16.0814 - val_loss: 16.3070\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 14.0154 - val_loss: 16.4977\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 13.8338 - val_loss: 12.7605\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 12.5508 - val_loss: 16.4454\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.8609 - val_loss: 11.7259\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2320 - val_loss: 10.0272\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0317 - val_loss: 8.7171\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0802 - val_loss: 8.2033\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4018 - val_loss: 7.4655\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.6785 - val_loss: 6.5188\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3139 - val_loss: 5.8450\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9203 - val_loss: 6.1129\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3290 - val_loss: 11.8836\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9744 - val_loss: 5.8752\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8481 - val_loss: 8.8962\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9566 - val_loss: 5.1554\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5471 - val_loss: 3.8774\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7817 - val_loss: 4.2606\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7506 - val_loss: 4.0239\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5049 - val_loss: 3.8833\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5644 - val_loss: 3.4069\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1857 - val_loss: 2.8380\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7877 - val_loss: 4.2070\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1684 - val_loss: 2.7668\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4254 - val_loss: 2.9459\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4332 - val_loss: 18.5080\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9793 - val_loss: 3.3023\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1940 - val_loss: 2.2010\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9622 - val_loss: 2.8956\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5471 - val_loss: 2.2559\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8900 - val_loss: 2.3070\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9303 - val_loss: 1.9670\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7239 - val_loss: 8.9875\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1652 - val_loss: 1.2911\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4248 - val_loss: 1.5726\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5973 - val_loss: 2.1871\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0026 - val_loss: 1.9128\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4176 - val_loss: 1.8046\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6756 - val_loss: 1.3113\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3134 - val_loss: 6.5290\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3561 - val_loss: 1.1905\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9466 - val_loss: 1.0155\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1784 - val_loss: 1.6015\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4152 - val_loss: 1.2591\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1981 - val_loss: 1.5525\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8517 - val_loss: 1.4269\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8737 - val_loss: 1.1063\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8954 - val_loss: 1.1963\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0127 - val_loss: 1.0844\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3968 - val_loss: 1.2719\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1940 - val_loss: 1.5958\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8061 - val_loss: 70.4486\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5412 - val_loss: 4.4709\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1078 - val_loss: 2.7960\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4072 - val_loss: 2.3951\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9697 - val_loss: 2.2657\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7192 - val_loss: 1.9260\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5565 - val_loss: 1.5029\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5646 - val_loss: 1.6883\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5763 - val_loss: 1.5935\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5349 - val_loss: 1.6009\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2929 - val_loss: 2.2974\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5782 - val_loss: 1.9497\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4147 - val_loss: 1.2746\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2062 - val_loss: 1.1068\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2213 - val_loss: 3.4914\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5006 - val_loss: 0.9769\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7503 - val_loss: 0.7770\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8121 - val_loss: 0.8481\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8064 - val_loss: 0.7348\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8700 - val_loss: 1.8994\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1490 - val_loss: 1.2011\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7994 - val_loss: 0.7624\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9198 - val_loss: 0.8370\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2693 - val_loss: 0.8386\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8969 - val_loss: 3.1093\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8115 - val_loss: 0.5720\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5091 - val_loss: 0.6539\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5275 - val_loss: 0.8007\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7906 - val_loss: 1.0933\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6069 - val_loss: 0.7886\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9385 - val_loss: 1.3290\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8491 - val_loss: 0.8454\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6807 - val_loss: 0.6029\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7454 - val_loss: 0.6168\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7946 - val_loss: 0.8263\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1284 - val_loss: 1.4705\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7697 - val_loss: 0.9115\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5961 - val_loss: 0.8859\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6950 - val_loss: 0.9457\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7919 - val_loss: 0.6512\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9176 - val_loss: 0.7440\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4778 - val_loss: 1.6204\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5080 - val_loss: 0.7035\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4716 - val_loss: 0.4331\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4272 - val_loss: 0.6197\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7259 - val_loss: 1.4639\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7688 - val_loss: 0.4766\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6071 - val_loss: 0.5754\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4109 - val_loss: 0.6404\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6905 - val_loss: 1.0706\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.2557 - val_loss: 0.9945\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5481 - val_loss: 0.5227\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3804 - val_loss: 0.4037\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3558 - val_loss: 0.4274\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3419 - val_loss: 0.5356\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4945 - val_loss: 0.6283\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4769 - val_loss: 0.3735\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4279 - val_loss: 0.6950\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5740 - val_loss: 0.7087\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.7087217202759053\n",
      "Mean Absolute Error (MAE): 0.6030928107084593\n",
      "Root Mean Squared Error (RMSE): 0.841856116136187\n",
      "Time taken: 1223.604558467865\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1373.4307 - val_loss: 1285.9001\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1215.8474 - val_loss: 1185.2284\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1127.6687 - val_loss: 1109.1903\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1062.5200 - val_loss: 1053.4561\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1014.3126 - val_loss: 1012.7639\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 979.1657 - val_loss: 983.6276\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 954.2574 - val_loss: 963.6069\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 937.5127 - val_loss: 951.0056\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.0217 - val_loss: 943.4398\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 920.8987 - val_loss: 939.3271\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.6492 - val_loss: 937.4221\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.0546 - val_loss: 936.6194\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.4035 - val_loss: 936.2795\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2095 - val_loss: 936.2604\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.1146 - val_loss: 936.3546\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0870 - val_loss: 936.3427\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0916 - val_loss: 936.3963\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0790 - val_loss: 936.3554\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 915.1052 - val_loss: 936.3785\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0679 - val_loss: 936.3207\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1053 - val_loss: 936.4233\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1015 - val_loss: 936.5599\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 915.1301 - val_loss: 936.4473\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.1128 - val_loss: 936.4482\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1233 - val_loss: 936.2833\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1353 - val_loss: 936.3467\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1210 - val_loss: 936.4132\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0833 - val_loss: 936.2605\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 915.1089 - val_loss: 936.3059\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.0928 - val_loss: 936.3517\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 915.0912 - val_loss: 936.2959\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.1431 - val_loss: 936.3423\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.0944 - val_loss: 936.3787\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.0640 - val_loss: 936.3280\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1139 - val_loss: 936.2890\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0903 - val_loss: 936.2314\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.0747 - val_loss: 936.2823\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0998 - val_loss: 936.3071\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0958 - val_loss: 936.3891\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 903.5436 - val_loss: 893.5836\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 862.7007 - val_loss: 871.6409\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 845.2148 - val_loss: 857.1700\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 833.0594 - val_loss: 843.3553\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 817.6163 - val_loss: 822.9777\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 798.1311 - val_loss: 796.7756\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 767.8665 - val_loss: 765.5953\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 751.7431 - val_loss: 722.3674\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 681.0391 - val_loss: 667.3300\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 628.3417 - val_loss: 614.5723\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 581.0687 - val_loss: 571.6525\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 540.2186 - val_loss: 531.5908\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 502.4529 - val_loss: 495.0338\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 466.4721 - val_loss: 458.9023\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 432.7469 - val_loss: 428.8236\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 399.4224 - val_loss: 393.8081\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 368.3628 - val_loss: 361.6248\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 340.1002 - val_loss: 333.8496\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 314.0031 - val_loss: 307.3092\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 287.2849 - val_loss: 282.3400\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 262.7661 - val_loss: 257.6188\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 240.6761 - val_loss: 236.4425\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 222.4719 - val_loss: 216.2744\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 200.7636 - val_loss: 196.1570\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 182.5961 - val_loss: 176.5061\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 165.6612 - val_loss: 161.9411\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 150.5942 - val_loss: 157.6658\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 139.2357 - val_loss: 138.8746\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 123.1611 - val_loss: 121.5479\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 109.9660 - val_loss: 106.9942\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 99.6111 - val_loss: 96.0320\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 87.1235 - val_loss: 85.4644\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 77.7299 - val_loss: 90.3973\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.2892 - val_loss: 67.1878\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 60.8286 - val_loss: 69.2703\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 58.5970 - val_loss: 53.2676\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 48.2334 - val_loss: 48.3536\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.5124 - val_loss: 43.6716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.1429 - val_loss: 38.8260\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.9642 - val_loss: 34.6989\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 32.1007 - val_loss: 30.5146\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.3651 - val_loss: 30.1542\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.1143 - val_loss: 26.4628\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.8669 - val_loss: 21.9992\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.2391 - val_loss: 19.5363\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.5806 - val_loss: 17.8294\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.7043 - val_loss: 16.9812\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7109 - val_loss: 16.0404\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2899 - val_loss: 15.4480\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.7229 - val_loss: 15.8659\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.5945 - val_loss: 18.7280\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.9478 - val_loss: 9.9716\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.1985 - val_loss: 11.2042\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3602 - val_loss: 7.9209\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.9590 - val_loss: 10.3248\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3087 - val_loss: 6.3145\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9735 - val_loss: 6.3710\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8231 - val_loss: 7.7485\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9734 - val_loss: 5.4023\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3773 - val_loss: 4.8322\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8587 - val_loss: 4.3533\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7401 - val_loss: 5.3471\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2751 - val_loss: 4.7557\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8104 - val_loss: 4.8459\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8397 - val_loss: 3.2666\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6671 - val_loss: 3.7065\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5671 - val_loss: 3.0864\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9043 - val_loss: 3.5175\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2732 - val_loss: 2.9416\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.3713 - val_loss: 4.1264\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6788 - val_loss: 2.4401\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2087 - val_loss: 2.2906\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2999 - val_loss: 2.5749\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4260 - val_loss: 2.0960\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3758 - val_loss: 2.9659\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5920 - val_loss: 2.0423\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7615 - val_loss: 1.7929\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0200 - val_loss: 3.5218\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5632 - val_loss: 1.6363\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9190 - val_loss: 1.4480\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1364 - val_loss: 1.9666\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5408 - val_loss: 1.6889\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7607 - val_loss: 1.9659\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9236 - val_loss: 5.0837\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9117 - val_loss: 1.5793\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7912 - val_loss: 1.2852\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1856 - val_loss: 1.6315\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8272 - val_loss: 1.6978\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1603 - val_loss: 1.4970\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3866 - val_loss: 1.2065\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3840 - val_loss: 1.0976\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2053 - val_loss: 1.6057\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5162 - val_loss: 0.9575\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2284 - val_loss: 0.9857\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5122 - val_loss: 6.1349\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4872 - val_loss: 1.0367\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7449 - val_loss: 0.7013\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7586 - val_loss: 0.7715\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0994 - val_loss: 1.4731\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7697 - val_loss: 1.4485\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2953 - val_loss: 0.7851\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6056 - val_loss: 0.6705\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7455 - val_loss: 0.8195\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7701 - val_loss: 1.9893\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0868 - val_loss: 0.6250\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6036 - val_loss: 0.8613\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4865 - val_loss: 0.5511\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5941 - val_loss: 0.8612\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6189 - val_loss: 0.7402\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.9108 - val_loss: 0.7410\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5825 - val_loss: 0.8060\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3226 - val_loss: 0.3843\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5545 - val_loss: 0.8946\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2710 - val_loss: 0.9182\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5978 - val_loss: 0.6403\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5673 - val_loss: 1.1058\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7758 - val_loss: 0.5853\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9455 - val_loss: 2.1554\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4471 - val_loss: 0.4046\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4548 - val_loss: 0.4253\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5910 - val_loss: 0.8267\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5480 - val_loss: 2.6750\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8033 - val_loss: 0.4179\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4817 - val_loss: 0.5846\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7177 - val_loss: 1.1414\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0596 - val_loss: 1.4720\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7185 - val_loss: 0.3554\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5672 - val_loss: 0.7192\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5113 - val_loss: 8.3600\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0635 - val_loss: 0.2578\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2439 - val_loss: 0.2404\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2261 - val_loss: 0.2682\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2477 - val_loss: 0.2541\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3948 - val_loss: 0.8261\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5652 - val_loss: 0.3510\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3707 - val_loss: 0.3617\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5116 - val_loss: 2.5022\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8364 - val_loss: 0.3412\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4404 - val_loss: 0.4536\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7223 - val_loss: 1.0682\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4591 - val_loss: 0.4532\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3260 - val_loss: 0.3828\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5213 - val_loss: 0.2992\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3617 - val_loss: 0.2674\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5557 - val_loss: 0.9667\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3323 - val_loss: 0.4175\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2769 - val_loss: 0.2560\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2017 - val_loss: 0.1750\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2088 - val_loss: 0.1481\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2568 - val_loss: 0.1979\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5397 - val_loss: 0.6263\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5403 - val_loss: 0.3061\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3917 - val_loss: 0.3516\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5005 - val_loss: 1.2538\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4689 - val_loss: 0.3565\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3527 - val_loss: 0.2167\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5867 - val_loss: 0.7630\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5285 - val_loss: 0.7017\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3581 - val_loss: 0.3477\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3823 - val_loss: 0.3639\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4059 - val_loss: 2.4771\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 2.4771355273133664\n",
      "Mean Absolute Error (MAE): 1.120263945233077\n",
      "Root Mean Squared Error (RMSE): 1.5738918410466987\n",
      "Time taken: 1201.4057216644287\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1377.3151 - val_loss: 1242.1715\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1214.6871 - val_loss: 1139.6611\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1129.1851 - val_loss: 1065.6262\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1066.3278 - val_loss: 1009.9297\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1019.9305 - val_loss: 969.0646\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 986.2654 - val_loss: 939.7253\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 962.8181 - val_loss: 919.4254\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 947.3228 - val_loss: 906.2109\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 937.5805 - val_loss: 898.0579\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 931.9486 - val_loss: 893.3652\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 929.0896 - val_loss: 890.9665\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.7313 - val_loss: 889.8160\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.2216 - val_loss: 889.2675\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9860 - val_loss: 889.0183\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9169 - val_loss: 888.9258\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8835 - val_loss: 888.8607\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9019 - val_loss: 888.8713\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8718 - val_loss: 888.8359\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8760 - val_loss: 888.8337\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9220 - val_loss: 888.8193\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9000 - val_loss: 888.8226\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9261 - val_loss: 888.8065\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8746 - val_loss: 888.8505\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9029 - val_loss: 888.7862\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8917 - val_loss: 888.8193\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9234 - val_loss: 888.8755\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8840 - val_loss: 888.8148\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8708 - val_loss: 888.8334\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 926.9002 - val_loss: 888.8126\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 926.9142 - val_loss: 888.8018\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 926.8754 - val_loss: 888.7991\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9045 - val_loss: 888.8233\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9175 - val_loss: 888.8410\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8886 - val_loss: 888.8652\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8826 - val_loss: 888.8225\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8796 - val_loss: 888.8248\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 926.9063 - val_loss: 888.8207\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8832 - val_loss: 888.8129\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8887 - val_loss: 888.8484\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 926.8831 - val_loss: 888.8122\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8898 - val_loss: 888.8054\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9307 - val_loss: 888.7941\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8824 - val_loss: 888.8305\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8932 - val_loss: 888.8432\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9243 - val_loss: 888.8395\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 926.8865 - val_loss: 888.8387\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9067 - val_loss: 888.8348\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.5072 - val_loss: 889.1160\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 926.9327 - val_loss: 888.8991\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8948 - val_loss: 888.8249\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 926.9430 - val_loss: 888.8090\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8980 - val_loss: 888.8029\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8826 - val_loss: 888.8381\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.8912 - val_loss: 888.8428\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8766 - val_loss: 888.8260\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8925 - val_loss: 888.8018\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 926.8701 - val_loss: 888.8188\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8790 - val_loss: 888.8167\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8952 - val_loss: 888.7789\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9002 - val_loss: 888.7947\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8706 - val_loss: 888.8109\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8928 - val_loss: 888.8212\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8834 - val_loss: 888.8312\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.9124 - val_loss: 888.8438\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.9006 - val_loss: 888.8413\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8885 - val_loss: 888.8044\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.8933 - val_loss: 888.8246\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 926.4565 - val_loss: 884.1886\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 879.5923 - val_loss: 816.1992\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 835.5708 - val_loss: 780.8371\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 796.6432 - val_loss: 739.4625\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 760.9873 - val_loss: 705.7627\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 728.1159 - val_loss: 676.3113\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 698.0838 - val_loss: 655.6991\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 664.4518 - val_loss: 613.3957\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 629.4928 - val_loss: 576.1121\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 595.0664 - val_loss: 545.4252\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 557.0927 - val_loss: 513.1352\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 530.1705 - val_loss: 478.3998\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 491.2642 - val_loss: 445.9380\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 458.2198 - val_loss: 417.8731\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 430.9117 - val_loss: 388.2972\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 400.7202 - val_loss: 375.8417\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 374.8491 - val_loss: 334.4977\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 350.7407 - val_loss: 313.4006\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 326.9013 - val_loss: 292.9586\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 301.5228 - val_loss: 272.4539\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 282.5658 - val_loss: 252.9801\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 260.4393 - val_loss: 232.7374\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 242.2332 - val_loss: 217.0768\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 224.8642 - val_loss: 199.7988\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 207.1578 - val_loss: 182.5023\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 193.9866 - val_loss: 173.4664\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 175.0057 - val_loss: 153.7209\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 160.4498 - val_loss: 139.3056\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 146.3773 - val_loss: 127.1785\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 133.6590 - val_loss: 115.2291\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 121.2384 - val_loss: 104.6303\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 109.8992 - val_loss: 97.3898\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 100.5745 - val_loss: 86.6072\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 90.1725 - val_loss: 77.1473\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 81.7216 - val_loss: 69.1308\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.5286 - val_loss: 63.2463\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 67.2621 - val_loss: 55.6153\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 60.8140 - val_loss: 50.4706\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 54.5507 - val_loss: 45.4119\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.4399 - val_loss: 41.9810\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 44.0932 - val_loss: 36.2060\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.7590 - val_loss: 31.8905\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.1239 - val_loss: 29.4625\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.2762 - val_loss: 25.6969\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.3894 - val_loss: 24.9879\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1491 - val_loss: 21.1065\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.6117 - val_loss: 18.6034\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.5260 - val_loss: 16.5502\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.7833 - val_loss: 16.9461\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.0682 - val_loss: 14.0646\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.8693 - val_loss: 11.6752\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2419 - val_loss: 11.8828\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.9581 - val_loss: 10.3260\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.8333 - val_loss: 8.5566\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.7597 - val_loss: 7.5719\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6977 - val_loss: 8.4945\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5668 - val_loss: 8.0454\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2476 - val_loss: 5.4597\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8751 - val_loss: 8.0521\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3357 - val_loss: 4.9250\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6031 - val_loss: 4.5855\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.2233 - val_loss: 4.0735\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1972 - val_loss: 4.3638\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4211 - val_loss: 7.0973\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4575 - val_loss: 3.4987\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8701 - val_loss: 2.7532\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7156 - val_loss: 3.2617\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4402 - val_loss: 3.1620\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3318 - val_loss: 2.1590\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7082 - val_loss: 2.4861\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7405 - val_loss: 2.2311\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5928 - val_loss: 2.3746\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6586 - val_loss: 1.7715\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0053 - val_loss: 1.5939\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5060 - val_loss: 2.1994\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3825 - val_loss: 1.8674\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8717 - val_loss: 2.1165\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8445 - val_loss: 1.5065\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3876 - val_loss: 2.2873\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4580 - val_loss: 1.3382\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2470 - val_loss: 1.6397\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.3003 - val_loss: 1.2765\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3949 - val_loss: 1.1394\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1834 - val_loss: 0.8688\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1981 - val_loss: 1.0350\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6416 - val_loss: 1.7968\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4480 - val_loss: 0.7967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0779 - val_loss: 1.3291\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4675 - val_loss: 1.7458\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3228 - val_loss: 1.0106\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9970 - val_loss: 0.9854\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1704 - val_loss: 0.7778\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1311 - val_loss: 0.9850\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8606 - val_loss: 0.8726\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0247 - val_loss: 1.2504\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.9830 - val_loss: 0.7979\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3824 - val_loss: 1.2613\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9414 - val_loss: 0.6482\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7787 - val_loss: 0.8105\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9504 - val_loss: 0.7825\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9897 - val_loss: 0.9758\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7048 - val_loss: 0.6741\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9848 - val_loss: 0.5488\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.8402 - val_loss: 1.2747\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.9492 - val_loss: 0.6313\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5283 - val_loss: 0.8422\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6049 - val_loss: 0.5500\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7458 - val_loss: 0.9182\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8189 - val_loss: 1.2462\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2178 - val_loss: 0.5509\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4678 - val_loss: 0.5339\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4945 - val_loss: 0.8932\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7033 - val_loss: 0.6462\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5862 - val_loss: 0.3981\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1878 - val_loss: 1.9189\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4646 - val_loss: 0.4060\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4276 - val_loss: 0.3657\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5251 - val_loss: 0.2680\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7924 - val_loss: 0.6256\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4916 - val_loss: 0.7300\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6013 - val_loss: 0.8079\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6776 - val_loss: 0.3261\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3794 - val_loss: 0.5562\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5284 - val_loss: 0.5800\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5328 - val_loss: 0.2336\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6265 - val_loss: 1.2070\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6386 - val_loss: 0.2994\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4899 - val_loss: 0.4529\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5312 - val_loss: 0.4582\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4809 - val_loss: 0.3474\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5296 - val_loss: 0.3385\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4147 - val_loss: 0.3465\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0073 - val_loss: 0.8407\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.8407453694817009\n",
      "Mean Absolute Error (MAE): 0.7024713345548994\n",
      "Root Mean Squared Error (RMSE): 0.9169216812147594\n",
      "Time taken: 1227.12237906456\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1392.2748 - val_loss: 1294.1074\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1231.2772 - val_loss: 1197.1229\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1142.2274 - val_loss: 1123.9073\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1073.9254 - val_loss: 1068.7413\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1022.7936 - val_loss: 1027.6870\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 984.5297 - val_loss: 997.4692\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 957.0737 - val_loss: 976.7291\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 938.4196 - val_loss: 963.0019\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.5232 - val_loss: 954.9340\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.4963 - val_loss: 950.7810\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.6055 - val_loss: 948.6767\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.7185 - val_loss: 947.8824\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.8677 - val_loss: 947.6512\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.5631 - val_loss: 947.7349\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4677 - val_loss: 947.8453\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4213 - val_loss: 947.7414\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4086 - val_loss: 948.0654\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3889 - val_loss: 948.0752\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3915 - val_loss: 948.2203\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4427 - val_loss: 947.9620\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4028 - val_loss: 948.1000\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 912.4046 - val_loss: 947.8394\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 912.4011 - val_loss: 948.0659\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4002 - val_loss: 948.0461\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4005 - val_loss: 947.8753\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4697 - val_loss: 947.8389\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.3849 - val_loss: 947.8985\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4244 - val_loss: 947.8550\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3717 - val_loss: 947.8734\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4127 - val_loss: 947.8945\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4039 - val_loss: 947.8805\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4200 - val_loss: 948.0516\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3943 - val_loss: 947.8041\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4105 - val_loss: 947.8320\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.3962 - val_loss: 947.8828\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4127 - val_loss: 948.1092\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3998 - val_loss: 947.9855\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4199 - val_loss: 947.9771\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3710 - val_loss: 947.9065\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4301 - val_loss: 947.9167\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0744 - val_loss: 942.7283\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 873.8490 - val_loss: 892.2368\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 848.9688 - val_loss: 878.5235\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 834.7454 - val_loss: 865.2714\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 810.5681 - val_loss: 818.9153\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 756.9501 - val_loss: 768.9818\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 710.8912 - val_loss: 722.7578\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 664.5084 - val_loss: 676.0848\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 619.1952 - val_loss: 627.0698\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 575.3169 - val_loss: 583.5148\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 534.4396 - val_loss: 541.4278\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 493.3735 - val_loss: 499.4347\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 454.4043 - val_loss: 462.6160\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 418.9118 - val_loss: 426.9009\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 387.7056 - val_loss: 396.0648\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 354.3944 - val_loss: 364.8431\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 327.3659 - val_loss: 332.9473\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 301.6933 - val_loss: 315.0380\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 273.9205 - val_loss: 283.5963\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 246.1069 - val_loss: 254.8439\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 224.5075 - val_loss: 232.7300\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.8248 - val_loss: 209.4468\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 191.8072 - val_loss: 192.0999\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 167.2131 - val_loss: 171.6608\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 150.6151 - val_loss: 156.3813\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 139.8424 - val_loss: 139.3941\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 126.1802 - val_loss: 127.2243\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 112.0976 - val_loss: 114.4261\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 100.8661 - val_loss: 103.2263\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 90.9230 - val_loss: 92.7438\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 82.2083 - val_loss: 83.5517\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.9411 - val_loss: 75.5196\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 68.6924 - val_loss: 70.7136\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.1515 - val_loss: 60.4684\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.5513 - val_loss: 52.6676\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.0052 - val_loss: 49.9353\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.2172 - val_loss: 43.1033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.5544 - val_loss: 37.1726\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.0564 - val_loss: 36.9147\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.4711 - val_loss: 31.9057\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.8418 - val_loss: 27.6875\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8524 - val_loss: 23.7834\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.5719 - val_loss: 25.1688\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.2480 - val_loss: 20.6711\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.4955 - val_loss: 20.6500\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.3427 - val_loss: 17.8517\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.5895 - val_loss: 14.7406\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.7636 - val_loss: 14.7458\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.9839 - val_loss: 13.6184\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.3199 - val_loss: 11.1132\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0895 - val_loss: 10.4108\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.8585 - val_loss: 10.6904\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2274 - val_loss: 9.4078\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1413 - val_loss: 7.0075\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3348 - val_loss: 6.2922\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0853 - val_loss: 6.4388\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6402 - val_loss: 5.6545\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4100 - val_loss: 5.2689\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4024 - val_loss: 9.5814\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7186 - val_loss: 4.7408\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0107 - val_loss: 4.1044\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6739 - val_loss: 4.4804\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4900 - val_loss: 3.3353\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5901 - val_loss: 3.6003\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2405 - val_loss: 3.3084\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2526 - val_loss: 6.3188\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0531 - val_loss: 2.4107\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2467 - val_loss: 2.3279\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9480 - val_loss: 3.5928\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6382 - val_loss: 2.0273\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2456 - val_loss: 3.2258\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4662 - val_loss: 1.7076\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0129 - val_loss: 3.8921\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3794 - val_loss: 1.6990\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2550 - val_loss: 3.6387\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2580 - val_loss: 1.2273\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7896 - val_loss: 1.8345\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0125 - val_loss: 1.1856\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4102 - val_loss: 2.7438\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2444 - val_loss: 4.7204\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3104 - val_loss: 0.8813\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3288 - val_loss: 1.5995\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2413 - val_loss: 2.0678\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6547 - val_loss: 1.2490\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7737 - val_loss: 1.1144\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9178 - val_loss: 1.9626\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0845 - val_loss: 2.5783\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2244 - val_loss: 0.9525\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9669 - val_loss: 0.8373\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0232 - val_loss: 1.1424\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9399 - val_loss: 1.0314\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0502 - val_loss: 2.7352\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3344 - val_loss: 1.0512\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2343 - val_loss: 1.4259\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2948 - val_loss: 0.6934\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6626 - val_loss: 1.0701\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9015 - val_loss: 1.1209\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1909 - val_loss: 2.5974\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4651 - val_loss: 0.9275\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7164 - val_loss: 0.6826\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6656 - val_loss: 1.0273\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9279 - val_loss: 0.6527\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9640 - val_loss: 0.8151\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7981 - val_loss: 0.9777\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2809 - val_loss: 0.4474\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5701 - val_loss: 1.1815\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4264 - val_loss: 0.8624\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6007 - val_loss: 0.5089\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6586 - val_loss: 0.9906\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1291 - val_loss: 1.2132\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6451 - val_loss: 0.7471\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7359 - val_loss: 1.0035\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6667 - val_loss: 0.5861\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7998 - val_loss: 1.1997\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7855 - val_loss: 0.4192\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6343 - val_loss: 0.6291\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8657 - val_loss: 0.3768\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7174 - val_loss: 2.8560\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5172 - val_loss: 0.4109\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3779 - val_loss: 0.3275\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5517 - val_loss: 0.2828\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5633 - val_loss: 3.1929\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8286 - val_loss: 0.4952\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3109 - val_loss: 0.2749\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2880 - val_loss: 0.2497\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4233 - val_loss: 0.3248\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6166 - val_loss: 1.1176\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0743 - val_loss: 0.5500\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3635 - val_loss: 0.6514\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7301 - val_loss: 0.8881\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5295 - val_loss: 0.6245\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0217 - val_loss: 0.3324\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3705 - val_loss: 0.3916\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6614 - val_loss: 0.3688\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3429 - val_loss: 1.0322\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6573 - val_loss: 0.8214\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4740 - val_loss: 0.8658\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6048 - val_loss: 0.6066\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3884 - val_loss: 16.4745\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9600 - val_loss: 0.3429\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2810 - val_loss: 0.2426\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2021 - val_loss: 0.1864\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1902 - val_loss: 0.2272\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2798 - val_loss: 0.2303\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2826 - val_loss: 0.3796\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4153 - val_loss: 0.3483\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4282 - val_loss: 0.3794\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5568 - val_loss: 0.4530\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4127 - val_loss: 0.3081\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5857 - val_loss: 1.3555\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2762 - val_loss: 0.1923\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2285 - val_loss: 0.2276\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3648 - val_loss: 0.5012\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4439 - val_loss: 0.4522\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4350 - val_loss: 0.4857\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5889 - val_loss: 0.1881\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3998 - val_loss: 0.6051\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7730 - val_loss: 0.5320\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3171 - val_loss: 0.5938\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4555 - val_loss: 0.3906\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.3906276890998009\n",
      "Mean Absolute Error (MAE): 0.48224304723461126\n",
      "Root Mean Squared Error (RMSE): 0.6250021512761383\n",
      "Time taken: 1193.8298935890198\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 18ms/step - loss: 1360.9999 - val_loss: 1279.6868\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1201.8834 - val_loss: 1176.3597\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1116.7977 - val_loss: 1098.4891\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1053.4674 - val_loss: 1041.9895\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1007.1973 - val_loss: 1000.1964\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 974.3141 - val_loss: 971.3420\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 951.6997 - val_loss: 951.5089\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 936.8978 - val_loss: 939.0375\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.9304 - val_loss: 931.4672\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.8799 - val_loss: 927.2136\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.3257 - val_loss: 925.0463\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.1912 - val_loss: 923.9387\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.6765 - val_loss: 923.5558\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5285 - val_loss: 923.3531\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4585 - val_loss: 923.3143\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4631 - val_loss: 923.2276\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.4529 - val_loss: 923.1730\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4507 - val_loss: 923.1784\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5030 - val_loss: 923.1471\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4743 - val_loss: 923.2519\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4653 - val_loss: 923.0203\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4610 - val_loss: 923.1270\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4405 - val_loss: 923.0865\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4474 - val_loss: 923.0458\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4454 - val_loss: 923.0812\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4435 - val_loss: 923.1174\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4698 - val_loss: 923.1609\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4322 - val_loss: 923.1779\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4871 - val_loss: 923.1080\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4443 - val_loss: 923.0963\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4545 - val_loss: 923.1244\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4318 - val_loss: 923.0691\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4619 - val_loss: 923.0720\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4235 - val_loss: 923.1215\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4530 - val_loss: 923.0910\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5131 - val_loss: 923.1049\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4368 - val_loss: 923.1303\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4954 - val_loss: 923.1415\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4315 - val_loss: 923.1166\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4699 - val_loss: 923.1078\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4522 - val_loss: 923.0719\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4471 - val_loss: 923.0231\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4858 - val_loss: 923.0099\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4464 - val_loss: 923.0674\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4541 - val_loss: 923.1377\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4664 - val_loss: 923.0652\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4949 - val_loss: 923.0289\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4325 - val_loss: 923.1355\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5029 - val_loss: 923.0304\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4565 - val_loss: 923.2048\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4506 - val_loss: 923.0618\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4749 - val_loss: 923.1184\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4929 - val_loss: 923.0903\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4371 - val_loss: 923.2195\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.1078 - val_loss: 925.7431\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.9048 - val_loss: 924.3107\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5565 - val_loss: 923.2192\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 904.7210 - val_loss: 880.4461\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 856.9393 - val_loss: 838.4335\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 804.0519 - val_loss: 776.0402\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 749.8756 - val_loss: 734.1157\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 711.4102 - val_loss: 698.4020\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 675.1368 - val_loss: 659.7404\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 639.4051 - val_loss: 625.0582\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 605.3440 - val_loss: 588.6710\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 569.8540 - val_loss: 554.6471\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 534.4848 - val_loss: 521.3865\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 500.4854 - val_loss: 485.7137\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 469.1410 - val_loss: 456.3006\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 437.7776 - val_loss: 424.7576\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 409.5307 - val_loss: 396.8405\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 382.7204 - val_loss: 370.0585\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 356.9442 - val_loss: 344.3600\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 333.3371 - val_loss: 321.9148\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 310.9213 - val_loss: 299.4066\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 291.1283 - val_loss: 279.6439\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 269.6050 - val_loss: 258.1709\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 250.8674 - val_loss: 241.4691\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 233.6709 - val_loss: 222.9504\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 217.7392 - val_loss: 207.4192\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 202.3617 - val_loss: 193.9240\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 188.4083 - val_loss: 179.0077\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 175.5387 - val_loss: 165.6680\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 162.3944 - val_loss: 153.9951\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 150.6659 - val_loss: 142.0772\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 139.5552 - val_loss: 130.6295\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 126.7544 - val_loss: 117.3949\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 114.5194 - val_loss: 106.6336\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 103.4002 - val_loss: 95.2696\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 93.9940 - val_loss: 87.8836\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 85.4366 - val_loss: 79.4198\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 77.3870 - val_loss: 71.4479\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.6975 - val_loss: 63.9849\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 63.6054 - val_loss: 58.7876\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.2101 - val_loss: 52.7071\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.2601 - val_loss: 48.2423\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 47.1630 - val_loss: 45.1363\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.9298 - val_loss: 40.6242\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.8100 - val_loss: 37.3397\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.1880 - val_loss: 32.8479\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.2001 - val_loss: 28.9013\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.1622 - val_loss: 26.0598\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.7762 - val_loss: 24.7886\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.4195 - val_loss: 23.0273\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.4308 - val_loss: 21.6705\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.4975 - val_loss: 19.7674\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.2945 - val_loss: 17.9439\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.9475 - val_loss: 15.0624\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.7518 - val_loss: 15.9772\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6491 - val_loss: 12.5433\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.7472 - val_loss: 11.4688\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5941 - val_loss: 10.4026\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1219 - val_loss: 9.9574\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3740 - val_loss: 8.4560\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.2412 - val_loss: 9.5110\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7185 - val_loss: 7.3154\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2623 - val_loss: 7.6327\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3956 - val_loss: 6.4165\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2322 - val_loss: 6.1289\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4442 - val_loss: 5.6691\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0169 - val_loss: 5.8110\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4087 - val_loss: 4.4205\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3492 - val_loss: 4.1147\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9036 - val_loss: 3.9067\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9291 - val_loss: 9.1711\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5243 - val_loss: 3.1477\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2537 - val_loss: 3.0979\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9856 - val_loss: 3.7201\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8964 - val_loss: 2.4904\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9177 - val_loss: 2.9958\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5547 - val_loss: 3.5603\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4628 - val_loss: 2.1324\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2935 - val_loss: 2.1763\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3101 - val_loss: 1.9111\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0681 - val_loss: 1.8663\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0307 - val_loss: 1.9429\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1613 - val_loss: 1.7157\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9370 - val_loss: 2.6104\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6278 - val_loss: 2.5814\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7479 - val_loss: 1.4025\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4888 - val_loss: 2.8619\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5267 - val_loss: 1.3802\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6730 - val_loss: 1.3382\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3247 - val_loss: 1.1804\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3369 - val_loss: 1.1821\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5262 - val_loss: 1.2557\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1287 - val_loss: 1.1893\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5747 - val_loss: 1.9188\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0287 - val_loss: 0.8051\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1292 - val_loss: 0.9651\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2900 - val_loss: 1.7943\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0523 - val_loss: 1.0811\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8318 - val_loss: 1.1025\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2173 - val_loss: 1.9379\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2222 - val_loss: 1.1409\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2139 - val_loss: 1.2127\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8054 - val_loss: 0.5384\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7883 - val_loss: 1.0076\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7154 - val_loss: 0.6000\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8573 - val_loss: 0.9833\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7582 - val_loss: 0.7102\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7037 - val_loss: 1.4230\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2030 - val_loss: 0.6045\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8856 - val_loss: 1.0284\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8207 - val_loss: 0.8561\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9116 - val_loss: 0.7925\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5239 - val_loss: 0.5410\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9704 - val_loss: 0.8305\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6250 - val_loss: 0.6187\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0057 - val_loss: 1.1687\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5847 - val_loss: 0.7171\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8076 - val_loss: 0.4668\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4789 - val_loss: 0.8371\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0498 - val_loss: 0.7972\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5033 - val_loss: 0.5376\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6267 - val_loss: 1.0281\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5824 - val_loss: 0.6724\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8429 - val_loss: 1.0615\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6239 - val_loss: 0.5103\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5328 - val_loss: 0.7126\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4645 - val_loss: 0.7073\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9807 - val_loss: 1.0801\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6077 - val_loss: 0.3626\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5333 - val_loss: 0.3892\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5373 - val_loss: 0.4221\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3876 - val_loss: 0.5373\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0871 - val_loss: 0.2828\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3699 - val_loss: 0.4183\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6000 - val_loss: 1.0288\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6735 - val_loss: 0.5410\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4453 - val_loss: 0.3560\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3683 - val_loss: 0.5077\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5876 - val_loss: 1.0384\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5080 - val_loss: 0.6188\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4825 - val_loss: 0.2239\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3879 - val_loss: 0.7154\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6369 - val_loss: 0.2524\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4392 - val_loss: 0.7219\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5149 - val_loss: 0.3687\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4223 - val_loss: 0.7165\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.7164513707088247\n",
      "Mean Absolute Error (MAE): 0.5971816760963244\n",
      "Root Mean Squared Error (RMSE): 0.8464345046776063\n",
      "Time taken: 1191.527464389801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_21732\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.708722  0.603093  0.841856  1223.604558\n",
      "1        2  2.477136  1.120264  1.573892  1201.405722\n",
      "2        3  0.840745  0.702471  0.916922  1227.122379\n",
      "3        4  0.390628  0.482243  0.625002  1193.829894\n",
      "4        5  0.716451  0.597182  0.846435  1191.527464\n",
      "5  Average  1.026736  0.701051  0.960821  1207.498003\n",
      "Results saved to 'DL_Result_PL_model_2_smoothing2_Reg1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_smoothing2_Reg1.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_2_smoothing2_Reg1.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
