{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_Scattered_Reg2.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.730466</td>\n",
       "      <td>89.453682</td>\n",
       "      <td>87.785620</td>\n",
       "      <td>76.587601</td>\n",
       "      <td>84.851017</td>\n",
       "      <td>69.058642</td>\n",
       "      <td>87.202550</td>\n",
       "      <td>71.783088</td>\n",
       "      <td>83.486579</td>\n",
       "      <td>67.510694</td>\n",
       "      <td>...</td>\n",
       "      <td>84.877108</td>\n",
       "      <td>82.619467</td>\n",
       "      <td>79.588797</td>\n",
       "      <td>63.378421</td>\n",
       "      <td>87.883257</td>\n",
       "      <td>81.792326</td>\n",
       "      <td>88.326395</td>\n",
       "      <td>80.556565</td>\n",
       "      <td>65.969870</td>\n",
       "      <td>79.732339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.102980</td>\n",
       "      <td>87.224094</td>\n",
       "      <td>82.082242</td>\n",
       "      <td>81.680587</td>\n",
       "      <td>101.897605</td>\n",
       "      <td>89.818785</td>\n",
       "      <td>81.507159</td>\n",
       "      <td>81.829383</td>\n",
       "      <td>73.644202</td>\n",
       "      <td>84.450240</td>\n",
       "      <td>...</td>\n",
       "      <td>73.485170</td>\n",
       "      <td>91.161226</td>\n",
       "      <td>87.614333</td>\n",
       "      <td>60.744124</td>\n",
       "      <td>102.136423</td>\n",
       "      <td>73.765738</td>\n",
       "      <td>75.956533</td>\n",
       "      <td>76.056711</td>\n",
       "      <td>60.898188</td>\n",
       "      <td>77.393442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.631890</td>\n",
       "      <td>66.257220</td>\n",
       "      <td>80.969001</td>\n",
       "      <td>72.930668</td>\n",
       "      <td>71.620128</td>\n",
       "      <td>94.681804</td>\n",
       "      <td>74.722719</td>\n",
       "      <td>80.515246</td>\n",
       "      <td>87.898047</td>\n",
       "      <td>83.039718</td>\n",
       "      <td>...</td>\n",
       "      <td>67.781748</td>\n",
       "      <td>78.291026</td>\n",
       "      <td>79.990390</td>\n",
       "      <td>70.183516</td>\n",
       "      <td>90.568165</td>\n",
       "      <td>69.355478</td>\n",
       "      <td>85.019282</td>\n",
       "      <td>79.626234</td>\n",
       "      <td>75.902175</td>\n",
       "      <td>84.732731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.862885</td>\n",
       "      <td>70.386298</td>\n",
       "      <td>66.226441</td>\n",
       "      <td>67.022090</td>\n",
       "      <td>82.123529</td>\n",
       "      <td>91.918278</td>\n",
       "      <td>81.225184</td>\n",
       "      <td>81.979229</td>\n",
       "      <td>87.241442</td>\n",
       "      <td>77.722305</td>\n",
       "      <td>...</td>\n",
       "      <td>94.550404</td>\n",
       "      <td>81.172616</td>\n",
       "      <td>72.587516</td>\n",
       "      <td>85.425354</td>\n",
       "      <td>94.797779</td>\n",
       "      <td>84.446558</td>\n",
       "      <td>75.434557</td>\n",
       "      <td>93.011442</td>\n",
       "      <td>69.124145</td>\n",
       "      <td>79.875870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.798557</td>\n",
       "      <td>82.741690</td>\n",
       "      <td>74.042955</td>\n",
       "      <td>94.149286</td>\n",
       "      <td>100.908840</td>\n",
       "      <td>95.783576</td>\n",
       "      <td>79.224920</td>\n",
       "      <td>67.167455</td>\n",
       "      <td>79.834331</td>\n",
       "      <td>80.872271</td>\n",
       "      <td>...</td>\n",
       "      <td>81.381189</td>\n",
       "      <td>69.274949</td>\n",
       "      <td>85.586719</td>\n",
       "      <td>63.042794</td>\n",
       "      <td>83.646354</td>\n",
       "      <td>82.237603</td>\n",
       "      <td>90.288501</td>\n",
       "      <td>81.855912</td>\n",
       "      <td>76.998217</td>\n",
       "      <td>84.330471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>82.108784</td>\n",
       "      <td>81.584543</td>\n",
       "      <td>86.016942</td>\n",
       "      <td>71.404227</td>\n",
       "      <td>101.371020</td>\n",
       "      <td>76.765853</td>\n",
       "      <td>77.869570</td>\n",
       "      <td>88.196777</td>\n",
       "      <td>77.081792</td>\n",
       "      <td>66.228624</td>\n",
       "      <td>...</td>\n",
       "      <td>77.961784</td>\n",
       "      <td>85.777203</td>\n",
       "      <td>84.200662</td>\n",
       "      <td>64.667301</td>\n",
       "      <td>75.258813</td>\n",
       "      <td>77.262059</td>\n",
       "      <td>85.273278</td>\n",
       "      <td>81.702915</td>\n",
       "      <td>68.892345</td>\n",
       "      <td>71.753840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>99.805372</td>\n",
       "      <td>85.037790</td>\n",
       "      <td>88.887016</td>\n",
       "      <td>76.925746</td>\n",
       "      <td>96.647426</td>\n",
       "      <td>83.254785</td>\n",
       "      <td>81.911410</td>\n",
       "      <td>77.866264</td>\n",
       "      <td>84.494610</td>\n",
       "      <td>73.992921</td>\n",
       "      <td>...</td>\n",
       "      <td>66.455703</td>\n",
       "      <td>73.287247</td>\n",
       "      <td>68.975757</td>\n",
       "      <td>59.690427</td>\n",
       "      <td>97.523042</td>\n",
       "      <td>72.543085</td>\n",
       "      <td>78.237935</td>\n",
       "      <td>86.620870</td>\n",
       "      <td>82.222681</td>\n",
       "      <td>60.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>81.841964</td>\n",
       "      <td>78.246787</td>\n",
       "      <td>78.208780</td>\n",
       "      <td>64.264379</td>\n",
       "      <td>78.449603</td>\n",
       "      <td>91.034501</td>\n",
       "      <td>83.279032</td>\n",
       "      <td>71.713930</td>\n",
       "      <td>89.748354</td>\n",
       "      <td>77.469063</td>\n",
       "      <td>...</td>\n",
       "      <td>81.866078</td>\n",
       "      <td>68.809098</td>\n",
       "      <td>72.848124</td>\n",
       "      <td>64.968223</td>\n",
       "      <td>70.571008</td>\n",
       "      <td>83.034651</td>\n",
       "      <td>83.789545</td>\n",
       "      <td>87.322471</td>\n",
       "      <td>70.519879</td>\n",
       "      <td>59.172377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>81.833020</td>\n",
       "      <td>81.762443</td>\n",
       "      <td>64.707536</td>\n",
       "      <td>70.649944</td>\n",
       "      <td>82.197035</td>\n",
       "      <td>76.806120</td>\n",
       "      <td>61.982283</td>\n",
       "      <td>97.342958</td>\n",
       "      <td>86.903924</td>\n",
       "      <td>84.735880</td>\n",
       "      <td>...</td>\n",
       "      <td>85.081986</td>\n",
       "      <td>81.865454</td>\n",
       "      <td>96.068783</td>\n",
       "      <td>73.198179</td>\n",
       "      <td>86.111218</td>\n",
       "      <td>88.545774</td>\n",
       "      <td>80.169550</td>\n",
       "      <td>76.045810</td>\n",
       "      <td>74.291673</td>\n",
       "      <td>56.386748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>81.962929</td>\n",
       "      <td>85.052762</td>\n",
       "      <td>71.584087</td>\n",
       "      <td>68.405848</td>\n",
       "      <td>73.879542</td>\n",
       "      <td>105.246037</td>\n",
       "      <td>80.553156</td>\n",
       "      <td>87.584474</td>\n",
       "      <td>90.150839</td>\n",
       "      <td>84.384220</td>\n",
       "      <td>...</td>\n",
       "      <td>85.666249</td>\n",
       "      <td>91.608497</td>\n",
       "      <td>69.734030</td>\n",
       "      <td>65.366162</td>\n",
       "      <td>85.565140</td>\n",
       "      <td>91.927970</td>\n",
       "      <td>81.254355</td>\n",
       "      <td>84.759455</td>\n",
       "      <td>74.441114</td>\n",
       "      <td>68.843831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3           4           5   \\\n",
       "0     65.730466  89.453682  87.785620  76.587601   84.851017   69.058642   \n",
       "1     66.102980  87.224094  82.082242  81.680587  101.897605   89.818785   \n",
       "2     87.631890  66.257220  80.969001  72.930668   71.620128   94.681804   \n",
       "3     78.862885  70.386298  66.226441  67.022090   82.123529   91.918278   \n",
       "4     78.798557  82.741690  74.042955  94.149286  100.908840   95.783576   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "2438  82.108784  81.584543  86.016942  71.404227  101.371020   76.765853   \n",
       "2439  99.805372  85.037790  88.887016  76.925746   96.647426   83.254785   \n",
       "2440  81.841964  78.246787  78.208780  64.264379   78.449603   91.034501   \n",
       "2441  81.833020  81.762443  64.707536  70.649944   82.197035   76.806120   \n",
       "2442  81.962929  85.052762  71.584087  68.405848   73.879542  105.246037   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     87.202550  71.783088  83.486579  67.510694  ...  84.877108  82.619467   \n",
       "1     81.507159  81.829383  73.644202  84.450240  ...  73.485170  91.161226   \n",
       "2     74.722719  80.515246  87.898047  83.039718  ...  67.781748  78.291026   \n",
       "3     81.225184  81.979229  87.241442  77.722305  ...  94.550404  81.172616   \n",
       "4     79.224920  67.167455  79.834331  80.872271  ...  81.381189  69.274949   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  77.869570  88.196777  77.081792  66.228624  ...  77.961784  85.777203   \n",
       "2439  81.911410  77.866264  84.494610  73.992921  ...  66.455703  73.287247   \n",
       "2440  83.279032  71.713930  89.748354  77.469063  ...  81.866078  68.809098   \n",
       "2441  61.982283  97.342958  86.903924  84.735880  ...  85.081986  81.865454   \n",
       "2442  80.553156  87.584474  90.150839  84.384220  ...  85.666249  91.608497   \n",
       "\n",
       "             40         41          42         43         44         45  \\\n",
       "0     79.588797  63.378421   87.883257  81.792326  88.326395  80.556565   \n",
       "1     87.614333  60.744124  102.136423  73.765738  75.956533  76.056711   \n",
       "2     79.990390  70.183516   90.568165  69.355478  85.019282  79.626234   \n",
       "3     72.587516  85.425354   94.797779  84.446558  75.434557  93.011442   \n",
       "4     85.586719  63.042794   83.646354  82.237603  90.288501  81.855912   \n",
       "...         ...        ...         ...        ...        ...        ...   \n",
       "2438  84.200662  64.667301   75.258813  77.262059  85.273278  81.702915   \n",
       "2439  68.975757  59.690427   97.523042  72.543085  78.237935  86.620870   \n",
       "2440  72.848124  64.968223   70.571008  83.034651  83.789545  87.322471   \n",
       "2441  96.068783  73.198179   86.111218  88.545774  80.169550  76.045810   \n",
       "2442  69.734030  65.366162   85.565140  91.927970  81.254355  84.759455   \n",
       "\n",
       "             46         47  \n",
       "0     65.969870  79.732339  \n",
       "1     60.898188  77.393442  \n",
       "2     75.902175  84.732731  \n",
       "3     69.124145  79.875870  \n",
       "4     76.998217  84.330471  \n",
       "...         ...        ...  \n",
       "2438  68.892345  71.753840  \n",
       "2439  82.222681  60.468085  \n",
       "2440  70.519879  59.172377  \n",
       "2441  74.291673  56.386748  \n",
       "2442  74.441114  68.843831  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.730466</td>\n",
       "      <td>89.453682</td>\n",
       "      <td>87.785620</td>\n",
       "      <td>76.587601</td>\n",
       "      <td>84.851017</td>\n",
       "      <td>69.058642</td>\n",
       "      <td>87.202550</td>\n",
       "      <td>71.783088</td>\n",
       "      <td>83.486579</td>\n",
       "      <td>67.510694</td>\n",
       "      <td>...</td>\n",
       "      <td>84.877108</td>\n",
       "      <td>82.619467</td>\n",
       "      <td>79.588797</td>\n",
       "      <td>63.378421</td>\n",
       "      <td>87.883257</td>\n",
       "      <td>81.792326</td>\n",
       "      <td>88.326395</td>\n",
       "      <td>80.556565</td>\n",
       "      <td>65.969870</td>\n",
       "      <td>79.732339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.102980</td>\n",
       "      <td>87.224094</td>\n",
       "      <td>82.082242</td>\n",
       "      <td>81.680587</td>\n",
       "      <td>101.897605</td>\n",
       "      <td>89.818785</td>\n",
       "      <td>81.507159</td>\n",
       "      <td>81.829383</td>\n",
       "      <td>73.644202</td>\n",
       "      <td>84.450240</td>\n",
       "      <td>...</td>\n",
       "      <td>73.485170</td>\n",
       "      <td>91.161226</td>\n",
       "      <td>87.614333</td>\n",
       "      <td>60.744124</td>\n",
       "      <td>102.136423</td>\n",
       "      <td>73.765738</td>\n",
       "      <td>75.956533</td>\n",
       "      <td>76.056711</td>\n",
       "      <td>60.898188</td>\n",
       "      <td>77.393442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.631890</td>\n",
       "      <td>66.257220</td>\n",
       "      <td>80.969001</td>\n",
       "      <td>72.930668</td>\n",
       "      <td>71.620128</td>\n",
       "      <td>94.681804</td>\n",
       "      <td>74.722719</td>\n",
       "      <td>80.515246</td>\n",
       "      <td>87.898047</td>\n",
       "      <td>83.039718</td>\n",
       "      <td>...</td>\n",
       "      <td>67.781748</td>\n",
       "      <td>78.291026</td>\n",
       "      <td>79.990390</td>\n",
       "      <td>70.183516</td>\n",
       "      <td>90.568165</td>\n",
       "      <td>69.355478</td>\n",
       "      <td>85.019282</td>\n",
       "      <td>79.626234</td>\n",
       "      <td>75.902175</td>\n",
       "      <td>84.732731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.862885</td>\n",
       "      <td>70.386298</td>\n",
       "      <td>66.226441</td>\n",
       "      <td>67.022090</td>\n",
       "      <td>82.123529</td>\n",
       "      <td>91.918278</td>\n",
       "      <td>81.225184</td>\n",
       "      <td>81.979229</td>\n",
       "      <td>87.241442</td>\n",
       "      <td>77.722305</td>\n",
       "      <td>...</td>\n",
       "      <td>94.550404</td>\n",
       "      <td>81.172616</td>\n",
       "      <td>72.587516</td>\n",
       "      <td>85.425354</td>\n",
       "      <td>94.797779</td>\n",
       "      <td>84.446558</td>\n",
       "      <td>75.434557</td>\n",
       "      <td>93.011442</td>\n",
       "      <td>69.124145</td>\n",
       "      <td>79.875870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.798557</td>\n",
       "      <td>82.741690</td>\n",
       "      <td>74.042955</td>\n",
       "      <td>94.149286</td>\n",
       "      <td>100.908840</td>\n",
       "      <td>95.783576</td>\n",
       "      <td>79.224920</td>\n",
       "      <td>67.167455</td>\n",
       "      <td>79.834331</td>\n",
       "      <td>80.872271</td>\n",
       "      <td>...</td>\n",
       "      <td>81.381189</td>\n",
       "      <td>69.274949</td>\n",
       "      <td>85.586719</td>\n",
       "      <td>63.042794</td>\n",
       "      <td>83.646354</td>\n",
       "      <td>82.237603</td>\n",
       "      <td>90.288501</td>\n",
       "      <td>81.855912</td>\n",
       "      <td>76.998217</td>\n",
       "      <td>84.330471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>82.108784</td>\n",
       "      <td>81.584543</td>\n",
       "      <td>86.016942</td>\n",
       "      <td>71.404227</td>\n",
       "      <td>101.371020</td>\n",
       "      <td>76.765853</td>\n",
       "      <td>77.869570</td>\n",
       "      <td>88.196777</td>\n",
       "      <td>77.081792</td>\n",
       "      <td>66.228624</td>\n",
       "      <td>...</td>\n",
       "      <td>77.961784</td>\n",
       "      <td>85.777203</td>\n",
       "      <td>84.200662</td>\n",
       "      <td>64.667301</td>\n",
       "      <td>75.258813</td>\n",
       "      <td>77.262059</td>\n",
       "      <td>85.273278</td>\n",
       "      <td>81.702915</td>\n",
       "      <td>68.892345</td>\n",
       "      <td>71.753840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>99.805372</td>\n",
       "      <td>85.037790</td>\n",
       "      <td>88.887016</td>\n",
       "      <td>76.925746</td>\n",
       "      <td>96.647426</td>\n",
       "      <td>83.254785</td>\n",
       "      <td>81.911410</td>\n",
       "      <td>77.866264</td>\n",
       "      <td>84.494610</td>\n",
       "      <td>73.992921</td>\n",
       "      <td>...</td>\n",
       "      <td>66.455703</td>\n",
       "      <td>73.287247</td>\n",
       "      <td>68.975757</td>\n",
       "      <td>59.690427</td>\n",
       "      <td>97.523042</td>\n",
       "      <td>72.543085</td>\n",
       "      <td>78.237935</td>\n",
       "      <td>86.620870</td>\n",
       "      <td>82.222681</td>\n",
       "      <td>60.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>81.841964</td>\n",
       "      <td>78.246787</td>\n",
       "      <td>78.208780</td>\n",
       "      <td>64.264379</td>\n",
       "      <td>78.449603</td>\n",
       "      <td>91.034501</td>\n",
       "      <td>83.279032</td>\n",
       "      <td>71.713930</td>\n",
       "      <td>89.748354</td>\n",
       "      <td>77.469063</td>\n",
       "      <td>...</td>\n",
       "      <td>81.866078</td>\n",
       "      <td>68.809098</td>\n",
       "      <td>72.848124</td>\n",
       "      <td>64.968223</td>\n",
       "      <td>70.571008</td>\n",
       "      <td>83.034651</td>\n",
       "      <td>83.789545</td>\n",
       "      <td>87.322471</td>\n",
       "      <td>70.519879</td>\n",
       "      <td>59.172377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>81.833020</td>\n",
       "      <td>81.762443</td>\n",
       "      <td>64.707536</td>\n",
       "      <td>70.649944</td>\n",
       "      <td>82.197035</td>\n",
       "      <td>76.806120</td>\n",
       "      <td>61.982283</td>\n",
       "      <td>97.342958</td>\n",
       "      <td>86.903924</td>\n",
       "      <td>84.735880</td>\n",
       "      <td>...</td>\n",
       "      <td>85.081986</td>\n",
       "      <td>81.865454</td>\n",
       "      <td>96.068783</td>\n",
       "      <td>73.198179</td>\n",
       "      <td>86.111218</td>\n",
       "      <td>88.545774</td>\n",
       "      <td>80.169550</td>\n",
       "      <td>76.045810</td>\n",
       "      <td>74.291673</td>\n",
       "      <td>56.386748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>81.962929</td>\n",
       "      <td>85.052762</td>\n",
       "      <td>71.584087</td>\n",
       "      <td>68.405848</td>\n",
       "      <td>73.879542</td>\n",
       "      <td>105.246037</td>\n",
       "      <td>80.553156</td>\n",
       "      <td>87.584474</td>\n",
       "      <td>90.150839</td>\n",
       "      <td>84.384220</td>\n",
       "      <td>...</td>\n",
       "      <td>85.666249</td>\n",
       "      <td>91.608497</td>\n",
       "      <td>69.734030</td>\n",
       "      <td>65.366162</td>\n",
       "      <td>85.565140</td>\n",
       "      <td>91.927970</td>\n",
       "      <td>81.254355</td>\n",
       "      <td>84.759455</td>\n",
       "      <td>74.441114</td>\n",
       "      <td>68.843831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4     sensor5     sensor6  \\\n",
       "0     65.730466  89.453682  87.785620  76.587601   84.851017   69.058642   \n",
       "1     66.102980  87.224094  82.082242  81.680587  101.897605   89.818785   \n",
       "2     87.631890  66.257220  80.969001  72.930668   71.620128   94.681804   \n",
       "3     78.862885  70.386298  66.226441  67.022090   82.123529   91.918278   \n",
       "4     78.798557  82.741690  74.042955  94.149286  100.908840   95.783576   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "2438  82.108784  81.584543  86.016942  71.404227  101.371020   76.765853   \n",
       "2439  99.805372  85.037790  88.887016  76.925746   96.647426   83.254785   \n",
       "2440  81.841964  78.246787  78.208780  64.264379   78.449603   91.034501   \n",
       "2441  81.833020  81.762443  64.707536  70.649944   82.197035   76.806120   \n",
       "2442  81.962929  85.052762  71.584087  68.405848   73.879542  105.246037   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     87.202550  71.783088  83.486579  67.510694  ...  84.877108  82.619467   \n",
       "1     81.507159  81.829383  73.644202  84.450240  ...  73.485170  91.161226   \n",
       "2     74.722719  80.515246  87.898047  83.039718  ...  67.781748  78.291026   \n",
       "3     81.225184  81.979229  87.241442  77.722305  ...  94.550404  81.172616   \n",
       "4     79.224920  67.167455  79.834331  80.872271  ...  81.381189  69.274949   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  77.869570  88.196777  77.081792  66.228624  ...  77.961784  85.777203   \n",
       "2439  81.911410  77.866264  84.494610  73.992921  ...  66.455703  73.287247   \n",
       "2440  83.279032  71.713930  89.748354  77.469063  ...  81.866078  68.809098   \n",
       "2441  61.982283  97.342958  86.903924  84.735880  ...  85.081986  81.865454   \n",
       "2442  80.553156  87.584474  90.150839  84.384220  ...  85.666249  91.608497   \n",
       "\n",
       "       sensor41   sensor42    sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     79.588797  63.378421   87.883257  81.792326  88.326395  80.556565   \n",
       "1     87.614333  60.744124  102.136423  73.765738  75.956533  76.056711   \n",
       "2     79.990390  70.183516   90.568165  69.355478  85.019282  79.626234   \n",
       "3     72.587516  85.425354   94.797779  84.446558  75.434557  93.011442   \n",
       "4     85.586719  63.042794   83.646354  82.237603  90.288501  81.855912   \n",
       "...         ...        ...         ...        ...        ...        ...   \n",
       "2438  84.200662  64.667301   75.258813  77.262059  85.273278  81.702915   \n",
       "2439  68.975757  59.690427   97.523042  72.543085  78.237935  86.620870   \n",
       "2440  72.848124  64.968223   70.571008  83.034651  83.789545  87.322471   \n",
       "2441  96.068783  73.198179   86.111218  88.545774  80.169550  76.045810   \n",
       "2442  69.734030  65.366162   85.565140  91.927970  81.254355  84.759455   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     65.969870  79.732339  \n",
       "1     60.898188  77.393442  \n",
       "2     75.902175  84.732731  \n",
       "3     69.124145  79.875870  \n",
       "4     76.998217  84.330471  \n",
       "...         ...        ...  \n",
       "2438  68.892345  71.753840  \n",
       "2439  82.222681  60.468085  \n",
       "2440  70.519879  59.172377  \n",
       "2441  74.291673  56.386748  \n",
       "2442  74.441114  68.843831  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 18ms/step - loss: 4066.8398 - val_loss: 3908.2539\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3751.5271 - val_loss: 3682.6082\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3545.6641 - val_loss: 3492.4780\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3371.3743 - val_loss: 3329.7847\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3221.3667 - val_loss: 3188.5312\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3091.8716 - val_loss: 3066.3875\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2980.0640 - val_loss: 2960.6575\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2884.1155 - val_loss: 2869.8254\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2802.4072 - val_loss: 2792.4990\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2733.6052 - val_loss: 2727.3198\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2676.4583 - val_loss: 2673.3926\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2630.2439 - val_loss: 2629.8320\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2593.6362 - val_loss: 2595.5210\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2565.3469 - val_loss: 2569.3237\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2544.6106 - val_loss: 2549.9780\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2530.0837 - val_loss: 2536.4436\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2520.6489 - val_loss: 2527.5371\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.8135 - val_loss: 2522.1138\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.5967 - val_loss: 2518.9150\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2510.0371 - val_loss: 2517.3408\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2509.2908 - val_loss: 2516.4644\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2509.0317 - val_loss: 2515.9565\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9758 - val_loss: 2515.7236\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9204 - val_loss: 2515.6184\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9072 - val_loss: 2515.6980\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8584 - val_loss: 2515.5503\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9028 - val_loss: 2515.6240\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2508.8779 - val_loss: 2515.5176\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8635 - val_loss: 2515.8501\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8816 - val_loss: 2515.6274\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8455 - val_loss: 2515.8118\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8833 - val_loss: 2515.7234\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8767 - val_loss: 2515.7729\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9414 - val_loss: 2515.6558\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8972 - val_loss: 2515.7764\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8997 - val_loss: 2515.9189\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8997 - val_loss: 2515.7441\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8889 - val_loss: 2515.9668\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9529 - val_loss: 2515.8403\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8606 - val_loss: 2515.8838\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8601 - val_loss: 2515.7600\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9141 - val_loss: 2515.8169\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2508.8945 - val_loss: 2515.8347\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9004 - val_loss: 2515.9087\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8362 - val_loss: 2515.9026\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8706 - val_loss: 2515.7905\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8694 - val_loss: 2515.7227\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.9451 - val_loss: 2515.9268\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8774 - val_loss: 2515.7354\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8762 - val_loss: 2515.8582\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8589 - val_loss: 2515.8047\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8625 - val_loss: 2515.7283\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9055 - val_loss: 2515.6953\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8323 - val_loss: 2515.7847\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8511 - val_loss: 2515.6692\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8699 - val_loss: 2515.7236\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8940 - val_loss: 2515.6606\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8738 - val_loss: 2515.7615\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9109 - val_loss: 2515.6606\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9048 - val_loss: 2515.6521\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8982 - val_loss: 2515.7126\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8159 - val_loss: 2515.6375\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9080 - val_loss: 2515.8240\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9202 - val_loss: 2515.7056\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9255 - val_loss: 2515.8318\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9819 - val_loss: 2515.7847\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9072 - val_loss: 2515.8008\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8616 - val_loss: 2515.7214\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8762 - val_loss: 2515.8340\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9011 - val_loss: 2515.7507\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9104 - val_loss: 2515.5854\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8975 - val_loss: 2515.7751\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8787 - val_loss: 2515.7803\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8655 - val_loss: 2515.7498\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.9019 - val_loss: 2515.7454\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8572 - val_loss: 2515.7866\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8904 - val_loss: 2515.8103\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8921 - val_loss: 2515.7559\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8904 - val_loss: 2515.8142\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8918 - val_loss: 2515.7241\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.8752 - val_loss: 2515.6858\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2508.9277 - val_loss: 2515.7109\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8984 - val_loss: 2515.7307\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8743 - val_loss: 2515.6831\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9109 - val_loss: 2515.6880\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9160 - val_loss: 2515.6558\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2509.1411 - val_loss: 2517.7549\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2509.4431 - val_loss: 2516.0356\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.9480 - val_loss: 2515.9011\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9172 - val_loss: 2515.6682\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8779 - val_loss: 2515.5613\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8774 - val_loss: 2515.6409\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8801 - val_loss: 2515.5852\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8879 - val_loss: 2515.7173\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8652 - val_loss: 2515.7244\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2508.8970 - val_loss: 2515.7178\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.9238 - val_loss: 2515.6697\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8516 - val_loss: 2515.6426\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8362 - val_loss: 2515.8306\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.2773 - val_loss: 2513.6865\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2497.5759 - val_loss: 2493.2551\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2484.2058 - val_loss: 2481.2959\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2475.3950 - val_loss: 2475.4250\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2471.0322 - val_loss: 2468.0464\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2463.5090 - val_loss: 2458.8840\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2457.9607 - val_loss: 2454.1921\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2452.3533 - val_loss: 2447.7124\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2447.8525 - val_loss: 2443.2327\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2443.1018 - val_loss: 2437.5308\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 2437.6792 - val_loss: 2432.5422\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2432.4153 - val_loss: 2430.9333\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2428.4077 - val_loss: 2425.9888\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2423.5208 - val_loss: 2422.2710\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2419.6226 - val_loss: 2416.8220\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2417.0222 - val_loss: 2415.6282\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2413.1748 - val_loss: 2414.1628\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2407.3467 - val_loss: 2412.8928\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2404.1912 - val_loss: 2408.4495\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2399.6411 - val_loss: 2403.2964\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2395.6082 - val_loss: 2402.0264\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2394.5234 - val_loss: 2397.8025\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2389.4631 - val_loss: 2399.6707\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2385.4485 - val_loss: 2398.2571\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2383.8093 - val_loss: 2394.4395\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2379.0000 - val_loss: 2390.8970\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2374.5481 - val_loss: 2401.4751\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2368.1880 - val_loss: 2382.2012\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2365.5320 - val_loss: 2389.9807\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2357.2590 - val_loss: 2374.7812\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2353.8882 - val_loss: 2361.9390\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2343.0232 - val_loss: 2355.3262\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2335.8818 - val_loss: 2353.6865\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2326.1553 - val_loss: 2321.8845\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2320.7053 - val_loss: 2318.2642\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2307.6785 - val_loss: 2325.6572\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2295.3855 - val_loss: 2282.5269\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2288.3435 - val_loss: 2334.5300\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2305.3372 - val_loss: 2288.8013\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2270.3362 - val_loss: 2264.1130\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2256.0974 - val_loss: 2244.0295\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2300.4241 - val_loss: 2270.0559\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2254.2673 - val_loss: 2218.0283\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2176.9092 - val_loss: 2146.6201\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2124.9622 - val_loss: 2096.9905\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2077.9758 - val_loss: 2051.0625\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2042.3104 - val_loss: 2042.8693\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2002.5360 - val_loss: 1965.9945\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1971.7325 - val_loss: 1939.5814\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1929.7520 - val_loss: 1910.4913\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 1890.1455 - val_loss: 1915.1379\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1873.3905 - val_loss: 1840.9916\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1839.5327 - val_loss: 1847.0980\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1810.6696 - val_loss: 1819.4846\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1785.6702 - val_loss: 1789.0411\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1773.8759 - val_loss: 1743.0667\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1744.4735 - val_loss: 1775.8955\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1710.1810 - val_loss: 1700.8505\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1688.5027 - val_loss: 1671.4659\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1668.6411 - val_loss: 1630.4719\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1642.2175 - val_loss: 1618.7965\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1616.9009 - val_loss: 1650.0099\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1606.8151 - val_loss: 1594.6572\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1584.0659 - val_loss: 1544.0017\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1551.7811 - val_loss: 1554.1782\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1521.5740 - val_loss: 1508.2169\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1507.1891 - val_loss: 1520.6804\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1485.5925 - val_loss: 1461.6342\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1479.3300 - val_loss: 1457.3984\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1446.1399 - val_loss: 1439.3699\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1421.8479 - val_loss: 1375.2999\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1393.1736 - val_loss: 1451.6859\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1408.7413 - val_loss: 1373.4910\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1356.6310 - val_loss: 1371.6879\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1362.2080 - val_loss: 1329.6378\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1340.0745 - val_loss: 1310.1823\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1328.2568 - val_loss: 1331.4746\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1311.3235 - val_loss: 1273.5667\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1297.0557 - val_loss: 1281.2314\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1278.9526 - val_loss: 1291.1771\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1262.5360 - val_loss: 1229.5715\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1259.6952 - val_loss: 1251.9468\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1245.8452 - val_loss: 1234.5958\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1230.1819 - val_loss: 1200.9828\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1226.8191 - val_loss: 1174.7377\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1226.5059 - val_loss: 1194.9657\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1205.2688 - val_loss: 1281.1648\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1196.5565 - val_loss: 1236.6858\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1172.0387 - val_loss: 1197.2651\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1159.8859 - val_loss: 1241.1394\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1154.8341 - val_loss: 1362.8676\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1152.0153 - val_loss: 1129.2904\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1135.7056 - val_loss: 1108.0094\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1136.8927 - val_loss: 1174.8457\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1121.4135 - val_loss: 1085.6661\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1111.2947 - val_loss: 1079.3422\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1104.2922 - val_loss: 1162.4619\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1104.1691 - val_loss: 1050.0205\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1092.4849 - val_loss: 1041.0945\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1078.1914 - val_loss: 1100.4271\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1088.0452 - val_loss: 1055.7135\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 1055.713490775917\n",
      "Mean Absolute Error (MAE): 25.55945909785409\n",
      "Root Mean Squared Error (RMSE): 32.49174496354293\n",
      "Time taken: 2457.3764340877533\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 51ms/step - loss: 4021.7009 - val_loss: 3867.7664\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3707.6655 - val_loss: 3647.0251\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3505.3655 - val_loss: 3468.2122\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3335.4067 - val_loss: 3307.4436\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3183.8855 - val_loss: 3170.9580\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 3056.9609 - val_loss: 3054.4578\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2948.4189 - val_loss: 2954.1228\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2855.7778 - val_loss: 2868.8013\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2777.1768 - val_loss: 2796.3904\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2711.2085 - val_loss: 2735.5115\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2656.8555 - val_loss: 2685.8176\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2612.8660 - val_loss: 2645.7876\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2578.4260 - val_loss: 2614.4722\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2551.7649 - val_loss: 2590.4341\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2532.6121 - val_loss: 2573.5015\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2519.5305 - val_loss: 2562.1926\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2511.0811 - val_loss: 2554.9229\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2506.0496 - val_loss: 2550.7029\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2503.4077 - val_loss: 2548.4836\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.9932 - val_loss: 2547.2954\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.5002 - val_loss: 2546.7720\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2568 - val_loss: 2546.5615\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2119 - val_loss: 2546.5559\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1965 - val_loss: 2546.5825\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2146 - val_loss: 2546.5493\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2292 - val_loss: 2546.6187\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2190 - val_loss: 2546.5557\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1599 - val_loss: 2546.6519\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1907 - val_loss: 2546.6626\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1853 - val_loss: 2546.5308\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1912 - val_loss: 2546.5940\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2432 - val_loss: 2546.6848\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1665 - val_loss: 2546.5796\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1826 - val_loss: 2546.7073\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2192 - val_loss: 2546.7334\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2173 - val_loss: 2546.6670\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2209 - val_loss: 2546.6760\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1970 - val_loss: 2546.8813\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2217 - val_loss: 2546.6528\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2205 - val_loss: 2546.6665\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2114 - val_loss: 2546.5779\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2227 - val_loss: 2546.5769\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1633 - val_loss: 2546.6174\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2085 - val_loss: 2546.7090\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1626 - val_loss: 2546.8013\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2034 - val_loss: 2546.8240\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1797 - val_loss: 2546.6892\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2275 - val_loss: 2546.9021\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2334 - val_loss: 2546.7686\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2134 - val_loss: 2546.7085\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2100 - val_loss: 2546.6689\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2087 - val_loss: 2546.7290\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2053 - val_loss: 2546.7620\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1467 - val_loss: 2546.6919\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2534 - val_loss: 2546.7188\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2249 - val_loss: 2546.6907\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2351 - val_loss: 2546.7954\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1956 - val_loss: 2546.7112\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1548 - val_loss: 2546.6040\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2053 - val_loss: 2546.8176\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2764 - val_loss: 2546.6797\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2339 - val_loss: 2546.7117\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2332 - val_loss: 2546.7969\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2778 - val_loss: 2546.7329\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1743 - val_loss: 2546.7070\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2485 - val_loss: 2546.6206\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1616 - val_loss: 2546.5066\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1838 - val_loss: 2546.6917\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2192 - val_loss: 2546.8479\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1863 - val_loss: 2546.6692\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2590 - val_loss: 2546.8569\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1921 - val_loss: 2546.8142\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1858 - val_loss: 2546.8062\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2141 - val_loss: 2546.7646\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1589 - val_loss: 2546.5801\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2053 - val_loss: 2546.8337\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2437 - val_loss: 2546.7925\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2501.2451 - val_loss: 2546.7632\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2029 - val_loss: 2546.7153\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2532 - val_loss: 2546.7722\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2004 - val_loss: 2546.7375\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2495 - val_loss: 2546.7490\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2524 - val_loss: 2546.7354\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.1848 - val_loss: 2546.7695\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.3142 - val_loss: 2546.7483\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2886 - val_loss: 2546.7742\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2510 - val_loss: 2546.8767\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2012 - val_loss: 2546.7830\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2180 - val_loss: 2546.7476\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.1797 - val_loss: 2546.7437\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2278 - val_loss: 2546.9458\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1716 - val_loss: 2546.8220\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2073 - val_loss: 2546.9480\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1904 - val_loss: 2546.8127\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2437 - val_loss: 2546.9185\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2122 - val_loss: 2546.7898\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1807 - val_loss: 2546.7944\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1890 - val_loss: 2546.8516\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2722 - val_loss: 2546.7336\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2522 - val_loss: 2546.8254\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2373 - val_loss: 2546.7141\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.1731 - val_loss: 2546.7666\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.2212 - val_loss: 2546.6648\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2004 - val_loss: 2546.8381\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2501.2219 - val_loss: 2546.6768\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2273 - val_loss: 2546.6760\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2202 - val_loss: 2546.7217\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2501.1907 - val_loss: 2546.9551\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2493 - val_loss: 2546.8608\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2214 - val_loss: 2546.7454\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1965 - val_loss: 2546.7524\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2361 - val_loss: 2546.7205\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2034 - val_loss: 2546.7302\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2620 - val_loss: 2546.6846\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2007 - val_loss: 2546.8088\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2214 - val_loss: 2546.7314\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1614 - val_loss: 2546.7891\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1938 - val_loss: 2546.8093\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.1807 - val_loss: 2546.7893\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2542 - val_loss: 2546.7280\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2488 - val_loss: 2546.6853\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1704 - val_loss: 2546.8088\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2217 - val_loss: 2546.6196\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2280 - val_loss: 2546.7524\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2720 - val_loss: 2546.6926\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2363 - val_loss: 2546.7126\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2175 - val_loss: 2546.8145\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2251 - val_loss: 2546.7400\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2170 - val_loss: 2546.6643\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2188 - val_loss: 2546.6763\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2175 - val_loss: 2546.6914\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2341 - val_loss: 2546.8276\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2266 - val_loss: 2546.7349\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1997 - val_loss: 2546.8381\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1592 - val_loss: 2546.9216\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2501.2126 - val_loss: 2546.8923\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2501.2068 - val_loss: 2546.8804\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1587 - val_loss: 2546.8867\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1670 - val_loss: 2546.9348\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2214 - val_loss: 2546.9106\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2476 - val_loss: 2546.7170\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1785 - val_loss: 2546.9060\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1812 - val_loss: 2546.7964\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1772 - val_loss: 2546.8352\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2029 - val_loss: 2546.8252\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2761 - val_loss: 2546.8933\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2607 - val_loss: 2546.9727\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2051 - val_loss: 2546.8811\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1975 - val_loss: 2546.8586\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2119 - val_loss: 2546.9585\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1892 - val_loss: 2546.7834\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2612 - val_loss: 2546.8438\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2501.1858 - val_loss: 2546.9009\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1770 - val_loss: 2546.7783\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2288 - val_loss: 2546.9368\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2249 - val_loss: 2546.9365\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.3076 - val_loss: 2546.8372\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1711 - val_loss: 2546.9568\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1689 - val_loss: 2546.7654\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1687 - val_loss: 2546.8970\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.3030 - val_loss: 2546.9443\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2341 - val_loss: 2546.6714\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1379 - val_loss: 2546.6301\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2451 - val_loss: 2546.6528\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1470 - val_loss: 2546.7876\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2759 - val_loss: 2546.7620\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1965 - val_loss: 2546.7976\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2041 - val_loss: 2546.7388\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1921 - val_loss: 2546.7832\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2292 - val_loss: 2546.6853\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1794 - val_loss: 2546.8411\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1580 - val_loss: 2546.9897\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1953 - val_loss: 2546.9114\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1943 - val_loss: 2546.7512\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1965 - val_loss: 2546.6047\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1919 - val_loss: 2546.7705\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2095 - val_loss: 2546.7986\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1812 - val_loss: 2546.7488\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2043 - val_loss: 2546.7935\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2366 - val_loss: 2546.8198\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2139 - val_loss: 2546.5676\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1685 - val_loss: 2546.6216\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2163 - val_loss: 2546.5623\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2036 - val_loss: 2546.6448\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1914 - val_loss: 2546.6123\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1965 - val_loss: 2546.6580\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1604 - val_loss: 2546.5828\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2666 - val_loss: 2546.7874\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2080 - val_loss: 2546.5347\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2202 - val_loss: 2546.7939\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2288 - val_loss: 2546.8076\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1545 - val_loss: 2546.7378\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2332 - val_loss: 2546.7253\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2869 - val_loss: 2546.9478\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2305 - val_loss: 2546.9500\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1650 - val_loss: 2546.8191\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2815 - val_loss: 2546.7815\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.1636 - val_loss: 2546.7366\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2668 - val_loss: 2546.8545\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.2168 - val_loss: 2546.8799\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 2546.8798202118764\n",
      "Mean Absolute Error (MAE): 39.621613359144305\n",
      "Root Mean Squared Error (RMSE): 50.46662085192426\n",
      "Time taken: 3840.1102771759033\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 51ms/step - loss: 4018.3601 - val_loss: 3859.6138\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3706.6184 - val_loss: 3636.6030\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3505.5649 - val_loss: 3452.0203\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3336.8777 - val_loss: 3293.6897\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3191.5378 - val_loss: 3156.8000\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3065.6680 - val_loss: 3038.7729\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2957.1455 - val_loss: 2937.4392\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2864.2598 - val_loss: 2850.9084\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2785.3318 - val_loss: 2778.2607\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2718.9604 - val_loss: 2717.4199\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2663.9253 - val_loss: 2667.8494\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2619.3850 - val_loss: 2628.4407\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2584.3728 - val_loss: 2597.7239\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2557.6094 - val_loss: 2575.0051\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2537.9539 - val_loss: 2558.5706\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2523.8140 - val_loss: 2547.6118\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2514.9580 - val_loss: 2540.8159\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2509.8152 - val_loss: 2537.1667\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2506.9470 - val_loss: 2535.2087\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2505.6333 - val_loss: 2534.1770\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.9958 - val_loss: 2533.8354\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.7737 - val_loss: 2533.6833\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.7610 - val_loss: 2533.8484\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.7053 - val_loss: 2533.7537\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6577 - val_loss: 2533.5610\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6472 - val_loss: 2533.5186\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6648 - val_loss: 2533.5388\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6790 - val_loss: 2533.3455\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6641 - val_loss: 2533.3359\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6821 - val_loss: 2533.3823\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6479 - val_loss: 2533.1511\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6628 - val_loss: 2533.4900\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.5813 - val_loss: 2533.2556\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6636 - val_loss: 2533.3130\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6741 - val_loss: 2533.4250\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6499 - val_loss: 2533.4373\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6311 - val_loss: 2533.3030\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6609 - val_loss: 2533.3784\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6541 - val_loss: 2533.3083\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6938 - val_loss: 2533.2896\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6548 - val_loss: 2533.1643\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6484 - val_loss: 2533.2449\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6587 - val_loss: 2533.2402\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6306 - val_loss: 2533.2114\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6689 - val_loss: 2533.1516\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6514 - val_loss: 2533.2336\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6443 - val_loss: 2533.1255\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6592 - val_loss: 2533.2939\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.7080 - val_loss: 2533.1873\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6497 - val_loss: 2533.3508\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.5981 - val_loss: 2533.2810\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6025 - val_loss: 2533.2993\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.8237 - val_loss: 2533.8193\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.7935 - val_loss: 2533.8477\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6953 - val_loss: 2533.5911\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6626 - val_loss: 2533.5471\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6763 - val_loss: 2533.5469\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6748 - val_loss: 2533.6453\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2504.6978 - val_loss: 2533.6824\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.0425 - val_loss: 2517.8792\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2486.5754 - val_loss: 2511.6211\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2480.9690 - val_loss: 2501.1948\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2477.3948 - val_loss: 2497.4402\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2474.0444 - val_loss: 2495.3545\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2470.6187 - val_loss: 2497.9382\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2467.6721 - val_loss: 2487.8733\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2463.4204 - val_loss: 2480.9812\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2458.7961 - val_loss: 2476.0867\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2451.8252 - val_loss: 2475.6052\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2446.7129 - val_loss: 2463.7273\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2439.5203 - val_loss: 2452.8311\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2430.4944 - val_loss: 2442.0925\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2416.7625 - val_loss: 2425.1863\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2387.9946 - val_loss: 2399.0339\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 2360.2068 - val_loss: 2372.1108\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2329.0994 - val_loss: 2346.4094\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2304.6350 - val_loss: 2319.4475\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2270.5044 - val_loss: 2278.9783\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2242.7397 - val_loss: 2251.8264\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2207.3054 - val_loss: 2222.7327\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2176.2925 - val_loss: 2262.6660\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2140.4895 - val_loss: 2145.7217\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2101.7546 - val_loss: 2122.9001\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2065.7456 - val_loss: 2105.9106\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2040.1685 - val_loss: 2066.7490\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2005.1844 - val_loss: 2058.9333\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1978.5436 - val_loss: 2002.2957\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1952.8221 - val_loss: 1974.5466\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1921.5004 - val_loss: 1965.1741\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1900.0171 - val_loss: 1947.3608\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1865.1010 - val_loss: 1926.5846\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1847.2017 - val_loss: 1879.7499\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1815.3622 - val_loss: 1851.9862\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1785.7864 - val_loss: 1898.7939\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1769.5109 - val_loss: 1842.1666\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1729.3021 - val_loss: 1776.3308\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1713.0686 - val_loss: 1800.8152\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1695.6405 - val_loss: 1757.6306\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1675.9718 - val_loss: 1695.1683\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1654.2692 - val_loss: 1704.4064\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1612.2834 - val_loss: 1659.3905\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1584.5088 - val_loss: 1620.2172\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1567.2775 - val_loss: 1620.9019\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1552.7681 - val_loss: 1599.3904\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1532.4796 - val_loss: 1541.7368\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1503.2244 - val_loss: 1559.3507\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1489.0095 - val_loss: 1567.0754\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1473.2166 - val_loss: 1511.9504\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1453.7178 - val_loss: 1503.3245\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1419.1536 - val_loss: 1468.5967\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1412.5441 - val_loss: 1462.6379\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1384.0304 - val_loss: 1446.3192\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1378.8469 - val_loss: 1464.9099\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1359.3967 - val_loss: 1457.9084\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1352.6112 - val_loss: 1410.2699\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1327.2533 - val_loss: 1426.8639\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1351.9153 - val_loss: 1382.3823\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1306.1437 - val_loss: 1353.6920\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1292.3230 - val_loss: 1362.2042\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1276.7577 - val_loss: 1354.5857\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1260.4990 - val_loss: 1328.7008\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1253.4473 - val_loss: 1305.6365\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1227.2821 - val_loss: 1293.7960\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1230.7146 - val_loss: 1321.1958\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1221.1956 - val_loss: 1295.5044\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1214.7505 - val_loss: 1299.3665\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1194.4552 - val_loss: 1251.6615\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1187.0789 - val_loss: 1234.8505\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1168.7949 - val_loss: 1249.5592\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1149.7815 - val_loss: 1358.8613\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1154.9714 - val_loss: 1228.9154\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1138.5448 - val_loss: 1230.4338\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1147.2640 - val_loss: 1226.0532\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1121.7341 - val_loss: 1186.4603\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1119.8691 - val_loss: 1209.7759\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1114.7552 - val_loss: 1193.5789\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1106.1912 - val_loss: 1189.1273\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1095.0886 - val_loss: 1185.2531\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1088.0713 - val_loss: 1186.3723\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1082.7899 - val_loss: 1144.9609\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1084.4135 - val_loss: 1175.5475\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1066.4213 - val_loss: 1140.9324\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1056.9801 - val_loss: 1260.5928\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1064.8442 - val_loss: 1160.4922\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1045.5955 - val_loss: 1110.8953\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1037.2830 - val_loss: 1175.7800\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1034.2837 - val_loss: 1104.9890\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1018.1296 - val_loss: 1087.8944\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 1018.7583 - val_loss: 1110.9243\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1008.9170 - val_loss: 1179.8661\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1007.2994 - val_loss: 1104.0385\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1018.3180 - val_loss: 1094.6122\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 995.9534 - val_loss: 1076.8684\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 990.2579 - val_loss: 1138.8630\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 985.1273 - val_loss: 1121.3176\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 984.8740 - val_loss: 1066.8271\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 968.8082 - val_loss: 1076.0399\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 961.1783 - val_loss: 1061.1296\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 962.4343 - val_loss: 1061.2705\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 968.8237 - val_loss: 1044.3202\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 969.2526 - val_loss: 1047.4318\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 940.8647 - val_loss: 1029.8052\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 937.5078 - val_loss: 1054.7728\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.0421 - val_loss: 1031.3267\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 916.6884 - val_loss: 1033.0509\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 935.8922 - val_loss: 1016.5178\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 905.6383 - val_loss: 1042.6910\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 902.0623 - val_loss: 1007.9772\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 885.8453 - val_loss: 1016.9744\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 880.6219 - val_loss: 1070.6028\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 882.7298 - val_loss: 986.5173\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 898.1083 - val_loss: 958.8500\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 872.8240 - val_loss: 963.1884\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 867.4263 - val_loss: 966.4011\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 863.2532 - val_loss: 984.6551\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 861.0444 - val_loss: 969.2369\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 855.9853 - val_loss: 977.4177\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 856.0980 - val_loss: 1037.0643\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 851.0482 - val_loss: 942.6852\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 856.6785 - val_loss: 980.7966\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 840.3593 - val_loss: 945.2160\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 845.1653 - val_loss: 987.0589\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 832.4202 - val_loss: 937.5202\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 825.6080 - val_loss: 932.6005\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 813.4365 - val_loss: 1009.4216\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 846.8693 - val_loss: 939.2111\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 815.4470 - val_loss: 921.4632\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 806.3909 - val_loss: 1004.2156\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 807.6881 - val_loss: 964.1759\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 809.1840 - val_loss: 960.8881\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 803.5074 - val_loss: 929.1844\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 798.7045 - val_loss: 926.9321\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 802.1772 - val_loss: 947.4333\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 789.4072 - val_loss: 928.7519\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 785.0600 - val_loss: 953.2718\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 788.5182 - val_loss: 927.2247\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 770.9041 - val_loss: 921.1985\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 766.8395 - val_loss: 932.0486\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 774.1744 - val_loss: 911.0854\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 778.2003 - val_loss: 912.5793\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 912.5793591522533\n",
      "Mean Absolute Error (MAE): 22.910065177096637\n",
      "Root Mean Squared Error (RMSE): 30.208928467462286\n",
      "Time taken: 3835.3740582466125\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 4135.9453 - val_loss: 3806.9680\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3811.1631 - val_loss: 3567.5461\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3597.8901 - val_loss: 3369.2793\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3418.2754 - val_loss: 3200.9268\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3264.3579 - val_loss: 3056.6680\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3131.8286 - val_loss: 2932.7493\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3017.8335 - val_loss: 2827.0842\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2919.9644 - val_loss: 2736.8630\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2836.6672 - val_loss: 2661.0520\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2766.3447 - val_loss: 2597.9543\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2708.1104 - val_loss: 2546.6880\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2660.8528 - val_loss: 2506.1978\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2623.2534 - val_loss: 2474.2358\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2594.3286 - val_loss: 2450.9541\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2572.9497 - val_loss: 2434.3401\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2557.8882 - val_loss: 2423.4954\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2547.9746 - val_loss: 2416.5435\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2541.7839 - val_loss: 2412.9077\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2538.4080 - val_loss: 2411.2295\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2536.6509 - val_loss: 2410.5999\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.9111 - val_loss: 2410.5471\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.6082 - val_loss: 2410.5046\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4988 - val_loss: 2410.5408\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4585 - val_loss: 2410.6248\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4827 - val_loss: 2410.5142\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4307 - val_loss: 2410.2854\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4099 - val_loss: 2410.4211\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4126 - val_loss: 2410.5093\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4170 - val_loss: 2410.4006\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4082 - val_loss: 2410.5874\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4851 - val_loss: 2410.4966\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4641 - val_loss: 2410.3467\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.3955 - val_loss: 2410.3958\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4563 - val_loss: 2410.2366\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4771 - val_loss: 2410.3625\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4199 - val_loss: 2410.2566\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4863 - val_loss: 2410.2949\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4502 - val_loss: 2410.2871\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4575 - val_loss: 2410.5334\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4524 - val_loss: 2410.5110\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4861 - val_loss: 2410.4475\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4453 - val_loss: 2410.5059\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4390 - val_loss: 2410.4775\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4387 - val_loss: 2410.4021\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4390 - val_loss: 2410.4211\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4380 - val_loss: 2410.2241\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4663 - val_loss: 2410.5188\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4570 - val_loss: 2410.2825\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4600 - val_loss: 2410.3508\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4832 - val_loss: 2410.3237\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4692 - val_loss: 2410.3708\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4775 - val_loss: 2410.3352\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4875 - val_loss: 2410.3013\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4670 - val_loss: 2410.2812\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4514 - val_loss: 2410.2559\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4136 - val_loss: 2410.1643\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4797 - val_loss: 2410.3413\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4907 - val_loss: 2410.3257\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4893 - val_loss: 2410.4924\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4934 - val_loss: 2410.2725\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4250 - val_loss: 2410.2708\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4390 - val_loss: 2410.3010\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4739 - val_loss: 2410.4155\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4163 - val_loss: 2410.3235\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4927 - val_loss: 2410.3818\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4319 - val_loss: 2410.4358\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.3928 - val_loss: 2410.2886\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4729 - val_loss: 2410.2483\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5217 - val_loss: 2410.3005\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4626 - val_loss: 2410.2043\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4482 - val_loss: 2410.0215\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5044 - val_loss: 2410.1492\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5059 - val_loss: 2410.1345\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4377 - val_loss: 2410.2793\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4709 - val_loss: 2410.4126\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5015 - val_loss: 2410.5076\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4607 - val_loss: 2410.3564\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5024 - val_loss: 2410.4990\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4380 - val_loss: 2410.4119\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4668 - val_loss: 2410.4360\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.3945 - val_loss: 2410.4250\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4546 - val_loss: 2410.3088\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2536.2783 - val_loss: 2412.3813\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2536.3582 - val_loss: 2410.9629\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.5825 - val_loss: 2410.9563\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.4500 - val_loss: 2410.7922\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.2422 - val_loss: 2410.4688\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2533.9526 - val_loss: 2407.9944\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2521.4880 - val_loss: 2410.8210\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2509.4058 - val_loss: 2397.1985\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.7791 - val_loss: 2395.3765\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.4521 - val_loss: 2390.7664\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2495.0627 - val_loss: 2389.0349\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2491.2961 - val_loss: 2386.5896\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2485.4417 - val_loss: 2383.0198\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2480.9924 - val_loss: 2379.2593\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2475.6106 - val_loss: 2375.0396\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2469.9500 - val_loss: 2367.6697\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2463.5530 - val_loss: 2364.1804\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2455.8464 - val_loss: 2352.8381\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2447.6455 - val_loss: 2344.2883\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2438.1052 - val_loss: 2337.9910\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2432.6367 - val_loss: 2328.7795\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2423.4780 - val_loss: 2324.8257\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2414.9492 - val_loss: 2315.9131\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2405.6069 - val_loss: 2307.2725\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2385.7471 - val_loss: 2279.9436\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2349.6001 - val_loss: 2270.9258\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2318.0649 - val_loss: 2221.6804\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2281.5034 - val_loss: 2168.7791\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2242.3674 - val_loss: 2143.6597\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2205.4680 - val_loss: 2109.1445\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2179.4236 - val_loss: 2079.5701\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2146.1150 - val_loss: 2078.9441\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2109.1680 - val_loss: 2023.4712\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2074.9114 - val_loss: 2043.3276\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2041.5474 - val_loss: 1953.8003\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2016.5023 - val_loss: 2000.2863\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1988.0806 - val_loss: 1898.3191\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1944.6367 - val_loss: 1900.2500\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1933.3630 - val_loss: 1846.2274\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1910.8893 - val_loss: 1805.2170\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1857.2822 - val_loss: 1783.7997\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1841.5291 - val_loss: 1786.4630\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1804.5385 - val_loss: 1731.6224\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1795.6147 - val_loss: 1744.0604\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1766.0491 - val_loss: 1733.5685\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1733.8414 - val_loss: 1676.0104\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1716.8555 - val_loss: 1660.4720\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1707.8903 - val_loss: 1645.0077\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1673.0197 - val_loss: 1637.3042\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1655.8760 - val_loss: 1605.8749\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1623.8286 - val_loss: 1589.9153\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1602.0508 - val_loss: 1561.1324\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1585.1836 - val_loss: 1558.9211\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1565.8767 - val_loss: 1572.3153\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1536.8286 - val_loss: 1562.3649\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1522.2961 - val_loss: 1503.4802\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1505.2131 - val_loss: 1450.5035\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1458.1375 - val_loss: 1440.0554\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1478.3690 - val_loss: 1415.9736\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1430.0564 - val_loss: 1417.5546\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1413.3326 - val_loss: 1371.2118\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1385.5189 - val_loss: 1353.0406\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1374.8424 - val_loss: 1373.6287\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1362.9844 - val_loss: 1326.4869\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1347.8868 - val_loss: 1308.3552\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1319.6965 - val_loss: 1316.4496\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 1320.7938 - val_loss: 1285.5594\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1305.0540 - val_loss: 1257.1553\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1271.6934 - val_loss: 1272.8093\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1268.1429 - val_loss: 1258.7438\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1254.5214 - val_loss: 1283.2313\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1225.6152 - val_loss: 1225.4067\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1219.4387 - val_loss: 1215.6376\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1201.3325 - val_loss: 1192.1890\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1201.7119 - val_loss: 1181.8723\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1177.7732 - val_loss: 1197.3116\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1162.8599 - val_loss: 1209.5649\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1151.7346 - val_loss: 1148.6239\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1139.6571 - val_loss: 1146.5332\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1117.7051 - val_loss: 1297.7642\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1136.0439 - val_loss: 1171.0448\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1112.1975 - val_loss: 1124.4456\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1108.0540 - val_loss: 1111.2375\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1101.4417 - val_loss: 1131.7462\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1088.6024 - val_loss: 1099.1534\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1082.9432 - val_loss: 1099.0774\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1063.5317 - val_loss: 1111.7238\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1073.0415 - val_loss: 1147.7087\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1049.8145 - val_loss: 1094.2924\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1045.5106 - val_loss: 1097.6193\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1035.9446 - val_loss: 1103.0836\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1029.2697 - val_loss: 1085.3221\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1022.8654 - val_loss: 1061.8899\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1021.7571 - val_loss: 1064.5134\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1016.6558 - val_loss: 1108.9592\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1007.7891 - val_loss: 1103.8768\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 993.9847 - val_loss: 1025.0027\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1000.7571 - val_loss: 1097.7231\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 988.3776 - val_loss: 1055.1901\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 987.5551 - val_loss: 1015.7136\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 983.5003 - val_loss: 1108.6354\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 990.6436 - val_loss: 1065.9481\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 964.3314 - val_loss: 1069.3416\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 975.9807 - val_loss: 999.9434\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 966.7148 - val_loss: 1021.5994\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 963.1367 - val_loss: 1025.2870\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 952.6491 - val_loss: 1009.0496\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 956.9420 - val_loss: 1022.8888\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 943.3152 - val_loss: 1025.7263\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 944.6693 - val_loss: 1028.7405\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 931.2776 - val_loss: 1010.6774\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 922.0754 - val_loss: 1080.5878\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 945.8647 - val_loss: 1003.2208\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.2014 - val_loss: 984.1461\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 912.6018 - val_loss: 966.4912\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 900.1683 - val_loss: 966.8762\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 909.1313 - val_loss: 1013.3282\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 900.8665 - val_loss: 1048.9644\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 1048.9808139295383\n",
      "Mean Absolute Error (MAE): 24.724887468938558\n",
      "Root Mean Squared Error (RMSE): 32.38797329147871\n",
      "Time taken: 3807.076354265213\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 4085.3274 - val_loss: 3883.0398\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3760.5515 - val_loss: 3659.3137\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3551.5286 - val_loss: 3474.1772\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3374.9954 - val_loss: 3315.8667\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3222.8430 - val_loss: 3179.2708\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3091.3179 - val_loss: 3059.5884\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2975.2004 - val_loss: 2955.8462\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2877.2542 - val_loss: 2869.2178\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2794.4670 - val_loss: 2796.2505\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2724.9312 - val_loss: 2735.6399\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2667.5359 - val_loss: 2686.1311\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2620.9739 - val_loss: 2646.6873\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2584.1785 - val_loss: 2616.2427\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2555.9150 - val_loss: 2593.5798\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2535.1201 - val_loss: 2577.3982\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2520.5522 - val_loss: 2566.8049\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2510.9009 - val_loss: 2560.4072\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2505.1135 - val_loss: 2556.7097\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.7598 - val_loss: 2555.1816\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2500.2207 - val_loss: 2554.5513\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.5491 - val_loss: 2554.5076\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.2422 - val_loss: 2554.5630\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1765 - val_loss: 2554.5874\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1460 - val_loss: 2554.7029\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.2375 - val_loss: 2554.8721\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1877 - val_loss: 2554.7500\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1089 - val_loss: 2554.6838\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1389 - val_loss: 2554.7258\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0979 - val_loss: 2555.0039\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1145 - val_loss: 2554.8687\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1040 - val_loss: 2554.7285\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1414 - val_loss: 2554.9697\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1030 - val_loss: 2554.8813\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0894 - val_loss: 2554.9290\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1040 - val_loss: 2554.7781\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2499.1199 - val_loss: 2554.8633\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2499.0669 - val_loss: 2554.8584\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2499.1099 - val_loss: 2554.8418\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1284 - val_loss: 2554.7991\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1665 - val_loss: 2554.8027\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1118 - val_loss: 2554.8689\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1150 - val_loss: 2554.8796\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1560 - val_loss: 2554.8782\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1550 - val_loss: 2554.8813\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.0896 - val_loss: 2554.8076\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1494 - val_loss: 2554.9583\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1538 - val_loss: 2554.8855\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1099 - val_loss: 2554.9424\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1436 - val_loss: 2554.9600\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1296 - val_loss: 2554.9854\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1482 - val_loss: 2555.0044\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1477 - val_loss: 2555.0930\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1035 - val_loss: 2554.9705\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1274 - val_loss: 2554.9656\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1416 - val_loss: 2554.8367\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1272 - val_loss: 2554.9617\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1384 - val_loss: 2554.9197\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1567 - val_loss: 2554.9392\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1719 - val_loss: 2554.8987\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.0613 - val_loss: 2554.8821\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.0979 - val_loss: 2554.8511\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1453 - val_loss: 2554.8560\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1401 - val_loss: 2554.9275\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.0955 - val_loss: 2554.8979\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1118 - val_loss: 2554.9236\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1018 - val_loss: 2554.8826\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1289 - val_loss: 2554.8301\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1528 - val_loss: 2554.8364\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0967 - val_loss: 2554.7812\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1680 - val_loss: 2554.8752\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1274 - val_loss: 2554.8706\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2499.1084 - val_loss: 2554.9592\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1013 - val_loss: 2554.9316\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1182 - val_loss: 2554.9883\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1067 - val_loss: 2554.9294\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1709 - val_loss: 2554.9043\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0859 - val_loss: 2554.8706\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1274 - val_loss: 2554.8262\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1321 - val_loss: 2554.8853\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1111 - val_loss: 2554.8770\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0706 - val_loss: 2554.9043\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 2499.1609 - val_loss: 2554.8589\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 2499.1348 - val_loss: 2554.9253\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 2499.1436 - val_loss: 2554.8435\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 2499.0732 - val_loss: 2554.9023\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2499.1135 - val_loss: 2554.9739\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2499.1277 - val_loss: 2554.9583\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1125 - val_loss: 2554.8230\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1182 - val_loss: 2554.7520\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1228 - val_loss: 2554.7876\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1804 - val_loss: 2554.8213\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1404 - val_loss: 2554.9502\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1509 - val_loss: 2554.8623\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1570 - val_loss: 2554.9316\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1221 - val_loss: 2554.8699\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.2219 - val_loss: 2554.9485\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1431 - val_loss: 2554.9641\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1042 - val_loss: 2554.9019\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0977 - val_loss: 2554.8782\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1497 - val_loss: 2554.8657\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1577 - val_loss: 2554.7732\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0972 - val_loss: 2554.7634\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1582 - val_loss: 2554.8435\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.0823 - val_loss: 2554.8655\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1663 - val_loss: 2555.0015\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1792 - val_loss: 2554.8276\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1306 - val_loss: 2554.9355\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2499.1191 - val_loss: 2554.8418\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1008 - val_loss: 2554.9263\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1606 - val_loss: 2554.9546\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0999 - val_loss: 2554.9802\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1296 - val_loss: 2554.7644\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1399 - val_loss: 2554.8311\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1553 - val_loss: 2555.0181\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1182 - val_loss: 2554.9675\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1423 - val_loss: 2554.9407\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0894 - val_loss: 2554.9856\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1523 - val_loss: 2554.8804\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1382 - val_loss: 2554.9351\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1345 - val_loss: 2555.1001\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1340 - val_loss: 2554.8860\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1311 - val_loss: 2554.9656\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2499.1506 - val_loss: 2554.9934\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1165 - val_loss: 2555.0085\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0957 - val_loss: 2554.9536\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1123 - val_loss: 2554.8918\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0864 - val_loss: 2555.0034\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0925 - val_loss: 2554.9619\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2499.1572 - val_loss: 2554.9600\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1086 - val_loss: 2554.9617\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1248 - val_loss: 2554.9844\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1233 - val_loss: 2554.8989\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1565 - val_loss: 2554.8738\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0525 - val_loss: 2554.9675\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1331 - val_loss: 2554.9648\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1843 - val_loss: 2554.9106\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1104 - val_loss: 2554.9487\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1160 - val_loss: 2554.9048\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1355 - val_loss: 2555.0193\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1250 - val_loss: 2554.9583\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1809 - val_loss: 2555.0559\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0977 - val_loss: 2555.0635\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1235 - val_loss: 2555.0464\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1184 - val_loss: 2554.9890\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1138 - val_loss: 2554.9041\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2499.1418 - val_loss: 2555.1060\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2499.1375 - val_loss: 2555.0112\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2499.1487 - val_loss: 2555.0310\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1357 - val_loss: 2554.9214\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2499.1096 - val_loss: 2555.0032\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0808 - val_loss: 2554.9578\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1775 - val_loss: 2555.0396\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1272 - val_loss: 2554.9937\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1230 - val_loss: 2554.9531\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1077 - val_loss: 2554.9878\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0825 - val_loss: 2554.8887\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1790 - val_loss: 2554.8423\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1128 - val_loss: 2554.7600\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1409 - val_loss: 2554.8511\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1279 - val_loss: 2554.8638\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1125 - val_loss: 2554.9370\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1099 - val_loss: 2554.8901\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1069 - val_loss: 2554.8628\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1497 - val_loss: 2555.0046\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1619 - val_loss: 2554.9863\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1089 - val_loss: 2554.8125\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1431 - val_loss: 2554.9683\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1738 - val_loss: 2554.8867\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1399 - val_loss: 2554.7991\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1121 - val_loss: 2554.7939\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1060 - val_loss: 2554.8186\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1350 - val_loss: 2555.0579\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.2092 - val_loss: 2555.0161\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1165 - val_loss: 2555.0256\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0964 - val_loss: 2554.9033\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0627 - val_loss: 2554.9331\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1323 - val_loss: 2554.9834\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.0811 - val_loss: 2554.9358\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1350 - val_loss: 2555.0352\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1233 - val_loss: 2554.9934\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1323 - val_loss: 2554.8381\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1174 - val_loss: 2554.8286\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1873 - val_loss: 2554.9355\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1184 - val_loss: 2554.9390\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1030 - val_loss: 2554.8499\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1104 - val_loss: 2554.8025\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1069 - val_loss: 2554.9011\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1169 - val_loss: 2554.7949\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1201 - val_loss: 2554.8699\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1509 - val_loss: 2554.9915\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1121 - val_loss: 2554.8594\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1509 - val_loss: 2554.9038\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1733 - val_loss: 2554.8118\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1135 - val_loss: 2554.9731\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1091 - val_loss: 2554.9868\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1375 - val_loss: 2554.8501\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1287 - val_loss: 2554.9053\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1328 - val_loss: 2555.0312\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1550 - val_loss: 2554.8630\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2499.1123 - val_loss: 2554.9326\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 2554.932761969918\n",
      "Mean Absolute Error (MAE): 39.79190685804604\n",
      "Root Mean Squared Error (RMSE): 50.546342716065205\n",
      "Time taken: 3843.8471269607544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_12856\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold          MSE        MAE       RMSE   Time taken\n",
      "0        1  1055.713491  25.559459  32.491745  2457.376434\n",
      "1        2  2546.879820  39.621613  50.466621  3840.110277\n",
      "2        3   912.579359  22.910065  30.208928  3835.374058\n",
      "3        4  1048.980814  24.724887  32.387973  3807.076354\n",
      "4        5  2554.932762  39.791907  50.546343  3843.847127\n",
      "5  Average  1623.817249  30.521586  39.220322  3556.756850\n",
      "Results saved to 'DL_Result_PL_model_2_Scattered_Reg2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_Scattered_Reg2.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_2_Scattered_Reg2.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
