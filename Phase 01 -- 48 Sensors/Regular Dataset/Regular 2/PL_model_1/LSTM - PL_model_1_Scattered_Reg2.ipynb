{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_Reg2.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172.360911</td>\n",
       "      <td>198.698898</td>\n",
       "      <td>121.363501</td>\n",
       "      <td>160.748513</td>\n",
       "      <td>202.499388</td>\n",
       "      <td>232.822808</td>\n",
       "      <td>168.632401</td>\n",
       "      <td>199.493193</td>\n",
       "      <td>164.679481</td>\n",
       "      <td>177.628746</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732384</td>\n",
       "      <td>160.030382</td>\n",
       "      <td>169.678207</td>\n",
       "      <td>135.263693</td>\n",
       "      <td>214.033213</td>\n",
       "      <td>180.031244</td>\n",
       "      <td>162.028360</td>\n",
       "      <td>189.460195</td>\n",
       "      <td>110.539577</td>\n",
       "      <td>152.588802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.296893</td>\n",
       "      <td>193.903892</td>\n",
       "      <td>115.600204</td>\n",
       "      <td>155.129616</td>\n",
       "      <td>214.455008</td>\n",
       "      <td>219.996785</td>\n",
       "      <td>169.183957</td>\n",
       "      <td>199.994701</td>\n",
       "      <td>161.466782</td>\n",
       "      <td>185.240605</td>\n",
       "      <td>...</td>\n",
       "      <td>165.732941</td>\n",
       "      <td>160.236901</td>\n",
       "      <td>172.716496</td>\n",
       "      <td>133.458960</td>\n",
       "      <td>212.467698</td>\n",
       "      <td>189.105376</td>\n",
       "      <td>158.462646</td>\n",
       "      <td>178.879499</td>\n",
       "      <td>114.537605</td>\n",
       "      <td>152.156101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176.651492</td>\n",
       "      <td>190.236048</td>\n",
       "      <td>116.590044</td>\n",
       "      <td>170.190377</td>\n",
       "      <td>193.769774</td>\n",
       "      <td>219.342484</td>\n",
       "      <td>168.475106</td>\n",
       "      <td>191.204758</td>\n",
       "      <td>164.559102</td>\n",
       "      <td>186.545886</td>\n",
       "      <td>...</td>\n",
       "      <td>160.350716</td>\n",
       "      <td>154.632700</td>\n",
       "      <td>178.171720</td>\n",
       "      <td>125.746017</td>\n",
       "      <td>206.064571</td>\n",
       "      <td>183.335022</td>\n",
       "      <td>148.591627</td>\n",
       "      <td>182.388360</td>\n",
       "      <td>110.232965</td>\n",
       "      <td>144.684928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169.240405</td>\n",
       "      <td>194.123707</td>\n",
       "      <td>129.025909</td>\n",
       "      <td>156.078737</td>\n",
       "      <td>200.534552</td>\n",
       "      <td>216.315637</td>\n",
       "      <td>175.937287</td>\n",
       "      <td>209.187138</td>\n",
       "      <td>169.290122</td>\n",
       "      <td>178.631353</td>\n",
       "      <td>...</td>\n",
       "      <td>165.758159</td>\n",
       "      <td>160.230700</td>\n",
       "      <td>169.323863</td>\n",
       "      <td>133.493971</td>\n",
       "      <td>208.195730</td>\n",
       "      <td>177.192519</td>\n",
       "      <td>164.454454</td>\n",
       "      <td>180.018405</td>\n",
       "      <td>96.014828</td>\n",
       "      <td>154.401898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161.039650</td>\n",
       "      <td>194.849887</td>\n",
       "      <td>126.197054</td>\n",
       "      <td>163.840713</td>\n",
       "      <td>204.367203</td>\n",
       "      <td>222.484022</td>\n",
       "      <td>170.983469</td>\n",
       "      <td>201.431757</td>\n",
       "      <td>162.756180</td>\n",
       "      <td>172.346699</td>\n",
       "      <td>...</td>\n",
       "      <td>163.355101</td>\n",
       "      <td>165.211009</td>\n",
       "      <td>170.248962</td>\n",
       "      <td>140.103333</td>\n",
       "      <td>201.208208</td>\n",
       "      <td>177.592137</td>\n",
       "      <td>157.491865</td>\n",
       "      <td>185.918652</td>\n",
       "      <td>117.597141</td>\n",
       "      <td>151.603599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>181.245214</td>\n",
       "      <td>164.287448</td>\n",
       "      <td>138.192933</td>\n",
       "      <td>108.488997</td>\n",
       "      <td>231.463336</td>\n",
       "      <td>216.307978</td>\n",
       "      <td>196.624613</td>\n",
       "      <td>173.587108</td>\n",
       "      <td>183.642126</td>\n",
       "      <td>171.838412</td>\n",
       "      <td>...</td>\n",
       "      <td>156.929008</td>\n",
       "      <td>182.575110</td>\n",
       "      <td>170.941661</td>\n",
       "      <td>117.944039</td>\n",
       "      <td>219.751293</td>\n",
       "      <td>182.235793</td>\n",
       "      <td>189.209821</td>\n",
       "      <td>173.955258</td>\n",
       "      <td>134.553951</td>\n",
       "      <td>109.122599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>195.037339</td>\n",
       "      <td>169.041186</td>\n",
       "      <td>133.922141</td>\n",
       "      <td>104.737884</td>\n",
       "      <td>230.084562</td>\n",
       "      <td>217.194307</td>\n",
       "      <td>193.064322</td>\n",
       "      <td>183.724746</td>\n",
       "      <td>193.400698</td>\n",
       "      <td>167.595070</td>\n",
       "      <td>...</td>\n",
       "      <td>166.544534</td>\n",
       "      <td>181.052463</td>\n",
       "      <td>167.221663</td>\n",
       "      <td>117.773440</td>\n",
       "      <td>222.948420</td>\n",
       "      <td>175.224586</td>\n",
       "      <td>190.740649</td>\n",
       "      <td>178.321505</td>\n",
       "      <td>136.214719</td>\n",
       "      <td>111.137296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>182.814296</td>\n",
       "      <td>181.623425</td>\n",
       "      <td>142.281230</td>\n",
       "      <td>111.597687</td>\n",
       "      <td>227.649577</td>\n",
       "      <td>213.521915</td>\n",
       "      <td>193.936353</td>\n",
       "      <td>175.111805</td>\n",
       "      <td>175.687648</td>\n",
       "      <td>174.554387</td>\n",
       "      <td>...</td>\n",
       "      <td>157.645839</td>\n",
       "      <td>177.192679</td>\n",
       "      <td>178.387387</td>\n",
       "      <td>115.644471</td>\n",
       "      <td>222.105130</td>\n",
       "      <td>172.307717</td>\n",
       "      <td>189.402601</td>\n",
       "      <td>173.823475</td>\n",
       "      <td>138.565298</td>\n",
       "      <td>108.035843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>181.395257</td>\n",
       "      <td>170.335565</td>\n",
       "      <td>136.894954</td>\n",
       "      <td>109.927367</td>\n",
       "      <td>236.149469</td>\n",
       "      <td>219.412274</td>\n",
       "      <td>199.783831</td>\n",
       "      <td>180.232828</td>\n",
       "      <td>174.865073</td>\n",
       "      <td>173.082784</td>\n",
       "      <td>...</td>\n",
       "      <td>151.768313</td>\n",
       "      <td>177.730222</td>\n",
       "      <td>166.470572</td>\n",
       "      <td>112.461426</td>\n",
       "      <td>220.258529</td>\n",
       "      <td>181.032729</td>\n",
       "      <td>188.063658</td>\n",
       "      <td>174.705130</td>\n",
       "      <td>140.198055</td>\n",
       "      <td>103.798161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>187.288652</td>\n",
       "      <td>170.548328</td>\n",
       "      <td>141.485074</td>\n",
       "      <td>107.160129</td>\n",
       "      <td>232.043615</td>\n",
       "      <td>215.286278</td>\n",
       "      <td>199.924494</td>\n",
       "      <td>190.494727</td>\n",
       "      <td>176.715670</td>\n",
       "      <td>171.357024</td>\n",
       "      <td>...</td>\n",
       "      <td>161.567614</td>\n",
       "      <td>187.842578</td>\n",
       "      <td>170.745940</td>\n",
       "      <td>107.742276</td>\n",
       "      <td>219.572528</td>\n",
       "      <td>180.629743</td>\n",
       "      <td>181.027870</td>\n",
       "      <td>161.388162</td>\n",
       "      <td>130.086955</td>\n",
       "      <td>109.264857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     172.360911  198.698898  121.363501  160.748513  202.499388  232.822808   \n",
       "1     166.296893  193.903892  115.600204  155.129616  214.455008  219.996785   \n",
       "2     176.651492  190.236048  116.590044  170.190377  193.769774  219.342484   \n",
       "3     169.240405  194.123707  129.025909  156.078737  200.534552  216.315637   \n",
       "4     161.039650  194.849887  126.197054  163.840713  204.367203  222.484022   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  181.245214  164.287448  138.192933  108.488997  231.463336  216.307978   \n",
       "2439  195.037339  169.041186  133.922141  104.737884  230.084562  217.194307   \n",
       "2440  182.814296  181.623425  142.281230  111.597687  227.649577  213.521915   \n",
       "2441  181.395257  170.335565  136.894954  109.927367  236.149469  219.412274   \n",
       "2442  187.288652  170.548328  141.485074  107.160129  232.043615  215.286278   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     168.632401  199.493193  164.679481  177.628746  ...  162.732384   \n",
       "1     169.183957  199.994701  161.466782  185.240605  ...  165.732941   \n",
       "2     168.475106  191.204758  164.559102  186.545886  ...  160.350716   \n",
       "3     175.937287  209.187138  169.290122  178.631353  ...  165.758159   \n",
       "4     170.983469  201.431757  162.756180  172.346699  ...  163.355101   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.624613  173.587108  183.642126  171.838412  ...  156.929008   \n",
       "2439  193.064322  183.724746  193.400698  167.595070  ...  166.544534   \n",
       "2440  193.936353  175.111805  175.687648  174.554387  ...  157.645839   \n",
       "2441  199.783831  180.232828  174.865073  173.082784  ...  151.768313   \n",
       "2442  199.924494  190.494727  176.715670  171.357024  ...  161.567614   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     160.030382  169.678207  135.263693  214.033213  180.031244  162.028360   \n",
       "1     160.236901  172.716496  133.458960  212.467698  189.105376  158.462646   \n",
       "2     154.632700  178.171720  125.746017  206.064571  183.335022  148.591627   \n",
       "3     160.230700  169.323863  133.493971  208.195730  177.192519  164.454454   \n",
       "4     165.211009  170.248962  140.103333  201.208208  177.592137  157.491865   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  182.575110  170.941661  117.944039  219.751293  182.235793  189.209821   \n",
       "2439  181.052463  167.221663  117.773440  222.948420  175.224586  190.740649   \n",
       "2440  177.192679  178.387387  115.644471  222.105130  172.307717  189.402601   \n",
       "2441  177.730222  166.470572  112.461426  220.258529  181.032729  188.063658   \n",
       "2442  187.842578  170.745940  107.742276  219.572528  180.629743  181.027870   \n",
       "\n",
       "              45          46          47  \n",
       "0     189.460195  110.539577  152.588802  \n",
       "1     178.879499  114.537605  152.156101  \n",
       "2     182.388360  110.232965  144.684928  \n",
       "3     180.018405   96.014828  154.401898  \n",
       "4     185.918652  117.597141  151.603599  \n",
       "...          ...         ...         ...  \n",
       "2438  173.955258  134.553951  109.122599  \n",
       "2439  178.321505  136.214719  111.137296  \n",
       "2440  173.823475  138.565298  108.035843  \n",
       "2441  174.705130  140.198055  103.798161  \n",
       "2442  161.388162  130.086955  109.264857  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172.360911</td>\n",
       "      <td>198.698898</td>\n",
       "      <td>121.363501</td>\n",
       "      <td>160.748513</td>\n",
       "      <td>202.499388</td>\n",
       "      <td>232.822808</td>\n",
       "      <td>168.632401</td>\n",
       "      <td>199.493193</td>\n",
       "      <td>164.679481</td>\n",
       "      <td>177.628746</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732384</td>\n",
       "      <td>160.030382</td>\n",
       "      <td>169.678207</td>\n",
       "      <td>135.263693</td>\n",
       "      <td>214.033213</td>\n",
       "      <td>180.031244</td>\n",
       "      <td>162.028360</td>\n",
       "      <td>189.460195</td>\n",
       "      <td>110.539577</td>\n",
       "      <td>152.588802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.296893</td>\n",
       "      <td>193.903892</td>\n",
       "      <td>115.600204</td>\n",
       "      <td>155.129616</td>\n",
       "      <td>214.455008</td>\n",
       "      <td>219.996785</td>\n",
       "      <td>169.183957</td>\n",
       "      <td>199.994701</td>\n",
       "      <td>161.466782</td>\n",
       "      <td>185.240605</td>\n",
       "      <td>...</td>\n",
       "      <td>165.732941</td>\n",
       "      <td>160.236901</td>\n",
       "      <td>172.716496</td>\n",
       "      <td>133.458960</td>\n",
       "      <td>212.467698</td>\n",
       "      <td>189.105376</td>\n",
       "      <td>158.462646</td>\n",
       "      <td>178.879499</td>\n",
       "      <td>114.537605</td>\n",
       "      <td>152.156101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176.651492</td>\n",
       "      <td>190.236048</td>\n",
       "      <td>116.590044</td>\n",
       "      <td>170.190377</td>\n",
       "      <td>193.769774</td>\n",
       "      <td>219.342484</td>\n",
       "      <td>168.475106</td>\n",
       "      <td>191.204758</td>\n",
       "      <td>164.559102</td>\n",
       "      <td>186.545886</td>\n",
       "      <td>...</td>\n",
       "      <td>160.350716</td>\n",
       "      <td>154.632700</td>\n",
       "      <td>178.171720</td>\n",
       "      <td>125.746017</td>\n",
       "      <td>206.064571</td>\n",
       "      <td>183.335022</td>\n",
       "      <td>148.591627</td>\n",
       "      <td>182.388360</td>\n",
       "      <td>110.232965</td>\n",
       "      <td>144.684928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169.240405</td>\n",
       "      <td>194.123707</td>\n",
       "      <td>129.025909</td>\n",
       "      <td>156.078737</td>\n",
       "      <td>200.534552</td>\n",
       "      <td>216.315637</td>\n",
       "      <td>175.937287</td>\n",
       "      <td>209.187138</td>\n",
       "      <td>169.290122</td>\n",
       "      <td>178.631353</td>\n",
       "      <td>...</td>\n",
       "      <td>165.758159</td>\n",
       "      <td>160.230700</td>\n",
       "      <td>169.323863</td>\n",
       "      <td>133.493971</td>\n",
       "      <td>208.195730</td>\n",
       "      <td>177.192519</td>\n",
       "      <td>164.454454</td>\n",
       "      <td>180.018405</td>\n",
       "      <td>96.014828</td>\n",
       "      <td>154.401898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161.039650</td>\n",
       "      <td>194.849887</td>\n",
       "      <td>126.197054</td>\n",
       "      <td>163.840713</td>\n",
       "      <td>204.367203</td>\n",
       "      <td>222.484022</td>\n",
       "      <td>170.983469</td>\n",
       "      <td>201.431757</td>\n",
       "      <td>162.756180</td>\n",
       "      <td>172.346699</td>\n",
       "      <td>...</td>\n",
       "      <td>163.355101</td>\n",
       "      <td>165.211009</td>\n",
       "      <td>170.248962</td>\n",
       "      <td>140.103333</td>\n",
       "      <td>201.208208</td>\n",
       "      <td>177.592137</td>\n",
       "      <td>157.491865</td>\n",
       "      <td>185.918652</td>\n",
       "      <td>117.597141</td>\n",
       "      <td>151.603599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>181.245214</td>\n",
       "      <td>164.287448</td>\n",
       "      <td>138.192933</td>\n",
       "      <td>108.488997</td>\n",
       "      <td>231.463336</td>\n",
       "      <td>216.307978</td>\n",
       "      <td>196.624613</td>\n",
       "      <td>173.587108</td>\n",
       "      <td>183.642126</td>\n",
       "      <td>171.838412</td>\n",
       "      <td>...</td>\n",
       "      <td>156.929008</td>\n",
       "      <td>182.575110</td>\n",
       "      <td>170.941661</td>\n",
       "      <td>117.944039</td>\n",
       "      <td>219.751293</td>\n",
       "      <td>182.235793</td>\n",
       "      <td>189.209821</td>\n",
       "      <td>173.955258</td>\n",
       "      <td>134.553951</td>\n",
       "      <td>109.122599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>195.037339</td>\n",
       "      <td>169.041186</td>\n",
       "      <td>133.922141</td>\n",
       "      <td>104.737884</td>\n",
       "      <td>230.084562</td>\n",
       "      <td>217.194307</td>\n",
       "      <td>193.064322</td>\n",
       "      <td>183.724746</td>\n",
       "      <td>193.400698</td>\n",
       "      <td>167.595070</td>\n",
       "      <td>...</td>\n",
       "      <td>166.544534</td>\n",
       "      <td>181.052463</td>\n",
       "      <td>167.221663</td>\n",
       "      <td>117.773440</td>\n",
       "      <td>222.948420</td>\n",
       "      <td>175.224586</td>\n",
       "      <td>190.740649</td>\n",
       "      <td>178.321505</td>\n",
       "      <td>136.214719</td>\n",
       "      <td>111.137296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>182.814296</td>\n",
       "      <td>181.623425</td>\n",
       "      <td>142.281230</td>\n",
       "      <td>111.597687</td>\n",
       "      <td>227.649577</td>\n",
       "      <td>213.521915</td>\n",
       "      <td>193.936353</td>\n",
       "      <td>175.111805</td>\n",
       "      <td>175.687648</td>\n",
       "      <td>174.554387</td>\n",
       "      <td>...</td>\n",
       "      <td>157.645839</td>\n",
       "      <td>177.192679</td>\n",
       "      <td>178.387387</td>\n",
       "      <td>115.644471</td>\n",
       "      <td>222.105130</td>\n",
       "      <td>172.307717</td>\n",
       "      <td>189.402601</td>\n",
       "      <td>173.823475</td>\n",
       "      <td>138.565298</td>\n",
       "      <td>108.035843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>181.395257</td>\n",
       "      <td>170.335565</td>\n",
       "      <td>136.894954</td>\n",
       "      <td>109.927367</td>\n",
       "      <td>236.149469</td>\n",
       "      <td>219.412274</td>\n",
       "      <td>199.783831</td>\n",
       "      <td>180.232828</td>\n",
       "      <td>174.865073</td>\n",
       "      <td>173.082784</td>\n",
       "      <td>...</td>\n",
       "      <td>151.768313</td>\n",
       "      <td>177.730222</td>\n",
       "      <td>166.470572</td>\n",
       "      <td>112.461426</td>\n",
       "      <td>220.258529</td>\n",
       "      <td>181.032729</td>\n",
       "      <td>188.063658</td>\n",
       "      <td>174.705130</td>\n",
       "      <td>140.198055</td>\n",
       "      <td>103.798161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>187.288652</td>\n",
       "      <td>170.548328</td>\n",
       "      <td>141.485074</td>\n",
       "      <td>107.160129</td>\n",
       "      <td>232.043615</td>\n",
       "      <td>215.286278</td>\n",
       "      <td>199.924494</td>\n",
       "      <td>190.494727</td>\n",
       "      <td>176.715670</td>\n",
       "      <td>171.357024</td>\n",
       "      <td>...</td>\n",
       "      <td>161.567614</td>\n",
       "      <td>187.842578</td>\n",
       "      <td>170.745940</td>\n",
       "      <td>107.742276</td>\n",
       "      <td>219.572528</td>\n",
       "      <td>180.629743</td>\n",
       "      <td>181.027870</td>\n",
       "      <td>161.388162</td>\n",
       "      <td>130.086955</td>\n",
       "      <td>109.264857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     172.360911  198.698898  121.363501  160.748513  202.499388  232.822808   \n",
       "1     166.296893  193.903892  115.600204  155.129616  214.455008  219.996785   \n",
       "2     176.651492  190.236048  116.590044  170.190377  193.769774  219.342484   \n",
       "3     169.240405  194.123707  129.025909  156.078737  200.534552  216.315637   \n",
       "4     161.039650  194.849887  126.197054  163.840713  204.367203  222.484022   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  181.245214  164.287448  138.192933  108.488997  231.463336  216.307978   \n",
       "2439  195.037339  169.041186  133.922141  104.737884  230.084562  217.194307   \n",
       "2440  182.814296  181.623425  142.281230  111.597687  227.649577  213.521915   \n",
       "2441  181.395257  170.335565  136.894954  109.927367  236.149469  219.412274   \n",
       "2442  187.288652  170.548328  141.485074  107.160129  232.043615  215.286278   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     168.632401  199.493193  164.679481  177.628746  ...  162.732384   \n",
       "1     169.183957  199.994701  161.466782  185.240605  ...  165.732941   \n",
       "2     168.475106  191.204758  164.559102  186.545886  ...  160.350716   \n",
       "3     175.937287  209.187138  169.290122  178.631353  ...  165.758159   \n",
       "4     170.983469  201.431757  162.756180  172.346699  ...  163.355101   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.624613  173.587108  183.642126  171.838412  ...  156.929008   \n",
       "2439  193.064322  183.724746  193.400698  167.595070  ...  166.544534   \n",
       "2440  193.936353  175.111805  175.687648  174.554387  ...  157.645839   \n",
       "2441  199.783831  180.232828  174.865073  173.082784  ...  151.768313   \n",
       "2442  199.924494  190.494727  176.715670  171.357024  ...  161.567614   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     160.030382  169.678207  135.263693  214.033213  180.031244  162.028360   \n",
       "1     160.236901  172.716496  133.458960  212.467698  189.105376  158.462646   \n",
       "2     154.632700  178.171720  125.746017  206.064571  183.335022  148.591627   \n",
       "3     160.230700  169.323863  133.493971  208.195730  177.192519  164.454454   \n",
       "4     165.211009  170.248962  140.103333  201.208208  177.592137  157.491865   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  182.575110  170.941661  117.944039  219.751293  182.235793  189.209821   \n",
       "2439  181.052463  167.221663  117.773440  222.948420  175.224586  190.740649   \n",
       "2440  177.192679  178.387387  115.644471  222.105130  172.307717  189.402601   \n",
       "2441  177.730222  166.470572  112.461426  220.258529  181.032729  188.063658   \n",
       "2442  187.842578  170.745940  107.742276  219.572528  180.629743  181.027870   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     189.460195  110.539577  152.588802  \n",
       "1     178.879499  114.537605  152.156101  \n",
       "2     182.388360  110.232965  144.684928  \n",
       "3     180.018405   96.014828  154.401898  \n",
       "4     185.918652  117.597141  151.603599  \n",
       "...          ...         ...         ...  \n",
       "2438  173.955258  134.553951  109.122599  \n",
       "2439  178.321505  136.214719  111.137296  \n",
       "2440  173.823475  138.565298  108.035843  \n",
       "2441  174.705130  140.198055  103.798161  \n",
       "2442  161.388162  130.086955  109.264857  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 20ms/step - loss: 4050.8914 - val_loss: 3815.3716\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3727.3306 - val_loss: 3597.3408\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3525.3628 - val_loss: 3412.2966\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3353.2722 - val_loss: 3253.0005\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3203.6475 - val_loss: 3116.1963\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3075.6404 - val_loss: 2999.0173\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2965.8406 - val_loss: 2899.0889\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2871.9888 - val_loss: 2813.5837\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2792.3958 - val_loss: 2741.6638\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2725.6062 - val_loss: 2681.8792\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2670.5039 - val_loss: 2633.1980\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2625.9753 - val_loss: 2594.4600\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2590.9233 - val_loss: 2564.2454\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2564.1858 - val_loss: 2542.3586\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2544.8088 - val_loss: 2526.7654\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2531.3914 - val_loss: 2516.5713\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.8191 - val_loss: 2510.1118\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2517.5569 - val_loss: 2506.6077\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.8872 - val_loss: 2504.9402\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.4146 - val_loss: 2504.1116\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.7805 - val_loss: 2504.1174\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.6375 - val_loss: 2503.8818\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.4885 - val_loss: 2504.1501\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.4355 - val_loss: 2504.3508\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.5042 - val_loss: 2504.3074\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.5256 - val_loss: 2504.1448\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.4543 - val_loss: 2504.6057\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.4753 - val_loss: 2504.5825\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2512.4951 - val_loss: 2504.3154\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.9062 - val_loss: 2504.0730\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2505.2009 - val_loss: 2480.5171\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2452.6536 - val_loss: 2407.4844\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2391.0806 - val_loss: 2361.5386\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2348.0859 - val_loss: 2319.3472\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2306.5745 - val_loss: 2277.6609\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2267.5022 - val_loss: 2268.8274\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2228.0190 - val_loss: 2197.8022\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2189.5735 - val_loss: 2160.9165\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2147.2881 - val_loss: 2109.2002\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2094.4170 - val_loss: 2045.6295\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2032.0607 - val_loss: 1981.2811\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1969.6624 - val_loss: 1920.9800\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1910.8821 - val_loss: 1852.9091\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1846.8201 - val_loss: 1790.3196\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1780.1832 - val_loss: 1720.2147\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1711.5988 - val_loss: 1658.3679\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1649.3093 - val_loss: 1593.2097\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1590.1078 - val_loss: 1533.9067\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1523.0532 - val_loss: 1460.4214\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1456.5389 - val_loss: 1403.4872\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1391.8090 - val_loss: 1336.6348\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1329.0325 - val_loss: 1277.0447\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1273.7448 - val_loss: 1212.2582\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1208.6554 - val_loss: 1157.4092\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1152.0626 - val_loss: 1098.2639\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1094.7609 - val_loss: 1042.5826\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1034.1062 - val_loss: 979.4296\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 972.4876 - val_loss: 922.1589\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.3704 - val_loss: 860.2009\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 856.6767 - val_loss: 815.9943\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 805.9920 - val_loss: 762.9498\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 757.0742 - val_loss: 719.0574\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 713.8824 - val_loss: 669.8436\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 665.8387 - val_loss: 626.5473\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 624.9699 - val_loss: 587.3705\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 585.3742 - val_loss: 553.2369\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 546.2012 - val_loss: 515.9096\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 506.2206 - val_loss: 474.7448\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 470.7754 - val_loss: 438.4296\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 438.4138 - val_loss: 408.7987\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 404.1240 - val_loss: 374.9034\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 373.1100 - val_loss: 349.1740\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 345.3562 - val_loss: 333.4000\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 319.6327 - val_loss: 297.3161\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 292.0678 - val_loss: 267.1311\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 265.4564 - val_loss: 244.3285\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 246.4248 - val_loss: 226.7122\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 224.0262 - val_loss: 211.8065\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 204.9346 - val_loss: 189.3921\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 187.6902 - val_loss: 179.4670\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 170.2611 - val_loss: 157.6016\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 158.4648 - val_loss: 147.2628\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 141.9421 - val_loss: 130.5887\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 131.2467 - val_loss: 119.6006\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 117.2812 - val_loss: 108.0139\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 105.1244 - val_loss: 98.6010\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 95.6583 - val_loss: 90.7797\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 87.1473 - val_loss: 88.4567\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 79.0817 - val_loss: 89.6560\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 75.1709 - val_loss: 93.0287\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.7401 - val_loss: 82.1837\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 58.8501 - val_loss: 56.0873\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.7518 - val_loss: 52.5125\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.5666 - val_loss: 59.6954\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 44.6335 - val_loss: 45.0200\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 44.3022 - val_loss: 43.1294\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.1965 - val_loss: 45.1567\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.3493 - val_loss: 61.9510\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.1463 - val_loss: 40.8435\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.8698 - val_loss: 31.0464\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.6608 - val_loss: 28.8870\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.0774 - val_loss: 34.2879\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.0017 - val_loss: 28.4470\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.5125 - val_loss: 23.7772\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.3964 - val_loss: 22.5459\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.8903 - val_loss: 23.1216\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.5150 - val_loss: 30.3173\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.4921 - val_loss: 23.7584\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.1930 - val_loss: 23.0108\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.9039 - val_loss: 22.5859\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.6100 - val_loss: 19.1143\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.4104 - val_loss: 23.8759\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5508 - val_loss: 21.8397\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.7218 - val_loss: 17.9232\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.6053 - val_loss: 18.2218\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.8870 - val_loss: 17.6609\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0144 - val_loss: 15.3045\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7381 - val_loss: 17.0637\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.4779 - val_loss: 13.5799\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.9411 - val_loss: 23.9197\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9712 - val_loss: 14.0323\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.9896 - val_loss: 20.3049\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.4121 - val_loss: 35.7528\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 14.9526 - val_loss: 13.2086\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.3167 - val_loss: 13.3088\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.6297 - val_loss: 13.4917\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.2882 - val_loss: 12.7829\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0901 - val_loss: 15.7823\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.7719 - val_loss: 65.8927\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.1779 - val_loss: 13.9756\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.9957 - val_loss: 13.7964\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3629 - val_loss: 17.7288\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5309 - val_loss: 12.1905\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2474 - val_loss: 11.3473\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1347 - val_loss: 11.0429\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.8993 - val_loss: 12.5787\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4812 - val_loss: 13.4631\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0478 - val_loss: 20.5357\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7767 - val_loss: 13.7918\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3829 - val_loss: 11.4019\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3798 - val_loss: 11.3921\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0586 - val_loss: 13.5982\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.8964 - val_loss: 11.4961\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7500 - val_loss: 11.2088\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.2345 - val_loss: 10.3388\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2327 - val_loss: 11.6687\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3234 - val_loss: 9.6502\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.3514 - val_loss: 14.0222\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0883 - val_loss: 9.3329\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3011 - val_loss: 10.6536\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7486 - val_loss: 9.3776\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4759 - val_loss: 21.4604\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 14.0365 - val_loss: 9.9841\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3056 - val_loss: 10.5666\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4211 - val_loss: 10.7308\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6787 - val_loss: 9.0664\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8046 - val_loss: 9.0273\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7787 - val_loss: 18.2540\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5042 - val_loss: 11.2492\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5028 - val_loss: 9.2314\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2215 - val_loss: 8.5174\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3188 - val_loss: 16.3243\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.4641 - val_loss: 10.0906\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.5362 - val_loss: 15.0717\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.0146 - val_loss: 18.4395\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4786 - val_loss: 8.8867\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6479 - val_loss: 11.0515\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8213 - val_loss: 11.0749\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.8396 - val_loss: 35.9781\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 11.4082 - val_loss: 8.3906\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6285 - val_loss: 9.3027\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3420 - val_loss: 7.7655\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5863 - val_loss: 8.4437\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7611 - val_loss: 8.9372\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0679 - val_loss: 9.0124\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8610 - val_loss: 8.4015\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3189 - val_loss: 67.9056\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3211 - val_loss: 8.1968\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8678 - val_loss: 8.9333\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1845 - val_loss: 7.4482\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9019 - val_loss: 7.6897\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.0457 - val_loss: 9.4831\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0766 - val_loss: 7.9899\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.9167 - val_loss: 14.2344\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5512 - val_loss: 8.2319\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2848 - val_loss: 8.0715\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0397 - val_loss: 7.4880\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6742 - val_loss: 7.7227\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6888 - val_loss: 7.3702\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5056 - val_loss: 14.0241\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7358 - val_loss: 8.5355\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7191 - val_loss: 9.9048\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5905 - val_loss: 9.4559\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4967 - val_loss: 8.5952\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9752 - val_loss: 9.5169\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8839 - val_loss: 9.1453\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2459 - val_loss: 7.1783\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8791 - val_loss: 9.4367\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1389 - val_loss: 8.3389\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4777 - val_loss: 8.5610\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 8.560950440996201\n",
      "Mean Absolute Error (MAE): 1.9470129943906913\n",
      "Root Mean Squared Error (RMSE): 2.925910190179494\n",
      "Time taken: 1244.8997621536255\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 20ms/step - loss: 4027.6165 - val_loss: 3830.7412\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3694.4038 - val_loss: 3607.1997\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3492.0652 - val_loss: 3422.8872\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3321.0938 - val_loss: 3265.2407\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3174.5642 - val_loss: 3129.3081\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3048.7683 - val_loss: 3012.7759\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2940.7827 - val_loss: 2912.9604\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2849.0037 - val_loss: 2828.2629\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2771.7783 - val_loss: 2757.3767\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2707.3550 - val_loss: 2698.3943\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2654.4734 - val_loss: 2650.5857\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2612.0750 - val_loss: 2612.4136\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2578.9341 - val_loss: 2583.1855\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2553.9475 - val_loss: 2561.3528\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2536.0583 - val_loss: 2546.1111\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.7051 - val_loss: 2535.7629\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2515.9275 - val_loss: 2529.4812\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.3738 - val_loss: 2525.9277\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8940 - val_loss: 2524.0872\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.7683 - val_loss: 2523.1785\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2507.2334 - val_loss: 2522.8513\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0916 - val_loss: 2522.6599\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0439 - val_loss: 2522.5894\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0481 - val_loss: 2522.5493\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.9661 - val_loss: 2522.6431\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0200 - val_loss: 2522.5540\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0234 - val_loss: 2522.7266\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.9561 - val_loss: 2522.6250\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.0093 - val_loss: 2522.7200\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.0193 - val_loss: 2522.6550\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.9758 - val_loss: 2522.7131\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.0325 - val_loss: 2523.2200\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.1846 - val_loss: 2522.8804\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.0286 - val_loss: 2522.7373\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.6401 - val_loss: 2522.5417\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.1838 - val_loss: 2522.3403\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2507.9939 - val_loss: 2522.1077\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.8623 - val_loss: 2518.6560\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.3020 - val_loss: 2498.2507\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2462.3418 - val_loss: 2452.1160\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2387.7393 - val_loss: 2371.8901\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2329.1746 - val_loss: 2321.9277\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2279.0117 - val_loss: 2269.7087\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2217.1179 - val_loss: 2191.2876\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2131.2004 - val_loss: 2100.5398\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2040.9492 - val_loss: 2014.8120\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1955.0328 - val_loss: 1930.6150\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1869.4130 - val_loss: 1865.4596\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1787.6960 - val_loss: 1757.6254\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1712.1536 - val_loss: 1681.7201\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1634.8331 - val_loss: 1617.9031\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1562.5803 - val_loss: 1538.2583\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1496.3092 - val_loss: 1472.2555\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1426.9622 - val_loss: 1406.2322\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1360.1519 - val_loss: 1339.1218\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1298.0687 - val_loss: 1282.5126\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1238.4835 - val_loss: 1218.3077\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1180.7494 - val_loss: 1157.7993\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1120.4634 - val_loss: 1097.5281\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1063.6271 - val_loss: 1046.6992\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1008.6353 - val_loss: 988.7627\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 953.9788 - val_loss: 941.3925\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 899.5407 - val_loss: 880.7704\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 849.9911 - val_loss: 829.9563\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 801.6391 - val_loss: 782.5930\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 756.7941 - val_loss: 740.4929\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 713.5610 - val_loss: 695.5949\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 666.4755 - val_loss: 650.0308\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 623.0483 - val_loss: 606.6681\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 586.5270 - val_loss: 571.3775\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 549.0677 - val_loss: 531.4048\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 513.1594 - val_loss: 498.5994\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 480.5597 - val_loss: 459.9404\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 443.5588 - val_loss: 428.7829\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 410.6708 - val_loss: 397.4971\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 382.3147 - val_loss: 370.4136\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 352.1904 - val_loss: 339.0035\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 327.6268 - val_loss: 316.4052\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 301.0379 - val_loss: 285.8300\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 278.4009 - val_loss: 265.0833\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 254.0784 - val_loss: 247.2011\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 237.3043 - val_loss: 226.3646\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 214.1836 - val_loss: 204.7342\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 197.8596 - val_loss: 185.5974\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 182.2070 - val_loss: 169.5095\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 165.8228 - val_loss: 157.9764\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 154.7090 - val_loss: 147.7774\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 139.3811 - val_loss: 131.3746\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 128.1628 - val_loss: 129.6029\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 118.9723 - val_loss: 113.7251\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 108.2024 - val_loss: 102.1009\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 98.1794 - val_loss: 93.8409\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 88.0484 - val_loss: 86.6183\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 81.3169 - val_loss: 86.6349\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 74.4064 - val_loss: 70.8827\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 67.9111 - val_loss: 64.5003\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 61.6617 - val_loss: 60.2239\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 56.6532 - val_loss: 53.2778\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.0666 - val_loss: 46.3440\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.7884 - val_loss: 44.3689\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 45.0840 - val_loss: 42.6811\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.6976 - val_loss: 41.6896\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.0059 - val_loss: 38.4985\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.0919 - val_loss: 36.6836\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.6866 - val_loss: 29.6027\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.7166 - val_loss: 30.6337\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.5033 - val_loss: 30.4994\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.1077 - val_loss: 23.8667\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.1511 - val_loss: 24.8254\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.2315 - val_loss: 23.3167\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.6568 - val_loss: 23.3697\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.5845 - val_loss: 23.8116\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.1623 - val_loss: 20.6091\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.7635 - val_loss: 19.7861\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 15.5798 - val_loss: 17.4760\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 14.5248 - val_loss: 27.5890\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 14.6894 - val_loss: 21.7584\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.2854 - val_loss: 23.8086\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.8618 - val_loss: 17.8586\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.3879 - val_loss: 14.8865\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 12.0526 - val_loss: 16.0695\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.4344 - val_loss: 15.3188\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0659 - val_loss: 14.9500\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.4794 - val_loss: 14.3393\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 11.0626 - val_loss: 13.7893\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.5217 - val_loss: 15.8914\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3080 - val_loss: 12.0437\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.4056 - val_loss: 22.7088\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.2129 - val_loss: 14.3039\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.7752 - val_loss: 12.4521\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8302 - val_loss: 13.2393\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8204 - val_loss: 10.9411\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 10.3704 - val_loss: 17.3072\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.6873 - val_loss: 14.4179\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 9.3503 - val_loss: 10.7806\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.4673 - val_loss: 11.5328\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.5245 - val_loss: 9.7662\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0103 - val_loss: 9.5772\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3672 - val_loss: 10.4837\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1874 - val_loss: 11.2141\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.7850 - val_loss: 12.0077\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2356 - val_loss: 10.2446\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3906 - val_loss: 9.1277\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1429 - val_loss: 11.3586\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2906 - val_loss: 10.4244\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8509 - val_loss: 9.6723\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1177 - val_loss: 9.7331\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8493 - val_loss: 10.4891\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5973 - val_loss: 14.8959\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8252 - val_loss: 10.7118\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7738 - val_loss: 12.4003\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.3688 - val_loss: 9.1930\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2623 - val_loss: 7.7496\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.0404 - val_loss: 7.4564\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.8393 - val_loss: 9.3742\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.3631 - val_loss: 9.3027\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7147 - val_loss: 11.2633\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.5260 - val_loss: 9.3651\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1915 - val_loss: 8.9687\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2320 - val_loss: 9.9547\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9386 - val_loss: 10.1492\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7187 - val_loss: 10.2329\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1688 - val_loss: 8.1152\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2617 - val_loss: 10.9304\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6527 - val_loss: 8.7015\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6169 - val_loss: 8.4172\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3558 - val_loss: 8.4172\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3924 - val_loss: 8.1185\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3991 - val_loss: 8.5744\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7598 - val_loss: 7.8882\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0588 - val_loss: 15.8175\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8038 - val_loss: 8.6904\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.2584 - val_loss: 9.6752\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2245 - val_loss: 7.6142\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.3103 - val_loss: 7.4280\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.8492 - val_loss: 9.2250\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9940 - val_loss: 7.5525\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8606 - val_loss: 8.6283\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5040 - val_loss: 11.6728\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0472 - val_loss: 11.3449\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7174 - val_loss: 7.9297\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1920 - val_loss: 7.0160\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8399 - val_loss: 7.6438\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0806 - val_loss: 7.5544\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6979 - val_loss: 8.2550\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0973 - val_loss: 7.5986\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9514 - val_loss: 10.3481\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5546 - val_loss: 9.7846\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9208 - val_loss: 7.1887\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5599 - val_loss: 6.8984\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9730 - val_loss: 8.3185\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5984 - val_loss: 7.7138\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.9316 - val_loss: 7.8283\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.3265 - val_loss: 7.8547\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1938 - val_loss: 8.0547\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8975 - val_loss: 8.8558\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8002 - val_loss: 6.7135\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6714 - val_loss: 29.8027\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9473 - val_loss: 8.7624\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3054 - val_loss: 6.5097\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 6.509663202267977\n",
      "Mean Absolute Error (MAE): 1.7095325766772052\n",
      "Root Mean Squared Error (RMSE): 2.5514041628616932\n",
      "Time taken: 1255.5195467472076\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 4071.0369 - val_loss: 3903.6614\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3742.5291 - val_loss: 3675.2251\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3537.1819 - val_loss: 3482.2476\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3360.4219 - val_loss: 3317.5491\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3209.9260 - val_loss: 3176.0737\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3080.6016 - val_loss: 3054.1409\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2969.2732 - val_loss: 2949.5923\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2873.9504 - val_loss: 2860.2642\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2793.0769 - val_loss: 2784.6477\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2725.1592 - val_loss: 2721.6982\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2669.0166 - val_loss: 2669.8591\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2623.6775 - val_loss: 2628.4922\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2588.0303 - val_loss: 2596.1455\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2560.5615 - val_loss: 2571.7529\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2540.4463 - val_loss: 2554.2014\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.5815 - val_loss: 2542.2498\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2517.3984 - val_loss: 2534.5667\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.9253 - val_loss: 2530.0186\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.8958 - val_loss: 2527.6467\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.4360 - val_loss: 2526.4004\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.7520 - val_loss: 2525.8442\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.5647 - val_loss: 2525.5105\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.4553 - val_loss: 2525.3921\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4524 - val_loss: 2525.3772\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4624 - val_loss: 2525.3677\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.4092 - val_loss: 2525.3508\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4729 - val_loss: 2525.3088\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.3931 - val_loss: 2525.3372\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2506.3755 - val_loss: 2525.2964\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2506.4673 - val_loss: 2525.2183\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2506.4031 - val_loss: 2525.2075\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2506.4333 - val_loss: 2525.2471\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4272 - val_loss: 2525.1753\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4556 - val_loss: 2525.1484\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.4023 - val_loss: 2525.2490\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.3899 - val_loss: 2525.2249\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2506.3755 - val_loss: 2525.1570\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2506.3938 - val_loss: 2525.1882\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2479.1909 - val_loss: 2445.5825\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2405.8284 - val_loss: 2411.0186\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2374.3462 - val_loss: 2383.4299\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2349.7458 - val_loss: 2363.4019\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2325.9297 - val_loss: 2332.2004\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2301.9902 - val_loss: 2304.8145\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2275.9900 - val_loss: 2278.4468\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2246.2783 - val_loss: 2238.1360\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2210.3875 - val_loss: 2191.6458\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2143.2173 - val_loss: 2102.4519\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2048.4043 - val_loss: 2011.4076\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1967.7847 - val_loss: 1937.9191\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1888.9948 - val_loss: 1860.1799\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1814.8835 - val_loss: 1785.8693\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1743.9041 - val_loss: 1718.4613\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1675.6422 - val_loss: 1651.2018\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1607.1624 - val_loss: 1583.7239\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1541.4803 - val_loss: 1512.5597\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1473.2521 - val_loss: 1446.5668\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1410.1971 - val_loss: 1388.7761\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1346.9877 - val_loss: 1322.3522\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1283.0977 - val_loss: 1257.8787\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1222.8087 - val_loss: 1198.7548\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1161.7489 - val_loss: 1138.9860\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1103.8844 - val_loss: 1081.8824\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1050.2960 - val_loss: 1026.7224\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 995.4614 - val_loss: 974.6721\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 945.3689 - val_loss: 927.8345\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 897.0461 - val_loss: 877.1766\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 849.7465 - val_loss: 827.7315\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 803.0778 - val_loss: 781.9152\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 756.5859 - val_loss: 737.6309\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 711.7009 - val_loss: 693.2350\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 669.9310 - val_loss: 655.1572\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 624.1689 - val_loss: 605.0201\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 583.7998 - val_loss: 566.8503\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 546.9393 - val_loss: 530.8487\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 509.7742 - val_loss: 502.8818\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 478.0348 - val_loss: 463.0041\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 444.8269 - val_loss: 429.6191\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 415.4148 - val_loss: 400.6124\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 384.4031 - val_loss: 369.1208\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 355.9433 - val_loss: 343.4658\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 328.9561 - val_loss: 315.1295\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 303.3964 - val_loss: 292.5461\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 280.5395 - val_loss: 273.1126\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 260.2937 - val_loss: 248.5215\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 239.8417 - val_loss: 228.8727\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 221.5166 - val_loss: 214.5916\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 204.3869 - val_loss: 204.5072\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 189.1273 - val_loss: 192.4678\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 171.6862 - val_loss: 170.4015\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 158.2938 - val_loss: 152.1014\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 144.8423 - val_loss: 148.5804\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 132.8501 - val_loss: 148.1183\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 123.5712 - val_loss: 123.6829\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 112.8136 - val_loss: 107.9495\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 103.3268 - val_loss: 96.5189\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 97.5385 - val_loss: 100.5682\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 89.2128 - val_loss: 85.5347\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 80.6537 - val_loss: 80.1216\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 75.4263 - val_loss: 72.6399\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 68.5683 - val_loss: 71.2755\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.6477 - val_loss: 60.3772\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.3042 - val_loss: 60.9076\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 54.9346 - val_loss: 51.8190\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.8178 - val_loss: 50.1002\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.0553 - val_loss: 43.0643\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 45.9700 - val_loss: 40.8947\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.5021 - val_loss: 41.5306\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 41.1841 - val_loss: 37.0440\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.9394 - val_loss: 38.1928\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.5215 - val_loss: 31.0716\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.7024 - val_loss: 32.2964\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.6997 - val_loss: 29.7208\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.1141 - val_loss: 28.0675\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.7286 - val_loss: 29.3488\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.5998 - val_loss: 24.2286\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8425 - val_loss: 23.7420\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.9495 - val_loss: 27.6503\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.6906 - val_loss: 20.6900\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.5235 - val_loss: 20.1368\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.8726 - val_loss: 20.9958\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3573 - val_loss: 26.6167\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.0832 - val_loss: 20.4237\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.2720 - val_loss: 23.3290\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.7920 - val_loss: 19.5459\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.1016 - val_loss: 18.9512\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.3984 - val_loss: 16.7402\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.8688 - val_loss: 16.1005\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.8047 - val_loss: 18.0248\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6442 - val_loss: 16.0090\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.8929 - val_loss: 18.8841\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.7136 - val_loss: 17.9156\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 14.7423 - val_loss: 14.6199\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.7116 - val_loss: 13.4124\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.3603 - val_loss: 13.5250\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.9218 - val_loss: 13.9157\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.2186 - val_loss: 12.6921\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.3802 - val_loss: 13.5458\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.1646 - val_loss: 11.7856\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.7569 - val_loss: 13.6995\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.4457 - val_loss: 12.9258\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.0519 - val_loss: 12.6293\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.0929 - val_loss: 14.2691\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.7923 - val_loss: 13.0277\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.1178 - val_loss: 12.4306\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6809 - val_loss: 12.3680\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 11.1099 - val_loss: 13.7542\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0637 - val_loss: 11.9497\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 10.3363 - val_loss: 15.0523\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.8309 - val_loss: 15.3998\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1687 - val_loss: 11.0648\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6987 - val_loss: 13.0824\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3912 - val_loss: 12.3889\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.9589 - val_loss: 10.7597\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5870 - val_loss: 10.6808\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1387 - val_loss: 14.8112\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.6239 - val_loss: 9.9005\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2259 - val_loss: 12.0994\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2367 - val_loss: 11.2515\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2398 - val_loss: 17.3262\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.7916 - val_loss: 10.9360\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8806 - val_loss: 9.8745\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.6267 - val_loss: 15.1123\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0594 - val_loss: 9.6933\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3545 - val_loss: 10.2388\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9709 - val_loss: 9.9566\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.3956 - val_loss: 12.1651\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 9.2314 - val_loss: 10.4694\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7421 - val_loss: 9.0339\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9139 - val_loss: 10.8936\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9389 - val_loss: 10.1208\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1447 - val_loss: 11.7271\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8580 - val_loss: 9.7673\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2566 - val_loss: 10.6734\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4374 - val_loss: 10.4454\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0644 - val_loss: 9.0952\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5471 - val_loss: 10.4326\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6050 - val_loss: 12.0782\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9676 - val_loss: 10.8918\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9039 - val_loss: 10.4510\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3769 - val_loss: 11.9727\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6683 - val_loss: 11.9199\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4171 - val_loss: 10.8886\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.5081 - val_loss: 11.0655\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.8142 - val_loss: 10.1805\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0100 - val_loss: 8.3324\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3576 - val_loss: 9.5932\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5311 - val_loss: 9.5533\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9739 - val_loss: 8.3679\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2622 - val_loss: 8.5134\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0211 - val_loss: 8.9477\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3903 - val_loss: 9.5319\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7683 - val_loss: 8.7367\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1573 - val_loss: 8.9976\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0734 - val_loss: 11.0102\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3255 - val_loss: 9.5159\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0077 - val_loss: 9.1473\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7397 - val_loss: 8.2084\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1042 - val_loss: 8.4540\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3494 - val_loss: 9.0290\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 9.029029134536755\n",
      "Mean Absolute Error (MAE): 2.085173961094585\n",
      "Root Mean Squared Error (RMSE): 3.004834294023009\n",
      "Time taken: 1245.9433674812317\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4072.0525 - val_loss: 3858.8032\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3754.0374 - val_loss: 3631.1541\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3545.9290 - val_loss: 3441.9016\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3370.8120 - val_loss: 3280.3694\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3220.2473 - val_loss: 3140.7246\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3090.3311 - val_loss: 3020.4917\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2978.5576 - val_loss: 2917.4622\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2882.8320 - val_loss: 2829.4731\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2801.4607 - val_loss: 2754.9565\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2733.0808 - val_loss: 2692.8052\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2676.6489 - val_loss: 2642.2676\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2630.8052 - val_loss: 2601.4358\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2594.6086 - val_loss: 2569.9844\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2567.0425 - val_loss: 2546.2290\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2546.6882 - val_loss: 2529.4539\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2532.5818 - val_loss: 2518.1436\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.2583 - val_loss: 2510.9399\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2517.6267 - val_loss: 2506.7937\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.5686 - val_loss: 2504.7368\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.0164 - val_loss: 2503.7549\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.3438 - val_loss: 2503.4573\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.0796 - val_loss: 2503.3018\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.9690 - val_loss: 2503.2415\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.0190 - val_loss: 2503.0483\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9824 - val_loss: 2503.2488\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9568 - val_loss: 2503.1548\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9580 - val_loss: 2503.1704\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.8303 - val_loss: 2502.6829\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.0312 - val_loss: 2502.7239\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.0364 - val_loss: 2502.7654\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9868 - val_loss: 2502.8286\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9861 - val_loss: 2502.8325\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9575 - val_loss: 2502.9082\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9412 - val_loss: 2502.8735\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9746 - val_loss: 2502.7805\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.9500 - val_loss: 2502.5686\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2465.3335 - val_loss: 2410.3804\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2386.1106 - val_loss: 2390.1736\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2339.7312 - val_loss: 2323.7097\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2283.6975 - val_loss: 2250.5068\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2196.9272 - val_loss: 2158.7153\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2116.8818 - val_loss: 2170.4275\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2064.5496 - val_loss: 2010.6272\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1959.2673 - val_loss: 1927.0061\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1889.6865 - val_loss: 1862.2115\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1823.2820 - val_loss: 1795.4626\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1749.9630 - val_loss: 1728.1230\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1683.5487 - val_loss: 1654.6216\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1620.2642 - val_loss: 1596.6536\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1556.9600 - val_loss: 1538.6064\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1501.3126 - val_loss: 1479.3137\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1436.7952 - val_loss: 1411.0565\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1372.6029 - val_loss: 1359.1250\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1320.4274 - val_loss: 1297.5005\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1262.0583 - val_loss: 1244.2194\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1208.1643 - val_loss: 1194.0707\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1152.7449 - val_loss: 1127.6938\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1090.8674 - val_loss: 1072.8606\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1034.4803 - val_loss: 1011.2809\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 978.1367 - val_loss: 1015.0012\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.3336 - val_loss: 918.0220\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 869.9371 - val_loss: 856.1627\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 817.7021 - val_loss: 800.9094\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 767.6839 - val_loss: 748.5932\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 716.1511 - val_loss: 697.8849\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 673.2147 - val_loss: 666.3274\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 629.3727 - val_loss: 613.2776\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 595.7697 - val_loss: 582.2000\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 554.3760 - val_loss: 572.2894\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 518.3582 - val_loss: 504.0641\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 485.6946 - val_loss: 479.4861\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 445.6251 - val_loss: 431.3325\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 414.1006 - val_loss: 402.8765\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 389.0883 - val_loss: 372.8386\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 357.4034 - val_loss: 356.4548\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 334.6634 - val_loss: 318.9495\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 307.2775 - val_loss: 337.2930\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 283.7471 - val_loss: 273.9694\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 261.7733 - val_loss: 249.8052\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 240.6385 - val_loss: 229.4631\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 222.9367 - val_loss: 224.0791\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 204.5531 - val_loss: 197.5935\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 188.9491 - val_loss: 194.6893\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 171.1321 - val_loss: 161.6410\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 158.2666 - val_loss: 153.0380\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 146.6499 - val_loss: 141.7231\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 134.2022 - val_loss: 126.4305\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 121.0774 - val_loss: 118.4287\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 115.0386 - val_loss: 105.5846\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 111.0801 - val_loss: 105.3085\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 96.0397 - val_loss: 89.1113\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.5702 - val_loss: 81.8214\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 79.4589 - val_loss: 84.1262\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.4269 - val_loss: 72.0711\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.8265 - val_loss: 65.4560\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.1867 - val_loss: 66.8251\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.3374 - val_loss: 63.0614\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.3600 - val_loss: 54.2966\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.0544 - val_loss: 49.8401\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.3335 - val_loss: 54.6379\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.5204 - val_loss: 48.2709\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.5078 - val_loss: 40.1055\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.6578 - val_loss: 40.3968\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.1929 - val_loss: 36.2375\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.0405 - val_loss: 46.1168\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.0278 - val_loss: 35.3890\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.3241 - val_loss: 33.6772\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.3872 - val_loss: 30.4417\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 34.5321 - val_loss: 28.1419\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.9219 - val_loss: 24.9321\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.0260 - val_loss: 23.7732\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.4806 - val_loss: 21.1699\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.3237 - val_loss: 53.6043\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.0620 - val_loss: 22.1896\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.0522 - val_loss: 24.3193\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.2299 - val_loss: 23.0994\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.0628 - val_loss: 18.7254\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.2720 - val_loss: 21.2862\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.9506 - val_loss: 20.4760\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.1418 - val_loss: 33.6083\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.4170 - val_loss: 17.9471\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.2507 - val_loss: 17.9903\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.3690 - val_loss: 18.1699\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.4927 - val_loss: 16.9854\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.1304 - val_loss: 13.1162\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.8903 - val_loss: 14.7094\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5353 - val_loss: 15.4797\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.7651 - val_loss: 12.7487\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.4527 - val_loss: 13.1026\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.1708 - val_loss: 14.3765\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.4810 - val_loss: 19.1140\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.8031 - val_loss: 17.4838\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.5441 - val_loss: 16.0207\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.5835 - val_loss: 20.8514\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2318 - val_loss: 18.7292\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.9665 - val_loss: 13.0757\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6290 - val_loss: 17.8264\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7492 - val_loss: 12.5243\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0012 - val_loss: 11.1173\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1698 - val_loss: 13.1096\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.4079 - val_loss: 12.5961\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0924 - val_loss: 10.8065\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3254 - val_loss: 13.5191\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6947 - val_loss: 14.6065\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0991 - val_loss: 12.9960\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.6776 - val_loss: 13.0836\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6124 - val_loss: 11.8482\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7066 - val_loss: 10.2449\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.8590 - val_loss: 24.4442\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.8300 - val_loss: 9.5051\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9207 - val_loss: 9.7952\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.0658 - val_loss: 10.7917\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6338 - val_loss: 11.6352\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9149 - val_loss: 9.9576\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9630 - val_loss: 12.2688\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.1591 - val_loss: 13.1252\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1704 - val_loss: 9.1747\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2408 - val_loss: 8.3504\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0090 - val_loss: 15.5976\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0653 - val_loss: 12.1832\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9019 - val_loss: 11.7387\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5602 - val_loss: 13.6079\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6576 - val_loss: 9.7428\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7855 - val_loss: 11.0904\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.2172 - val_loss: 10.0229\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2551 - val_loss: 12.7286\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6860 - val_loss: 9.8164\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3218 - val_loss: 11.9356\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6032 - val_loss: 9.9365\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9359 - val_loss: 9.0680\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1710 - val_loss: 8.9634\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.9006 - val_loss: 39.3266\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.2288 - val_loss: 9.1811\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5384 - val_loss: 8.8312\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1131 - val_loss: 10.1080\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.1415 - val_loss: 10.0477\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1668 - val_loss: 10.9701\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7105 - val_loss: 9.4774\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0684 - val_loss: 10.0010\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9603 - val_loss: 18.2180\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.8238 - val_loss: 22.3951\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2386 - val_loss: 10.3860\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9128 - val_loss: 9.0811\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3755 - val_loss: 12.9203\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5426 - val_loss: 10.2106\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4266 - val_loss: 10.1557\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3878 - val_loss: 9.4188\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7515 - val_loss: 11.0569\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7770 - val_loss: 7.7858\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2476 - val_loss: 9.1038\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6882 - val_loss: 8.0430\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0316 - val_loss: 7.4797\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9257 - val_loss: 8.4158\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.9963 - val_loss: 13.0236\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3959 - val_loss: 8.2754\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1774 - val_loss: 8.0808\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4906 - val_loss: 7.9263\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4729 - val_loss: 6.4203\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6194 - val_loss: 7.3043\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9388 - val_loss: 6.9160\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 6.91593022540803\n",
      "Mean Absolute Error (MAE): 1.8111896420246314\n",
      "Root Mean Squared Error (RMSE): 2.6298156257441376\n",
      "Time taken: 1195.7358322143555\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 4019.3599 - val_loss: 3784.8750\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3695.8127 - val_loss: 3564.7556\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3494.1921 - val_loss: 3383.4080\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3324.4766 - val_loss: 3227.6426\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3179.0500 - val_loss: 3094.7407\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3054.1743 - val_loss: 2980.4158\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2946.9951 - val_loss: 2882.8091\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2855.3691 - val_loss: 2799.6475\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2777.9443 - val_loss: 2730.0896\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2713.3799 - val_loss: 2672.7507\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2660.3135 - val_loss: 2626.0427\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2617.6606 - val_loss: 2589.4270\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2584.3445 - val_loss: 2561.0417\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2559.0139 - val_loss: 2540.5830\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2540.8391 - val_loss: 2526.1106\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2528.4092 - val_loss: 2516.9197\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2520.5073 - val_loss: 2511.3445\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2515.8667 - val_loss: 2508.4160\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.2778 - val_loss: 2506.9375\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.1353 - val_loss: 2506.4397\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2511.6240 - val_loss: 2506.2102\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2511.4307 - val_loss: 2506.1938\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2511.4070 - val_loss: 2506.3069\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2511.3906 - val_loss: 2506.2888\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2511.3245 - val_loss: 2506.3188\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.3420 - val_loss: 2506.1763\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.3547 - val_loss: 2506.3142\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.3313 - val_loss: 2506.3235\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.5581 - val_loss: 2506.2334\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.3862 - val_loss: 2506.3174\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2511.3684 - val_loss: 2506.2166\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.4380 - val_loss: 2506.2798\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.2654 - val_loss: 2506.4492\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.3369 - val_loss: 2506.4082\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.2161 - val_loss: 2506.4302\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2511.3762 - val_loss: 2506.2847\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2510.9243 - val_loss: 2506.4683\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2512.2358 - val_loss: 2505.5798\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2510.2979 - val_loss: 2486.5403\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2471.3774 - val_loss: 2431.8003\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2424.9729 - val_loss: 2390.0764\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2358.6716 - val_loss: 2289.6921\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2237.7371 - val_loss: 2180.8293\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2128.5371 - val_loss: 2067.9307\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2028.7559 - val_loss: 1969.1976\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1933.3016 - val_loss: 1877.6902\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1845.0861 - val_loss: 1795.3938\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1756.0453 - val_loss: 1711.3616\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1671.6624 - val_loss: 1648.2080\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1593.3201 - val_loss: 1542.0991\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1506.6938 - val_loss: 1459.3314\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1425.6798 - val_loss: 1381.3845\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1349.9861 - val_loss: 1306.7274\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1278.2307 - val_loss: 1236.6962\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1208.1808 - val_loss: 1163.5028\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1134.8452 - val_loss: 1097.1396\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1067.7666 - val_loss: 1031.2493\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1008.8577 - val_loss: 971.2316\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 941.2326 - val_loss: 910.9833\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 887.8640 - val_loss: 852.3441\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 829.8337 - val_loss: 799.1525\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 773.6598 - val_loss: 748.5372\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 721.4167 - val_loss: 699.4135\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 672.3530 - val_loss: 653.7718\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 626.0594 - val_loss: 607.3400\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 585.1359 - val_loss: 560.1720\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 540.1736 - val_loss: 519.1577\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 497.5414 - val_loss: 496.5350\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 461.5165 - val_loss: 443.7216\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 420.5064 - val_loss: 409.9198\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 391.7432 - val_loss: 381.8563\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 356.1255 - val_loss: 352.7300\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 329.6374 - val_loss: 333.8949\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 302.3832 - val_loss: 295.3628\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 274.5645 - val_loss: 262.8337\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 245.9402 - val_loss: 253.4566\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 234.6178 - val_loss: 219.4986\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 204.3000 - val_loss: 198.5580\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 186.4578 - val_loss: 181.2471\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 173.3952 - val_loss: 177.8933\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 162.6073 - val_loss: 162.7991\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 143.9065 - val_loss: 141.4012\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 125.3498 - val_loss: 127.7600\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 113.6335 - val_loss: 131.9736\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 111.3629 - val_loss: 115.8436\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 95.0451 - val_loss: 93.4599\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 86.0508 - val_loss: 88.2252\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 80.3350 - val_loss: 80.5528\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.3659 - val_loss: 90.0573\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.7989 - val_loss: 62.8456\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.9048 - val_loss: 60.0892\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.8819 - val_loss: 64.8754\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.5169 - val_loss: 54.9735\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.0377 - val_loss: 57.4231\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.2026 - val_loss: 55.3940\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.8886 - val_loss: 37.6110\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.9303 - val_loss: 40.5208\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.5891 - val_loss: 37.4966\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.1888 - val_loss: 35.1552\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.3720 - val_loss: 27.5678\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 34.7006 - val_loss: 32.4038\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.9038 - val_loss: 29.7033\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.2874 - val_loss: 27.9772\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.7542 - val_loss: 25.9156\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.7306 - val_loss: 25.1498\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.4608 - val_loss: 21.9516\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.3139 - val_loss: 19.8414\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.9242 - val_loss: 21.1859\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.9706 - val_loss: 32.0387\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.9710 - val_loss: 46.5517\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.0766 - val_loss: 18.6794\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.2039 - val_loss: 23.6956\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.8693 - val_loss: 19.6285\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.9747 - val_loss: 25.1867\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.3421 - val_loss: 15.9474\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.2337 - val_loss: 13.7962\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.6538 - val_loss: 13.8917\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2639 - val_loss: 16.5351\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.7102 - val_loss: 21.1396\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.2733 - val_loss: 18.6995\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.1990 - val_loss: 17.3304\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.7984 - val_loss: 14.9024\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.7760 - val_loss: 15.7936\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.0234 - val_loss: 22.8904\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.3201 - val_loss: 13.2422\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.9181 - val_loss: 13.2354\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3635 - val_loss: 13.0878\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3775 - val_loss: 13.9097\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.3753 - val_loss: 13.7581\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.4189 - val_loss: 16.6647\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.5669 - val_loss: 13.9707\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1201 - val_loss: 13.6607\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7616 - val_loss: 20.8674\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 13.9315 - val_loss: 13.3486\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.8715 - val_loss: 12.2813\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3774 - val_loss: 15.0784\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.4598 - val_loss: 13.5452\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8078 - val_loss: 11.4629\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8718 - val_loss: 11.5194\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0936 - val_loss: 12.8691\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4370 - val_loss: 10.4847\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0207 - val_loss: 13.6413\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3300 - val_loss: 15.6874\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3759 - val_loss: 11.1794\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.3863 - val_loss: 11.9201\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.5294 - val_loss: 10.5687\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0760 - val_loss: 13.1000\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5937 - val_loss: 10.0444\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1872 - val_loss: 10.4488\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0110 - val_loss: 20.7508\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.9961 - val_loss: 9.4667\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.1737 - val_loss: 10.5869\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4582 - val_loss: 11.3486\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4341 - val_loss: 11.2227\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4857 - val_loss: 10.6472\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9452 - val_loss: 11.0856\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7134 - val_loss: 22.9170\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0823 - val_loss: 11.8432\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0112 - val_loss: 8.5712\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1140 - val_loss: 12.6030\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.6782 - val_loss: 17.5965\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1618 - val_loss: 9.1376\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3646 - val_loss: 7.8457\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3354 - val_loss: 10.3216\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2153 - val_loss: 8.7392\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8355 - val_loss: 10.9130\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.2392 - val_loss: 15.1441\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.4249 - val_loss: 9.5072\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2282 - val_loss: 9.6855\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6247 - val_loss: 7.4208\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0910 - val_loss: 8.8531\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1469 - val_loss: 8.7737\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7726 - val_loss: 8.9115\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1101 - val_loss: 10.3185\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6938 - val_loss: 10.4758\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0872 - val_loss: 22.4525\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0796 - val_loss: 7.2782\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0788 - val_loss: 8.0082\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9263 - val_loss: 8.1682\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0343 - val_loss: 8.8213\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1959 - val_loss: 13.6992\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.2040 - val_loss: 14.5810\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.1377 - val_loss: 8.6153\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6757 - val_loss: 8.5802\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2695 - val_loss: 8.1055\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4927 - val_loss: 21.6516\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1673 - val_loss: 11.1287\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9214 - val_loss: 8.4158\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8171 - val_loss: 12.7606\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8462 - val_loss: 7.6563\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3806 - val_loss: 14.0008\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7961 - val_loss: 9.1143\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7872 - val_loss: 9.9391\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4388 - val_loss: 7.4910\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2003 - val_loss: 10.1102\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2876 - val_loss: 9.0408\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.5927 - val_loss: 9.9253\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6500 - val_loss: 8.0220\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5894 - val_loss: 7.0265\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.4922 - val_loss: 7.0803\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 7.080357816623521\n",
      "Mean Absolute Error (MAE): 1.7342880568917345\n",
      "Root Mean Squared Error (RMSE): 2.6608941761414564\n",
      "Time taken: 1206.8364157676697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_1992\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  8.560950  1.947013  2.925910  1244.899762\n",
      "1        2  6.509663  1.709533  2.551404  1255.519547\n",
      "2        3  9.029029  2.085174  3.004834  1245.943367\n",
      "3        4  6.915930  1.811190  2.629816  1195.735832\n",
      "4        5  7.080358  1.734288  2.660894  1206.836416\n",
      "5  Average  7.619186  1.857439  2.754572  1229.786985\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_Reg2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_Reg2.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_Reg2.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
