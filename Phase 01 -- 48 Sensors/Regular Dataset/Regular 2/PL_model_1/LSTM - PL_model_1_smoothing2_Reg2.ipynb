{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg2.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.243675</td>\n",
       "      <td>191.472930</td>\n",
       "      <td>120.844514</td>\n",
       "      <td>161.973862</td>\n",
       "      <td>203.309157</td>\n",
       "      <td>223.942286</td>\n",
       "      <td>172.272042</td>\n",
       "      <td>201.715548</td>\n",
       "      <td>165.343468</td>\n",
       "      <td>181.189056</td>\n",
       "      <td>...</td>\n",
       "      <td>162.676355</td>\n",
       "      <td>160.308657</td>\n",
       "      <td>171.779385</td>\n",
       "      <td>134.252465</td>\n",
       "      <td>210.414208</td>\n",
       "      <td>182.083076</td>\n",
       "      <td>155.812278</td>\n",
       "      <td>184.634597</td>\n",
       "      <td>110.600911</td>\n",
       "      <td>152.888042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.270370</td>\n",
       "      <td>191.619220</td>\n",
       "      <td>121.106482</td>\n",
       "      <td>162.092142</td>\n",
       "      <td>203.169182</td>\n",
       "      <td>223.871564</td>\n",
       "      <td>172.093343</td>\n",
       "      <td>201.375005</td>\n",
       "      <td>165.538999</td>\n",
       "      <td>181.421935</td>\n",
       "      <td>...</td>\n",
       "      <td>162.704305</td>\n",
       "      <td>160.165693</td>\n",
       "      <td>172.022959</td>\n",
       "      <td>134.354939</td>\n",
       "      <td>210.195337</td>\n",
       "      <td>181.743372</td>\n",
       "      <td>155.731854</td>\n",
       "      <td>184.679094</td>\n",
       "      <td>110.362424</td>\n",
       "      <td>152.731892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.302643</td>\n",
       "      <td>191.768979</td>\n",
       "      <td>121.366443</td>\n",
       "      <td>162.209259</td>\n",
       "      <td>203.026693</td>\n",
       "      <td>223.798630</td>\n",
       "      <td>171.910986</td>\n",
       "      <td>201.032445</td>\n",
       "      <td>165.733919</td>\n",
       "      <td>181.652777</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732617</td>\n",
       "      <td>160.022876</td>\n",
       "      <td>172.263765</td>\n",
       "      <td>134.456503</td>\n",
       "      <td>209.974871</td>\n",
       "      <td>181.403982</td>\n",
       "      <td>155.654399</td>\n",
       "      <td>184.723051</td>\n",
       "      <td>110.124378</td>\n",
       "      <td>152.574211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.340422</td>\n",
       "      <td>191.921973</td>\n",
       "      <td>121.624512</td>\n",
       "      <td>162.325496</td>\n",
       "      <td>202.881402</td>\n",
       "      <td>223.723243</td>\n",
       "      <td>171.725116</td>\n",
       "      <td>200.688059</td>\n",
       "      <td>165.928499</td>\n",
       "      <td>181.881396</td>\n",
       "      <td>...</td>\n",
       "      <td>162.761213</td>\n",
       "      <td>159.880347</td>\n",
       "      <td>172.501652</td>\n",
       "      <td>134.557184</td>\n",
       "      <td>209.752425</td>\n",
       "      <td>181.064774</td>\n",
       "      <td>155.579700</td>\n",
       "      <td>184.766445</td>\n",
       "      <td>109.886510</td>\n",
       "      <td>152.415073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.383411</td>\n",
       "      <td>192.077960</td>\n",
       "      <td>121.880971</td>\n",
       "      <td>162.441055</td>\n",
       "      <td>202.733207</td>\n",
       "      <td>223.645400</td>\n",
       "      <td>171.535999</td>\n",
       "      <td>200.342203</td>\n",
       "      <td>166.123080</td>\n",
       "      <td>182.107430</td>\n",
       "      <td>...</td>\n",
       "      <td>162.790045</td>\n",
       "      <td>159.738426</td>\n",
       "      <td>172.736318</td>\n",
       "      <td>134.657218</td>\n",
       "      <td>209.527727</td>\n",
       "      <td>180.725519</td>\n",
       "      <td>155.507653</td>\n",
       "      <td>184.809417</td>\n",
       "      <td>109.648511</td>\n",
       "      <td>152.254752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>183.851947</td>\n",
       "      <td>169.897148</td>\n",
       "      <td>138.281160</td>\n",
       "      <td>108.937324</td>\n",
       "      <td>232.694714</td>\n",
       "      <td>217.587301</td>\n",
       "      <td>196.776191</td>\n",
       "      <td>181.842620</td>\n",
       "      <td>177.813604</td>\n",
       "      <td>170.610492</td>\n",
       "      <td>...</td>\n",
       "      <td>162.979941</td>\n",
       "      <td>179.132099</td>\n",
       "      <td>171.526947</td>\n",
       "      <td>112.014481</td>\n",
       "      <td>220.481556</td>\n",
       "      <td>181.341623</td>\n",
       "      <td>187.811124</td>\n",
       "      <td>171.976998</td>\n",
       "      <td>136.395307</td>\n",
       "      <td>108.910087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>183.511357</td>\n",
       "      <td>169.679639</td>\n",
       "      <td>138.390898</td>\n",
       "      <td>109.054093</td>\n",
       "      <td>232.650925</td>\n",
       "      <td>217.531290</td>\n",
       "      <td>196.876168</td>\n",
       "      <td>182.050297</td>\n",
       "      <td>177.707430</td>\n",
       "      <td>170.562320</td>\n",
       "      <td>...</td>\n",
       "      <td>162.718275</td>\n",
       "      <td>179.004554</td>\n",
       "      <td>171.365925</td>\n",
       "      <td>112.141669</td>\n",
       "      <td>220.444739</td>\n",
       "      <td>181.449889</td>\n",
       "      <td>187.662169</td>\n",
       "      <td>171.959291</td>\n",
       "      <td>136.412017</td>\n",
       "      <td>109.095598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>183.169072</td>\n",
       "      <td>169.462522</td>\n",
       "      <td>138.500958</td>\n",
       "      <td>109.170887</td>\n",
       "      <td>232.606076</td>\n",
       "      <td>217.473657</td>\n",
       "      <td>196.976591</td>\n",
       "      <td>182.254254</td>\n",
       "      <td>177.601171</td>\n",
       "      <td>170.514881</td>\n",
       "      <td>...</td>\n",
       "      <td>162.454129</td>\n",
       "      <td>178.877790</td>\n",
       "      <td>171.205267</td>\n",
       "      <td>112.271075</td>\n",
       "      <td>220.409626</td>\n",
       "      <td>181.556351</td>\n",
       "      <td>187.512505</td>\n",
       "      <td>171.945252</td>\n",
       "      <td>136.426329</td>\n",
       "      <td>109.279941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>182.825475</td>\n",
       "      <td>169.246118</td>\n",
       "      <td>138.611468</td>\n",
       "      <td>109.287583</td>\n",
       "      <td>232.560118</td>\n",
       "      <td>217.414344</td>\n",
       "      <td>197.077582</td>\n",
       "      <td>182.454750</td>\n",
       "      <td>177.494501</td>\n",
       "      <td>170.468468</td>\n",
       "      <td>...</td>\n",
       "      <td>162.187125</td>\n",
       "      <td>178.751987</td>\n",
       "      <td>171.044821</td>\n",
       "      <td>112.402489</td>\n",
       "      <td>220.376130</td>\n",
       "      <td>181.660586</td>\n",
       "      <td>187.362046</td>\n",
       "      <td>171.934720</td>\n",
       "      <td>136.438444</td>\n",
       "      <td>109.462795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>182.480920</td>\n",
       "      <td>169.030592</td>\n",
       "      <td>138.722488</td>\n",
       "      <td>109.404024</td>\n",
       "      <td>232.513155</td>\n",
       "      <td>217.353402</td>\n",
       "      <td>197.179353</td>\n",
       "      <td>182.652230</td>\n",
       "      <td>177.387208</td>\n",
       "      <td>170.423237</td>\n",
       "      <td>...</td>\n",
       "      <td>161.917079</td>\n",
       "      <td>178.627367</td>\n",
       "      <td>170.884336</td>\n",
       "      <td>112.535554</td>\n",
       "      <td>220.344095</td>\n",
       "      <td>181.762446</td>\n",
       "      <td>187.210616</td>\n",
       "      <td>171.927396</td>\n",
       "      <td>136.448449</td>\n",
       "      <td>109.643939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     165.243675  191.472930  120.844514  161.973862  203.309157  223.942286   \n",
       "1     165.270370  191.619220  121.106482  162.092142  203.169182  223.871564   \n",
       "2     165.302643  191.768979  121.366443  162.209259  203.026693  223.798630   \n",
       "3     165.340422  191.921973  121.624512  162.325496  202.881402  223.723243   \n",
       "4     165.383411  192.077960  121.880971  162.441055  202.733207  223.645400   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  183.851947  169.897148  138.281160  108.937324  232.694714  217.587301   \n",
       "2439  183.511357  169.679639  138.390898  109.054093  232.650925  217.531290   \n",
       "2440  183.169072  169.462522  138.500958  109.170887  232.606076  217.473657   \n",
       "2441  182.825475  169.246118  138.611468  109.287583  232.560118  217.414344   \n",
       "2442  182.480920  169.030592  138.722488  109.404024  232.513155  217.353402   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     172.272042  201.715548  165.343468  181.189056  ...  162.676355   \n",
       "1     172.093343  201.375005  165.538999  181.421935  ...  162.704305   \n",
       "2     171.910986  201.032445  165.733919  181.652777  ...  162.732617   \n",
       "3     171.725116  200.688059  165.928499  181.881396  ...  162.761213   \n",
       "4     171.535999  200.342203  166.123080  182.107430  ...  162.790045   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.776191  181.842620  177.813604  170.610492  ...  162.979941   \n",
       "2439  196.876168  182.050297  177.707430  170.562320  ...  162.718275   \n",
       "2440  196.976591  182.254254  177.601171  170.514881  ...  162.454129   \n",
       "2441  197.077582  182.454750  177.494501  170.468468  ...  162.187125   \n",
       "2442  197.179353  182.652230  177.387208  170.423237  ...  161.917079   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     160.308657  171.779385  134.252465  210.414208  182.083076  155.812278   \n",
       "1     160.165693  172.022959  134.354939  210.195337  181.743372  155.731854   \n",
       "2     160.022876  172.263765  134.456503  209.974871  181.403982  155.654399   \n",
       "3     159.880347  172.501652  134.557184  209.752425  181.064774  155.579700   \n",
       "4     159.738426  172.736318  134.657218  209.527727  180.725519  155.507653   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  179.132099  171.526947  112.014481  220.481556  181.341623  187.811124   \n",
       "2439  179.004554  171.365925  112.141669  220.444739  181.449889  187.662169   \n",
       "2440  178.877790  171.205267  112.271075  220.409626  181.556351  187.512505   \n",
       "2441  178.751987  171.044821  112.402489  220.376130  181.660586  187.362046   \n",
       "2442  178.627367  170.884336  112.535554  220.344095  181.762446  187.210616   \n",
       "\n",
       "              45          46          47  \n",
       "0     184.634597  110.600911  152.888042  \n",
       "1     184.679094  110.362424  152.731892  \n",
       "2     184.723051  110.124378  152.574211  \n",
       "3     184.766445  109.886510  152.415073  \n",
       "4     184.809417  109.648511  152.254752  \n",
       "...          ...         ...         ...  \n",
       "2438  171.976998  136.395307  108.910087  \n",
       "2439  171.959291  136.412017  109.095598  \n",
       "2440  171.945252  136.426329  109.279941  \n",
       "2441  171.934720  136.438444  109.462795  \n",
       "2442  171.927396  136.448449  109.643939  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.243675</td>\n",
       "      <td>191.472930</td>\n",
       "      <td>120.844514</td>\n",
       "      <td>161.973862</td>\n",
       "      <td>203.309157</td>\n",
       "      <td>223.942286</td>\n",
       "      <td>172.272042</td>\n",
       "      <td>201.715548</td>\n",
       "      <td>165.343468</td>\n",
       "      <td>181.189056</td>\n",
       "      <td>...</td>\n",
       "      <td>162.676355</td>\n",
       "      <td>160.308657</td>\n",
       "      <td>171.779385</td>\n",
       "      <td>134.252465</td>\n",
       "      <td>210.414208</td>\n",
       "      <td>182.083076</td>\n",
       "      <td>155.812278</td>\n",
       "      <td>184.634597</td>\n",
       "      <td>110.600911</td>\n",
       "      <td>152.888042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.270370</td>\n",
       "      <td>191.619220</td>\n",
       "      <td>121.106482</td>\n",
       "      <td>162.092142</td>\n",
       "      <td>203.169182</td>\n",
       "      <td>223.871564</td>\n",
       "      <td>172.093343</td>\n",
       "      <td>201.375005</td>\n",
       "      <td>165.538999</td>\n",
       "      <td>181.421935</td>\n",
       "      <td>...</td>\n",
       "      <td>162.704305</td>\n",
       "      <td>160.165693</td>\n",
       "      <td>172.022959</td>\n",
       "      <td>134.354939</td>\n",
       "      <td>210.195337</td>\n",
       "      <td>181.743372</td>\n",
       "      <td>155.731854</td>\n",
       "      <td>184.679094</td>\n",
       "      <td>110.362424</td>\n",
       "      <td>152.731892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.302643</td>\n",
       "      <td>191.768979</td>\n",
       "      <td>121.366443</td>\n",
       "      <td>162.209259</td>\n",
       "      <td>203.026693</td>\n",
       "      <td>223.798630</td>\n",
       "      <td>171.910986</td>\n",
       "      <td>201.032445</td>\n",
       "      <td>165.733919</td>\n",
       "      <td>181.652777</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732617</td>\n",
       "      <td>160.022876</td>\n",
       "      <td>172.263765</td>\n",
       "      <td>134.456503</td>\n",
       "      <td>209.974871</td>\n",
       "      <td>181.403982</td>\n",
       "      <td>155.654399</td>\n",
       "      <td>184.723051</td>\n",
       "      <td>110.124378</td>\n",
       "      <td>152.574211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.340422</td>\n",
       "      <td>191.921973</td>\n",
       "      <td>121.624512</td>\n",
       "      <td>162.325496</td>\n",
       "      <td>202.881402</td>\n",
       "      <td>223.723243</td>\n",
       "      <td>171.725116</td>\n",
       "      <td>200.688059</td>\n",
       "      <td>165.928499</td>\n",
       "      <td>181.881396</td>\n",
       "      <td>...</td>\n",
       "      <td>162.761213</td>\n",
       "      <td>159.880347</td>\n",
       "      <td>172.501652</td>\n",
       "      <td>134.557184</td>\n",
       "      <td>209.752425</td>\n",
       "      <td>181.064774</td>\n",
       "      <td>155.579700</td>\n",
       "      <td>184.766445</td>\n",
       "      <td>109.886510</td>\n",
       "      <td>152.415073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.383411</td>\n",
       "      <td>192.077960</td>\n",
       "      <td>121.880971</td>\n",
       "      <td>162.441055</td>\n",
       "      <td>202.733207</td>\n",
       "      <td>223.645400</td>\n",
       "      <td>171.535999</td>\n",
       "      <td>200.342203</td>\n",
       "      <td>166.123080</td>\n",
       "      <td>182.107430</td>\n",
       "      <td>...</td>\n",
       "      <td>162.790045</td>\n",
       "      <td>159.738426</td>\n",
       "      <td>172.736318</td>\n",
       "      <td>134.657218</td>\n",
       "      <td>209.527727</td>\n",
       "      <td>180.725519</td>\n",
       "      <td>155.507653</td>\n",
       "      <td>184.809417</td>\n",
       "      <td>109.648511</td>\n",
       "      <td>152.254752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>183.851947</td>\n",
       "      <td>169.897148</td>\n",
       "      <td>138.281160</td>\n",
       "      <td>108.937324</td>\n",
       "      <td>232.694714</td>\n",
       "      <td>217.587301</td>\n",
       "      <td>196.776191</td>\n",
       "      <td>181.842620</td>\n",
       "      <td>177.813604</td>\n",
       "      <td>170.610492</td>\n",
       "      <td>...</td>\n",
       "      <td>162.979941</td>\n",
       "      <td>179.132099</td>\n",
       "      <td>171.526947</td>\n",
       "      <td>112.014481</td>\n",
       "      <td>220.481556</td>\n",
       "      <td>181.341623</td>\n",
       "      <td>187.811124</td>\n",
       "      <td>171.976998</td>\n",
       "      <td>136.395307</td>\n",
       "      <td>108.910087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>183.511357</td>\n",
       "      <td>169.679639</td>\n",
       "      <td>138.390898</td>\n",
       "      <td>109.054093</td>\n",
       "      <td>232.650925</td>\n",
       "      <td>217.531290</td>\n",
       "      <td>196.876168</td>\n",
       "      <td>182.050297</td>\n",
       "      <td>177.707430</td>\n",
       "      <td>170.562320</td>\n",
       "      <td>...</td>\n",
       "      <td>162.718275</td>\n",
       "      <td>179.004554</td>\n",
       "      <td>171.365925</td>\n",
       "      <td>112.141669</td>\n",
       "      <td>220.444739</td>\n",
       "      <td>181.449889</td>\n",
       "      <td>187.662169</td>\n",
       "      <td>171.959291</td>\n",
       "      <td>136.412017</td>\n",
       "      <td>109.095598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>183.169072</td>\n",
       "      <td>169.462522</td>\n",
       "      <td>138.500958</td>\n",
       "      <td>109.170887</td>\n",
       "      <td>232.606076</td>\n",
       "      <td>217.473657</td>\n",
       "      <td>196.976591</td>\n",
       "      <td>182.254254</td>\n",
       "      <td>177.601171</td>\n",
       "      <td>170.514881</td>\n",
       "      <td>...</td>\n",
       "      <td>162.454129</td>\n",
       "      <td>178.877790</td>\n",
       "      <td>171.205267</td>\n",
       "      <td>112.271075</td>\n",
       "      <td>220.409626</td>\n",
       "      <td>181.556351</td>\n",
       "      <td>187.512505</td>\n",
       "      <td>171.945252</td>\n",
       "      <td>136.426329</td>\n",
       "      <td>109.279941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>182.825475</td>\n",
       "      <td>169.246118</td>\n",
       "      <td>138.611468</td>\n",
       "      <td>109.287583</td>\n",
       "      <td>232.560118</td>\n",
       "      <td>217.414344</td>\n",
       "      <td>197.077582</td>\n",
       "      <td>182.454750</td>\n",
       "      <td>177.494501</td>\n",
       "      <td>170.468468</td>\n",
       "      <td>...</td>\n",
       "      <td>162.187125</td>\n",
       "      <td>178.751987</td>\n",
       "      <td>171.044821</td>\n",
       "      <td>112.402489</td>\n",
       "      <td>220.376130</td>\n",
       "      <td>181.660586</td>\n",
       "      <td>187.362046</td>\n",
       "      <td>171.934720</td>\n",
       "      <td>136.438444</td>\n",
       "      <td>109.462795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>182.480920</td>\n",
       "      <td>169.030592</td>\n",
       "      <td>138.722488</td>\n",
       "      <td>109.404024</td>\n",
       "      <td>232.513155</td>\n",
       "      <td>217.353402</td>\n",
       "      <td>197.179353</td>\n",
       "      <td>182.652230</td>\n",
       "      <td>177.387208</td>\n",
       "      <td>170.423237</td>\n",
       "      <td>...</td>\n",
       "      <td>161.917079</td>\n",
       "      <td>178.627367</td>\n",
       "      <td>170.884336</td>\n",
       "      <td>112.535554</td>\n",
       "      <td>220.344095</td>\n",
       "      <td>181.762446</td>\n",
       "      <td>187.210616</td>\n",
       "      <td>171.927396</td>\n",
       "      <td>136.448449</td>\n",
       "      <td>109.643939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.243675  191.472930  120.844514  161.973862  203.309157  223.942286   \n",
       "1     165.270370  191.619220  121.106482  162.092142  203.169182  223.871564   \n",
       "2     165.302643  191.768979  121.366443  162.209259  203.026693  223.798630   \n",
       "3     165.340422  191.921973  121.624512  162.325496  202.881402  223.723243   \n",
       "4     165.383411  192.077960  121.880971  162.441055  202.733207  223.645400   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  183.851947  169.897148  138.281160  108.937324  232.694714  217.587301   \n",
       "2439  183.511357  169.679639  138.390898  109.054093  232.650925  217.531290   \n",
       "2440  183.169072  169.462522  138.500958  109.170887  232.606076  217.473657   \n",
       "2441  182.825475  169.246118  138.611468  109.287583  232.560118  217.414344   \n",
       "2442  182.480920  169.030592  138.722488  109.404024  232.513155  217.353402   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     172.272042  201.715548  165.343468  181.189056  ...  162.676355   \n",
       "1     172.093343  201.375005  165.538999  181.421935  ...  162.704305   \n",
       "2     171.910986  201.032445  165.733919  181.652777  ...  162.732617   \n",
       "3     171.725116  200.688059  165.928499  181.881396  ...  162.761213   \n",
       "4     171.535999  200.342203  166.123080  182.107430  ...  162.790045   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.776191  181.842620  177.813604  170.610492  ...  162.979941   \n",
       "2439  196.876168  182.050297  177.707430  170.562320  ...  162.718275   \n",
       "2440  196.976591  182.254254  177.601171  170.514881  ...  162.454129   \n",
       "2441  197.077582  182.454750  177.494501  170.468468  ...  162.187125   \n",
       "2442  197.179353  182.652230  177.387208  170.423237  ...  161.917079   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     160.308657  171.779385  134.252465  210.414208  182.083076  155.812278   \n",
       "1     160.165693  172.022959  134.354939  210.195337  181.743372  155.731854   \n",
       "2     160.022876  172.263765  134.456503  209.974871  181.403982  155.654399   \n",
       "3     159.880347  172.501652  134.557184  209.752425  181.064774  155.579700   \n",
       "4     159.738426  172.736318  134.657218  209.527727  180.725519  155.507653   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  179.132099  171.526947  112.014481  220.481556  181.341623  187.811124   \n",
       "2439  179.004554  171.365925  112.141669  220.444739  181.449889  187.662169   \n",
       "2440  178.877790  171.205267  112.271075  220.409626  181.556351  187.512505   \n",
       "2441  178.751987  171.044821  112.402489  220.376130  181.660586  187.362046   \n",
       "2442  178.627367  170.884336  112.535554  220.344095  181.762446  187.210616   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     184.634597  110.600911  152.888042  \n",
       "1     184.679094  110.362424  152.731892  \n",
       "2     184.723051  110.124378  152.574211  \n",
       "3     184.766445  109.886510  152.415073  \n",
       "4     184.809417  109.648511  152.254752  \n",
       "...          ...         ...         ...  \n",
       "2438  171.976998  136.395307  108.910087  \n",
       "2439  171.959291  136.412017  109.095598  \n",
       "2440  171.945252  136.426329  109.279941  \n",
       "2441  171.934720  136.438444  109.462795  \n",
       "2442  171.927396  136.448449  109.643939  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 4083.9895 - val_loss: 3782.4800\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3756.3237 - val_loss: 3559.4875\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3545.9875 - val_loss: 3374.3997\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3369.7249 - val_loss: 3218.4141\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3219.3518 - val_loss: 3084.8284\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3089.7744 - val_loss: 2970.0264\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2978.3667 - val_loss: 2871.5867\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2882.9187 - val_loss: 2788.1567\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2801.9795 - val_loss: 2718.0295\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2734.0315 - val_loss: 2660.0613\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2677.8679 - val_loss: 2612.8210\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2632.2722 - val_loss: 2575.5942\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2596.3037 - val_loss: 2546.9084\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2568.7781 - val_loss: 2526.0000\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2548.5437 - val_loss: 2511.4241\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2534.5381 - val_loss: 2502.2883\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2525.4934 - val_loss: 2496.8870\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2519.8738 - val_loss: 2494.1797\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2516.8374 - val_loss: 2493.2014\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2515.3821 - val_loss: 2492.9795\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.6138 - val_loss: 2493.0681\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.3875 - val_loss: 2493.3601\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2979 - val_loss: 2493.4453\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2451 - val_loss: 2493.5906\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2427 - val_loss: 2493.6938\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2539 - val_loss: 2493.7446\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2124 - val_loss: 2493.7188\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2439 - val_loss: 2493.7644\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2319 - val_loss: 2493.7134\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2292 - val_loss: 2493.8662\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2514.2573 - val_loss: 2493.8394\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2861 - val_loss: 2493.8765\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.1978 - val_loss: 2493.8301\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2361 - val_loss: 2493.8418\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2166 - val_loss: 2493.7937\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2939 - val_loss: 2493.8010\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2869 - val_loss: 2493.8159\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2000 - val_loss: 2493.9629\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2134 - val_loss: 2493.8364\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2487.5515 - val_loss: 2400.5305\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2398.4683 - val_loss: 2362.1650\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2367.3589 - val_loss: 2341.4602\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2342.9651 - val_loss: 2316.5342\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2312.3525 - val_loss: 2281.1182\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2261.4321 - val_loss: 2215.5930\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2194.8499 - val_loss: 2141.1802\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2127.8289 - val_loss: 2077.3755\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2062.4529 - val_loss: 2008.5306\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1998.5400 - val_loss: 1952.4276\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1942.5277 - val_loss: 1889.4348\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1870.4098 - val_loss: 1815.0487\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1799.3550 - val_loss: 1745.0167\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1722.8762 - val_loss: 1665.5859\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1652.1191 - val_loss: 1595.6600\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1579.5892 - val_loss: 1531.4880\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1523.7175 - val_loss: 1461.5214\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1449.4890 - val_loss: 1397.3077\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1384.6180 - val_loss: 1332.0776\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1320.3872 - val_loss: 1267.8862\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1258.7948 - val_loss: 1215.3715\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1197.6223 - val_loss: 1147.9404\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1138.4861 - val_loss: 1090.8379\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1083.3026 - val_loss: 1035.4901\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1031.4088 - val_loss: 981.2487\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 973.2505 - val_loss: 927.0659\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.0121 - val_loss: 873.7111\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 869.2303 - val_loss: 825.1201\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 818.1956 - val_loss: 786.1750\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 772.9168 - val_loss: 741.3574\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 723.7025 - val_loss: 684.6904\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 678.4628 - val_loss: 639.1771\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 634.1899 - val_loss: 601.5596\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 592.5821 - val_loss: 571.4553\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 554.9151 - val_loss: 518.5127\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 513.3267 - val_loss: 484.8105\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 477.1295 - val_loss: 443.4522\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 436.9795 - val_loss: 410.9829\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 404.6013 - val_loss: 378.1814\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 377.2037 - val_loss: 349.8744\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 357.4460 - val_loss: 329.8189\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 319.1442 - val_loss: 296.7955\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 290.1081 - val_loss: 272.2515\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 265.2188 - val_loss: 245.3767\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 240.2899 - val_loss: 223.8463\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 219.4214 - val_loss: 205.7181\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.1511 - val_loss: 185.9168\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 181.4067 - val_loss: 171.0778\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 165.9993 - val_loss: 159.4878\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 149.6251 - val_loss: 166.4794\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 136.5778 - val_loss: 128.5492\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 122.6005 - val_loss: 119.3051\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 114.3814 - val_loss: 107.5638\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 101.1259 - val_loss: 95.2905\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.7898 - val_loss: 86.6485\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 84.4093 - val_loss: 77.7617\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 72.1280 - val_loss: 71.9821\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.6325 - val_loss: 64.4882\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.1134 - val_loss: 58.8582\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.3831 - val_loss: 69.3246\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.2528 - val_loss: 46.8363\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.6193 - val_loss: 44.6403\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.0344 - val_loss: 46.4095\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.2166 - val_loss: 34.8980\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.4069 - val_loss: 32.1531\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.1863 - val_loss: 28.7652\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.8064 - val_loss: 28.3913\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.8185 - val_loss: 25.8446\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.1200 - val_loss: 22.9988\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6126 - val_loss: 20.2295\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.2835 - val_loss: 17.3318\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0835 - val_loss: 16.2077\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.3530 - val_loss: 16.2159\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.4767 - val_loss: 13.3363\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.2689 - val_loss: 12.8212\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.1862 - val_loss: 10.5882\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0234 - val_loss: 10.0590\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3969 - val_loss: 16.6588\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.5471 - val_loss: 8.0601\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2628 - val_loss: 8.1901\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6861 - val_loss: 6.7806\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6711 - val_loss: 6.3735\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6797 - val_loss: 6.1340\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4498 - val_loss: 5.9353\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0748 - val_loss: 5.4398\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0368 - val_loss: 4.7982\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5825 - val_loss: 7.6103\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1415 - val_loss: 4.9439\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5249 - val_loss: 4.2128\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8672 - val_loss: 3.0886\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9038 - val_loss: 4.6765\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8766 - val_loss: 3.8811\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8671 - val_loss: 2.3001\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8137 - val_loss: 2.5945\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8113 - val_loss: 2.2034\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7181 - val_loss: 2.3665\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8994 - val_loss: 1.7011\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1802 - val_loss: 2.0329\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0096 - val_loss: 1.4839\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7020 - val_loss: 1.1730\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8075 - val_loss: 1.6566\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.8332 - val_loss: 2.4187\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0195 - val_loss: 0.9023\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5979 - val_loss: 1.3161\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8460 - val_loss: 2.0060\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4032 - val_loss: 1.0109\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9734 - val_loss: 1.5650\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1938 - val_loss: 1.5015\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1471 - val_loss: 2.5705\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7454 - val_loss: 0.8056\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6133 - val_loss: 0.8459\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7809 - val_loss: 1.3769\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2462 - val_loss: 0.7024\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4566 - val_loss: 0.5449\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3718 - val_loss: 0.6356\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6720 - val_loss: 1.2084\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4853 - val_loss: 0.8627\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8313 - val_loss: 0.6999\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9050 - val_loss: 1.4237\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8741 - val_loss: 0.6079\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5179 - val_loss: 0.4704\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0370 - val_loss: 3.2449\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5423 - val_loss: 0.3247\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2451 - val_loss: 0.3720\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3449 - val_loss: 0.5377\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6725 - val_loss: 2.0688\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1723 - val_loss: 0.6307\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3165 - val_loss: 0.4040\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5685 - val_loss: 0.5050\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1840 - val_loss: 2.5860\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2578 - val_loss: 0.5252\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3191 - val_loss: 0.2945\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0762 - val_loss: 4.5707\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6464 - val_loss: 0.3152\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2828 - val_loss: 0.6437\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3242 - val_loss: 0.5049\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2428 - val_loss: 0.3792\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0817 - val_loss: 1.9064\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6256 - val_loss: 0.4433\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6920 - val_loss: 2.2480\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3389 - val_loss: 0.3423\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4605 - val_loss: 0.3787\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8275 - val_loss: 0.8412\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3883 - val_loss: 0.3463\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9482 - val_loss: 1.5833\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8757 - val_loss: 0.4978\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8368 - val_loss: 0.6743\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4467 - val_loss: 1.8507\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2284 - val_loss: 1.3266\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4293 - val_loss: 0.2191\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6471 - val_loss: 0.7539\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7677 - val_loss: 0.6213\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6551 - val_loss: 0.7237\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6052 - val_loss: 0.5125\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5768 - val_loss: 0.5253\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.9629 - val_loss: 0.6777\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7011 - val_loss: 1.1073\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7272 - val_loss: 1.2927\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5180 - val_loss: 0.3320\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6307 - val_loss: 0.8898\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8835 - val_loss: 0.8056\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.8055636134437133\n",
      "Mean Absolute Error (MAE): 0.6798097491607638\n",
      "Root Mean Squared Error (RMSE): 0.8975319567813245\n",
      "Time taken: 1228.782513141632\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 19ms/step - loss: 4097.1323 - val_loss: 3761.2556\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3769.1292 - val_loss: 3537.2698\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3556.6257 - val_loss: 3352.8196\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3380.1523 - val_loss: 3195.8171\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3229.1243 - val_loss: 3060.8787\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3099.1753 - val_loss: 2944.4355\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2987.8125 - val_loss: 2844.5935\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2892.6201 - val_loss: 2759.4707\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2811.9509 - val_loss: 2687.4958\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2744.1870 - val_loss: 2627.4490\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2688.2883 - val_loss: 2578.2471\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2642.7974 - val_loss: 2539.0332\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2607.2734 - val_loss: 2508.6077\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2580.1646 - val_loss: 2485.7200\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2560.2214 - val_loss: 2469.6455\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2546.5334 - val_loss: 2458.6479\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2537.4436 - val_loss: 2451.9895\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2532.0583 - val_loss: 2448.1016\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2529.1318 - val_loss: 2446.0820\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2527.6760 - val_loss: 2445.3960\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2527.0601 - val_loss: 2445.0889\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.8655 - val_loss: 2445.0569\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.7229 - val_loss: 2444.8486\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.7991 - val_loss: 2444.8708\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.7202 - val_loss: 2445.0051\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.6909 - val_loss: 2444.9619\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.7712 - val_loss: 2444.9263\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.6755 - val_loss: 2445.0972\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.7229 - val_loss: 2445.1172\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.6978 - val_loss: 2445.0652\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.6926 - val_loss: 2445.0752\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.7305 - val_loss: 2445.1313\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.7693 - val_loss: 2445.1287\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.7087 - val_loss: 2445.1736\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.7124 - val_loss: 2445.1978\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.6904 - val_loss: 2445.1890\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.6794 - val_loss: 2445.0000\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.6758 - val_loss: 2445.2263\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2526.7053 - val_loss: 2445.1829\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2526.7954 - val_loss: 2445.2190\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2526.4797 - val_loss: 2443.5437\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2455.8193 - val_loss: 2322.4580\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2399.8342 - val_loss: 2293.1340\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2370.2495 - val_loss: 2268.2495\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2344.9011 - val_loss: 2246.4146\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2320.0369 - val_loss: 2226.1736\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2298.4712 - val_loss: 2206.6960\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2275.2737 - val_loss: 2177.5623\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2251.1882 - val_loss: 2163.7761\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2224.6904 - val_loss: 2134.3215\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2198.9448 - val_loss: 2109.1619\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2157.8057 - val_loss: 2056.9268\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2088.1184 - val_loss: 1984.6709\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2021.8020 - val_loss: 1927.1533\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1965.2653 - val_loss: 1872.3152\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1904.9692 - val_loss: 1811.0922\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1843.8057 - val_loss: 1753.4709\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1784.8193 - val_loss: 1699.7937\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1728.4233 - val_loss: 1650.6003\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1668.7286 - val_loss: 1580.7799\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1604.8552 - val_loss: 1527.4801\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1545.9585 - val_loss: 1468.4442\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1496.4076 - val_loss: 1415.4888\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1430.5789 - val_loss: 1357.4027\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1373.3916 - val_loss: 1306.7151\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1318.7521 - val_loss: 1253.2217\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1263.3396 - val_loss: 1200.8374\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1213.4875 - val_loss: 1153.5894\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1160.5018 - val_loss: 1100.4196\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1110.1621 - val_loss: 1050.6935\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1057.8999 - val_loss: 1001.7664\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1007.2166 - val_loss: 953.7626\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 958.3419 - val_loss: 908.1310\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 910.8022 - val_loss: 863.0243\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 865.4144 - val_loss: 819.0997\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 816.4185 - val_loss: 768.3233\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 765.8018 - val_loss: 724.5460\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 722.3785 - val_loss: 678.2657\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 677.0826 - val_loss: 640.2201\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 640.5098 - val_loss: 605.6508\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 603.7441 - val_loss: 571.1753\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 569.4259 - val_loss: 546.4282\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 536.7740 - val_loss: 516.2286\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 505.2964 - val_loss: 480.8813\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 473.3090 - val_loss: 455.2559\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 443.5191 - val_loss: 419.8488\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 413.6892 - val_loss: 392.9309\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 386.8336 - val_loss: 370.8781\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 361.8069 - val_loss: 350.7091\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 337.8464 - val_loss: 320.7773\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 314.1943 - val_loss: 300.5674\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 291.4391 - val_loss: 280.4321\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 272.0894 - val_loss: 263.7630\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 252.6040 - val_loss: 240.4178\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 232.4111 - val_loss: 223.7767\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 215.2685 - val_loss: 208.3811\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 200.2088 - val_loss: 192.5304\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 183.1536 - val_loss: 176.6393\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 169.5673 - val_loss: 163.7748\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 156.1831 - val_loss: 154.2407\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 143.4244 - val_loss: 140.1772\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 131.5512 - val_loss: 128.2347\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 120.0769 - val_loss: 116.0361\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 109.1511 - val_loss: 105.7296\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 102.5138 - val_loss: 96.1584\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.8211 - val_loss: 87.6364\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 82.7128 - val_loss: 82.1668\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 74.7662 - val_loss: 73.3588\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 68.5316 - val_loss: 67.4977\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.2766 - val_loss: 62.0323\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.3964 - val_loss: 55.9940\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 50.8491 - val_loss: 50.4649\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.5951 - val_loss: 48.9907\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.8971 - val_loss: 43.1915\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 37.9653 - val_loss: 39.1009\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.2596 - val_loss: 34.7477\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.2085 - val_loss: 30.7864\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.7398 - val_loss: 28.2797\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.7518 - val_loss: 26.7334\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.1080 - val_loss: 24.8825\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.9879 - val_loss: 21.4329\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.0171 - val_loss: 19.2685\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.4286 - val_loss: 17.7680\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.4805 - val_loss: 15.4976\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.7419 - val_loss: 14.2283\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.2531 - val_loss: 13.5143\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7230 - val_loss: 11.6065\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5412 - val_loss: 10.3321\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.1258 - val_loss: 9.0993\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2782 - val_loss: 9.6025\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2338 - val_loss: 12.9224\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7207 - val_loss: 6.2917\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1770 - val_loss: 7.4887\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4227 - val_loss: 5.2583\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9221 - val_loss: 4.8524\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3180 - val_loss: 4.6824\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.2963 - val_loss: 4.4252\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6251 - val_loss: 3.3763\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2576 - val_loss: 3.3842\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8034 - val_loss: 2.7595\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6893 - val_loss: 2.9827\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0308 - val_loss: 2.6168\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2291 - val_loss: 2.3241\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7772 - val_loss: 2.9996\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0640 - val_loss: 2.0530\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8296 - val_loss: 2.6359\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1034 - val_loss: 1.5143\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5835 - val_loss: 1.4406\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4791 - val_loss: 2.4026\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2515 - val_loss: 1.3422\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2066 - val_loss: 3.2291\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6929 - val_loss: 1.0064\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5122 - val_loss: 3.0753\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3561 - val_loss: 0.6936\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8028 - val_loss: 0.6873\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8573 - val_loss: 2.4118\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1153 - val_loss: 1.5382\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9698 - val_loss: 0.7509\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6866 - val_loss: 0.7253\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0397 - val_loss: 1.8716\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2959 - val_loss: 0.9207\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8279 - val_loss: 0.5542\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8506 - val_loss: 1.2251\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1214 - val_loss: 0.7524\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8695 - val_loss: 0.7538\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4881 - val_loss: 0.6082\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5294 - val_loss: 1.8338\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7216 - val_loss: 0.3817\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6435 - val_loss: 1.3098\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6550 - val_loss: 0.4368\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5600 - val_loss: 0.9198\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2459 - val_loss: 1.1235\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8447 - val_loss: 0.9654\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4791 - val_loss: 0.4079\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1205 - val_loss: 0.4957\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5291 - val_loss: 0.8222\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6784 - val_loss: 0.5939\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7208 - val_loss: 0.6459\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8510 - val_loss: 0.6798\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5175 - val_loss: 0.4436\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2376 - val_loss: 0.4846\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4591 - val_loss: 1.4925\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2120 - val_loss: 0.3853\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4973 - val_loss: 0.4315\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4069 - val_loss: 0.3590\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1357 - val_loss: 0.9820\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4765 - val_loss: 0.5636\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2974 - val_loss: 0.4323\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4391 - val_loss: 0.6003\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3359 - val_loss: 0.2535\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3442 - val_loss: 0.3641\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3981 - val_loss: 0.8311\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2109 - val_loss: 0.4281\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4408 - val_loss: 0.7315\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4218 - val_loss: 0.1991\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4435 - val_loss: 0.5286\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9617 - val_loss: 0.9075\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3725 - val_loss: 0.3067\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4355 - val_loss: 0.6592\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6837 - val_loss: 0.8823\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.8823363373067142\n",
      "Mean Absolute Error (MAE): 0.7152264428911117\n",
      "Root Mean Squared Error (RMSE): 0.9393275985015633\n",
      "Time taken: 1221.6120777130127\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 3995.5635 - val_loss: 3943.9597\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3674.0652 - val_loss: 3712.6602\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3471.0503 - val_loss: 3522.0752\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3301.3467 - val_loss: 3359.7686\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3155.9155 - val_loss: 3221.0874\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3030.6443 - val_loss: 3101.1851\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2923.4348 - val_loss: 2999.1411\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2832.0544 - val_loss: 2912.7327\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2754.7883 - val_loss: 2839.9695\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2690.0735 - val_loss: 2779.5342\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2637.0176 - val_loss: 2730.4763\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2594.4341 - val_loss: 2691.5793\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2561.0249 - val_loss: 2661.6096\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2535.8518 - val_loss: 2639.3247\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2517.6909 - val_loss: 2623.8542\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2505.2705 - val_loss: 2613.2576\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2497.2666 - val_loss: 2606.7410\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2492.6536 - val_loss: 2603.0923\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2490.2068 - val_loss: 2601.1282\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2489.0601 - val_loss: 2600.1445\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.5200 - val_loss: 2599.7305\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.3606 - val_loss: 2599.5054\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2505 - val_loss: 2599.2144\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2466 - val_loss: 2599.2915\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2664 - val_loss: 2599.4480\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2646 - val_loss: 2599.1990\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2400 - val_loss: 2599.2053\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.3110 - val_loss: 2599.1785\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2830 - val_loss: 2599.3020\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.3364 - val_loss: 2599.2805\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.2915 - val_loss: 2599.1379\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.2454 - val_loss: 2599.0176\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.2227 - val_loss: 2599.0840\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2488.4880 - val_loss: 2598.8694\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2491.0024 - val_loss: 2599.7822\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.3862 - val_loss: 2599.3765\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2489.9521 - val_loss: 2599.8359\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2486.3652 - val_loss: 2599.2031\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2491.6440 - val_loss: 2619.1292\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.8127 - val_loss: 2600.9272\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.5569 - val_loss: 2599.3210\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2488.5261 - val_loss: 2600.6851\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2488.0339 - val_loss: 2588.6450\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2465.0393 - val_loss: 2554.8203\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2390.4429 - val_loss: 2411.2959\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2248.3337 - val_loss: 2290.7292\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2136.0454 - val_loss: 2218.2085\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2035.9420 - val_loss: 2116.3186\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1953.8661 - val_loss: 2016.0454\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1858.5103 - val_loss: 1915.8729\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1753.8448 - val_loss: 1803.1145\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1658.3525 - val_loss: 1712.5543\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1566.8727 - val_loss: 1615.8623\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1481.9795 - val_loss: 1522.4136\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1388.9050 - val_loss: 1468.9829\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1312.2290 - val_loss: 1364.1083\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1234.5560 - val_loss: 1279.4294\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1155.8682 - val_loss: 1205.1637\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1082.0957 - val_loss: 1125.7943\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1011.4316 - val_loss: 1055.3350\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 944.6609 - val_loss: 988.5199\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 887.6558 - val_loss: 932.6953\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 823.2433 - val_loss: 866.2888\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 769.7807 - val_loss: 816.7636\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 713.6086 - val_loss: 759.0087\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 663.4366 - val_loss: 696.9633\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 610.5288 - val_loss: 644.2189\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 562.7375 - val_loss: 597.0074\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 522.5559 - val_loss: 549.9813\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 475.7707 - val_loss: 504.6150\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 435.2793 - val_loss: 467.8021\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 400.7770 - val_loss: 425.4458\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 378.1140 - val_loss: 421.4537\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 337.4479 - val_loss: 356.7804\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 301.1717 - val_loss: 323.2806\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 273.9716 - val_loss: 293.8051\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 246.8624 - val_loss: 266.3525\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 226.9522 - val_loss: 242.1746\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.7154 - val_loss: 223.6677\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 184.8603 - val_loss: 194.6493\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 161.8731 - val_loss: 175.6072\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 146.1753 - val_loss: 173.1346\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 158.0806 - val_loss: 142.3543\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 116.1025 - val_loss: 124.2129\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 102.2049 - val_loss: 110.5014\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.9717 - val_loss: 97.2572\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 82.0044 - val_loss: 85.5939\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.0492 - val_loss: 82.2491\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.3374 - val_loss: 69.7594\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.1016 - val_loss: 58.1124\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.8354 - val_loss: 57.5073\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 47.1297 - val_loss: 48.3361\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.4924 - val_loss: 38.8455\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.8806 - val_loss: 36.2067\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.1048 - val_loss: 29.1843\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.0956 - val_loss: 28.6149\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.3445 - val_loss: 23.6204\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.8895 - val_loss: 21.1094\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.2004 - val_loss: 18.8717\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.7440 - val_loss: 13.4501\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6088 - val_loss: 13.5630\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.9875 - val_loss: 15.9505\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2011 - val_loss: 8.9429\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6005 - val_loss: 11.3175\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2892 - val_loss: 6.6931\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5770 - val_loss: 13.3787\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7163 - val_loss: 5.7547\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8712 - val_loss: 10.6531\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8466 - val_loss: 4.2543\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0061 - val_loss: 3.1099\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3359 - val_loss: 2.5672\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.7498 - val_loss: 4.2874\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.3701 - val_loss: 2.3433\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.1811 - val_loss: 1.7238\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6372 - val_loss: 3.2214\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.1337 - val_loss: 3.0002\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4047 - val_loss: 1.5861\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1484 - val_loss: 3.9561\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8166 - val_loss: 1.6344\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8620 - val_loss: 4.5811\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0147 - val_loss: 1.3304\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8721 - val_loss: 1.7866\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4873 - val_loss: 1.2483\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2156 - val_loss: 4.6346\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4992 - val_loss: 1.9089\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7939 - val_loss: 1.3754\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9752 - val_loss: 2.3716\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5984 - val_loss: 1.0498\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7969 - val_loss: 1.7094\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7596 - val_loss: 19.3920\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5578 - val_loss: 1.0808\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1774 - val_loss: 1.5420\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2617 - val_loss: 0.6121\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7042 - val_loss: 3.0627\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0498 - val_loss: 1.5733\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2658 - val_loss: 0.6903\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3859 - val_loss: 3.5791\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2771 - val_loss: 4.4271\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9372 - val_loss: 0.9359\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7735 - val_loss: 0.6415\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6488 - val_loss: 3.4193\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9176 - val_loss: 1.1922\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4011 - val_loss: 2.0542\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2697 - val_loss: 0.7627\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6923 - val_loss: 1.4864\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9513 - val_loss: 3.6672\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0207 - val_loss: 0.4292\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4537 - val_loss: 0.4258\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5930 - val_loss: 0.7665\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6099 - val_loss: 0.9018\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5754 - val_loss: 0.7284\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3109 - val_loss: 1.9454\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5006 - val_loss: 0.5939\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6011 - val_loss: 1.1049\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0030 - val_loss: 1.9439\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6029 - val_loss: 1.4610\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2403 - val_loss: 2.5658\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5877 - val_loss: 0.3738\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6479 - val_loss: 1.7388\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9220 - val_loss: 5.0938\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8849 - val_loss: 0.2948\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2332 - val_loss: 1.0251\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2102 - val_loss: 1.2534\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5764 - val_loss: 0.5470\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1116 - val_loss: 1.5802\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8283 - val_loss: 0.7788\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5151 - val_loss: 0.5942\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5464 - val_loss: 1.5654\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7065 - val_loss: 0.3695\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3691 - val_loss: 0.2396\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5073 - val_loss: 0.3244\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4077 - val_loss: 0.2884\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1265 - val_loss: 3.7493\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1994 - val_loss: 0.4904\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5062 - val_loss: 0.7735\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2644 - val_loss: 36.9329\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5890 - val_loss: 0.9382\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4529 - val_loss: 0.4115\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2894 - val_loss: 0.3722\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2619 - val_loss: 0.2055\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2378 - val_loss: 0.1784\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5327 - val_loss: 0.7867\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3925 - val_loss: 1.8432\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6297 - val_loss: 0.2178\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8123 - val_loss: 1.4975\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2235 - val_loss: 0.3461\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6334 - val_loss: 0.5032\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8422 - val_loss: 0.6236\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4426 - val_loss: 0.4627\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.6220 - val_loss: 1.2133\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 17.6907 - val_loss: 39.0413\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.4100 - val_loss: 0.5943\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4284 - val_loss: 0.2918\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.2858 - val_loss: 0.2467\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2434 - val_loss: 0.3269\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5581 - val_loss: 0.8915\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.5636 - val_loss: 0.4593\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3928 - val_loss: 0.2440\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6572 - val_loss: 0.9479\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9192 - val_loss: 0.7039\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.7038725443953423\n",
      "Mean Absolute Error (MAE): 0.5480723092631423\n",
      "Root Mean Squared Error (RMSE): 0.8389711225038334\n",
      "Time taken: 1301.819459438324\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 18s 37ms/step - loss: 3999.0371 - val_loss: 3876.6021\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3673.6262 - val_loss: 3658.9019\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3473.2576 - val_loss: 3475.8408\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3303.6221 - val_loss: 3320.0232\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3158.6023 - val_loss: 3185.6174\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3034.1082 - val_loss: 3069.7222\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2927.4084 - val_loss: 2970.5867\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2836.3479 - val_loss: 2885.6660\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2759.2820 - val_loss: 2814.1072\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2694.9871 - val_loss: 2754.3928\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2642.3191 - val_loss: 2705.5254\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2599.8818 - val_loss: 2666.5227\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2566.8098 - val_loss: 2636.3748\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2541.9768 - val_loss: 2613.6487\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2523.9446 - val_loss: 2597.5747\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2511.7114 - val_loss: 2586.6294\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2503.8174 - val_loss: 2579.6470\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2499.2158 - val_loss: 2575.4585\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2496.8462 - val_loss: 2573.4910\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.7463 - val_loss: 2572.4053\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.2458 - val_loss: 2571.7820\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.0444 - val_loss: 2571.7632\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9680 - val_loss: 2571.5002\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9321 - val_loss: 2571.4712\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.0071 - val_loss: 2571.4753\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.0437 - val_loss: 2571.5806\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9519 - val_loss: 2571.7639\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9839 - val_loss: 2571.7678\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9197 - val_loss: 2571.7576\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9983 - val_loss: 2571.7229\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2494.9607 - val_loss: 2571.6460\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.1775 - val_loss: 2575.4585\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2496.2361 - val_loss: 2572.9277\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2495.3086 - val_loss: 2571.8623\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2496.6177 - val_loss: 2572.1289\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2489.9277 - val_loss: 2548.1855\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2463.5906 - val_loss: 2522.1619\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2433.5762 - val_loss: 2469.0544\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2308.8604 - val_loss: 2306.5093\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2200.3872 - val_loss: 2217.6194\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2118.5457 - val_loss: 2140.3188\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2046.3563 - val_loss: 2066.8259\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1969.0907 - val_loss: 1987.3689\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1891.3672 - val_loss: 1904.1117\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1810.4016 - val_loss: 1826.0011\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1728.3479 - val_loss: 1743.4172\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1647.7275 - val_loss: 1656.4214\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1570.3499 - val_loss: 1577.4702\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1494.7983 - val_loss: 1504.0294\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1420.6555 - val_loss: 1431.1722\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1346.1567 - val_loss: 1350.2979\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1275.6931 - val_loss: 1285.4960\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1209.9578 - val_loss: 1214.2419\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1143.6594 - val_loss: 1150.0310\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1081.4587 - val_loss: 1087.6401\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1019.2073 - val_loss: 1023.5159\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 962.1033 - val_loss: 969.0012\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 908.7857 - val_loss: 913.3122\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 855.0245 - val_loss: 859.3104\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 801.4207 - val_loss: 801.8702\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 743.9365 - val_loss: 745.3372\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 692.0037 - val_loss: 697.8546\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 644.0953 - val_loss: 642.8281\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 596.3123 - val_loss: 607.4329\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 550.4286 - val_loss: 552.5041\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 517.4484 - val_loss: 513.7350\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 474.5472 - val_loss: 477.0570\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 439.9129 - val_loss: 447.6430\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 408.5062 - val_loss: 408.5188\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 377.4084 - val_loss: 387.0131\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 349.0380 - val_loss: 347.2184\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 316.6485 - val_loss: 334.9984\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 292.1686 - val_loss: 300.5782\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 266.2508 - val_loss: 265.4510\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 242.1304 - val_loss: 253.7223\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 34ms/step - loss: 222.4187 - val_loss: 223.8812\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 201.2138 - val_loss: 202.7217\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 182.1941 - val_loss: 185.4723\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 170.9253 - val_loss: 167.3690\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 149.7002 - val_loss: 150.5103\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 134.3523 - val_loss: 135.5634\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 127.6438 - val_loss: 128.0146\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 110.7023 - val_loss: 110.5376\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 99.2757 - val_loss: 99.8713\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 90.8221 - val_loss: 90.3870\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 82.1211 - val_loss: 85.8346\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 73.8657 - val_loss: 72.0736\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 66.6021 - val_loss: 66.8742\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 62.4949 - val_loss: 59.7404\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 51.0490 - val_loss: 50.3573\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 45.6212 - val_loss: 47.0992\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 41.6852 - val_loss: 40.9268\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 37.9798 - val_loss: 38.4268\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 36.5149 - val_loss: 31.6550\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 28.1335 - val_loss: 30.7463\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 27.0362 - val_loss: 25.8558\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 24.5589 - val_loss: 23.3694\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 20.2098 - val_loss: 19.9611\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 18.4087 - val_loss: 18.2261\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 23.0064 - val_loss: 21.0207\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 15.3548 - val_loss: 13.2545\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 12.2918 - val_loss: 12.0137\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 10.8830 - val_loss: 9.9859\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 10.0125 - val_loss: 9.2656\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 9.0348 - val_loss: 8.3289\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 7.9718 - val_loss: 7.1368\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 9.2443 - val_loss: 7.1040\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 9.5491 - val_loss: 6.0062\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.0213 - val_loss: 4.6178\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.2811 - val_loss: 4.4139\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.2135 - val_loss: 4.8402\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 15s 37ms/step - loss: 4.8733 - val_loss: 6.2597\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.8173 - val_loss: 2.9063\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.9140 - val_loss: 3.1795\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 8.6907 - val_loss: 5.9223\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.7732 - val_loss: 1.7896\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0929 - val_loss: 1.5086\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1938 - val_loss: 1.4217\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7071 - val_loss: 2.0125\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.3510 - val_loss: 1.8987\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.3935 - val_loss: 4.9216\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.0782 - val_loss: 1.1382\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7604 - val_loss: 1.6882\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0175 - val_loss: 2.1865\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.9587 - val_loss: 1.0496\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.3492 - val_loss: 1.0160\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8413 - val_loss: 0.5822\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7002 - val_loss: 0.6350\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.3198 - val_loss: 0.9749\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5013 - val_loss: 0.6101\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9579 - val_loss: 1.3654\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1167 - val_loss: 0.9414\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.3990 - val_loss: 1.5080\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9073 - val_loss: 0.6140\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3613 - val_loss: 0.8300\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7620 - val_loss: 0.4402\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.5887 - val_loss: 0.5708\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.8330 - val_loss: 0.7597\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.2306 - val_loss: 0.7116\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3711 - val_loss: 3.6001\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.3400 - val_loss: 0.4852\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3344 - val_loss: 0.3094\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9200 - val_loss: 0.6719\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0853 - val_loss: 0.3025\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7823 - val_loss: 1.0184\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.2537 - val_loss: 0.7718\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3932 - val_loss: 0.3484\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8282 - val_loss: 1.7957\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8354 - val_loss: 2.6657\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0141 - val_loss: 0.4734\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4698 - val_loss: 0.4501\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8212 - val_loss: 2.5972\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0209 - val_loss: 0.6937\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4602 - val_loss: 0.2212\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.2332 - val_loss: 0.4379\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3501 - val_loss: 0.2107\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.2907 - val_loss: 0.2911\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7697 - val_loss: 1.3309\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9124 - val_loss: 0.6155\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6451 - val_loss: 2.1702\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3050 - val_loss: 0.3506\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3079 - val_loss: 0.3721\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.4322 - val_loss: 0.4881\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7239 - val_loss: 1.4610\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.8697 - val_loss: 0.3625\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4343 - val_loss: 0.1506\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4439 - val_loss: 1.6674\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 6.7135 - val_loss: 0.4893\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4895 - val_loss: 0.2434\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3530 - val_loss: 0.1438\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3113 - val_loss: 0.8622\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6772 - val_loss: 1.2035\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1184 - val_loss: 0.5558\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6953 - val_loss: 0.1918\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3614 - val_loss: 0.7036\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.6613 - val_loss: 0.9343\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3142 - val_loss: 0.4063\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6764 - val_loss: 2.4936\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9517 - val_loss: 0.3164\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9888 - val_loss: 0.3115\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4091 - val_loss: 0.4591\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0614 - val_loss: 4.0308\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6638 - val_loss: 0.1882\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1391 - val_loss: 0.1944\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3970 - val_loss: 1.4657\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6009 - val_loss: 0.4756\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1699 - val_loss: 1.8397\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6465 - val_loss: 0.5125\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7994 - val_loss: 0.7449\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.5928 - val_loss: 2.0853\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.5053 - val_loss: 0.1935\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1263 - val_loss: 0.0757\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1570 - val_loss: 0.7407\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6577 - val_loss: 0.4662\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8654 - val_loss: 0.4813\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.2216 - val_loss: 0.1273\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1479 - val_loss: 0.6413\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3767 - val_loss: 0.1969\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1876 - val_loss: 2.9105\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0336 - val_loss: 0.1213\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.12132214211517119\n",
      "Mean Absolute Error (MAE): 0.2564759094247772\n",
      "Root Mean Squared Error (RMSE): 0.3483132815658501\n",
      "Time taken: 2693.9297664165497\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4085.9404 - val_loss: 3865.5767\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3770.1362 - val_loss: 3630.0635\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3562.2566 - val_loss: 3431.2405\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3387.4668 - val_loss: 3264.0137\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3237.7952 - val_loss: 3119.9617\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3108.6130 - val_loss: 2995.6926\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2997.2649 - val_loss: 2889.1221\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2901.6631 - val_loss: 2797.8018\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2820.2217 - val_loss: 2720.6858\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2751.4529 - val_loss: 2656.0205\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2694.4990 - val_loss: 2603.0000\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2648.0742 - val_loss: 2559.9004\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2611.1458 - val_loss: 2526.1921\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2582.7493 - val_loss: 2500.8892\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2561.8357 - val_loss: 2482.6824\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2547.1287 - val_loss: 2470.1165\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2537.3638 - val_loss: 2461.7156\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2531.3738 - val_loss: 2456.9563\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2528.0652 - val_loss: 2454.2021\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2526.3943 - val_loss: 2452.8157\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.5957 - val_loss: 2452.1875\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.3337 - val_loss: 2451.7888\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.1912 - val_loss: 2451.5083\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.1882 - val_loss: 2451.4390\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1221 - val_loss: 2451.3130\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1921 - val_loss: 2451.1304\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1531 - val_loss: 2451.1243\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1960 - val_loss: 2451.2917\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1443 - val_loss: 2451.2192\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1248 - val_loss: 2451.1997\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1826 - val_loss: 2451.2034\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2525.1772 - val_loss: 2451.2854\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2517.7449 - val_loss: 2409.9541\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2424.7644 - val_loss: 2344.0017\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2366.0569 - val_loss: 2276.8975\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2279.8811 - val_loss: 2177.0371\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2195.2700 - val_loss: 2098.9595\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2107.9941 - val_loss: 2014.2809\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2024.8595 - val_loss: 1942.0448\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1952.1052 - val_loss: 1859.2360\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1875.5730 - val_loss: 1782.7783\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1800.0944 - val_loss: 1712.0035\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1724.0127 - val_loss: 1637.4418\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1646.2583 - val_loss: 1558.0483\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1571.5269 - val_loss: 1495.6000\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1502.7240 - val_loss: 1419.6473\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1434.8813 - val_loss: 1357.7887\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1367.6566 - val_loss: 1286.2566\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1304.0022 - val_loss: 1222.7543\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1231.8298 - val_loss: 1152.1851\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1176.0016 - val_loss: 1092.3550\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1124.1144 - val_loss: 1035.2397\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1048.7006 - val_loss: 976.8827\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 989.8889 - val_loss: 917.7048\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 928.3156 - val_loss: 855.6045\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 870.4192 - val_loss: 805.5315\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 815.5486 - val_loss: 748.3155\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 767.9624 - val_loss: 711.7094\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 726.0074 - val_loss: 660.9031\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 670.8826 - val_loss: 609.3304\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 626.2026 - val_loss: 567.3948\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 584.7838 - val_loss: 537.3755\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 546.0901 - val_loss: 490.9741\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 506.8940 - val_loss: 457.1181\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 470.6564 - val_loss: 422.3196\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 439.0484 - val_loss: 386.1436\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 402.3748 - val_loss: 353.6135\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 366.4257 - val_loss: 318.6414\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 335.9428 - val_loss: 291.7683\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 303.7545 - val_loss: 259.6209\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 275.1541 - val_loss: 234.7431\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 252.4335 - val_loss: 214.7416\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 230.4311 - val_loss: 195.7792\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 213.5207 - val_loss: 176.0293\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 191.4018 - val_loss: 173.4120\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 176.4979 - val_loss: 144.9494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 155.2455 - val_loss: 127.2680\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 141.5119 - val_loss: 112.1455\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 125.2997 - val_loss: 102.1252\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 113.9632 - val_loss: 90.8133\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 121.7412 - val_loss: 82.5427\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 91.5518 - val_loss: 71.9564\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 81.1958 - val_loss: 62.6660\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 73.5208 - val_loss: 58.6815\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 65.7349 - val_loss: 52.9328\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 58.1157 - val_loss: 43.3469\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 52.7304 - val_loss: 45.6304\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.3351 - val_loss: 34.0271\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.3768 - val_loss: 28.8548\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.0181 - val_loss: 28.0755\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.6043 - val_loss: 24.4777\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.9123 - val_loss: 19.0177\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.3370 - val_loss: 19.1526\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.7589 - val_loss: 16.7934\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.9632 - val_loss: 12.3847\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.3939 - val_loss: 14.5147\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.5038 - val_loss: 10.3813\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.5397 - val_loss: 13.7629\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.7459 - val_loss: 8.4226\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5030 - val_loss: 7.3142\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1674 - val_loss: 20.1937\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7193 - val_loss: 6.2525\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3489 - val_loss: 4.7218\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5728 - val_loss: 3.7148\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9387 - val_loss: 4.4103\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0178 - val_loss: 2.8546\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7815 - val_loss: 2.6098\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1073 - val_loss: 6.2625\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7064 - val_loss: 2.6126\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6034 - val_loss: 2.0156\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6206 - val_loss: 3.9288\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7912 - val_loss: 1.3916\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0333 - val_loss: 2.3997\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2226 - val_loss: 1.0369\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6070 - val_loss: 3.5039\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5658 - val_loss: 3.5002\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5666 - val_loss: 2.5307\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6704 - val_loss: 1.9567\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6280 - val_loss: 1.7771\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7110 - val_loss: 1.5345\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6950 - val_loss: 3.9367\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7736 - val_loss: 0.9432\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2506 - val_loss: 1.4824\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5591 - val_loss: 1.3074\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6814 - val_loss: 1.2429\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3870 - val_loss: 0.8767\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9260 - val_loss: 1.0511\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7976 - val_loss: 1.0147\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5974 - val_loss: 1.1913\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2710 - val_loss: 0.5398\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4038 - val_loss: 10.9833\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7510 - val_loss: 0.6966\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4261 - val_loss: 0.5954\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0026 - val_loss: 0.4877\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5689 - val_loss: 0.4761\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1128 - val_loss: 1.1984\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8062 - val_loss: 0.7026\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8424 - val_loss: 1.4654\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0198 - val_loss: 0.5198\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1404 - val_loss: 6.6794\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4844 - val_loss: 0.2751\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2652 - val_loss: 0.3113\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5848 - val_loss: 1.2132\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6175 - val_loss: 1.2998\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5377 - val_loss: 0.4317\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.0049 - val_loss: 6.8701\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.1432 - val_loss: 0.4441\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3608 - val_loss: 0.4040\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3154 - val_loss: 0.3388\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2948 - val_loss: 0.2110\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3763 - val_loss: 0.7960\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0339 - val_loss: 0.4155\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4323 - val_loss: 0.3543\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0763 - val_loss: 0.7563\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4544 - val_loss: 0.3367\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8075 - val_loss: 0.6331\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7094 - val_loss: 0.6173\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1482 - val_loss: 0.2106\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3507 - val_loss: 0.1993\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9473 - val_loss: 0.6887\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5909 - val_loss: 0.3193\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5903 - val_loss: 1.2462\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6747 - val_loss: 0.2687\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2144 - val_loss: 0.1229\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2414 - val_loss: 0.2109\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6909 - val_loss: 0.5224\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9563 - val_loss: 4.4564\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0466 - val_loss: 0.1651\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1585 - val_loss: 0.1330\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1358 - val_loss: 0.1837\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7442 - val_loss: 2.0269\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8975 - val_loss: 0.3636\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2966 - val_loss: 0.3034\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2365 - val_loss: 0.1913\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2541 - val_loss: 0.5659\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8020 - val_loss: 0.2801\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4003 - val_loss: 0.5383\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3621 - val_loss: 0.4805\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8450 - val_loss: 0.2457\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4109 - val_loss: 1.1131\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2193 - val_loss: 0.4005\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4106 - val_loss: 0.2625\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3471 - val_loss: 0.4513\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8164 - val_loss: 0.4576\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9379 - val_loss: 4.9429\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2203 - val_loss: 0.1488\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2041 - val_loss: 0.9472\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4873 - val_loss: 0.1666\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3870 - val_loss: 0.4062\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9051 - val_loss: 0.4480\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2709 - val_loss: 0.3546\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5773 - val_loss: 1.4460\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7974 - val_loss: 3.3411\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6594 - val_loss: 0.3152\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2531 - val_loss: 0.8459\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9153 - val_loss: 2.5492\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4620 - val_loss: 0.1004\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1910 - val_loss: 0.1509\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1940 - val_loss: 0.1637\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3870 - val_loss: 0.6195\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.6193725190232632\n",
      "Mean Absolute Error (MAE): 0.583664031963845\n",
      "Root Mean Squared Error (RMSE): 0.7870022357168137\n",
      "Time taken: 1235.701430797577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_20984\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.805564  0.679810  0.897532  1228.782513\n",
      "1        2  0.882336  0.715226  0.939328  1221.612078\n",
      "2        3  0.703873  0.548072  0.838971  1301.819459\n",
      "3        4  0.121322  0.256476  0.348313  2693.929766\n",
      "4        5  0.619373  0.583664  0.787002  1235.701431\n",
      "5  Average  0.626493  0.556650  0.762229  1536.369050\n",
      "Results saved to 'DL_Result_PL_model_1_smoothing2_Reg2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_Reg2.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_smoothing2_Reg2.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
