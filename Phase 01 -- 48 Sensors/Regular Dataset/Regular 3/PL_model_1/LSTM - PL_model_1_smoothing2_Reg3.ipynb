{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217.774599</td>\n",
       "      <td>242.930668</td>\n",
       "      <td>180.930066</td>\n",
       "      <td>209.165430</td>\n",
       "      <td>263.740760</td>\n",
       "      <td>282.804639</td>\n",
       "      <td>234.185665</td>\n",
       "      <td>251.670782</td>\n",
       "      <td>225.920056</td>\n",
       "      <td>236.535232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.275029</td>\n",
       "      <td>198.288288</td>\n",
       "      <td>226.977928</td>\n",
       "      <td>188.625021</td>\n",
       "      <td>269.387212</td>\n",
       "      <td>240.345304</td>\n",
       "      <td>199.864666</td>\n",
       "      <td>222.641881</td>\n",
       "      <td>152.935187</td>\n",
       "      <td>181.772174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.170207</td>\n",
       "      <td>243.162352</td>\n",
       "      <td>181.188283</td>\n",
       "      <td>209.229425</td>\n",
       "      <td>263.645224</td>\n",
       "      <td>282.566208</td>\n",
       "      <td>233.849222</td>\n",
       "      <td>251.399776</td>\n",
       "      <td>226.070043</td>\n",
       "      <td>236.605439</td>\n",
       "      <td>...</td>\n",
       "      <td>206.327307</td>\n",
       "      <td>198.312659</td>\n",
       "      <td>227.204584</td>\n",
       "      <td>188.755642</td>\n",
       "      <td>269.186951</td>\n",
       "      <td>239.957409</td>\n",
       "      <td>199.826290</td>\n",
       "      <td>222.569317</td>\n",
       "      <td>152.659342</td>\n",
       "      <td>181.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.566640</td>\n",
       "      <td>243.391827</td>\n",
       "      <td>181.448150</td>\n",
       "      <td>209.293887</td>\n",
       "      <td>263.546981</td>\n",
       "      <td>282.329663</td>\n",
       "      <td>233.513263</td>\n",
       "      <td>251.127383</td>\n",
       "      <td>226.217667</td>\n",
       "      <td>236.678232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.381960</td>\n",
       "      <td>198.338475</td>\n",
       "      <td>227.430371</td>\n",
       "      <td>188.887730</td>\n",
       "      <td>268.984770</td>\n",
       "      <td>239.570115</td>\n",
       "      <td>199.788536</td>\n",
       "      <td>222.498237</td>\n",
       "      <td>152.383094</td>\n",
       "      <td>181.396746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.963998</td>\n",
       "      <td>243.619173</td>\n",
       "      <td>181.709370</td>\n",
       "      <td>209.359166</td>\n",
       "      <td>263.446095</td>\n",
       "      <td>282.095104</td>\n",
       "      <td>233.177442</td>\n",
       "      <td>250.853437</td>\n",
       "      <td>226.363208</td>\n",
       "      <td>236.753423</td>\n",
       "      <td>...</td>\n",
       "      <td>206.438990</td>\n",
       "      <td>198.365888</td>\n",
       "      <td>227.655468</td>\n",
       "      <td>189.021122</td>\n",
       "      <td>268.780495</td>\n",
       "      <td>239.183495</td>\n",
       "      <td>199.751706</td>\n",
       "      <td>222.428767</td>\n",
       "      <td>152.106454</td>\n",
       "      <td>181.212740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.362447</td>\n",
       "      <td>243.844582</td>\n",
       "      <td>181.971595</td>\n",
       "      <td>209.425648</td>\n",
       "      <td>263.342829</td>\n",
       "      <td>281.862540</td>\n",
       "      <td>232.841279</td>\n",
       "      <td>250.577746</td>\n",
       "      <td>226.507030</td>\n",
       "      <td>236.830717</td>\n",
       "      <td>...</td>\n",
       "      <td>206.498603</td>\n",
       "      <td>198.394939</td>\n",
       "      <td>227.880017</td>\n",
       "      <td>189.155630</td>\n",
       "      <td>268.573997</td>\n",
       "      <td>238.797611</td>\n",
       "      <td>199.715905</td>\n",
       "      <td>222.361175</td>\n",
       "      <td>151.829327</td>\n",
       "      <td>181.031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.842192</td>\n",
       "      <td>225.493872</td>\n",
       "      <td>186.161293</td>\n",
       "      <td>168.746854</td>\n",
       "      <td>290.125966</td>\n",
       "      <td>278.359253</td>\n",
       "      <td>251.284803</td>\n",
       "      <td>238.057134</td>\n",
       "      <td>234.084081</td>\n",
       "      <td>227.072033</td>\n",
       "      <td>...</td>\n",
       "      <td>208.063014</td>\n",
       "      <td>218.150333</td>\n",
       "      <td>227.500042</td>\n",
       "      <td>171.740167</td>\n",
       "      <td>279.291011</td>\n",
       "      <td>241.091078</td>\n",
       "      <td>226.577232</td>\n",
       "      <td>211.259677</td>\n",
       "      <td>167.182356</td>\n",
       "      <td>144.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.838339</td>\n",
       "      <td>225.193644</td>\n",
       "      <td>186.158236</td>\n",
       "      <td>168.801166</td>\n",
       "      <td>290.099684</td>\n",
       "      <td>278.240476</td>\n",
       "      <td>251.416575</td>\n",
       "      <td>238.186267</td>\n",
       "      <td>233.905906</td>\n",
       "      <td>227.057967</td>\n",
       "      <td>...</td>\n",
       "      <td>207.847027</td>\n",
       "      <td>218.029800</td>\n",
       "      <td>227.372573</td>\n",
       "      <td>171.813916</td>\n",
       "      <td>279.132031</td>\n",
       "      <td>241.322583</td>\n",
       "      <td>226.512762</td>\n",
       "      <td>211.122695</td>\n",
       "      <td>167.361938</td>\n",
       "      <td>144.194156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.835359</td>\n",
       "      <td>224.890461</td>\n",
       "      <td>186.152840</td>\n",
       "      <td>168.856330</td>\n",
       "      <td>290.072086</td>\n",
       "      <td>278.120520</td>\n",
       "      <td>251.546598</td>\n",
       "      <td>238.314937</td>\n",
       "      <td>233.725835</td>\n",
       "      <td>227.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>207.630922</td>\n",
       "      <td>217.908241</td>\n",
       "      <td>227.248109</td>\n",
       "      <td>171.889770</td>\n",
       "      <td>278.973606</td>\n",
       "      <td>241.555246</td>\n",
       "      <td>226.448731</td>\n",
       "      <td>210.986402</td>\n",
       "      <td>167.540383</td>\n",
       "      <td>144.375144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.833573</td>\n",
       "      <td>224.584238</td>\n",
       "      <td>186.145056</td>\n",
       "      <td>168.912353</td>\n",
       "      <td>290.043592</td>\n",
       "      <td>277.999301</td>\n",
       "      <td>251.674989</td>\n",
       "      <td>238.443174</td>\n",
       "      <td>233.543693</td>\n",
       "      <td>227.032526</td>\n",
       "      <td>...</td>\n",
       "      <td>207.414766</td>\n",
       "      <td>217.785837</td>\n",
       "      <td>227.126460</td>\n",
       "      <td>171.968173</td>\n",
       "      <td>278.815774</td>\n",
       "      <td>241.789214</td>\n",
       "      <td>226.385133</td>\n",
       "      <td>210.850667</td>\n",
       "      <td>167.717528</td>\n",
       "      <td>144.556286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.833291</td>\n",
       "      <td>224.274990</td>\n",
       "      <td>186.134915</td>\n",
       "      <td>168.969321</td>\n",
       "      <td>290.014623</td>\n",
       "      <td>277.876785</td>\n",
       "      <td>251.801863</td>\n",
       "      <td>238.570923</td>\n",
       "      <td>233.359245</td>\n",
       "      <td>227.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>207.198785</td>\n",
       "      <td>217.662767</td>\n",
       "      <td>227.007411</td>\n",
       "      <td>172.049382</td>\n",
       "      <td>278.658692</td>\n",
       "      <td>242.024537</td>\n",
       "      <td>226.321951</td>\n",
       "      <td>210.715291</td>\n",
       "      <td>167.893351</td>\n",
       "      <td>144.737845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     217.774599  242.930668  180.930066  209.165430  263.740760  282.804639   \n",
       "1     218.170207  243.162352  181.188283  209.229425  263.645224  282.566208   \n",
       "2     218.566640  243.391827  181.448150  209.293887  263.546981  282.329663   \n",
       "3     218.963998  243.619173  181.709370  209.359166  263.446095  282.095104   \n",
       "4     219.362447  243.844582  181.971595  209.425648  263.342829  281.862540   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.842192  225.493872  186.161293  168.746854  290.125966  278.359253   \n",
       "2439  240.838339  225.193644  186.158236  168.801166  290.099684  278.240476   \n",
       "2440  240.835359  224.890461  186.152840  168.856330  290.072086  278.120520   \n",
       "2441  240.833573  224.584238  186.145056  168.912353  290.043592  277.999301   \n",
       "2442  240.833291  224.274990  186.134915  168.969321  290.014623  277.876785   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     234.185665  251.670782  225.920056  236.535232  ...  206.275029   \n",
       "1     233.849222  251.399776  226.070043  236.605439  ...  206.327307   \n",
       "2     233.513263  251.127383  226.217667  236.678232  ...  206.381960   \n",
       "3     233.177442  250.853437  226.363208  236.753423  ...  206.438990   \n",
       "4     232.841279  250.577746  226.507030  236.830717  ...  206.498603   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  251.284803  238.057134  234.084081  227.072033  ...  208.063014   \n",
       "2439  251.416575  238.186267  233.905906  227.057967  ...  207.847027   \n",
       "2440  251.546598  238.314937  233.725835  227.044989  ...  207.630922   \n",
       "2441  251.674989  238.443174  233.543693  227.032526  ...  207.414766   \n",
       "2442  251.801863  238.570923  233.359245  227.020120  ...  207.198785   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     198.288288  226.977928  188.625021  269.387212  240.345304  199.864666   \n",
       "1     198.312659  227.204584  188.755642  269.186951  239.957409  199.826290   \n",
       "2     198.338475  227.430371  188.887730  268.984770  239.570115  199.788536   \n",
       "3     198.365888  227.655468  189.021122  268.780495  239.183495  199.751706   \n",
       "4     198.394939  227.880017  189.155630  268.573997  238.797611  199.715905   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  218.150333  227.500042  171.740167  279.291011  241.091078  226.577232   \n",
       "2439  218.029800  227.372573  171.813916  279.132031  241.322583  226.512762   \n",
       "2440  217.908241  227.248109  171.889770  278.973606  241.555246  226.448731   \n",
       "2441  217.785837  227.126460  171.968173  278.815774  241.789214  226.385133   \n",
       "2442  217.662767  227.007411  172.049382  278.658692  242.024537  226.321951   \n",
       "\n",
       "              45          46          47  \n",
       "0     222.641881  152.935187  181.772174  \n",
       "1     222.569317  152.659342  181.583242  \n",
       "2     222.498237  152.383094  181.396746  \n",
       "3     222.428767  152.106454  181.212740  \n",
       "4     222.361175  151.829327  181.031371  \n",
       "...          ...         ...         ...  \n",
       "2438  211.259677  167.182356  144.012906  \n",
       "2439  211.122695  167.361938  144.194156  \n",
       "2440  210.986402  167.540383  144.375144  \n",
       "2441  210.850667  167.717528  144.556286  \n",
       "2442  210.715291  167.893351  144.737845  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217.774599</td>\n",
       "      <td>242.930668</td>\n",
       "      <td>180.930066</td>\n",
       "      <td>209.165430</td>\n",
       "      <td>263.740760</td>\n",
       "      <td>282.804639</td>\n",
       "      <td>234.185665</td>\n",
       "      <td>251.670782</td>\n",
       "      <td>225.920056</td>\n",
       "      <td>236.535232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.275029</td>\n",
       "      <td>198.288288</td>\n",
       "      <td>226.977928</td>\n",
       "      <td>188.625021</td>\n",
       "      <td>269.387212</td>\n",
       "      <td>240.345304</td>\n",
       "      <td>199.864666</td>\n",
       "      <td>222.641881</td>\n",
       "      <td>152.935187</td>\n",
       "      <td>181.772174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.170207</td>\n",
       "      <td>243.162352</td>\n",
       "      <td>181.188283</td>\n",
       "      <td>209.229425</td>\n",
       "      <td>263.645224</td>\n",
       "      <td>282.566208</td>\n",
       "      <td>233.849222</td>\n",
       "      <td>251.399776</td>\n",
       "      <td>226.070043</td>\n",
       "      <td>236.605439</td>\n",
       "      <td>...</td>\n",
       "      <td>206.327307</td>\n",
       "      <td>198.312659</td>\n",
       "      <td>227.204584</td>\n",
       "      <td>188.755642</td>\n",
       "      <td>269.186951</td>\n",
       "      <td>239.957409</td>\n",
       "      <td>199.826290</td>\n",
       "      <td>222.569317</td>\n",
       "      <td>152.659342</td>\n",
       "      <td>181.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.566640</td>\n",
       "      <td>243.391827</td>\n",
       "      <td>181.448150</td>\n",
       "      <td>209.293887</td>\n",
       "      <td>263.546981</td>\n",
       "      <td>282.329663</td>\n",
       "      <td>233.513263</td>\n",
       "      <td>251.127383</td>\n",
       "      <td>226.217667</td>\n",
       "      <td>236.678232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.381960</td>\n",
       "      <td>198.338475</td>\n",
       "      <td>227.430371</td>\n",
       "      <td>188.887730</td>\n",
       "      <td>268.984770</td>\n",
       "      <td>239.570115</td>\n",
       "      <td>199.788536</td>\n",
       "      <td>222.498237</td>\n",
       "      <td>152.383094</td>\n",
       "      <td>181.396746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.963998</td>\n",
       "      <td>243.619173</td>\n",
       "      <td>181.709370</td>\n",
       "      <td>209.359166</td>\n",
       "      <td>263.446095</td>\n",
       "      <td>282.095104</td>\n",
       "      <td>233.177442</td>\n",
       "      <td>250.853437</td>\n",
       "      <td>226.363208</td>\n",
       "      <td>236.753423</td>\n",
       "      <td>...</td>\n",
       "      <td>206.438990</td>\n",
       "      <td>198.365888</td>\n",
       "      <td>227.655468</td>\n",
       "      <td>189.021122</td>\n",
       "      <td>268.780495</td>\n",
       "      <td>239.183495</td>\n",
       "      <td>199.751706</td>\n",
       "      <td>222.428767</td>\n",
       "      <td>152.106454</td>\n",
       "      <td>181.212740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.362447</td>\n",
       "      <td>243.844582</td>\n",
       "      <td>181.971595</td>\n",
       "      <td>209.425648</td>\n",
       "      <td>263.342829</td>\n",
       "      <td>281.862540</td>\n",
       "      <td>232.841279</td>\n",
       "      <td>250.577746</td>\n",
       "      <td>226.507030</td>\n",
       "      <td>236.830717</td>\n",
       "      <td>...</td>\n",
       "      <td>206.498603</td>\n",
       "      <td>198.394939</td>\n",
       "      <td>227.880017</td>\n",
       "      <td>189.155630</td>\n",
       "      <td>268.573997</td>\n",
       "      <td>238.797611</td>\n",
       "      <td>199.715905</td>\n",
       "      <td>222.361175</td>\n",
       "      <td>151.829327</td>\n",
       "      <td>181.031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.842192</td>\n",
       "      <td>225.493872</td>\n",
       "      <td>186.161293</td>\n",
       "      <td>168.746854</td>\n",
       "      <td>290.125966</td>\n",
       "      <td>278.359253</td>\n",
       "      <td>251.284803</td>\n",
       "      <td>238.057134</td>\n",
       "      <td>234.084081</td>\n",
       "      <td>227.072033</td>\n",
       "      <td>...</td>\n",
       "      <td>208.063014</td>\n",
       "      <td>218.150333</td>\n",
       "      <td>227.500042</td>\n",
       "      <td>171.740167</td>\n",
       "      <td>279.291011</td>\n",
       "      <td>241.091078</td>\n",
       "      <td>226.577232</td>\n",
       "      <td>211.259677</td>\n",
       "      <td>167.182356</td>\n",
       "      <td>144.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.838339</td>\n",
       "      <td>225.193644</td>\n",
       "      <td>186.158236</td>\n",
       "      <td>168.801166</td>\n",
       "      <td>290.099684</td>\n",
       "      <td>278.240476</td>\n",
       "      <td>251.416575</td>\n",
       "      <td>238.186267</td>\n",
       "      <td>233.905906</td>\n",
       "      <td>227.057967</td>\n",
       "      <td>...</td>\n",
       "      <td>207.847027</td>\n",
       "      <td>218.029800</td>\n",
       "      <td>227.372573</td>\n",
       "      <td>171.813916</td>\n",
       "      <td>279.132031</td>\n",
       "      <td>241.322583</td>\n",
       "      <td>226.512762</td>\n",
       "      <td>211.122695</td>\n",
       "      <td>167.361938</td>\n",
       "      <td>144.194156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.835359</td>\n",
       "      <td>224.890461</td>\n",
       "      <td>186.152840</td>\n",
       "      <td>168.856330</td>\n",
       "      <td>290.072086</td>\n",
       "      <td>278.120520</td>\n",
       "      <td>251.546598</td>\n",
       "      <td>238.314937</td>\n",
       "      <td>233.725835</td>\n",
       "      <td>227.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>207.630922</td>\n",
       "      <td>217.908241</td>\n",
       "      <td>227.248109</td>\n",
       "      <td>171.889770</td>\n",
       "      <td>278.973606</td>\n",
       "      <td>241.555246</td>\n",
       "      <td>226.448731</td>\n",
       "      <td>210.986402</td>\n",
       "      <td>167.540383</td>\n",
       "      <td>144.375144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.833573</td>\n",
       "      <td>224.584238</td>\n",
       "      <td>186.145056</td>\n",
       "      <td>168.912353</td>\n",
       "      <td>290.043592</td>\n",
       "      <td>277.999301</td>\n",
       "      <td>251.674989</td>\n",
       "      <td>238.443174</td>\n",
       "      <td>233.543693</td>\n",
       "      <td>227.032526</td>\n",
       "      <td>...</td>\n",
       "      <td>207.414766</td>\n",
       "      <td>217.785837</td>\n",
       "      <td>227.126460</td>\n",
       "      <td>171.968173</td>\n",
       "      <td>278.815774</td>\n",
       "      <td>241.789214</td>\n",
       "      <td>226.385133</td>\n",
       "      <td>210.850667</td>\n",
       "      <td>167.717528</td>\n",
       "      <td>144.556286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.833291</td>\n",
       "      <td>224.274990</td>\n",
       "      <td>186.134915</td>\n",
       "      <td>168.969321</td>\n",
       "      <td>290.014623</td>\n",
       "      <td>277.876785</td>\n",
       "      <td>251.801863</td>\n",
       "      <td>238.570923</td>\n",
       "      <td>233.359245</td>\n",
       "      <td>227.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>207.198785</td>\n",
       "      <td>217.662767</td>\n",
       "      <td>227.007411</td>\n",
       "      <td>172.049382</td>\n",
       "      <td>278.658692</td>\n",
       "      <td>242.024537</td>\n",
       "      <td>226.321951</td>\n",
       "      <td>210.715291</td>\n",
       "      <td>167.893351</td>\n",
       "      <td>144.737845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     217.774599  242.930668  180.930066  209.165430  263.740760  282.804639   \n",
       "1     218.170207  243.162352  181.188283  209.229425  263.645224  282.566208   \n",
       "2     218.566640  243.391827  181.448150  209.293887  263.546981  282.329663   \n",
       "3     218.963998  243.619173  181.709370  209.359166  263.446095  282.095104   \n",
       "4     219.362447  243.844582  181.971595  209.425648  263.342829  281.862540   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.842192  225.493872  186.161293  168.746854  290.125966  278.359253   \n",
       "2439  240.838339  225.193644  186.158236  168.801166  290.099684  278.240476   \n",
       "2440  240.835359  224.890461  186.152840  168.856330  290.072086  278.120520   \n",
       "2441  240.833573  224.584238  186.145056  168.912353  290.043592  277.999301   \n",
       "2442  240.833291  224.274990  186.134915  168.969321  290.014623  277.876785   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     234.185665  251.670782  225.920056  236.535232  ...  206.275029   \n",
       "1     233.849222  251.399776  226.070043  236.605439  ...  206.327307   \n",
       "2     233.513263  251.127383  226.217667  236.678232  ...  206.381960   \n",
       "3     233.177442  250.853437  226.363208  236.753423  ...  206.438990   \n",
       "4     232.841279  250.577746  226.507030  236.830717  ...  206.498603   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  251.284803  238.057134  234.084081  227.072033  ...  208.063014   \n",
       "2439  251.416575  238.186267  233.905906  227.057967  ...  207.847027   \n",
       "2440  251.546598  238.314937  233.725835  227.044989  ...  207.630922   \n",
       "2441  251.674989  238.443174  233.543693  227.032526  ...  207.414766   \n",
       "2442  251.801863  238.570923  233.359245  227.020120  ...  207.198785   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     198.288288  226.977928  188.625021  269.387212  240.345304  199.864666   \n",
       "1     198.312659  227.204584  188.755642  269.186951  239.957409  199.826290   \n",
       "2     198.338475  227.430371  188.887730  268.984770  239.570115  199.788536   \n",
       "3     198.365888  227.655468  189.021122  268.780495  239.183495  199.751706   \n",
       "4     198.394939  227.880017  189.155630  268.573997  238.797611  199.715905   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  218.150333  227.500042  171.740167  279.291011  241.091078  226.577232   \n",
       "2439  218.029800  227.372573  171.813916  279.132031  241.322583  226.512762   \n",
       "2440  217.908241  227.248109  171.889770  278.973606  241.555246  226.448731   \n",
       "2441  217.785837  227.126460  171.968173  278.815774  241.789214  226.385133   \n",
       "2442  217.662767  227.007411  172.049382  278.658692  242.024537  226.321951   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     222.641881  152.935187  181.772174  \n",
       "1     222.569317  152.659342  181.583242  \n",
       "2     222.498237  152.383094  181.396746  \n",
       "3     222.428767  152.106454  181.212740  \n",
       "4     222.361175  151.829327  181.031371  \n",
       "...          ...         ...         ...  \n",
       "2438  211.259677  167.182356  144.012906  \n",
       "2439  211.122695  167.361938  144.194156  \n",
       "2440  210.986402  167.540383  144.375144  \n",
       "2441  210.850667  167.717528  144.556286  \n",
       "2442  210.715291  167.893351  144.737845  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 50ms/step - loss: 3985.7051 - val_loss: 3856.2542\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3667.2778 - val_loss: 3628.4602\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3470.5769 - val_loss: 3442.4453\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3304.5813 - val_loss: 3283.6348\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3161.8384 - val_loss: 3147.1619\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3038.7834 - val_loss: 3030.1233\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2932.9792 - val_loss: 2929.6736\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2842.6775 - val_loss: 2845.3008\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2766.3145 - val_loss: 2774.1177\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2702.5483 - val_loss: 2715.2517\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2649.9751 - val_loss: 2667.5107\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2607.8179 - val_loss: 2629.8789\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2574.7778 - val_loss: 2601.0999\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2549.8613 - val_loss: 2579.5664\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2531.8557 - val_loss: 2564.7847\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2519.5959 - val_loss: 2555.0959\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2511.8428 - val_loss: 2549.2161\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2507.2397 - val_loss: 2545.9382\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2504.7893 - val_loss: 2544.3997\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.6023 - val_loss: 2543.6394\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.0837 - val_loss: 2543.4392\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9319 - val_loss: 2543.0503\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8442 - val_loss: 2543.0337\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9243 - val_loss: 2543.3210\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8259 - val_loss: 2543.1536\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8875 - val_loss: 2543.3442\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8533 - val_loss: 2543.1128\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8384 - val_loss: 2543.1228\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8384 - val_loss: 2542.8254\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8435 - val_loss: 2543.0193\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9360 - val_loss: 2542.8894\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8757 - val_loss: 2543.0183\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8435 - val_loss: 2543.2068\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.6006 - val_loss: 2543.3264\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.3281 - val_loss: 2542.8315\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.0227 - val_loss: 2542.8367\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9744 - val_loss: 2542.6445\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9419 - val_loss: 2543.0076\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8948 - val_loss: 2542.7498\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8833 - val_loss: 2542.8230\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8040 - val_loss: 2542.9663\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.7983 - val_loss: 2542.8472\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.0920 - val_loss: 2542.9717\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9709 - val_loss: 2542.9172\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9258 - val_loss: 2542.7373\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.9404 - val_loss: 2542.6833\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2502.8582 - val_loss: 2542.7366\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.5327 - val_loss: 2544.3796\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.5483 - val_loss: 2543.9385\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2504.3638 - val_loss: 2544.8960\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2503.1262 - val_loss: 2544.5400\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2503.0554 - val_loss: 2547.4446\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2501.4812 - val_loss: 2539.6138\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2440.5327 - val_loss: 2423.7026\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2348.4326 - val_loss: 2364.1353\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2281.6074 - val_loss: 2282.6594\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2206.1714 - val_loss: 2204.8970\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2142.9456 - val_loss: 2144.0654\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2175.8921 - val_loss: 2088.9617\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2022.5978 - val_loss: 2021.2089\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1960.9252 - val_loss: 1954.2426\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1907.4497 - val_loss: 1896.7081\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1845.1891 - val_loss: 1846.0767\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1795.1389 - val_loss: 1785.9946\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1742.7689 - val_loss: 1731.9148\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1699.5414 - val_loss: 1686.5110\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1654.0762 - val_loss: 1640.6696\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1612.1696 - val_loss: 1600.4789\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1569.7922 - val_loss: 1554.8347\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1529.8436 - val_loss: 1515.4055\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1500.3622 - val_loss: 1477.4567\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1455.3169 - val_loss: 1436.9310\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1425.7573 - val_loss: 1401.8187\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1388.3591 - val_loss: 1370.6830\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 1354.7982 - val_loss: 1333.7576\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1323.5505 - val_loss: 1307.2235\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1301.0463 - val_loss: 1271.8544\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1265.1178 - val_loss: 1243.8733\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1236.2408 - val_loss: 1240.1700\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1209.8674 - val_loss: 1176.7988\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1174.2126 - val_loss: 1144.7286\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1141.8510 - val_loss: 1113.2340\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1118.2687 - val_loss: 1081.4363\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1088.3647 - val_loss: 1053.3966\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1050.9475 - val_loss: 1010.9410\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1007.7598 - val_loss: 969.4295\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 972.7300 - val_loss: 940.1626\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 947.4918 - val_loss: 912.5354\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 921.2137 - val_loss: 887.1180\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 893.3988 - val_loss: 863.3021\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 858.7795 - val_loss: 823.6350\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 828.8036 - val_loss: 798.9089\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 792.0414 - val_loss: 766.2828\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 777.8550 - val_loss: 793.0542\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 735.1597 - val_loss: 697.4719\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 691.4668 - val_loss: 650.2446\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 657.0926 - val_loss: 614.3162\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 618.3119 - val_loss: 576.2974\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 582.4405 - val_loss: 541.1533\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 545.6010 - val_loss: 510.6584\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 517.4521 - val_loss: 482.0910\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 487.6308 - val_loss: 449.9768\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 464.5142 - val_loss: 420.9100\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 426.0476 - val_loss: 401.4393\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 402.7208 - val_loss: 384.9543\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 375.4821 - val_loss: 343.3038\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 349.0008 - val_loss: 320.1262\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 331.2920 - val_loss: 314.4986\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 302.1992 - val_loss: 274.3982\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 281.1779 - val_loss: 254.6174\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 257.9152 - val_loss: 239.1561\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 240.7945 - val_loss: 221.3272\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 254.6293 - val_loss: 203.9274\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 204.6997 - val_loss: 183.8668\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 187.6987 - val_loss: 173.3295\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 174.6978 - val_loss: 159.5338\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 160.3338 - val_loss: 140.1531\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 151.3671 - val_loss: 129.0535\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 133.2023 - val_loss: 119.2880\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 132.9256 - val_loss: 107.6484\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 111.6814 - val_loss: 96.9420\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 102.0298 - val_loss: 88.7402\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 92.6453 - val_loss: 82.6056\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 83.7152 - val_loss: 71.6363\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 76.5163 - val_loss: 68.4606\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 72.5291 - val_loss: 57.4758\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 60.0409 - val_loss: 51.2262\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 52.7626 - val_loss: 46.6841\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 48.9637 - val_loss: 45.0684\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 55.3740 - val_loss: 35.5162\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 38.4878 - val_loss: 33.7134\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 43.1282 - val_loss: 54.7819\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 34.6067 - val_loss: 26.4798\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 26.9321 - val_loss: 23.6733\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 24.1917 - val_loss: 23.1396\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 23.6497 - val_loss: 19.7263\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 19.4297 - val_loss: 17.5089\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 18.4647 - val_loss: 15.5893\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 15.0654 - val_loss: 15.3972\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 14.0475 - val_loss: 10.8247\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 11.6560 - val_loss: 10.3002\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 11.5777 - val_loss: 15.3502\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 10.0447 - val_loss: 7.7182\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 8.2739 - val_loss: 8.9763\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 19.5598 - val_loss: 13.1641\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 6.5117 - val_loss: 4.7938\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 5.4219 - val_loss: 4.7194\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.9189 - val_loss: 4.5311\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.8134 - val_loss: 3.2724\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 19.7993 - val_loss: 6.7845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.8013 - val_loss: 3.0501\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.5466 - val_loss: 3.4675\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.7413 - val_loss: 1.9918\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.7506 - val_loss: 1.8489\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.5575 - val_loss: 1.8120\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.5840 - val_loss: 2.9260\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.8678 - val_loss: 1.4446\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.5579 - val_loss: 1.9537\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.1083 - val_loss: 2.7350\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.6572 - val_loss: 8.0366\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.6901 - val_loss: 2.0550\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6873 - val_loss: 0.9617\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.5965 - val_loss: 2.2876\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.5568 - val_loss: 0.9316\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.5428 - val_loss: 1.0718\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1381 - val_loss: 1.0578\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.3765 - val_loss: 1.3431\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2651 - val_loss: 1.3167\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2767 - val_loss: 0.6946\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.1707 - val_loss: 23.1937\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.0573 - val_loss: 0.5042\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9853 - val_loss: 0.7832\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.8747 - val_loss: 0.5741\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9856 - val_loss: 1.1807\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2629 - val_loss: 1.5182\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2968 - val_loss: 1.0207\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6878 - val_loss: 0.9303\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 18.7275 - val_loss: 1.5413\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9212 - val_loss: 0.5476\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5502 - val_loss: 0.3621\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5596 - val_loss: 0.6504\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6560 - val_loss: 0.7904\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8510 - val_loss: 1.0386\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0028 - val_loss: 4.4837\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.5531 - val_loss: 1.1852\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7248 - val_loss: 0.4170\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1271 - val_loss: 1.1337\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.3389 - val_loss: 1.2897\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1319 - val_loss: 1.1575\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.1037 - val_loss: 0.3466\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4569 - val_loss: 0.2648\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7210 - val_loss: 0.8175\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0204 - val_loss: 1.5244\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.7576 - val_loss: 3.9701\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8660 - val_loss: 0.2804\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8367 - val_loss: 1.0963\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1622 - val_loss: 0.6207\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7951 - val_loss: 0.6355\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 10.0760 - val_loss: 0.5174\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4264 - val_loss: 0.2662\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.26616143255634006\n",
      "Mean Absolute Error (MAE): 0.38227479066358644\n",
      "Root Mean Squared Error (RMSE): 0.515908356742106\n",
      "Time taken: 3764.3896555900574\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 17s 33ms/step - loss: 4112.7563 - val_loss: 3825.2808\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3787.6665 - val_loss: 3597.1443\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 3582.3337 - val_loss: 3407.9868\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3408.4004 - val_loss: 3245.0425\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3258.2009 - val_loss: 3103.8833\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3127.9626 - val_loss: 2981.6904\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3015.1194 - val_loss: 2876.1438\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2917.8835 - val_loss: 2785.5317\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2834.8835 - val_loss: 2708.3896\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2764.5491 - val_loss: 2643.7739\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2705.9570 - val_loss: 2590.0698\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2658.1011 - val_loss: 2546.8704\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2619.8560 - val_loss: 2512.8438\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2590.3638 - val_loss: 2486.9436\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2568.3103 - val_loss: 2468.0481\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2552.6990 - val_loss: 2455.0066\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2542.2786 - val_loss: 2446.4409\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2535.7078 - val_loss: 2441.4927\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2532.0020 - val_loss: 2438.7266\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2530.0881 - val_loss: 2437.3245\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2529.2720 - val_loss: 2436.6897\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.9121 - val_loss: 2436.3694\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7695 - val_loss: 2436.2493\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6970 - val_loss: 2436.2327\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7188 - val_loss: 2436.1838\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6628 - val_loss: 2436.2937\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6672 - val_loss: 2436.1963\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6575 - val_loss: 2436.1995\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7444 - val_loss: 2436.2083\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7539 - val_loss: 2436.1453\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2528.6853 - val_loss: 2436.1550\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2528.6262 - val_loss: 2436.0752\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7239 - val_loss: 2436.1436\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6555 - val_loss: 2436.0984\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6694 - val_loss: 2436.1619\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6650 - val_loss: 2436.0974\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6416 - val_loss: 2436.0110\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7698 - val_loss: 2436.0225\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6768 - val_loss: 2436.0579\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6748 - val_loss: 2436.0586\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7266 - val_loss: 2436.0332\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6401 - val_loss: 2436.0652\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7122 - val_loss: 2436.0288\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6694 - val_loss: 2436.0984\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6758 - val_loss: 2436.1326\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7444 - val_loss: 2436.0767\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6868 - val_loss: 2436.1042\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6970 - val_loss: 2436.0811\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.6870 - val_loss: 2436.0720\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7007 - val_loss: 2436.1086\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2529.3289 - val_loss: 2438.1194\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2529.2812 - val_loss: 2437.1863\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.9915 - val_loss: 2436.8840\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2528.7510 - val_loss: 2436.4873\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2526.5535 - val_loss: 2427.2268\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2489.7231 - val_loss: 2372.1655\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2435.1929 - val_loss: 2334.2949\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2383.3416 - val_loss: 2278.5005\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2339.8525 - val_loss: 2221.3145\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2277.8872 - val_loss: 2161.8735\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2214.6626 - val_loss: 2106.9700\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2147.1492 - val_loss: 2036.0443\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2077.2537 - val_loss: 1975.6821\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2016.4309 - val_loss: 1905.1307\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1954.8091 - val_loss: 1851.1526\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1901.2968 - val_loss: 1797.5499\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1844.9060 - val_loss: 1746.1914\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1793.8104 - val_loss: 1688.4803\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1725.8191 - val_loss: 1630.7291\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1667.4835 - val_loss: 1563.4750\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1599.2654 - val_loss: 1505.9868\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1536.5188 - val_loss: 1436.7369\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1475.4446 - val_loss: 1396.4248\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1416.3652 - val_loss: 1325.2632\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 31ms/step - loss: 1358.3660 - val_loss: 1253.0314\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1286.7234 - val_loss: 1193.9202\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1226.6716 - val_loss: 1142.1395\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1167.1849 - val_loss: 1078.3035\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1106.9220 - val_loss: 1025.5128\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1055.2749 - val_loss: 970.5418\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 999.3281 - val_loss: 924.8400\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 950.5555 - val_loss: 871.1335\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 896.8330 - val_loss: 818.7518\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 834.4739 - val_loss: 758.2335\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 786.2685 - val_loss: 744.9617\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 744.3935 - val_loss: 669.7313\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 695.3885 - val_loss: 629.2661\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 651.8985 - val_loss: 587.2183\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 618.3552 - val_loss: 545.4622\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 569.8515 - val_loss: 508.5799\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 535.1297 - val_loss: 474.2975\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 500.8532 - val_loss: 452.6797\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 466.9191 - val_loss: 407.8537\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 433.1229 - val_loss: 380.6835\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 402.6462 - val_loss: 350.4188\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 375.3553 - val_loss: 327.2638\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 348.6539 - val_loss: 299.0587\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 321.2501 - val_loss: 274.2611\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 293.4773 - val_loss: 251.3640\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 271.3794 - val_loss: 231.2798\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 253.7862 - val_loss: 214.7190\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 229.4124 - val_loss: 191.3570\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 214.0705 - val_loss: 180.3437\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 193.1742 - val_loss: 157.1352\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 173.0819 - val_loss: 147.7017\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 156.1745 - val_loss: 128.5938\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 144.2829 - val_loss: 116.0313\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 131.5561 - val_loss: 111.2981\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 117.1215 - val_loss: 95.4209\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 108.8272 - val_loss: 88.3176\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 97.4421 - val_loss: 88.3678\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 87.5592 - val_loss: 69.5098\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 80.9721 - val_loss: 67.4975\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 72.2722 - val_loss: 57.7103\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 64.8167 - val_loss: 52.7025\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 66.7193 - val_loss: 60.3583\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 53.0213 - val_loss: 39.0939\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 45.8374 - val_loss: 34.3160\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 42.1278 - val_loss: 32.1220\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 45.2478 - val_loss: 29.8821\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 34.8744 - val_loss: 27.1610\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 30.9393 - val_loss: 22.3238\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 26.9732 - val_loss: 21.8403\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 25.5738 - val_loss: 20.4806\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 21.9941 - val_loss: 15.2101\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 23.2061 - val_loss: 21.1981\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 43.9069 - val_loss: 16.1718\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 17.1370 - val_loss: 11.9835\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 14.4767 - val_loss: 11.2785\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 14.2446 - val_loss: 9.7549\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 13.0150 - val_loss: 10.8796\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 10.9308 - val_loss: 7.5532\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 11.5825 - val_loss: 8.8818\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 9.1155 - val_loss: 6.4909\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 6.9009 - val_loss: 6.2597\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 7.5158 - val_loss: 10.1614\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 7.3752 - val_loss: 4.0035\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 5.2952 - val_loss: 3.3781\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 5.9374 - val_loss: 6.8853\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 5.1727 - val_loss: 3.5061\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 4.9309 - val_loss: 6.7447\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 13.0820 - val_loss: 2.5042\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.7352 - val_loss: 2.1299\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.4758 - val_loss: 2.0019\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.3470 - val_loss: 1.7680\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.0297 - val_loss: 3.7627\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.0096 - val_loss: 2.2018\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.0560 - val_loss: 2.7695\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 3.6536 - val_loss: 3.9925\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.2105 - val_loss: 1.5810\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 31ms/step - loss: 3.3033 - val_loss: 4.4072\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.2732 - val_loss: 1.6804\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.6935 - val_loss: 2.8131\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 4.6591 - val_loss: 1.0126\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 1.1212 - val_loss: 0.7562\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.2763 - val_loss: 3.3522\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 1.8314 - val_loss: 2.6273\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.6628 - val_loss: 2.0747\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.7071 - val_loss: 1.5653\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4519 - val_loss: 5.9756\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 10s 25ms/step - loss: 3.7304 - val_loss: 1.7390\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.4183 - val_loss: 1.3164\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6418 - val_loss: 1.0583\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.7208 - val_loss: 1.0795\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3931 - val_loss: 1.8844\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.0040 - val_loss: 0.7563\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5265 - val_loss: 3.4554\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.8610 - val_loss: 1.4063\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.1530 - val_loss: 0.5757\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1512 - val_loss: 0.6969\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9078 - val_loss: 0.8852\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9735 - val_loss: 1.2745\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8299 - val_loss: 0.5137\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 5.1892 - val_loss: 0.7367\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.7974 - val_loss: 0.9573\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6534 - val_loss: 0.8730\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8710 - val_loss: 1.5426\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8757 - val_loss: 0.3838\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6711 - val_loss: 1.0456\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8698 - val_loss: 0.4720\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7082 - val_loss: 8.9386\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6435 - val_loss: 1.0859\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9018 - val_loss: 0.7896\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9177 - val_loss: 0.9115\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6382 - val_loss: 1.1719\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.2299 - val_loss: 1.1488\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6977 - val_loss: 1.2083\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8883 - val_loss: 2.2046\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5194 - val_loss: 1.3517\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1756 - val_loss: 1.5756\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8263 - val_loss: 1.2616\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0198 - val_loss: 1.1541\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4959 - val_loss: 0.7001\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5563 - val_loss: 0.3697\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6255 - val_loss: 0.7117\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7789 - val_loss: 0.6165\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7184 - val_loss: 24.9939\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7944 - val_loss: 1.3368\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3863 - val_loss: 0.4124\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2809 - val_loss: 0.4292\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.429224866676557\n",
      "Mean Absolute Error (MAE): 0.49138092705863806\n",
      "Root Mean Squared Error (RMSE): 0.6551525522170825\n",
      "Time taken: 2194.282128572464\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 4102.1245 - val_loss: 3903.8879\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3776.2278 - val_loss: 3680.0642\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3568.9028 - val_loss: 3493.4209\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3392.3757 - val_loss: 3330.0796\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3237.7407 - val_loss: 3189.5156\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3105.1748 - val_loss: 3068.7424\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2990.7407 - val_loss: 2964.8833\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2892.2861 - val_loss: 2876.2139\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2808.2124 - val_loss: 2801.2725\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2737.3574 - val_loss: 2738.6057\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2678.3594 - val_loss: 2687.2979\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2630.2202 - val_loss: 2646.0593\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2591.7795 - val_loss: 2613.9487\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2562.1150 - val_loss: 2590.1472\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2540.0688 - val_loss: 2572.9407\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2766 - val_loss: 2561.5251\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2513.8567 - val_loss: 2554.4558\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2507.4165 - val_loss: 2550.6155\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.7839 - val_loss: 2548.7971\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2501.9851 - val_loss: 2548.0779\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2501.1812 - val_loss: 2547.9272\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.7441 - val_loss: 2547.9956\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.7466 - val_loss: 2548.0481\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.6321 - val_loss: 2548.1216\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.7556 - val_loss: 2548.1853\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.5300 - val_loss: 2548.1538\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5808 - val_loss: 2548.1934\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.5420 - val_loss: 2547.7776\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2500.7168 - val_loss: 2547.8674\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.6350 - val_loss: 2547.9805\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5610 - val_loss: 2548.0740\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.6279 - val_loss: 2548.0713\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5989 - val_loss: 2548.0747\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5952 - val_loss: 2548.0740\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.6760 - val_loss: 2547.9846\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.7170 - val_loss: 2548.2178\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.9705 - val_loss: 2547.5305\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5146 - val_loss: 2548.1826\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.5164 - val_loss: 2548.1697\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.5703 - val_loss: 2547.9365\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2500.6968 - val_loss: 2548.1152\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2500.4531 - val_loss: 2547.6838\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2498.6177 - val_loss: 2548.2898\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.1936 - val_loss: 2548.0261\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2500.1814 - val_loss: 2542.5168\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2499.3501 - val_loss: 2548.3484\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2499.1582 - val_loss: 2548.8606\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2496.0801 - val_loss: 2538.3594\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2495.9099 - val_loss: 2547.8877\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2499.5410 - val_loss: 2539.5303\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2499.2520 - val_loss: 2545.7825\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2489.7942 - val_loss: 2519.8330\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2487.1257 - val_loss: 2550.3687\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2497.6328 - val_loss: 2518.1028\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2472.4688 - val_loss: 2511.2817\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2456.2678 - val_loss: 2497.4248\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2447.8208 - val_loss: 2492.4570\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2439.6765 - val_loss: 2479.0151\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2412.6633 - val_loss: 2417.0256\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2325.0496 - val_loss: 2328.8008\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2228.4053 - val_loss: 2219.3672\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2125.9685 - val_loss: 2130.4639\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2030.6470 - val_loss: 2040.0900\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1954.9161 - val_loss: 1945.9386\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1875.8859 - val_loss: 1872.4075\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1797.4825 - val_loss: 1804.0182\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1724.9105 - val_loss: 1716.8984\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1652.8899 - val_loss: 1648.0745\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1587.1147 - val_loss: 1575.7042\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1508.9768 - val_loss: 1507.6637\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1443.6863 - val_loss: 1431.9513\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1374.3868 - val_loss: 1361.3547\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1321.4391 - val_loss: 1328.9052\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1248.5939 - val_loss: 1234.7261\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1180.7268 - val_loss: 1174.7190\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 1123.4343 - val_loss: 1380.4879\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1102.4855 - val_loss: 1065.4373\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1013.2735 - val_loss: 1009.7786\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 961.5359 - val_loss: 958.5173\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 915.6752 - val_loss: 905.5106\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 864.1378 - val_loss: 874.3220\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 818.9653 - val_loss: 809.9176\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 774.5355 - val_loss: 765.5804\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 737.6028 - val_loss: 729.4637\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 690.0195 - val_loss: 682.1194\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 656.2280 - val_loss: 635.1572\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 602.7345 - val_loss: 596.6513\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 566.3698 - val_loss: 560.7640\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 532.7143 - val_loss: 527.7143\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 506.8853 - val_loss: 494.7224\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 459.4802 - val_loss: 454.8304\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 434.8573 - val_loss: 423.6210\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 398.2860 - val_loss: 396.5273\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 370.6634 - val_loss: 370.8871\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 344.7986 - val_loss: 341.3386\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 326.5973 - val_loss: 317.0162\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 289.2447 - val_loss: 291.2982\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 266.5233 - val_loss: 264.1153\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 253.4005 - val_loss: 251.7794\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 227.6077 - val_loss: 229.9269\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 209.3954 - val_loss: 207.7390\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 195.2666 - val_loss: 195.3044\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 180.8418 - val_loss: 173.7435\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 161.4921 - val_loss: 163.4527\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 154.5424 - val_loss: 157.4188\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 138.9480 - val_loss: 135.6303\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 122.8060 - val_loss: 121.1373\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 112.1641 - val_loss: 120.3969\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 118.3300 - val_loss: 157.0103\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.8639 - val_loss: 93.4472\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 85.5562 - val_loss: 89.2162\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 78.0280 - val_loss: 77.2945\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 69.9128 - val_loss: 68.8266\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 64.7178 - val_loss: 65.6384\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 59.4465 - val_loss: 60.8909\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 55.7513 - val_loss: 52.1669\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.2196 - val_loss: 47.2488\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 50.4331 - val_loss: 42.0699\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.0257 - val_loss: 37.3450\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.1208 - val_loss: 42.4431\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.6521 - val_loss: 29.8469\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.5480 - val_loss: 40.7220\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 39.5830 - val_loss: 24.4045\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.9691 - val_loss: 23.0374\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.5258 - val_loss: 27.1348\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 19.3752 - val_loss: 18.5597\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 15.7420 - val_loss: 15.5873\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.8141 - val_loss: 35.7657\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.5039 - val_loss: 12.8579\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1741 - val_loss: 11.1138\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 10.4951 - val_loss: 12.1805\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.8794 - val_loss: 10.0380\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 11.7770 - val_loss: 10.8073\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 7.6627 - val_loss: 7.8013\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1865 - val_loss: 7.4390\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.7065 - val_loss: 6.7730\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.5229 - val_loss: 19.5890\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9410 - val_loss: 5.1154\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.1502 - val_loss: 4.4704\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.0201 - val_loss: 4.8070\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3206 - val_loss: 4.5198\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9522 - val_loss: 3.6048\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2996 - val_loss: 3.3261\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3800 - val_loss: 5.7045\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.0313 - val_loss: 2.6041\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2823 - val_loss: 8.4725\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.9031 - val_loss: 2.7813\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.0335 - val_loss: 2.1624\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6786 - val_loss: 2.2615\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0079 - val_loss: 2.8038\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3945 - val_loss: 1.8560\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.1559 - val_loss: 2.1294\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2859 - val_loss: 4.1622\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 12.3278 - val_loss: 2.8847\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6772 - val_loss: 1.5042\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3301 - val_loss: 1.3958\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1052 - val_loss: 1.2880\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0630 - val_loss: 2.0082\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8050 - val_loss: 3.6526\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4061 - val_loss: 3.4622\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5019 - val_loss: 1.5577\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3805 - val_loss: 2.0612\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7022 - val_loss: 4.6766\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6261 - val_loss: 0.8427\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7698 - val_loss: 1.2784\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3058 - val_loss: 1.1889\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4376 - val_loss: 1.0492\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0241 - val_loss: 1.3889\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0926 - val_loss: 1.7494\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3473 - val_loss: 0.9221\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7169 - val_loss: 2.4137\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6625 - val_loss: 1.3857\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2443 - val_loss: 1.6670\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.8490 - val_loss: 0.6869\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4816 - val_loss: 0.4798\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5701 - val_loss: 1.0230\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4353 - val_loss: 2.3894\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6353 - val_loss: 1.2233\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.8039 - val_loss: 0.6567\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4917 - val_loss: 0.6306\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3903 - val_loss: 0.5416\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4066 - val_loss: 0.4105\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4574 - val_loss: 0.5530\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8926 - val_loss: 0.4665\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5620 - val_loss: 1.1999\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2276 - val_loss: 0.5204\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2843 - val_loss: 1.2047\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2446 - val_loss: 0.3936\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.9319 - val_loss: 0.7241\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2189 - val_loss: 10.7397\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.8134 - val_loss: 1.2882\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6255 - val_loss: 0.7786\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6264 - val_loss: 0.4713\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6442 - val_loss: 1.1499\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0573 - val_loss: 0.4033\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7970 - val_loss: 0.4897\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2975 - val_loss: 3.3019\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.0146 - val_loss: 0.5432\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6497 - val_loss: 0.4076\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4459 - val_loss: 2.2130\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 2.212963201302229\n",
      "Mean Absolute Error (MAE): 1.1648827659765246\n",
      "Root Mean Squared Error (RMSE): 1.4876031733302497\n",
      "Time taken: 1291.6837136745453\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4037.4031 - val_loss: 3726.2522\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3715.4375 - val_loss: 3509.8628\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3513.6228 - val_loss: 3331.2771\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3343.4912 - val_loss: 3178.7805\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3197.4319 - val_loss: 3047.3159\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3071.6841 - val_loss: 2934.0791\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2963.5227 - val_loss: 2837.0686\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2871.2749 - val_loss: 2754.6699\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2793.1970 - val_loss: 2685.2534\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2727.9297 - val_loss: 2627.7939\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2674.2686 - val_loss: 2580.9873\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2631.1123 - val_loss: 2543.7151\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2597.3789 - val_loss: 2515.3167\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2571.7766 - val_loss: 2494.4014\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2553.3284 - val_loss: 2479.7073\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2540.5569 - val_loss: 2469.9617\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2532.3911 - val_loss: 2464.0862\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2527.6104 - val_loss: 2461.0388\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.0496 - val_loss: 2459.5979\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.8435 - val_loss: 2459.0049\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.3967 - val_loss: 2458.8110\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0874 - val_loss: 2458.7488\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0549 - val_loss: 2458.9124\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2523.0374 - val_loss: 2459.0208\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9702 - val_loss: 2458.8918\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9756 - val_loss: 2458.9333\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9756 - val_loss: 2459.0820\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9592 - val_loss: 2459.0938\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2523.0000 - val_loss: 2459.1030\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9529 - val_loss: 2459.0178\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0022 - val_loss: 2458.9946\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0215 - val_loss: 2459.0593\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.1025 - val_loss: 2459.0266\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9902 - val_loss: 2459.0747\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0754 - val_loss: 2459.0664\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2522.9998 - val_loss: 2459.1921\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2522.9817 - val_loss: 2459.1804\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.9900 - val_loss: 2459.2117\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0532 - val_loss: 2458.8196\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0154 - val_loss: 2458.8489\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2522.9739 - val_loss: 2458.9658\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.0496 - val_loss: 2459.0225\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2522.5645 - val_loss: 2458.9526\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.2412 - val_loss: 2453.3967\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2519.0298 - val_loss: 2458.0615\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.3396 - val_loss: 2456.6287\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2496.4390 - val_loss: 2393.3347\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2413.3076 - val_loss: 2314.5581\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2362.5085 - val_loss: 2277.3179\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2327.2273 - val_loss: 2244.8748\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2301.3752 - val_loss: 2242.2617\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2266.2378 - val_loss: 2183.6663\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2223.4822 - val_loss: 2123.5122\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2160.4355 - val_loss: 2091.8572\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2107.2695 - val_loss: 2014.8549\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2050.8918 - val_loss: 1952.8615\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1975.4771 - val_loss: 1902.0688\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1914.9702 - val_loss: 1809.1666\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1840.7275 - val_loss: 1748.0938\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1773.2903 - val_loss: 1685.5573\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1711.9733 - val_loss: 1626.1425\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1652.9291 - val_loss: 1568.9105\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1588.0730 - val_loss: 1515.8878\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1524.9565 - val_loss: 1447.1401\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1468.0302 - val_loss: 1391.0737\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1414.1721 - val_loss: 1334.2657\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1351.4547 - val_loss: 1274.4418\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1282.3325 - val_loss: 1218.2031\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1224.9062 - val_loss: 1158.3267\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1167.3440 - val_loss: 1101.1449\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1107.9907 - val_loss: 1037.2700\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1042.3525 - val_loss: 978.2973\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 987.5017 - val_loss: 928.5790\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 926.3162 - val_loss: 873.4934\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 873.7111 - val_loss: 822.5659\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 817.8491 - val_loss: 776.5617\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 761.9604 - val_loss: 714.9350\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 716.0803 - val_loss: 672.2100\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 666.7491 - val_loss: 627.4989\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 625.0764 - val_loss: 592.3658\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 584.1023 - val_loss: 556.4503\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 535.4168 - val_loss: 500.8573\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 496.2133 - val_loss: 507.4221\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 459.8564 - val_loss: 421.5361\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 415.2002 - val_loss: 390.5048\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 382.9520 - val_loss: 361.6800\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 368.7617 - val_loss: 380.2296\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 329.7595 - val_loss: 303.1348\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 296.3662 - val_loss: 280.0725\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 271.7012 - val_loss: 256.6153\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 247.0935 - val_loss: 233.9509\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 249.0165 - val_loss: 213.1572\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.1198 - val_loss: 190.2080\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 181.7249 - val_loss: 169.6789\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 162.5411 - val_loss: 152.5762\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 152.0713 - val_loss: 151.3736\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 131.6671 - val_loss: 124.6929\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 117.3480 - val_loss: 111.0839\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 112.0898 - val_loss: 105.1030\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 95.9987 - val_loss: 87.9026\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 83.5656 - val_loss: 81.4956\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 85.9886 - val_loss: 79.6556\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 68.3132 - val_loss: 65.0901\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.8174 - val_loss: 57.9503\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.5552 - val_loss: 48.5058\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.9947 - val_loss: 42.8371\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.7765 - val_loss: 38.1884\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.2994 - val_loss: 36.5015\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.3685 - val_loss: 39.1656\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.7624 - val_loss: 26.2812\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.0292 - val_loss: 24.0261\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.0204 - val_loss: 20.4268\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.9594 - val_loss: 19.3207\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.2227 - val_loss: 17.4793\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.7060 - val_loss: 13.6735\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.1859 - val_loss: 12.5650\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2788 - val_loss: 45.9179\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.5233 - val_loss: 11.5440\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5783 - val_loss: 9.1632\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4413 - val_loss: 8.1914\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8917 - val_loss: 15.8372\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1144 - val_loss: 6.5047\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1654 - val_loss: 7.7468\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8379 - val_loss: 7.5527\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3502 - val_loss: 13.3834\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8588 - val_loss: 4.3016\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0116 - val_loss: 5.6238\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3487 - val_loss: 3.6438\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5262 - val_loss: 7.1144\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0276 - val_loss: 15.9428\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7614 - val_loss: 3.1499\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8910 - val_loss: 4.5250\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9738 - val_loss: 4.7007\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9763 - val_loss: 2.8495\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2737 - val_loss: 3.1374\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7496 - val_loss: 4.4502\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8330 - val_loss: 6.4339\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3894 - val_loss: 2.8883\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5966 - val_loss: 4.8562\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1533 - val_loss: 2.6925\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8096 - val_loss: 1.2552\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7001 - val_loss: 1.5599\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6286 - val_loss: 1.8964\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3066 - val_loss: 2.8544\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.7805 - val_loss: 4.0291\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4905 - val_loss: 3.1539\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6136 - val_loss: 2.3861\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5751 - val_loss: 3.3632\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2217 - val_loss: 1.3052\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4909 - val_loss: 1.2986\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4140 - val_loss: 2.4466\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.6280 - val_loss: 9.8432\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4150 - val_loss: 2.4766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0674 - val_loss: 1.4190\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7770 - val_loss: 2.0856\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4525 - val_loss: 2.1265\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7965 - val_loss: 1.8012\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1705 - val_loss: 1.9740\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8835 - val_loss: 1.1212\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2598 - val_loss: 1.3115\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0964 - val_loss: 2.7277\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9044 - val_loss: 11.9204\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.3042 - val_loss: 1.4567\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7222 - val_loss: 1.7356\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0916 - val_loss: 1.4681\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6393 - val_loss: 1.0718\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2663 - val_loss: 1.1768\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8837 - val_loss: 1.6143\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8435 - val_loss: 1.7855\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0022 - val_loss: 3.6664\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8386 - val_loss: 0.7439\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7524 - val_loss: 0.9263\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2431 - val_loss: 1.3828\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.8997 - val_loss: 0.7068\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.2005 - val_loss: 2.0770\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4939 - val_loss: 2.6959\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6683 - val_loss: 1.1952\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9293 - val_loss: 5.3532\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2366 - val_loss: 7.3419\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9714 - val_loss: 0.9706\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8878 - val_loss: 0.5387\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3656 - val_loss: 4.0794\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9361 - val_loss: 4.9841\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7312 - val_loss: 0.7035\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5951 - val_loss: 0.9643\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6958 - val_loss: 1.6926\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1591 - val_loss: 7.6605\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3900 - val_loss: 0.9292\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3595 - val_loss: 0.2512\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5287 - val_loss: 1.2704\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0970 - val_loss: 1.6060\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3999 - val_loss: 2.8171\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9018 - val_loss: 1.3805\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7761 - val_loss: 0.6088\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7995 - val_loss: 0.9657\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6785 - val_loss: 0.4236\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5409 - val_loss: 0.5912\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8169 - val_loss: 0.5303\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9916 - val_loss: 2.2332\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6860 - val_loss: 0.5977\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.5976451746163338\n",
      "Mean Absolute Error (MAE): 0.566452178842883\n",
      "Root Mean Squared Error (RMSE): 0.7730751416365254\n",
      "Time taken: 1205.7821190357208\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4035.6084 - val_loss: 3919.9534\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3707.7859 - val_loss: 3696.8406\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3502.5193 - val_loss: 3511.5049\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3328.9531 - val_loss: 3352.1704\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3180.3862 - val_loss: 3214.5957\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3052.6990 - val_loss: 3096.5054\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2943.0508 - val_loss: 2994.5166\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2849.4045 - val_loss: 2907.0723\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2770.1724 - val_loss: 2833.1211\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2703.8618 - val_loss: 2771.1904\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2649.4185 - val_loss: 2720.2551\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2605.4783 - val_loss: 2679.4138\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2571.0710 - val_loss: 2647.4692\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2544.9143 - val_loss: 2623.1448\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.7854 - val_loss: 2605.5688\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2512.6228 - val_loss: 2593.2795\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2504.1724 - val_loss: 2585.6008\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2499.1165 - val_loss: 2580.6914\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2496.3354 - val_loss: 2577.9041\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2495.0444 - val_loss: 2576.4875\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.4766 - val_loss: 2575.9685\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.2170 - val_loss: 2575.4775\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1753 - val_loss: 2575.2834\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1868 - val_loss: 2575.4536\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1230 - val_loss: 2575.4209\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1633 - val_loss: 2575.4304\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1458 - val_loss: 2575.2905\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.1558 - val_loss: 2575.2837\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1213 - val_loss: 2575.3428\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.2292 - val_loss: 2575.2229\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.0906 - val_loss: 2575.3428\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1296 - val_loss: 2575.4773\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2494.1433 - val_loss: 2575.3101\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.0293 - val_loss: 2576.0134\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2495.5654 - val_loss: 2575.7861\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.3640 - val_loss: 2575.1086\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.2402 - val_loss: 2574.9270\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1016 - val_loss: 2574.7395\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.7222 - val_loss: 2574.6184\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2474.5869 - val_loss: 2574.4458\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.5208 - val_loss: 2574.4001\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.2527 - val_loss: 2574.4087\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2494.2659 - val_loss: 2574.5898\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2495.0156 - val_loss: 2574.0957\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.6948 - val_loss: 2573.4236\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2494.3918 - val_loss: 2573.8037\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2486.9299 - val_loss: 2497.4524\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2386.7439 - val_loss: 2437.3467\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2337.8386 - val_loss: 2394.0850\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2297.3433 - val_loss: 2353.1394\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2258.6699 - val_loss: 2315.0801\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2214.2341 - val_loss: 2258.4692\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2152.7605 - val_loss: 2182.7720\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2085.1562 - val_loss: 2126.1809\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2019.2478 - val_loss: 2053.9460\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1952.2371 - val_loss: 2016.2783\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1887.7632 - val_loss: 1925.4841\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1827.7432 - val_loss: 1864.6113\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1768.2368 - val_loss: 1801.6178\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1706.5023 - val_loss: 1744.5518\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1652.7965 - val_loss: 1691.0730\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1596.7098 - val_loss: 1633.9835\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1541.9575 - val_loss: 1578.7057\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1490.9070 - val_loss: 1526.3702\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1438.4899 - val_loss: 1478.9106\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1385.2543 - val_loss: 1424.8041\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1329.6677 - val_loss: 1363.1632\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1277.2390 - val_loss: 1310.0841\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1225.9304 - val_loss: 1263.3046\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1177.5338 - val_loss: 1210.7426\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1129.3463 - val_loss: 1166.1360\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1081.5756 - val_loss: 1117.4745\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1036.0216 - val_loss: 1077.6088\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 990.5944 - val_loss: 1012.7203\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 937.8741 - val_loss: 970.6877\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 895.2115 - val_loss: 941.3367\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 853.8707 - val_loss: 885.1966\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 815.8315 - val_loss: 849.9543\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 775.1069 - val_loss: 803.6160\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 739.8804 - val_loss: 769.6501\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 705.4287 - val_loss: 732.1918\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 669.2601 - val_loss: 698.7909\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 637.4092 - val_loss: 663.6276\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 604.5984 - val_loss: 629.6959\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 575.0709 - val_loss: 599.9755\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 545.5265 - val_loss: 568.9021\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 516.6207 - val_loss: 543.7813\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 488.1244 - val_loss: 511.1768\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 461.6612 - val_loss: 484.7766\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 435.6796 - val_loss: 459.2587\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 414.5273 - val_loss: 432.2987\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 386.9828 - val_loss: 407.1069\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 370.0972 - val_loss: 386.0063\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 343.3730 - val_loss: 363.2520\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 323.3677 - val_loss: 341.2077\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 302.6567 - val_loss: 320.4637\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 285.4395 - val_loss: 302.1050\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 267.3107 - val_loss: 286.0191\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 254.7731 - val_loss: 266.2026\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 234.3472 - val_loss: 249.9451\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 220.0756 - val_loss: 234.2654\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 204.1248 - val_loss: 219.0190\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 191.4377 - val_loss: 207.4089\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 178.8844 - val_loss: 196.5523\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 166.4489 - val_loss: 179.8683\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 154.5158 - val_loss: 168.3387\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 145.6398 - val_loss: 156.1064\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 133.1119 - val_loss: 145.0878\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 124.1258 - val_loss: 135.9934\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 114.2719 - val_loss: 125.5233\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 108.8960 - val_loss: 125.4181\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.2273 - val_loss: 123.9150\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 93.4577 - val_loss: 103.2066\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 84.8556 - val_loss: 95.3134\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 80.3959 - val_loss: 87.7697\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.3082 - val_loss: 81.0085\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 67.7998 - val_loss: 76.2127\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.5957 - val_loss: 70.3011\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.1779 - val_loss: 64.1625\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 52.2917 - val_loss: 59.9221\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.6513 - val_loss: 56.4157\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.4884 - val_loss: 51.1069\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.8626 - val_loss: 49.5251\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 37.1593 - val_loss: 43.1290\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.2939 - val_loss: 40.5869\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.9165 - val_loss: 36.2665\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.9142 - val_loss: 38.2710\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.2280 - val_loss: 30.4323\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.9866 - val_loss: 28.5934\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.3693 - val_loss: 25.9305\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.7194 - val_loss: 24.9669\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.4042 - val_loss: 22.2880\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.9336 - val_loss: 20.1705\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.8946 - val_loss: 18.3047\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.3610 - val_loss: 16.7569\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6719 - val_loss: 17.3018\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.0338 - val_loss: 18.4735\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5039 - val_loss: 12.5126\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5249 - val_loss: 12.5708\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3116 - val_loss: 13.0293\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4892 - val_loss: 11.2411\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.8968 - val_loss: 9.1670\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5236 - val_loss: 8.0347\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.9170 - val_loss: 7.3737\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7889 - val_loss: 6.8222\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.8286 - val_loss: 6.5332\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1507 - val_loss: 8.3300\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0713 - val_loss: 5.9991\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4753 - val_loss: 11.9472\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4567 - val_loss: 6.3149\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8251 - val_loss: 4.1939\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4502 - val_loss: 3.7860\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2005 - val_loss: 4.1930\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2707 - val_loss: 5.0523\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5865 - val_loss: 3.7486\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8674 - val_loss: 3.8490\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1361 - val_loss: 3.0191\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3600 - val_loss: 3.1814\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2317 - val_loss: 3.3060\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2332 - val_loss: 2.5656\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7599 - val_loss: 3.2125\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2390 - val_loss: 3.3018\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3111 - val_loss: 6.4885\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5277 - val_loss: 17.4016\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1560 - val_loss: 2.0051\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6651 - val_loss: 1.7369\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7702 - val_loss: 3.2834\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1840 - val_loss: 2.2040\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9030 - val_loss: 2.4240\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3125 - val_loss: 1.4057\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2799 - val_loss: 1.2549\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7642 - val_loss: 6.4889\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8991 - val_loss: 1.1345\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0811 - val_loss: 1.3426\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.3520 - val_loss: 3.1352\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6429 - val_loss: 1.1779\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4543 - val_loss: 1.8042\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5289 - val_loss: 0.9813\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7918 - val_loss: 0.7833\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2551 - val_loss: 3.3994\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9378 - val_loss: 0.9706\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9443 - val_loss: 0.9335\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3061 - val_loss: 1.5489\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5485 - val_loss: 0.9400\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7692 - val_loss: 0.6425\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6120 - val_loss: 0.5927\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7185 - val_loss: 0.7261\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4342 - val_loss: 4.8259\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2017 - val_loss: 0.6442\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5699 - val_loss: 0.5346\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5872 - val_loss: 0.6418\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2927 - val_loss: 1.6806\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4521 - val_loss: 1.0922\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7578 - val_loss: 0.7927\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7513 - val_loss: 1.0745\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5491 - val_loss: 0.3984\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4959 - val_loss: 0.5180\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5178 - val_loss: 0.7170\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3221 - val_loss: 1.4077\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6433 - val_loss: 0.3777\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.37769114781547053\n",
      "Mean Absolute Error (MAE): 0.42205475033496564\n",
      "Root Mean Squared Error (RMSE): 0.6145658205721097\n",
      "Time taken: 1217.8984100818634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_8568\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.266161  0.382275  0.515908  3764.389656\n",
      "1        2  0.429225  0.491381  0.655153  2194.282129\n",
      "2        3  2.212963  1.164883  1.487603  1291.683714\n",
      "3        4  0.597645  0.566452  0.773075  1205.782119\n",
      "4        5  0.377691  0.422055  0.614566  1217.898410\n",
      "5  Average  0.776737  0.605409  0.809261  1934.807205\n",
      "Results saved to 'DL_Result_PL_model_1_smoothing2_Reg3.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_Reg3.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_smoothing2_Reg3.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
