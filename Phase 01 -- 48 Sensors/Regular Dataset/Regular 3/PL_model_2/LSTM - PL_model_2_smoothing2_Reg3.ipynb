{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_Reg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.362170</td>\n",
       "      <td>92.059179</td>\n",
       "      <td>81.639492</td>\n",
       "      <td>83.046279</td>\n",
       "      <td>87.557608</td>\n",
       "      <td>88.829065</td>\n",
       "      <td>86.304978</td>\n",
       "      <td>91.903081</td>\n",
       "      <td>90.974425</td>\n",
       "      <td>87.964761</td>\n",
       "      <td>...</td>\n",
       "      <td>80.716972</td>\n",
       "      <td>82.980276</td>\n",
       "      <td>91.631634</td>\n",
       "      <td>83.478056</td>\n",
       "      <td>87.786838</td>\n",
       "      <td>89.127069</td>\n",
       "      <td>80.956526</td>\n",
       "      <td>92.613983</td>\n",
       "      <td>77.410061</td>\n",
       "      <td>81.618024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.314653</td>\n",
       "      <td>91.868510</td>\n",
       "      <td>81.753510</td>\n",
       "      <td>83.102770</td>\n",
       "      <td>87.667622</td>\n",
       "      <td>89.014976</td>\n",
       "      <td>86.405655</td>\n",
       "      <td>91.761364</td>\n",
       "      <td>90.912489</td>\n",
       "      <td>88.054400</td>\n",
       "      <td>...</td>\n",
       "      <td>80.811359</td>\n",
       "      <td>82.990759</td>\n",
       "      <td>91.367214</td>\n",
       "      <td>83.430809</td>\n",
       "      <td>87.885892</td>\n",
       "      <td>88.975725</td>\n",
       "      <td>81.054012</td>\n",
       "      <td>92.393965</td>\n",
       "      <td>77.261457</td>\n",
       "      <td>81.568429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.265068</td>\n",
       "      <td>91.681240</td>\n",
       "      <td>81.863257</td>\n",
       "      <td>83.160659</td>\n",
       "      <td>87.776139</td>\n",
       "      <td>89.204368</td>\n",
       "      <td>86.502722</td>\n",
       "      <td>91.621445</td>\n",
       "      <td>90.850914</td>\n",
       "      <td>88.142340</td>\n",
       "      <td>...</td>\n",
       "      <td>80.909673</td>\n",
       "      <td>82.999959</td>\n",
       "      <td>91.107171</td>\n",
       "      <td>83.381318</td>\n",
       "      <td>87.981265</td>\n",
       "      <td>88.825590</td>\n",
       "      <td>81.151156</td>\n",
       "      <td>92.174215</td>\n",
       "      <td>77.116598</td>\n",
       "      <td>81.522525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.213900</td>\n",
       "      <td>91.497346</td>\n",
       "      <td>81.968488</td>\n",
       "      <td>83.220587</td>\n",
       "      <td>87.883528</td>\n",
       "      <td>89.397035</td>\n",
       "      <td>86.596130</td>\n",
       "      <td>91.482991</td>\n",
       "      <td>90.789162</td>\n",
       "      <td>88.228029</td>\n",
       "      <td>...</td>\n",
       "      <td>81.011722</td>\n",
       "      <td>83.008250</td>\n",
       "      <td>90.851560</td>\n",
       "      <td>83.329554</td>\n",
       "      <td>88.072869</td>\n",
       "      <td>88.677138</td>\n",
       "      <td>81.247568</td>\n",
       "      <td>91.955906</td>\n",
       "      <td>76.975967</td>\n",
       "      <td>81.480128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.161546</td>\n",
       "      <td>91.316921</td>\n",
       "      <td>82.068671</td>\n",
       "      <td>83.283040</td>\n",
       "      <td>87.990020</td>\n",
       "      <td>89.592487</td>\n",
       "      <td>86.686142</td>\n",
       "      <td>91.345397</td>\n",
       "      <td>90.726820</td>\n",
       "      <td>88.311044</td>\n",
       "      <td>...</td>\n",
       "      <td>81.117124</td>\n",
       "      <td>83.015760</td>\n",
       "      <td>90.600178</td>\n",
       "      <td>83.275815</td>\n",
       "      <td>88.160725</td>\n",
       "      <td>88.530710</td>\n",
       "      <td>81.342925</td>\n",
       "      <td>91.740266</td>\n",
       "      <td>76.839861</td>\n",
       "      <td>81.440740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>91.606026</td>\n",
       "      <td>88.069788</td>\n",
       "      <td>79.531699</td>\n",
       "      <td>85.419493</td>\n",
       "      <td>91.976891</td>\n",
       "      <td>92.243208</td>\n",
       "      <td>87.175784</td>\n",
       "      <td>88.590557</td>\n",
       "      <td>91.681913</td>\n",
       "      <td>90.119286</td>\n",
       "      <td>...</td>\n",
       "      <td>84.995612</td>\n",
       "      <td>87.574274</td>\n",
       "      <td>86.411497</td>\n",
       "      <td>79.268845</td>\n",
       "      <td>90.031115</td>\n",
       "      <td>86.998545</td>\n",
       "      <td>84.367165</td>\n",
       "      <td>86.580171</td>\n",
       "      <td>78.319799</td>\n",
       "      <td>79.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>91.746546</td>\n",
       "      <td>88.072322</td>\n",
       "      <td>79.441336</td>\n",
       "      <td>85.618475</td>\n",
       "      <td>91.938881</td>\n",
       "      <td>92.245196</td>\n",
       "      <td>87.057803</td>\n",
       "      <td>88.739174</td>\n",
       "      <td>91.794750</td>\n",
       "      <td>90.221004</td>\n",
       "      <td>...</td>\n",
       "      <td>85.070026</td>\n",
       "      <td>87.674484</td>\n",
       "      <td>86.261152</td>\n",
       "      <td>79.277755</td>\n",
       "      <td>89.975204</td>\n",
       "      <td>86.894393</td>\n",
       "      <td>84.210259</td>\n",
       "      <td>86.641084</td>\n",
       "      <td>78.327737</td>\n",
       "      <td>79.590486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>91.887422</td>\n",
       "      <td>88.071942</td>\n",
       "      <td>79.354928</td>\n",
       "      <td>85.820510</td>\n",
       "      <td>91.904210</td>\n",
       "      <td>92.245867</td>\n",
       "      <td>86.937141</td>\n",
       "      <td>88.888750</td>\n",
       "      <td>91.904207</td>\n",
       "      <td>90.319187</td>\n",
       "      <td>...</td>\n",
       "      <td>85.149368</td>\n",
       "      <td>87.775550</td>\n",
       "      <td>86.108531</td>\n",
       "      <td>79.287771</td>\n",
       "      <td>89.921318</td>\n",
       "      <td>86.790128</td>\n",
       "      <td>84.051367</td>\n",
       "      <td>86.698728</td>\n",
       "      <td>78.334857</td>\n",
       "      <td>79.743283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.028156</td>\n",
       "      <td>88.068195</td>\n",
       "      <td>79.272625</td>\n",
       "      <td>86.026023</td>\n",
       "      <td>91.872413</td>\n",
       "      <td>92.245207</td>\n",
       "      <td>86.814066</td>\n",
       "      <td>89.039560</td>\n",
       "      <td>92.010556</td>\n",
       "      <td>90.413832</td>\n",
       "      <td>...</td>\n",
       "      <td>85.234268</td>\n",
       "      <td>87.876485</td>\n",
       "      <td>85.954382</td>\n",
       "      <td>79.298734</td>\n",
       "      <td>89.869081</td>\n",
       "      <td>86.686079</td>\n",
       "      <td>83.891148</td>\n",
       "      <td>86.753257</td>\n",
       "      <td>78.341213</td>\n",
       "      <td>79.895198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.168162</td>\n",
       "      <td>88.060630</td>\n",
       "      <td>79.194408</td>\n",
       "      <td>86.235405</td>\n",
       "      <td>91.843085</td>\n",
       "      <td>92.243320</td>\n",
       "      <td>86.689144</td>\n",
       "      <td>89.192029</td>\n",
       "      <td>92.114117</td>\n",
       "      <td>90.504833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.324974</td>\n",
       "      <td>87.976567</td>\n",
       "      <td>85.799219</td>\n",
       "      <td>79.310773</td>\n",
       "      <td>89.818353</td>\n",
       "      <td>86.582394</td>\n",
       "      <td>83.730161</td>\n",
       "      <td>86.805090</td>\n",
       "      <td>78.347071</td>\n",
       "      <td>80.045589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     87.362170  92.059179  81.639492  83.046279  87.557608  88.829065   \n",
       "1     87.314653  91.868510  81.753510  83.102770  87.667622  89.014976   \n",
       "2     87.265068  91.681240  81.863257  83.160659  87.776139  89.204368   \n",
       "3     87.213900  91.497346  81.968488  83.220587  87.883528  89.397035   \n",
       "4     87.161546  91.316921  82.068671  83.283040  87.990020  89.592487   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  91.606026  88.069788  79.531699  85.419493  91.976891  92.243208   \n",
       "2439  91.746546  88.072322  79.441336  85.618475  91.938881  92.245196   \n",
       "2440  91.887422  88.071942  79.354928  85.820510  91.904210  92.245867   \n",
       "2441  92.028156  88.068195  79.272625  86.026023  91.872413  92.245207   \n",
       "2442  92.168162  88.060630  79.194408  86.235405  91.843085  92.243320   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     86.304978  91.903081  90.974425  87.964761  ...  80.716972  82.980276   \n",
       "1     86.405655  91.761364  90.912489  88.054400  ...  80.811359  82.990759   \n",
       "2     86.502722  91.621445  90.850914  88.142340  ...  80.909673  82.999959   \n",
       "3     86.596130  91.482991  90.789162  88.228029  ...  81.011722  83.008250   \n",
       "4     86.686142  91.345397  90.726820  88.311044  ...  81.117124  83.015760   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.175784  88.590557  91.681913  90.119286  ...  84.995612  87.574274   \n",
       "2439  87.057803  88.739174  91.794750  90.221004  ...  85.070026  87.674484   \n",
       "2440  86.937141  88.888750  91.904207  90.319187  ...  85.149368  87.775550   \n",
       "2441  86.814066  89.039560  92.010556  90.413832  ...  85.234268  87.876485   \n",
       "2442  86.689144  89.192029  92.114117  90.504833  ...  85.324974  87.976567   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     91.631634  83.478056  87.786838  89.127069  80.956526  92.613983   \n",
       "1     91.367214  83.430809  87.885892  88.975725  81.054012  92.393965   \n",
       "2     91.107171  83.381318  87.981265  88.825590  81.151156  92.174215   \n",
       "3     90.851560  83.329554  88.072869  88.677138  81.247568  91.955906   \n",
       "4     90.600178  83.275815  88.160725  88.530710  81.342925  91.740266   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  86.411497  79.268845  90.031115  86.998545  84.367165  86.580171   \n",
       "2439  86.261152  79.277755  89.975204  86.894393  84.210259  86.641084   \n",
       "2440  86.108531  79.287771  89.921318  86.790128  84.051367  86.698728   \n",
       "2441  85.954382  79.298734  89.869081  86.686079  83.891148  86.753257   \n",
       "2442  85.799219  79.310773  89.818353  86.582394  83.730161  86.805090   \n",
       "\n",
       "             46         47  \n",
       "0     77.410061  81.618024  \n",
       "1     77.261457  81.568429  \n",
       "2     77.116598  81.522525  \n",
       "3     76.975967  81.480128  \n",
       "4     76.839861  81.440740  \n",
       "...         ...        ...  \n",
       "2438  78.319799  79.437264  \n",
       "2439  78.327737  79.590486  \n",
       "2440  78.334857  79.743283  \n",
       "2441  78.341213  79.895198  \n",
       "2442  78.347071  80.045589  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.362170</td>\n",
       "      <td>92.059179</td>\n",
       "      <td>81.639492</td>\n",
       "      <td>83.046279</td>\n",
       "      <td>87.557608</td>\n",
       "      <td>88.829065</td>\n",
       "      <td>86.304978</td>\n",
       "      <td>91.903081</td>\n",
       "      <td>90.974425</td>\n",
       "      <td>87.964761</td>\n",
       "      <td>...</td>\n",
       "      <td>80.716972</td>\n",
       "      <td>82.980276</td>\n",
       "      <td>91.631634</td>\n",
       "      <td>83.478056</td>\n",
       "      <td>87.786838</td>\n",
       "      <td>89.127069</td>\n",
       "      <td>80.956526</td>\n",
       "      <td>92.613983</td>\n",
       "      <td>77.410061</td>\n",
       "      <td>81.618024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.314653</td>\n",
       "      <td>91.868510</td>\n",
       "      <td>81.753510</td>\n",
       "      <td>83.102770</td>\n",
       "      <td>87.667622</td>\n",
       "      <td>89.014976</td>\n",
       "      <td>86.405655</td>\n",
       "      <td>91.761364</td>\n",
       "      <td>90.912489</td>\n",
       "      <td>88.054400</td>\n",
       "      <td>...</td>\n",
       "      <td>80.811359</td>\n",
       "      <td>82.990759</td>\n",
       "      <td>91.367214</td>\n",
       "      <td>83.430809</td>\n",
       "      <td>87.885892</td>\n",
       "      <td>88.975725</td>\n",
       "      <td>81.054012</td>\n",
       "      <td>92.393965</td>\n",
       "      <td>77.261457</td>\n",
       "      <td>81.568429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.265068</td>\n",
       "      <td>91.681240</td>\n",
       "      <td>81.863257</td>\n",
       "      <td>83.160659</td>\n",
       "      <td>87.776139</td>\n",
       "      <td>89.204368</td>\n",
       "      <td>86.502722</td>\n",
       "      <td>91.621445</td>\n",
       "      <td>90.850914</td>\n",
       "      <td>88.142340</td>\n",
       "      <td>...</td>\n",
       "      <td>80.909673</td>\n",
       "      <td>82.999959</td>\n",
       "      <td>91.107171</td>\n",
       "      <td>83.381318</td>\n",
       "      <td>87.981265</td>\n",
       "      <td>88.825590</td>\n",
       "      <td>81.151156</td>\n",
       "      <td>92.174215</td>\n",
       "      <td>77.116598</td>\n",
       "      <td>81.522525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.213900</td>\n",
       "      <td>91.497346</td>\n",
       "      <td>81.968488</td>\n",
       "      <td>83.220587</td>\n",
       "      <td>87.883528</td>\n",
       "      <td>89.397035</td>\n",
       "      <td>86.596130</td>\n",
       "      <td>91.482991</td>\n",
       "      <td>90.789162</td>\n",
       "      <td>88.228029</td>\n",
       "      <td>...</td>\n",
       "      <td>81.011722</td>\n",
       "      <td>83.008250</td>\n",
       "      <td>90.851560</td>\n",
       "      <td>83.329554</td>\n",
       "      <td>88.072869</td>\n",
       "      <td>88.677138</td>\n",
       "      <td>81.247568</td>\n",
       "      <td>91.955906</td>\n",
       "      <td>76.975967</td>\n",
       "      <td>81.480128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.161546</td>\n",
       "      <td>91.316921</td>\n",
       "      <td>82.068671</td>\n",
       "      <td>83.283040</td>\n",
       "      <td>87.990020</td>\n",
       "      <td>89.592487</td>\n",
       "      <td>86.686142</td>\n",
       "      <td>91.345397</td>\n",
       "      <td>90.726820</td>\n",
       "      <td>88.311044</td>\n",
       "      <td>...</td>\n",
       "      <td>81.117124</td>\n",
       "      <td>83.015760</td>\n",
       "      <td>90.600178</td>\n",
       "      <td>83.275815</td>\n",
       "      <td>88.160725</td>\n",
       "      <td>88.530710</td>\n",
       "      <td>81.342925</td>\n",
       "      <td>91.740266</td>\n",
       "      <td>76.839861</td>\n",
       "      <td>81.440740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>91.606026</td>\n",
       "      <td>88.069788</td>\n",
       "      <td>79.531699</td>\n",
       "      <td>85.419493</td>\n",
       "      <td>91.976891</td>\n",
       "      <td>92.243208</td>\n",
       "      <td>87.175784</td>\n",
       "      <td>88.590557</td>\n",
       "      <td>91.681913</td>\n",
       "      <td>90.119286</td>\n",
       "      <td>...</td>\n",
       "      <td>84.995612</td>\n",
       "      <td>87.574274</td>\n",
       "      <td>86.411497</td>\n",
       "      <td>79.268845</td>\n",
       "      <td>90.031115</td>\n",
       "      <td>86.998545</td>\n",
       "      <td>84.367165</td>\n",
       "      <td>86.580171</td>\n",
       "      <td>78.319799</td>\n",
       "      <td>79.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>91.746546</td>\n",
       "      <td>88.072322</td>\n",
       "      <td>79.441336</td>\n",
       "      <td>85.618475</td>\n",
       "      <td>91.938881</td>\n",
       "      <td>92.245196</td>\n",
       "      <td>87.057803</td>\n",
       "      <td>88.739174</td>\n",
       "      <td>91.794750</td>\n",
       "      <td>90.221004</td>\n",
       "      <td>...</td>\n",
       "      <td>85.070026</td>\n",
       "      <td>87.674484</td>\n",
       "      <td>86.261152</td>\n",
       "      <td>79.277755</td>\n",
       "      <td>89.975204</td>\n",
       "      <td>86.894393</td>\n",
       "      <td>84.210259</td>\n",
       "      <td>86.641084</td>\n",
       "      <td>78.327737</td>\n",
       "      <td>79.590486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>91.887422</td>\n",
       "      <td>88.071942</td>\n",
       "      <td>79.354928</td>\n",
       "      <td>85.820510</td>\n",
       "      <td>91.904210</td>\n",
       "      <td>92.245867</td>\n",
       "      <td>86.937141</td>\n",
       "      <td>88.888750</td>\n",
       "      <td>91.904207</td>\n",
       "      <td>90.319187</td>\n",
       "      <td>...</td>\n",
       "      <td>85.149368</td>\n",
       "      <td>87.775550</td>\n",
       "      <td>86.108531</td>\n",
       "      <td>79.287771</td>\n",
       "      <td>89.921318</td>\n",
       "      <td>86.790128</td>\n",
       "      <td>84.051367</td>\n",
       "      <td>86.698728</td>\n",
       "      <td>78.334857</td>\n",
       "      <td>79.743283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.028156</td>\n",
       "      <td>88.068195</td>\n",
       "      <td>79.272625</td>\n",
       "      <td>86.026023</td>\n",
       "      <td>91.872413</td>\n",
       "      <td>92.245207</td>\n",
       "      <td>86.814066</td>\n",
       "      <td>89.039560</td>\n",
       "      <td>92.010556</td>\n",
       "      <td>90.413832</td>\n",
       "      <td>...</td>\n",
       "      <td>85.234268</td>\n",
       "      <td>87.876485</td>\n",
       "      <td>85.954382</td>\n",
       "      <td>79.298734</td>\n",
       "      <td>89.869081</td>\n",
       "      <td>86.686079</td>\n",
       "      <td>83.891148</td>\n",
       "      <td>86.753257</td>\n",
       "      <td>78.341213</td>\n",
       "      <td>79.895198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.168162</td>\n",
       "      <td>88.060630</td>\n",
       "      <td>79.194408</td>\n",
       "      <td>86.235405</td>\n",
       "      <td>91.843085</td>\n",
       "      <td>92.243320</td>\n",
       "      <td>86.689144</td>\n",
       "      <td>89.192029</td>\n",
       "      <td>92.114117</td>\n",
       "      <td>90.504833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.324974</td>\n",
       "      <td>87.976567</td>\n",
       "      <td>85.799219</td>\n",
       "      <td>79.310773</td>\n",
       "      <td>89.818353</td>\n",
       "      <td>86.582394</td>\n",
       "      <td>83.730161</td>\n",
       "      <td>86.805090</td>\n",
       "      <td>78.347071</td>\n",
       "      <td>80.045589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     87.362170  92.059179  81.639492  83.046279  87.557608  88.829065   \n",
       "1     87.314653  91.868510  81.753510  83.102770  87.667622  89.014976   \n",
       "2     87.265068  91.681240  81.863257  83.160659  87.776139  89.204368   \n",
       "3     87.213900  91.497346  81.968488  83.220587  87.883528  89.397035   \n",
       "4     87.161546  91.316921  82.068671  83.283040  87.990020  89.592487   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  91.606026  88.069788  79.531699  85.419493  91.976891  92.243208   \n",
       "2439  91.746546  88.072322  79.441336  85.618475  91.938881  92.245196   \n",
       "2440  91.887422  88.071942  79.354928  85.820510  91.904210  92.245867   \n",
       "2441  92.028156  88.068195  79.272625  86.026023  91.872413  92.245207   \n",
       "2442  92.168162  88.060630  79.194408  86.235405  91.843085  92.243320   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     86.304978  91.903081  90.974425  87.964761  ...  80.716972  82.980276   \n",
       "1     86.405655  91.761364  90.912489  88.054400  ...  80.811359  82.990759   \n",
       "2     86.502722  91.621445  90.850914  88.142340  ...  80.909673  82.999959   \n",
       "3     86.596130  91.482991  90.789162  88.228029  ...  81.011722  83.008250   \n",
       "4     86.686142  91.345397  90.726820  88.311044  ...  81.117124  83.015760   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.175784  88.590557  91.681913  90.119286  ...  84.995612  87.574274   \n",
       "2439  87.057803  88.739174  91.794750  90.221004  ...  85.070026  87.674484   \n",
       "2440  86.937141  88.888750  91.904207  90.319187  ...  85.149368  87.775550   \n",
       "2441  86.814066  89.039560  92.010556  90.413832  ...  85.234268  87.876485   \n",
       "2442  86.689144  89.192029  92.114117  90.504833  ...  85.324974  87.976567   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     91.631634  83.478056  87.786838  89.127069  80.956526  92.613983   \n",
       "1     91.367214  83.430809  87.885892  88.975725  81.054012  92.393965   \n",
       "2     91.107171  83.381318  87.981265  88.825590  81.151156  92.174215   \n",
       "3     90.851560  83.329554  88.072869  88.677138  81.247568  91.955906   \n",
       "4     90.600178  83.275815  88.160725  88.530710  81.342925  91.740266   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  86.411497  79.268845  90.031115  86.998545  84.367165  86.580171   \n",
       "2439  86.261152  79.277755  89.975204  86.894393  84.210259  86.641084   \n",
       "2440  86.108531  79.287771  89.921318  86.790128  84.051367  86.698728   \n",
       "2441  85.954382  79.298734  89.869081  86.686079  83.891148  86.753257   \n",
       "2442  85.799219  79.310773  89.818353  86.582394  83.730161  86.805090   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     77.410061  81.618024  \n",
       "1     77.261457  81.568429  \n",
       "2     77.116598  81.522525  \n",
       "3     76.975967  81.480128  \n",
       "4     76.839861  81.440740  \n",
       "...         ...        ...  \n",
       "2438  78.319799  79.437264  \n",
       "2439  78.327737  79.590486  \n",
       "2440  78.334857  79.743283  \n",
       "2441  78.341213  79.895198  \n",
       "2442  78.347071  80.045589  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 20ms/step - loss: 4088.7522 - val_loss: 3759.8098\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3765.8440 - val_loss: 3539.9626\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3559.9949 - val_loss: 3357.7634\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3386.2322 - val_loss: 3201.4636\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3236.1604 - val_loss: 3066.1460\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3106.4404 - val_loss: 2949.6814\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2994.6660 - val_loss: 2849.4163\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2898.8267 - val_loss: 2764.3701\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2817.1152 - val_loss: 2692.0291\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2748.3440 - val_loss: 2631.9890\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2691.3682 - val_loss: 2583.1328\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2645.1221 - val_loss: 2543.8848\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2608.5579 - val_loss: 2513.7173\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2580.3882 - val_loss: 2491.2874\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2559.8542 - val_loss: 2475.5615\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2545.3706 - val_loss: 2465.1221\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2536.0112 - val_loss: 2459.0542\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2530.2104 - val_loss: 2455.6299\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2526.9492 - val_loss: 2454.1208\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2525.3396 - val_loss: 2453.6160\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.6555 - val_loss: 2453.4272\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.4280 - val_loss: 2453.8508\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2827 - val_loss: 2453.9170\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2395 - val_loss: 2454.0454\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2593 - val_loss: 2454.0022\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2070 - val_loss: 2454.1313\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2832 - val_loss: 2454.1292\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2749 - val_loss: 2454.0889\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2812 - val_loss: 2454.2412\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2405 - val_loss: 2454.1631\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2959 - val_loss: 2454.1328\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2690 - val_loss: 2454.0613\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2185 - val_loss: 2454.0781\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2339 - val_loss: 2454.2190\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2378 - val_loss: 2454.1882\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2375 - val_loss: 2454.2595\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.1924 - val_loss: 2454.1689\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2395 - val_loss: 2454.2749\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2446 - val_loss: 2454.2512\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2524.2231 - val_loss: 2454.2290\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2366 - val_loss: 2454.2996\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2524.2446 - val_loss: 2454.3550\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2178 - val_loss: 2454.2458\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2524.2034 - val_loss: 2454.2937\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2256 - val_loss: 2454.1897\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2546 - val_loss: 2454.3123\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2400 - val_loss: 2454.3281\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2622 - val_loss: 2454.1504\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2310 - val_loss: 2454.1316\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2292 - val_loss: 2454.1987\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2524.2227 - val_loss: 2454.2476\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2524.3054 - val_loss: 2454.3245\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2524.2476 - val_loss: 2454.3567\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2886 - val_loss: 2454.3596\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2546 - val_loss: 2454.3079\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2810 - val_loss: 2454.2595\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.3491 - val_loss: 2454.1663\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2275 - val_loss: 2454.1340\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2524.2480 - val_loss: 2454.2837\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2295 - val_loss: 2454.2493\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.3303 - val_loss: 2454.3525\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2769 - val_loss: 2454.2598\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2268 - val_loss: 2454.2166\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2578 - val_loss: 2454.1404\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2551 - val_loss: 2454.1467\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2800 - val_loss: 2454.2529\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2131 - val_loss: 2454.2402\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2786 - val_loss: 2454.3784\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2524.2603 - val_loss: 2454.2485\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2637 - val_loss: 2454.2678\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2515 - val_loss: 2454.1631\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2537 - val_loss: 2454.3115\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2979 - val_loss: 2454.3374\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2524.2866 - val_loss: 2454.2864\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2524.2380 - val_loss: 2454.3372\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2524 - val_loss: 2454.3083\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2637 - val_loss: 2454.1477\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2588 - val_loss: 2454.2695\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2656 - val_loss: 2454.3188\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.3027 - val_loss: 2454.2878\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2859 - val_loss: 2454.2837\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2505 - val_loss: 2454.2141\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2061 - val_loss: 2454.3743\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2285 - val_loss: 2454.2922\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2593 - val_loss: 2454.2339\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2900 - val_loss: 2454.2385\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2542 - val_loss: 2454.3430\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2043 - val_loss: 2454.3157\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2224 - val_loss: 2454.3130\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2825 - val_loss: 2454.2905\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2095 - val_loss: 2454.2605\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2620 - val_loss: 2454.3401\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2507 - val_loss: 2454.2681\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2471 - val_loss: 2454.3596\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2703 - val_loss: 2454.2295\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2217 - val_loss: 2454.2478\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2029 - val_loss: 2454.2371\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2944 - val_loss: 2454.2766\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2524.2603 - val_loss: 2454.1504\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2664 - val_loss: 2454.2312\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2449 - val_loss: 2454.2820\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.1982 - val_loss: 2454.2825\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2769 - val_loss: 2454.2710\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2458 - val_loss: 2454.3330\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2598 - val_loss: 2454.2639\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2561 - val_loss: 2454.1592\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2749 - val_loss: 2454.3127\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2490 - val_loss: 2454.2024\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2642 - val_loss: 2454.2480\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2524.2971 - val_loss: 2454.1394\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2375 - val_loss: 2454.2812\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2100 - val_loss: 2454.1641\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2366 - val_loss: 2454.2000\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2644 - val_loss: 2454.1777\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2424 - val_loss: 2454.2490\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2500 - val_loss: 2454.1111\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2524.2236 - val_loss: 2454.2412\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2302 - val_loss: 2454.3062\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2290 - val_loss: 2454.3127\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2729 - val_loss: 2454.3713\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2524.2598 - val_loss: 2454.2183\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2769 - val_loss: 2454.2175\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2537 - val_loss: 2454.2180\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2373 - val_loss: 2454.1157\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2808 - val_loss: 2454.1123\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2100 - val_loss: 2454.2551\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.3154 - val_loss: 2454.2151\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2644 - val_loss: 2454.1921\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2336 - val_loss: 2454.2070\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2107 - val_loss: 2454.2815\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2058 - val_loss: 2454.1919\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2866 - val_loss: 2454.3000\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2483 - val_loss: 2454.2427\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2097 - val_loss: 2454.3254\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2466 - val_loss: 2454.2217\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2437 - val_loss: 2454.2097\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2595 - val_loss: 2454.2566\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2234 - val_loss: 2454.1780\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.2585 - val_loss: 2454.0696\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2524.1978 - val_loss: 2454.1963\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2202 - val_loss: 2454.2559\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.2288 - val_loss: 2454.2424\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2524.2764 - val_loss: 2454.2585\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 2524.2646 - val_loss: 2454.2820\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 2524.2219 - val_loss: 2454.1089\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 2524.2925 - val_loss: 2454.1382\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2524.2285 - val_loss: 2454.3604\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 2524.2185 - val_loss: 2454.2642\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 2524.2405 - val_loss: 2454.2166\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 21s 55ms/step - loss: 2524.2791 - val_loss: 2454.3384\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2524.2158 - val_loss: 2454.2686\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 2524.2358 - val_loss: 2454.3091\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2524.2251 - val_loss: 2454.3118\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2524.2407 - val_loss: 2454.2649\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 2524.3533 - val_loss: 2454.3467\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2341 - val_loss: 2454.2930\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 2524.2146 - val_loss: 2454.2351\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.2405 - val_loss: 2454.2515\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.2188 - val_loss: 2454.2307\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.3076 - val_loss: 2454.2041\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2864 - val_loss: 2454.1987\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.3020 - val_loss: 2454.2056\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2246 - val_loss: 2454.1926\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2271 - val_loss: 2454.3442\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2927 - val_loss: 2454.2227\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2830 - val_loss: 2454.3113\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2859 - val_loss: 2454.2461\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2908 - val_loss: 2454.1475\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.1990 - val_loss: 2454.2896\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2495 - val_loss: 2454.3079\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2717 - val_loss: 2454.3411\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2524.2893 - val_loss: 2454.2986\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2876 - val_loss: 2454.1570\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2563 - val_loss: 2454.2983\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2524.2334 - val_loss: 2454.2900\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2524.2559 - val_loss: 2454.3447\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2524.2554 - val_loss: 2454.3135\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.2625 - val_loss: 2454.2302\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2874 - val_loss: 2454.3169\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2485 - val_loss: 2454.2007\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2891 - val_loss: 2454.1951\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2734 - val_loss: 2454.3208\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2634 - val_loss: 2454.2129\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2578 - val_loss: 2454.2683\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2678 - val_loss: 2454.2986\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2524.2510 - val_loss: 2454.2332\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2144 - val_loss: 2454.3240\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2524.2534 - val_loss: 2454.2832\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2524.2725 - val_loss: 2454.2356\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2024 - val_loss: 2454.3550\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 2524.2385 - val_loss: 2454.3992\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 2524.2278 - val_loss: 2454.2808\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.2366 - val_loss: 2454.3782\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2283 - val_loss: 2454.2891\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.3066 - val_loss: 2454.4358\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2524.2971 - val_loss: 2454.4956\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.2539 - val_loss: 2454.2725\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2524.3298 - val_loss: 2454.3074\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2524.2056 - val_loss: 2454.2556\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2524.3237 - val_loss: 2454.3438\n",
      "16/16 [==============================] - 2s 33ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 2454.3435481126976\n",
      "Mean Absolute Error (MAE): 38.91258538669438\n",
      "Root Mean Squared Error (RMSE): 49.54133171517191\n",
      "Time taken: 2042.8201830387115\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 53ms/step - loss: 4076.6848 - val_loss: 3834.3770\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3749.3911 - val_loss: 3606.6897\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3542.7468 - val_loss: 3419.7578\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 3368.8372 - val_loss: 3259.3835\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3219.1255 - val_loss: 3121.2725\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 3090.0242 - val_loss: 3001.9697\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2978.9524 - val_loss: 2899.8245\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2883.9141 - val_loss: 2812.5447\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2803.0164 - val_loss: 2738.6145\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2734.9880 - val_loss: 2677.3237\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2678.8481 - val_loss: 2627.0186\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2633.4011 - val_loss: 2586.9053\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2597.3647 - val_loss: 2555.6350\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2569.8423 - val_loss: 2532.0779\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2549.6440 - val_loss: 2515.5542\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2535.6023 - val_loss: 2504.3367\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2526.4253 - val_loss: 2497.2058\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2520.8760 - val_loss: 2493.3425\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 2517.8318 - val_loss: 2491.4124\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2516.3645 - val_loss: 2490.5339\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2515.7290 - val_loss: 2490.2192\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2515.4507 - val_loss: 2490.2139\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2515.3916 - val_loss: 2490.0542\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2515.3809 - val_loss: 2490.0454\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 2515.3540 - val_loss: 2490.0757\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 2515.2993 - val_loss: 2489.9771\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2515.3669 - val_loss: 2489.9622\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 2515.2881 - val_loss: 2490.0281\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 2515.3198 - val_loss: 2489.9399\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2515.2974 - val_loss: 2489.9001\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2515.3516 - val_loss: 2489.9551\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3301 - val_loss: 2490.0605\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3342 - val_loss: 2489.8687\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2969 - val_loss: 2489.9045\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3418 - val_loss: 2489.9207\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3469 - val_loss: 2489.9590\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3367 - val_loss: 2489.9460\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3447 - val_loss: 2489.8760\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.2852 - val_loss: 2490.0205\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3074 - val_loss: 2489.9282\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3647 - val_loss: 2489.9924\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2693 - val_loss: 2489.9399\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3669 - val_loss: 2490.0688\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3213 - val_loss: 2489.9570\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3706 - val_loss: 2489.9790\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3701 - val_loss: 2490.0640\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3262 - val_loss: 2489.8586\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3647 - val_loss: 2490.0688\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3323 - val_loss: 2489.9224\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3342 - val_loss: 2489.8662\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3467 - val_loss: 2489.9102\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3345 - val_loss: 2489.9207\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3179 - val_loss: 2489.9351\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3469 - val_loss: 2489.9277\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.2917 - val_loss: 2489.9746\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3508 - val_loss: 2489.8323\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3057 - val_loss: 2489.9614\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3015 - val_loss: 2489.9949\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3201 - val_loss: 2489.9055\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3083 - val_loss: 2490.0095\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3098 - val_loss: 2489.9954\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3843 - val_loss: 2490.0439\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2825 - val_loss: 2490.0017\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3157 - val_loss: 2489.9204\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3123 - val_loss: 2490.0010\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3677 - val_loss: 2490.0151\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3501 - val_loss: 2489.9736\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3210 - val_loss: 2490.0383\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3730 - val_loss: 2490.0286\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3677 - val_loss: 2489.9626\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3093 - val_loss: 2489.9387\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2925 - val_loss: 2490.0239\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3552 - val_loss: 2489.9844\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2639 - val_loss: 2489.9329\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2925 - val_loss: 2489.9514\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3242 - val_loss: 2490.0059\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3201 - val_loss: 2489.9907\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2937 - val_loss: 2489.9941\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3396 - val_loss: 2490.0037\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3022 - val_loss: 2490.0183\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3369 - val_loss: 2490.1162\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3254 - val_loss: 2489.9167\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3000 - val_loss: 2490.0652\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2927 - val_loss: 2489.8845\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2776 - val_loss: 2490.0178\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3337 - val_loss: 2489.9001\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3569 - val_loss: 2489.9707\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3420 - val_loss: 2489.9998\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3025 - val_loss: 2489.9421\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3018 - val_loss: 2489.8599\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3040 - val_loss: 2489.9077\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3801 - val_loss: 2489.8877\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3535 - val_loss: 2489.8198\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.4131 - val_loss: 2489.9729\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3245 - val_loss: 2489.9346\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3203 - val_loss: 2489.8872\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3071 - val_loss: 2489.8713\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3042 - val_loss: 2489.9260\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3452 - val_loss: 2490.0532\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2930 - val_loss: 2489.8811\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2864 - val_loss: 2489.9456\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3411 - val_loss: 2489.9287\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3022 - val_loss: 2489.9067\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3708 - val_loss: 2489.9648\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3250 - val_loss: 2490.0056\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.4119 - val_loss: 2489.8953\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3899 - val_loss: 2489.9746\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3257 - val_loss: 2489.9695\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2827 - val_loss: 2489.8540\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2903 - val_loss: 2489.9541\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3379 - val_loss: 2489.9365\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3760 - val_loss: 2489.9827\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3066 - val_loss: 2489.8350\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3601 - val_loss: 2489.9041\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3047 - val_loss: 2490.0203\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3672 - val_loss: 2489.8838\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3782 - val_loss: 2489.9192\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2703 - val_loss: 2490.0242\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3008 - val_loss: 2489.9575\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3535 - val_loss: 2489.9851\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3127 - val_loss: 2489.9875\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3110 - val_loss: 2489.9871\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3721 - val_loss: 2490.0667\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.2966 - val_loss: 2489.9094\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.2935 - val_loss: 2490.0283\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3220 - val_loss: 2489.9888\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3179 - val_loss: 2490.0181\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3889 - val_loss: 2490.0793\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3357 - val_loss: 2489.9031\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3306 - val_loss: 2489.9309\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.4219 - val_loss: 2489.8308\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3499 - val_loss: 2489.8931\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3735 - val_loss: 2489.9109\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3411 - val_loss: 2490.0247\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3240 - val_loss: 2489.8750\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2861 - val_loss: 2490.0420\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3188 - val_loss: 2489.9399\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3442 - val_loss: 2489.9932\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3013 - val_loss: 2489.8633\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3149 - val_loss: 2489.9392\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3257 - val_loss: 2490.1196\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3630 - val_loss: 2490.0637\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3206 - val_loss: 2489.9109\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3181 - val_loss: 2489.8948\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3533 - val_loss: 2489.9268\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3369 - val_loss: 2490.0232\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3152 - val_loss: 2489.9397\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3459 - val_loss: 2489.9397\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3567 - val_loss: 2489.9553\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3040 - val_loss: 2489.8789\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3279 - val_loss: 2489.9053\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3586 - val_loss: 2489.9885\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3501 - val_loss: 2489.9753\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.4097 - val_loss: 2489.8328\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3340 - val_loss: 2489.9331\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3384 - val_loss: 2489.8796\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3193 - val_loss: 2489.9409\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3071 - val_loss: 2489.9023\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3284 - val_loss: 2489.8313\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2949 - val_loss: 2489.9214\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3438 - val_loss: 2489.9846\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3333 - val_loss: 2489.9666\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3440 - val_loss: 2489.9331\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3066 - val_loss: 2490.0474\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3374 - val_loss: 2489.9890\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.4263 - val_loss: 2489.9167\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3730 - val_loss: 2489.9907\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2888 - val_loss: 2489.8257\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3535 - val_loss: 2489.8513\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3511 - val_loss: 2489.8103\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3049 - val_loss: 2489.7976\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.2944 - val_loss: 2489.8328\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3086 - val_loss: 2489.7615\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3628 - val_loss: 2489.8337\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3315 - val_loss: 2489.8950\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3667 - val_loss: 2489.8857\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3352 - val_loss: 2489.8196\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3074 - val_loss: 2489.9319\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3257 - val_loss: 2489.7781\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3220 - val_loss: 2489.8562\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.4023 - val_loss: 2489.9150\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3569 - val_loss: 2489.8652\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3186 - val_loss: 2489.8628\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3464 - val_loss: 2489.8774\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3296 - val_loss: 2489.8518\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3154 - val_loss: 2489.9502\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3530 - val_loss: 2489.8408\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3162 - val_loss: 2489.9067\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2969 - val_loss: 2489.8635\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3320 - val_loss: 2490.0291\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3633 - val_loss: 2489.9951\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3215 - val_loss: 2489.8792\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3630 - val_loss: 2490.0200\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2515.3010 - val_loss: 2489.9580\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.2878 - val_loss: 2489.8826\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3308 - val_loss: 2489.9678\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 2515.3530 - val_loss: 2489.9521\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3027 - val_loss: 2490.0784\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3357 - val_loss: 2489.9194\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 2515.3081 - val_loss: 2489.9897\n",
      "16/16 [==============================] - 1s 18ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 2489.98967658635\n",
      "Mean Absolute Error (MAE): 39.345086467482815\n",
      "Root Mean Squared Error (RMSE): 49.89979635816513\n",
      "Time taken: 2555.9699630737305\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 20ms/step - loss: 4079.2141 - val_loss: 3978.7522\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 3760.9636 - val_loss: 3742.1572\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 3550.1252 - val_loss: 3537.9370\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3369.6943 - val_loss: 3362.5071\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3214.5295 - val_loss: 3214.4253\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3082.8149 - val_loss: 3087.4836\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2970.1719 - val_loss: 2979.1501\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2873.8982 - val_loss: 2886.6787\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2792.2097 - val_loss: 2808.5732\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2723.8069 - val_loss: 2743.5300\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2667.4214 - val_loss: 2690.3318\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2621.7151 - val_loss: 2647.8127\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2585.6118 - val_loss: 2614.4604\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2558.0603 - val_loss: 2589.1584\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2537.7009 - val_loss: 2570.7070\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2523.5349 - val_loss: 2558.2261\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2514.3567 - val_loss: 2549.9968\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2508.7771 - val_loss: 2545.1724\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2505.6743 - val_loss: 2542.3726\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2504.2026 - val_loss: 2541.0259\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.4851 - val_loss: 2540.2061\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.2053 - val_loss: 2539.6689\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1497 - val_loss: 2539.6299\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1550 - val_loss: 2539.5708\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0720 - val_loss: 2539.5508\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0725 - val_loss: 2539.3748\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0723 - val_loss: 2539.5264\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1118 - val_loss: 2539.5293\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1147 - val_loss: 2539.4341\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0476 - val_loss: 2539.3384\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0334 - val_loss: 2539.3140\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0547 - val_loss: 2539.2766\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1213 - val_loss: 2539.4290\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1501 - val_loss: 2539.5444\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0872 - val_loss: 2539.3904\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1311 - val_loss: 2539.1938\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1543 - val_loss: 2539.3586\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0837 - val_loss: 2539.3662\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1316 - val_loss: 2539.4233\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0757 - val_loss: 2539.3953\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0891 - val_loss: 2539.5149\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1301 - val_loss: 2539.3176\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.0757 - val_loss: 2539.5022\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.2141 - val_loss: 2539.4568\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1619 - val_loss: 2539.4138\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1267 - val_loss: 2539.4258\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1426 - val_loss: 2539.4568\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1086 - val_loss: 2539.4661\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0769 - val_loss: 2539.4443\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1157 - val_loss: 2539.3762\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0908 - val_loss: 2539.3901\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1047 - val_loss: 2539.3853\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1104 - val_loss: 2539.3137\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2503.1021 - val_loss: 2539.4207\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0618 - val_loss: 2539.4285\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1379 - val_loss: 2539.4343\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0779 - val_loss: 2539.2590\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0952 - val_loss: 2539.2590\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1184 - val_loss: 2539.4734\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1335 - val_loss: 2539.3557\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1069 - val_loss: 2539.3416\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1323 - val_loss: 2539.5161\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0964 - val_loss: 2539.3340\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0879 - val_loss: 2539.4790\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1135 - val_loss: 2539.4927\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.1208 - val_loss: 2539.4697\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0732 - val_loss: 2539.6140\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0891 - val_loss: 2539.4575\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1035 - val_loss: 2539.4531\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0627 - val_loss: 2539.4080\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0808 - val_loss: 2539.4495\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0371 - val_loss: 2539.4292\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0945 - val_loss: 2539.4243\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0986 - val_loss: 2539.5862\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0562 - val_loss: 2539.4941\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1145 - val_loss: 2539.5806\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1541 - val_loss: 2539.4614\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0332 - val_loss: 2539.4524\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0911 - val_loss: 2539.4209\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.2280 - val_loss: 2539.3450\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0903 - val_loss: 2539.4785\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0791 - val_loss: 2539.4556\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1050 - val_loss: 2539.3621\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0735 - val_loss: 2539.3132\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0737 - val_loss: 2539.2537\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0500 - val_loss: 2539.3484\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0972 - val_loss: 2539.3145\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1069 - val_loss: 2539.4792\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1609 - val_loss: 2539.5007\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1484 - val_loss: 2539.4175\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0928 - val_loss: 2539.5977\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1030 - val_loss: 2539.5684\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0840 - val_loss: 2539.5947\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0588 - val_loss: 2539.4829\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0898 - val_loss: 2539.5107\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1052 - val_loss: 2539.5085\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1145 - val_loss: 2539.5261\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1189 - val_loss: 2539.4866\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1106 - val_loss: 2539.3757\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0696 - val_loss: 2539.3479\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0659 - val_loss: 2539.4314\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.1355 - val_loss: 2539.4412\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.0269 - val_loss: 2539.4312\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0557 - val_loss: 2539.2388\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.1194 - val_loss: 2539.4827\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1060 - val_loss: 2539.4097\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1379 - val_loss: 2539.3931\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0925 - val_loss: 2539.4990\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1274 - val_loss: 2539.5986\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1506 - val_loss: 2539.4072\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1467 - val_loss: 2539.5002\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1265 - val_loss: 2539.5337\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0837 - val_loss: 2539.3333\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0889 - val_loss: 2539.4905\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1245 - val_loss: 2539.4944\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1194 - val_loss: 2539.5588\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1658 - val_loss: 2539.5903\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1499 - val_loss: 2539.5046\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1436 - val_loss: 2539.6348\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0979 - val_loss: 2539.3142\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0447 - val_loss: 2539.4172\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2503.1140 - val_loss: 2539.4854\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1064 - val_loss: 2539.5317\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1365 - val_loss: 2539.5168\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1008 - val_loss: 2539.6543\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1543 - val_loss: 2539.5730\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0518 - val_loss: 2539.5723\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0967 - val_loss: 2539.6550\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0867 - val_loss: 2539.3865\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0547 - val_loss: 2539.3872\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0879 - val_loss: 2539.2659\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0933 - val_loss: 2539.3176\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1580 - val_loss: 2539.3464\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1299 - val_loss: 2539.3430\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1418 - val_loss: 2539.4736\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0938 - val_loss: 2539.5803\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0803 - val_loss: 2539.4187\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0918 - val_loss: 2539.4009\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0703 - val_loss: 2539.2878\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0781 - val_loss: 2539.5312\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1233 - val_loss: 2539.5156\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1746 - val_loss: 2539.4333\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.0222 - val_loss: 2539.4160\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.0991 - val_loss: 2539.5427\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.0820 - val_loss: 2539.4958\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0701 - val_loss: 2539.4419\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1016 - val_loss: 2539.2827\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0527 - val_loss: 2539.3005\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1064 - val_loss: 2539.3586\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0779 - val_loss: 2539.3093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1248 - val_loss: 2539.3726\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0833 - val_loss: 2539.3828\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1558 - val_loss: 2539.3823\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0417 - val_loss: 2539.4053\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0811 - val_loss: 2539.5198\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1350 - val_loss: 2539.4055\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0540 - val_loss: 2539.3149\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1396 - val_loss: 2539.3608\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1619 - val_loss: 2539.3281\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1099 - val_loss: 2539.1995\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1204 - val_loss: 2539.3474\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2503.1509 - val_loss: 2539.6238\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0444 - val_loss: 2539.3286\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1123 - val_loss: 2539.4246\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0710 - val_loss: 2539.4980\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1428 - val_loss: 2539.4260\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1423 - val_loss: 2539.2395\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1235 - val_loss: 2539.5205\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0369 - val_loss: 2539.4290\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0608 - val_loss: 2539.4805\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1431 - val_loss: 2539.4243\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1118 - val_loss: 2539.4441\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1189 - val_loss: 2539.2783\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0867 - val_loss: 2539.3528\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1470 - val_loss: 2539.4968\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.1194 - val_loss: 2539.3003\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1133 - val_loss: 2539.3013\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1160 - val_loss: 2539.4490\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1372 - val_loss: 2539.3118\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0833 - val_loss: 2539.2742\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1238 - val_loss: 2539.3306\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1135 - val_loss: 2539.3264\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0928 - val_loss: 2539.1614\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0605 - val_loss: 2539.2561\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2503.1292 - val_loss: 2539.2412\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0520 - val_loss: 2539.2952\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0950 - val_loss: 2539.3264\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1069 - val_loss: 2539.2070\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0872 - val_loss: 2539.3357\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0833 - val_loss: 2539.2463\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0835 - val_loss: 2539.4263\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0786 - val_loss: 2539.3486\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2503.1084 - val_loss: 2539.3118\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0430 - val_loss: 2539.3721\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.0759 - val_loss: 2539.3357\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2503.0381 - val_loss: 2539.5254\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0781 - val_loss: 2539.3003\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.1140 - val_loss: 2539.3064\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2503.1047 - val_loss: 2539.4805\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2503.0989 - val_loss: 2539.4949\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 2539.494691611712\n",
      "Mean Absolute Error (MAE): 39.49695893873564\n",
      "Root Mean Squared Error (RMSE): 50.393399286133814\n",
      "Time taken: 1215.4703187942505\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4043.1106 - val_loss: 3825.7571\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3725.9802 - val_loss: 3605.6028\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3524.7341 - val_loss: 3418.3174\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3351.7839 - val_loss: 3259.0254\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3204.2991 - val_loss: 3121.8511\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3077.2751 - val_loss: 3003.7498\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2967.9077 - val_loss: 2901.9993\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2874.1602 - val_loss: 2815.0369\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2794.6279 - val_loss: 2742.1445\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2727.8799 - val_loss: 2680.9622\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2672.7314 - val_loss: 2631.0098\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2628.0786 - val_loss: 2591.0503\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2593.0093 - val_loss: 2560.2427\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2566.3438 - val_loss: 2537.0801\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2546.8162 - val_loss: 2520.5154\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2533.2903 - val_loss: 2509.4919\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2524.4719 - val_loss: 2502.4094\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2519.1868 - val_loss: 2498.4446\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2516.3279 - val_loss: 2496.2856\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2514.8889 - val_loss: 2495.3293\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.2612 - val_loss: 2494.8752\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0354 - val_loss: 2494.7522\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9270 - val_loss: 2494.6821\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.8987 - val_loss: 2494.6575\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9160 - val_loss: 2494.6575\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9661 - val_loss: 2494.6570\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9187 - val_loss: 2494.6521\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9514 - val_loss: 2494.6526\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9858 - val_loss: 2494.6460\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9541 - val_loss: 2494.6389\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9292 - val_loss: 2494.6458\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2514.0317 - val_loss: 2494.6436\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9353 - val_loss: 2494.6418\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9626 - val_loss: 2494.6384\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9617 - val_loss: 2494.6311\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9561 - val_loss: 2494.6313\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9331 - val_loss: 2494.6465\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9573 - val_loss: 2494.6372\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9482 - val_loss: 2494.6113\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9351 - val_loss: 2494.6560\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9148 - val_loss: 2494.6331\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9268 - val_loss: 2494.6499\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9458 - val_loss: 2494.6650\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0046 - val_loss: 2494.6501\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9673 - val_loss: 2494.6355\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9385 - val_loss: 2494.6418\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8831 - val_loss: 2494.6479\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8931 - val_loss: 2494.6343\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9563 - val_loss: 2494.6450\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.8857 - val_loss: 2494.6377\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9241 - val_loss: 2494.6406\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9526 - val_loss: 2494.6292\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.8955 - val_loss: 2494.6179\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9583 - val_loss: 2494.6260\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9504 - val_loss: 2494.6404\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9373 - val_loss: 2494.6411\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9346 - val_loss: 2494.6509\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9663 - val_loss: 2494.6287\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9570 - val_loss: 2494.6448\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9214 - val_loss: 2494.6365\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9380 - val_loss: 2494.6470\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9375 - val_loss: 2494.6331\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9204 - val_loss: 2494.6448\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.8850 - val_loss: 2494.6348\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9641 - val_loss: 2494.6387\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9102 - val_loss: 2494.6311\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9861 - val_loss: 2494.6450\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9524 - val_loss: 2494.6348\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9768 - val_loss: 2494.6384\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9038 - val_loss: 2494.6575\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9285 - val_loss: 2494.6523\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9106 - val_loss: 2494.6357\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9292 - val_loss: 2494.6235\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9160 - val_loss: 2494.6387\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9438 - val_loss: 2494.6184\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9421 - val_loss: 2494.6084\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9602 - val_loss: 2494.6235\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9175 - val_loss: 2494.6270\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0234 - val_loss: 2494.6138\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8982 - val_loss: 2494.6216\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9326 - val_loss: 2494.6113\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9766 - val_loss: 2494.6135\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9719 - val_loss: 2494.6089\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9180 - val_loss: 2494.6235\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9250 - val_loss: 2494.6147\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9153 - val_loss: 2494.6436\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9241 - val_loss: 2494.6272\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9578 - val_loss: 2494.6152\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9141 - val_loss: 2494.6160\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9170 - val_loss: 2494.6301\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9849 - val_loss: 2494.6562\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9214 - val_loss: 2494.6331\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9387 - val_loss: 2494.6213\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9685 - val_loss: 2494.6399\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9871 - val_loss: 2494.6375\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9431 - val_loss: 2494.6313\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9460 - val_loss: 2494.6404\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9263 - val_loss: 2494.6287\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9399 - val_loss: 2494.6199\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9668 - val_loss: 2494.6204\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9824 - val_loss: 2494.6375\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9424 - val_loss: 2494.6289\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9602 - val_loss: 2494.6406\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9675 - val_loss: 2494.6360\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9253 - val_loss: 2494.6450\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9407 - val_loss: 2494.6423\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9336 - val_loss: 2494.6360\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8806 - val_loss: 2494.6357\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9092 - val_loss: 2494.6409\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9905 - val_loss: 2494.6370\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9678 - val_loss: 2494.6238\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9783 - val_loss: 2494.6538\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9475 - val_loss: 2494.6360\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9792 - val_loss: 2494.6421\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9575 - val_loss: 2494.6531\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9817 - val_loss: 2494.6360\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8955 - val_loss: 2494.6438\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9658 - val_loss: 2494.6245\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9531 - val_loss: 2494.6450\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9692 - val_loss: 2494.6416\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9072 - val_loss: 2494.6458\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9592 - val_loss: 2494.6499\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9111 - val_loss: 2494.6497\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9387 - val_loss: 2494.6550\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9519 - val_loss: 2494.6255\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9451 - val_loss: 2494.6421\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9453 - val_loss: 2494.6404\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9360 - val_loss: 2494.6248\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9658 - val_loss: 2494.6460\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9412 - val_loss: 2494.6299\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9268 - val_loss: 2494.6389\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9163 - val_loss: 2494.6370\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9976 - val_loss: 2494.6396\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9624 - val_loss: 2494.6228\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9248 - val_loss: 2494.6445\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9663 - val_loss: 2494.6514\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9236 - val_loss: 2494.6472\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9729 - val_loss: 2494.6450\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9531 - val_loss: 2494.6455\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9021 - val_loss: 2494.6316\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9492 - val_loss: 2494.6392\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9272 - val_loss: 2494.6338\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9626 - val_loss: 2494.6409\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9324 - val_loss: 2494.6416\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.8936 - val_loss: 2494.6353\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9419 - val_loss: 2494.6545\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0015 - val_loss: 2494.6372\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9487 - val_loss: 2494.6577\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9529 - val_loss: 2494.6414\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9500 - val_loss: 2494.6453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0178 - val_loss: 2494.6230\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9238 - val_loss: 2494.6313\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9487 - val_loss: 2494.6409\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9629 - val_loss: 2494.6370\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9658 - val_loss: 2494.6213\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9500 - val_loss: 2494.6296\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9731 - val_loss: 2494.6479\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9253 - val_loss: 2494.6235\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9631 - val_loss: 2494.6360\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9829 - val_loss: 2494.6355\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9573 - val_loss: 2494.6423\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8831 - val_loss: 2494.6384\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9104 - val_loss: 2494.6467\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9495 - val_loss: 2494.6323\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9324 - val_loss: 2494.6606\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9138 - val_loss: 2494.6370\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9690 - val_loss: 2494.6313\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9241 - val_loss: 2494.6238\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9165 - val_loss: 2494.6355\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0000 - val_loss: 2494.6150\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9458 - val_loss: 2494.6426\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9026 - val_loss: 2494.6284\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9785 - val_loss: 2494.6421\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9033 - val_loss: 2494.6287\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9702 - val_loss: 2494.6292\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9341 - val_loss: 2494.6504\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9399 - val_loss: 2494.6191\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2513.9307 - val_loss: 2494.6292\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9419 - val_loss: 2494.6416\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9292 - val_loss: 2494.6514\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8931 - val_loss: 2494.6506\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9819 - val_loss: 2494.6479\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9885 - val_loss: 2494.6584\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9131 - val_loss: 2494.6477\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9177 - val_loss: 2494.6382\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9370 - val_loss: 2494.6418\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9084 - val_loss: 2494.6353\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.8967 - val_loss: 2494.6475\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9062 - val_loss: 2494.6487\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9272 - val_loss: 2494.6226\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9136 - val_loss: 2494.6248\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9465 - val_loss: 2494.6377\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9341 - val_loss: 2494.6257\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9995 - val_loss: 2494.6453\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0388 - val_loss: 2494.6426\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2513.9465 - val_loss: 2494.6245\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9749 - val_loss: 2494.6150\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9666 - val_loss: 2494.6335\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9470 - val_loss: 2494.6453\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2513.9365 - val_loss: 2494.6394\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 2494.6398169403583\n",
      "Mean Absolute Error (MAE): 39.539107606376525\n",
      "Root Mean Squared Error (RMSE): 49.94636940699853\n",
      "Time taken: 1179.7932958602905\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 4056.5315 - val_loss: 3927.1936\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3735.9465 - val_loss: 3706.3965\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3529.6394 - val_loss: 3519.2261\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3354.6792 - val_loss: 3360.0996\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3205.5000 - val_loss: 3223.0996\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3075.5325 - val_loss: 3101.0247\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2960.9502 - val_loss: 2997.5894\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2864.9744 - val_loss: 2909.3711\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2783.5161 - val_loss: 2835.0125\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2715.2754 - val_loss: 2772.4836\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2658.6826 - val_loss: 2721.6787\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2612.9434 - val_loss: 2680.3469\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2576.6306 - val_loss: 2648.2295\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2548.8669 - val_loss: 2623.9778\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2528.4956 - val_loss: 2606.5249\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2514.0864 - val_loss: 2594.6702\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2504.6707 - val_loss: 2586.8542\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2499.0605 - val_loss: 2582.4683\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2495.8711 - val_loss: 2580.1614\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.2900 - val_loss: 2578.8911\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.5796 - val_loss: 2578.4934\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3955 - val_loss: 2578.2056\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2524 - val_loss: 2578.2478\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2751 - val_loss: 2578.2703\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2109 - val_loss: 2578.3521\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2485 - val_loss: 2578.2554\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2808 - val_loss: 2578.3770\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2271 - val_loss: 2578.5278\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2068 - val_loss: 2578.5127\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3123 - val_loss: 2578.4915\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1975 - val_loss: 2578.5916\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2417 - val_loss: 2578.3784\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2319 - val_loss: 2578.5295\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2397 - val_loss: 2578.4583\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2036 - val_loss: 2578.6125\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2493.2634 - val_loss: 2578.5781\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2378 - val_loss: 2578.6262\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2061 - val_loss: 2578.5759\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2119 - val_loss: 2578.4995\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2522 - val_loss: 2578.6658\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1960 - val_loss: 2578.6311\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2913 - val_loss: 2578.5354\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2188 - val_loss: 2578.6299\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3281 - val_loss: 2578.4326\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2119 - val_loss: 2578.5208\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2073 - val_loss: 2578.6382\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2249 - val_loss: 2578.5642\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2087 - val_loss: 2578.6096\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2087 - val_loss: 2578.6384\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2402 - val_loss: 2578.5449\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2026 - val_loss: 2578.6714\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1780 - val_loss: 2578.6523\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2104 - val_loss: 2578.5352\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1868 - val_loss: 2578.7710\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2551 - val_loss: 2578.6707\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2751 - val_loss: 2578.5212\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2087 - val_loss: 2578.6460\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2317 - val_loss: 2578.4324\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2014 - val_loss: 2578.5081\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2021 - val_loss: 2578.5642\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2510 - val_loss: 2578.5276\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2310 - val_loss: 2578.5254\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1785 - val_loss: 2578.4810\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2314 - val_loss: 2578.5842\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2380 - val_loss: 2578.5378\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2419 - val_loss: 2578.5610\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1707 - val_loss: 2578.5767\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2366 - val_loss: 2578.6135\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2480 - val_loss: 2578.4075\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2415 - val_loss: 2578.5081\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1699 - val_loss: 2578.4482\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2151 - val_loss: 2578.6316\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1968 - val_loss: 2578.5002\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2358 - val_loss: 2578.4509\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1970 - val_loss: 2578.4495\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1677 - val_loss: 2578.6082\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2375 - val_loss: 2578.4856\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2131 - val_loss: 2578.4255\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1965 - val_loss: 2578.5168\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2327 - val_loss: 2578.5276\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1946 - val_loss: 2578.6768\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1816 - val_loss: 2578.4963\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2041 - val_loss: 2578.5173\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1997 - val_loss: 2578.5396\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2634 - val_loss: 2578.4299\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1873 - val_loss: 2578.4392\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2100 - val_loss: 2578.5071\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2571 - val_loss: 2578.4321\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1997 - val_loss: 2578.6057\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2400 - val_loss: 2578.4700\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2268 - val_loss: 2578.5405\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2266 - val_loss: 2578.5464\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1677 - val_loss: 2578.6230\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2314 - val_loss: 2578.6370\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2388 - val_loss: 2578.5449\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2344 - val_loss: 2578.5459\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2146 - val_loss: 2578.5500\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2290 - val_loss: 2578.5007\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2451 - val_loss: 2578.5093\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2090 - val_loss: 2578.5039\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2107 - val_loss: 2578.6257\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.9473 - val_loss: 2579.4204\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2494.1106 - val_loss: 2578.1462\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.5098 - val_loss: 2577.8882\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3513 - val_loss: 2577.9753\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2258 - val_loss: 2578.0784\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2373 - val_loss: 2578.1272\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2637 - val_loss: 2578.2844\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2576 - val_loss: 2578.3914\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2837 - val_loss: 2578.3240\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2422 - val_loss: 2578.4329\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2537 - val_loss: 2578.4866\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1970 - val_loss: 2578.4829\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2507 - val_loss: 2578.5225\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1882 - val_loss: 2578.4478\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2610 - val_loss: 2578.4136\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2319 - val_loss: 2578.4570\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2041 - val_loss: 2578.5205\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2290 - val_loss: 2578.5750\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1565 - val_loss: 2578.6616\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2297 - val_loss: 2578.6055\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2083 - val_loss: 2578.5850\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2239 - val_loss: 2578.6855\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2258 - val_loss: 2578.6038\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2739 - val_loss: 2578.6021\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1865 - val_loss: 2578.7036\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.3086 - val_loss: 2578.5781\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2319 - val_loss: 2578.6851\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1755 - val_loss: 2578.6243\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2046 - val_loss: 2578.5417\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2107 - val_loss: 2578.5305\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2505 - val_loss: 2578.5447\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2219 - val_loss: 2578.6802\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2031 - val_loss: 2578.7019\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2458 - val_loss: 2578.5310\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2283 - val_loss: 2578.5830\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2261 - val_loss: 2578.5918\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2761 - val_loss: 2578.5862\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2388 - val_loss: 2578.7119\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2778 - val_loss: 2578.6055\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2878 - val_loss: 2578.5662\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2739 - val_loss: 2578.6191\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2505 - val_loss: 2578.6914\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1953 - val_loss: 2578.5383\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2144 - val_loss: 2578.6184\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1794 - val_loss: 2578.6646\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1870 - val_loss: 2578.5779\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1650 - val_loss: 2578.5918\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2354 - val_loss: 2578.6482\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2200 - val_loss: 2578.4673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1746 - val_loss: 2578.6609\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2395 - val_loss: 2578.6418\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2117 - val_loss: 2578.6018\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2227 - val_loss: 2578.6160\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1731 - val_loss: 2578.6555\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2283 - val_loss: 2578.7688\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2139 - val_loss: 2578.6633\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2798 - val_loss: 2578.6655\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2493.2280 - val_loss: 2578.7610\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2483 - val_loss: 2578.5837\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1687 - val_loss: 2578.6404\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2397 - val_loss: 2578.5173\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1697 - val_loss: 2578.6257\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1758 - val_loss: 2578.6597\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2122 - val_loss: 2578.5737\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2109 - val_loss: 2578.6157\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1787 - val_loss: 2578.4993\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1638 - val_loss: 2578.4619\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2788 - val_loss: 2578.6238\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1853 - val_loss: 2578.5098\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1782 - val_loss: 2578.5708\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2170 - val_loss: 2578.6572\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2439 - val_loss: 2578.6494\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1914 - val_loss: 2578.6299\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2075 - val_loss: 2578.5554\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2063 - val_loss: 2578.7126\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2156 - val_loss: 2578.6089\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2458 - val_loss: 2578.5867\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1772 - val_loss: 2578.6023\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2429 - val_loss: 2578.4678\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1792 - val_loss: 2578.5742\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2793 - val_loss: 2578.4299\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2786 - val_loss: 2578.3972\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1868 - val_loss: 2578.5674\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2002 - val_loss: 2578.7053\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.1938 - val_loss: 2578.6392\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1990 - val_loss: 2578.7227\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2493.2437 - val_loss: 2578.4023\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3237 - val_loss: 2578.4326\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1841 - val_loss: 2578.4536\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3008 - val_loss: 2578.5164\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.1873 - val_loss: 2578.5796\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2368 - val_loss: 2578.7107\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2087 - val_loss: 2578.7634\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2222 - val_loss: 2578.4780\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2532 - val_loss: 2578.7041\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2039 - val_loss: 2578.5735\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2246 - val_loss: 2578.6609\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.3140 - val_loss: 2578.5317\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2493.2383 - val_loss: 2578.5771\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 2578.5775101712156\n",
      "Mean Absolute Error (MAE): 39.90696517864918\n",
      "Root Mean Squared Error (RMSE): 50.77969584559576\n",
      "Time taken: 1190.2947082519531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_15632\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold          MSE        MAE       RMSE   Time taken\n",
      "0        1  2454.343548  38.912585  49.541332  2042.820183\n",
      "1        2  2489.989677  39.345086  49.899796  2555.969963\n",
      "2        3  2539.494692  39.496959  50.393399  1215.470319\n",
      "3        4  2494.639817  39.539108  49.946369  1179.793296\n",
      "4        5  2578.577510  39.906965  50.779696  1190.294708\n",
      "5  Average  2511.409049  39.440141  50.112119  1636.869694\n",
      "Results saved to 'DL_Result_PL_model_2_smoothing2_Reg3.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_smoothing2_Reg3.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_2_smoothing2_Reg3.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
