{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f_obese.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.504459</td>\n",
       "      <td>86.399597</td>\n",
       "      <td>79.080492</td>\n",
       "      <td>87.102449</td>\n",
       "      <td>86.771802</td>\n",
       "      <td>89.147780</td>\n",
       "      <td>85.664094</td>\n",
       "      <td>91.059360</td>\n",
       "      <td>83.770213</td>\n",
       "      <td>88.197770</td>\n",
       "      <td>...</td>\n",
       "      <td>81.239405</td>\n",
       "      <td>78.048687</td>\n",
       "      <td>84.186534</td>\n",
       "      <td>79.617342</td>\n",
       "      <td>88.756527</td>\n",
       "      <td>87.301170</td>\n",
       "      <td>84.126977</td>\n",
       "      <td>87.795904</td>\n",
       "      <td>67.633187</td>\n",
       "      <td>78.108970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.519259</td>\n",
       "      <td>86.593020</td>\n",
       "      <td>79.101701</td>\n",
       "      <td>86.823706</td>\n",
       "      <td>86.758531</td>\n",
       "      <td>89.178741</td>\n",
       "      <td>85.666877</td>\n",
       "      <td>90.877329</td>\n",
       "      <td>83.697448</td>\n",
       "      <td>88.085323</td>\n",
       "      <td>...</td>\n",
       "      <td>81.155170</td>\n",
       "      <td>78.370631</td>\n",
       "      <td>84.187540</td>\n",
       "      <td>79.793881</td>\n",
       "      <td>88.724417</td>\n",
       "      <td>87.359682</td>\n",
       "      <td>84.163713</td>\n",
       "      <td>87.768586</td>\n",
       "      <td>67.625241</td>\n",
       "      <td>78.155214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.537091</td>\n",
       "      <td>86.782692</td>\n",
       "      <td>79.118162</td>\n",
       "      <td>86.546519</td>\n",
       "      <td>86.749143</td>\n",
       "      <td>89.213281</td>\n",
       "      <td>85.665636</td>\n",
       "      <td>90.703549</td>\n",
       "      <td>83.626904</td>\n",
       "      <td>87.974585</td>\n",
       "      <td>...</td>\n",
       "      <td>81.075011</td>\n",
       "      <td>78.688196</td>\n",
       "      <td>84.187046</td>\n",
       "      <td>79.965533</td>\n",
       "      <td>88.695040</td>\n",
       "      <td>87.420345</td>\n",
       "      <td>84.200228</td>\n",
       "      <td>87.739953</td>\n",
       "      <td>67.616693</td>\n",
       "      <td>78.199453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.557419</td>\n",
       "      <td>86.968268</td>\n",
       "      <td>79.130002</td>\n",
       "      <td>86.270870</td>\n",
       "      <td>86.743626</td>\n",
       "      <td>89.251556</td>\n",
       "      <td>85.660263</td>\n",
       "      <td>90.537891</td>\n",
       "      <td>83.558588</td>\n",
       "      <td>87.865334</td>\n",
       "      <td>...</td>\n",
       "      <td>80.998904</td>\n",
       "      <td>79.000898</td>\n",
       "      <td>84.184876</td>\n",
       "      <td>80.132052</td>\n",
       "      <td>88.668364</td>\n",
       "      <td>87.483563</td>\n",
       "      <td>84.236669</td>\n",
       "      <td>87.710200</td>\n",
       "      <td>67.608228</td>\n",
       "      <td>78.241218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.579760</td>\n",
       "      <td>87.149185</td>\n",
       "      <td>79.137413</td>\n",
       "      <td>85.996588</td>\n",
       "      <td>86.742103</td>\n",
       "      <td>89.293636</td>\n",
       "      <td>85.650554</td>\n",
       "      <td>90.379940</td>\n",
       "      <td>83.492659</td>\n",
       "      <td>87.757402</td>\n",
       "      <td>...</td>\n",
       "      <td>80.927080</td>\n",
       "      <td>79.308376</td>\n",
       "      <td>84.181101</td>\n",
       "      <td>80.293286</td>\n",
       "      <td>88.644123</td>\n",
       "      <td>87.549464</td>\n",
       "      <td>84.272948</td>\n",
       "      <td>87.679623</td>\n",
       "      <td>67.600649</td>\n",
       "      <td>78.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>92.020713</td>\n",
       "      <td>87.437522</td>\n",
       "      <td>78.460299</td>\n",
       "      <td>75.652257</td>\n",
       "      <td>91.864426</td>\n",
       "      <td>93.264508</td>\n",
       "      <td>87.532459</td>\n",
       "      <td>84.719985</td>\n",
       "      <td>82.388790</td>\n",
       "      <td>83.227223</td>\n",
       "      <td>...</td>\n",
       "      <td>78.864388</td>\n",
       "      <td>80.495710</td>\n",
       "      <td>80.470719</td>\n",
       "      <td>81.113167</td>\n",
       "      <td>91.693359</td>\n",
       "      <td>91.156410</td>\n",
       "      <td>84.697309</td>\n",
       "      <td>85.748841</td>\n",
       "      <td>76.423079</td>\n",
       "      <td>71.173074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>92.240183</td>\n",
       "      <td>87.534982</td>\n",
       "      <td>78.470896</td>\n",
       "      <td>75.771393</td>\n",
       "      <td>91.834236</td>\n",
       "      <td>93.460502</td>\n",
       "      <td>87.600872</td>\n",
       "      <td>84.600752</td>\n",
       "      <td>82.230454</td>\n",
       "      <td>83.128281</td>\n",
       "      <td>...</td>\n",
       "      <td>78.829764</td>\n",
       "      <td>80.388767</td>\n",
       "      <td>80.310960</td>\n",
       "      <td>81.125860</td>\n",
       "      <td>91.847410</td>\n",
       "      <td>91.294340</td>\n",
       "      <td>84.606032</td>\n",
       "      <td>85.729828</td>\n",
       "      <td>76.590800</td>\n",
       "      <td>71.336844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>92.461814</td>\n",
       "      <td>87.630229</td>\n",
       "      <td>78.486618</td>\n",
       "      <td>75.894408</td>\n",
       "      <td>91.807209</td>\n",
       "      <td>93.653126</td>\n",
       "      <td>87.672303</td>\n",
       "      <td>84.480406</td>\n",
       "      <td>82.073045</td>\n",
       "      <td>83.028264</td>\n",
       "      <td>...</td>\n",
       "      <td>78.797137</td>\n",
       "      <td>80.284453</td>\n",
       "      <td>80.149074</td>\n",
       "      <td>81.137419</td>\n",
       "      <td>92.003863</td>\n",
       "      <td>91.430279</td>\n",
       "      <td>84.514645</td>\n",
       "      <td>85.711048</td>\n",
       "      <td>76.759045</td>\n",
       "      <td>71.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.685277</td>\n",
       "      <td>87.722960</td>\n",
       "      <td>78.507761</td>\n",
       "      <td>76.021213</td>\n",
       "      <td>91.783334</td>\n",
       "      <td>93.842537</td>\n",
       "      <td>87.746920</td>\n",
       "      <td>84.359183</td>\n",
       "      <td>81.916747</td>\n",
       "      <td>82.926738</td>\n",
       "      <td>...</td>\n",
       "      <td>78.767266</td>\n",
       "      <td>80.182617</td>\n",
       "      <td>79.985102</td>\n",
       "      <td>81.147198</td>\n",
       "      <td>92.162897</td>\n",
       "      <td>91.563879</td>\n",
       "      <td>84.422441</td>\n",
       "      <td>85.691773</td>\n",
       "      <td>76.928062</td>\n",
       "      <td>71.674240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.909981</td>\n",
       "      <td>87.812998</td>\n",
       "      <td>78.534559</td>\n",
       "      <td>76.151518</td>\n",
       "      <td>91.762590</td>\n",
       "      <td>94.029071</td>\n",
       "      <td>87.824930</td>\n",
       "      <td>84.237114</td>\n",
       "      <td>81.761408</td>\n",
       "      <td>82.823075</td>\n",
       "      <td>...</td>\n",
       "      <td>78.740971</td>\n",
       "      <td>80.082791</td>\n",
       "      <td>79.819085</td>\n",
       "      <td>81.154675</td>\n",
       "      <td>92.324398</td>\n",
       "      <td>91.694908</td>\n",
       "      <td>84.328841</td>\n",
       "      <td>85.671334</td>\n",
       "      <td>77.098275</td>\n",
       "      <td>71.847731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     83.504459  86.399597  79.080492  87.102449  86.771802  89.147780   \n",
       "1     83.519259  86.593020  79.101701  86.823706  86.758531  89.178741   \n",
       "2     83.537091  86.782692  79.118162  86.546519  86.749143  89.213281   \n",
       "3     83.557419  86.968268  79.130002  86.270870  86.743626  89.251556   \n",
       "4     83.579760  87.149185  79.137413  85.996588  86.742103  89.293636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  92.020713  87.437522  78.460299  75.652257  91.864426  93.264508   \n",
       "2439  92.240183  87.534982  78.470896  75.771393  91.834236  93.460502   \n",
       "2440  92.461814  87.630229  78.486618  75.894408  91.807209  93.653126   \n",
       "2441  92.685277  87.722960  78.507761  76.021213  91.783334  93.842537   \n",
       "2442  92.909981  87.812998  78.534559  76.151518  91.762590  94.029071   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     85.664094  91.059360  83.770213  88.197770  ...  81.239405  78.048687   \n",
       "1     85.666877  90.877329  83.697448  88.085323  ...  81.155170  78.370631   \n",
       "2     85.665636  90.703549  83.626904  87.974585  ...  81.075011  78.688196   \n",
       "3     85.660263  90.537891  83.558588  87.865334  ...  80.998904  79.000898   \n",
       "4     85.650554  90.379940  83.492659  87.757402  ...  80.927080  79.308376   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.532459  84.719985  82.388790  83.227223  ...  78.864388  80.495710   \n",
       "2439  87.600872  84.600752  82.230454  83.128281  ...  78.829764  80.388767   \n",
       "2440  87.672303  84.480406  82.073045  83.028264  ...  78.797137  80.284453   \n",
       "2441  87.746920  84.359183  81.916747  82.926738  ...  78.767266  80.182617   \n",
       "2442  87.824930  84.237114  81.761408  82.823075  ...  78.740971  80.082791   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     84.186534  79.617342  88.756527  87.301170  84.126977  87.795904   \n",
       "1     84.187540  79.793881  88.724417  87.359682  84.163713  87.768586   \n",
       "2     84.187046  79.965533  88.695040  87.420345  84.200228  87.739953   \n",
       "3     84.184876  80.132052  88.668364  87.483563  84.236669  87.710200   \n",
       "4     84.181101  80.293286  88.644123  87.549464  84.272948  87.679623   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  80.470719  81.113167  91.693359  91.156410  84.697309  85.748841   \n",
       "2439  80.310960  81.125860  91.847410  91.294340  84.606032  85.729828   \n",
       "2440  80.149074  81.137419  92.003863  91.430279  84.514645  85.711048   \n",
       "2441  79.985102  81.147198  92.162897  91.563879  84.422441  85.691773   \n",
       "2442  79.819085  81.154675  92.324398  91.694908  84.328841  85.671334   \n",
       "\n",
       "             46         47  \n",
       "0     67.633187  78.108970  \n",
       "1     67.625241  78.155214  \n",
       "2     67.616693  78.199453  \n",
       "3     67.608228  78.241218  \n",
       "4     67.600649  78.280300  \n",
       "...         ...        ...  \n",
       "2438  76.423079  71.173074  \n",
       "2439  76.590800  71.336844  \n",
       "2440  76.759045  71.503928  \n",
       "2441  76.928062  71.674240  \n",
       "2442  77.098275  71.847731  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.504459</td>\n",
       "      <td>86.399597</td>\n",
       "      <td>79.080492</td>\n",
       "      <td>87.102449</td>\n",
       "      <td>86.771802</td>\n",
       "      <td>89.147780</td>\n",
       "      <td>85.664094</td>\n",
       "      <td>91.059360</td>\n",
       "      <td>83.770213</td>\n",
       "      <td>88.197770</td>\n",
       "      <td>...</td>\n",
       "      <td>81.239405</td>\n",
       "      <td>78.048687</td>\n",
       "      <td>84.186534</td>\n",
       "      <td>79.617342</td>\n",
       "      <td>88.756527</td>\n",
       "      <td>87.301170</td>\n",
       "      <td>84.126977</td>\n",
       "      <td>87.795904</td>\n",
       "      <td>67.633187</td>\n",
       "      <td>78.108970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.519259</td>\n",
       "      <td>86.593020</td>\n",
       "      <td>79.101701</td>\n",
       "      <td>86.823706</td>\n",
       "      <td>86.758531</td>\n",
       "      <td>89.178741</td>\n",
       "      <td>85.666877</td>\n",
       "      <td>90.877329</td>\n",
       "      <td>83.697448</td>\n",
       "      <td>88.085323</td>\n",
       "      <td>...</td>\n",
       "      <td>81.155170</td>\n",
       "      <td>78.370631</td>\n",
       "      <td>84.187540</td>\n",
       "      <td>79.793881</td>\n",
       "      <td>88.724417</td>\n",
       "      <td>87.359682</td>\n",
       "      <td>84.163713</td>\n",
       "      <td>87.768586</td>\n",
       "      <td>67.625241</td>\n",
       "      <td>78.155214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.537091</td>\n",
       "      <td>86.782692</td>\n",
       "      <td>79.118162</td>\n",
       "      <td>86.546519</td>\n",
       "      <td>86.749143</td>\n",
       "      <td>89.213281</td>\n",
       "      <td>85.665636</td>\n",
       "      <td>90.703549</td>\n",
       "      <td>83.626904</td>\n",
       "      <td>87.974585</td>\n",
       "      <td>...</td>\n",
       "      <td>81.075011</td>\n",
       "      <td>78.688196</td>\n",
       "      <td>84.187046</td>\n",
       "      <td>79.965533</td>\n",
       "      <td>88.695040</td>\n",
       "      <td>87.420345</td>\n",
       "      <td>84.200228</td>\n",
       "      <td>87.739953</td>\n",
       "      <td>67.616693</td>\n",
       "      <td>78.199453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.557419</td>\n",
       "      <td>86.968268</td>\n",
       "      <td>79.130002</td>\n",
       "      <td>86.270870</td>\n",
       "      <td>86.743626</td>\n",
       "      <td>89.251556</td>\n",
       "      <td>85.660263</td>\n",
       "      <td>90.537891</td>\n",
       "      <td>83.558588</td>\n",
       "      <td>87.865334</td>\n",
       "      <td>...</td>\n",
       "      <td>80.998904</td>\n",
       "      <td>79.000898</td>\n",
       "      <td>84.184876</td>\n",
       "      <td>80.132052</td>\n",
       "      <td>88.668364</td>\n",
       "      <td>87.483563</td>\n",
       "      <td>84.236669</td>\n",
       "      <td>87.710200</td>\n",
       "      <td>67.608228</td>\n",
       "      <td>78.241218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.579760</td>\n",
       "      <td>87.149185</td>\n",
       "      <td>79.137413</td>\n",
       "      <td>85.996588</td>\n",
       "      <td>86.742103</td>\n",
       "      <td>89.293636</td>\n",
       "      <td>85.650554</td>\n",
       "      <td>90.379940</td>\n",
       "      <td>83.492659</td>\n",
       "      <td>87.757402</td>\n",
       "      <td>...</td>\n",
       "      <td>80.927080</td>\n",
       "      <td>79.308376</td>\n",
       "      <td>84.181101</td>\n",
       "      <td>80.293286</td>\n",
       "      <td>88.644123</td>\n",
       "      <td>87.549464</td>\n",
       "      <td>84.272948</td>\n",
       "      <td>87.679623</td>\n",
       "      <td>67.600649</td>\n",
       "      <td>78.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>92.020713</td>\n",
       "      <td>87.437522</td>\n",
       "      <td>78.460299</td>\n",
       "      <td>75.652257</td>\n",
       "      <td>91.864426</td>\n",
       "      <td>93.264508</td>\n",
       "      <td>87.532459</td>\n",
       "      <td>84.719985</td>\n",
       "      <td>82.388790</td>\n",
       "      <td>83.227223</td>\n",
       "      <td>...</td>\n",
       "      <td>78.864388</td>\n",
       "      <td>80.495710</td>\n",
       "      <td>80.470719</td>\n",
       "      <td>81.113167</td>\n",
       "      <td>91.693359</td>\n",
       "      <td>91.156410</td>\n",
       "      <td>84.697309</td>\n",
       "      <td>85.748841</td>\n",
       "      <td>76.423079</td>\n",
       "      <td>71.173074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>92.240183</td>\n",
       "      <td>87.534982</td>\n",
       "      <td>78.470896</td>\n",
       "      <td>75.771393</td>\n",
       "      <td>91.834236</td>\n",
       "      <td>93.460502</td>\n",
       "      <td>87.600872</td>\n",
       "      <td>84.600752</td>\n",
       "      <td>82.230454</td>\n",
       "      <td>83.128281</td>\n",
       "      <td>...</td>\n",
       "      <td>78.829764</td>\n",
       "      <td>80.388767</td>\n",
       "      <td>80.310960</td>\n",
       "      <td>81.125860</td>\n",
       "      <td>91.847410</td>\n",
       "      <td>91.294340</td>\n",
       "      <td>84.606032</td>\n",
       "      <td>85.729828</td>\n",
       "      <td>76.590800</td>\n",
       "      <td>71.336844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>92.461814</td>\n",
       "      <td>87.630229</td>\n",
       "      <td>78.486618</td>\n",
       "      <td>75.894408</td>\n",
       "      <td>91.807209</td>\n",
       "      <td>93.653126</td>\n",
       "      <td>87.672303</td>\n",
       "      <td>84.480406</td>\n",
       "      <td>82.073045</td>\n",
       "      <td>83.028264</td>\n",
       "      <td>...</td>\n",
       "      <td>78.797137</td>\n",
       "      <td>80.284453</td>\n",
       "      <td>80.149074</td>\n",
       "      <td>81.137419</td>\n",
       "      <td>92.003863</td>\n",
       "      <td>91.430279</td>\n",
       "      <td>84.514645</td>\n",
       "      <td>85.711048</td>\n",
       "      <td>76.759045</td>\n",
       "      <td>71.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.685277</td>\n",
       "      <td>87.722960</td>\n",
       "      <td>78.507761</td>\n",
       "      <td>76.021213</td>\n",
       "      <td>91.783334</td>\n",
       "      <td>93.842537</td>\n",
       "      <td>87.746920</td>\n",
       "      <td>84.359183</td>\n",
       "      <td>81.916747</td>\n",
       "      <td>82.926738</td>\n",
       "      <td>...</td>\n",
       "      <td>78.767266</td>\n",
       "      <td>80.182617</td>\n",
       "      <td>79.985102</td>\n",
       "      <td>81.147198</td>\n",
       "      <td>92.162897</td>\n",
       "      <td>91.563879</td>\n",
       "      <td>84.422441</td>\n",
       "      <td>85.691773</td>\n",
       "      <td>76.928062</td>\n",
       "      <td>71.674240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.909981</td>\n",
       "      <td>87.812998</td>\n",
       "      <td>78.534559</td>\n",
       "      <td>76.151518</td>\n",
       "      <td>91.762590</td>\n",
       "      <td>94.029071</td>\n",
       "      <td>87.824930</td>\n",
       "      <td>84.237114</td>\n",
       "      <td>81.761408</td>\n",
       "      <td>82.823075</td>\n",
       "      <td>...</td>\n",
       "      <td>78.740971</td>\n",
       "      <td>80.082791</td>\n",
       "      <td>79.819085</td>\n",
       "      <td>81.154675</td>\n",
       "      <td>92.324398</td>\n",
       "      <td>91.694908</td>\n",
       "      <td>84.328841</td>\n",
       "      <td>85.671334</td>\n",
       "      <td>77.098275</td>\n",
       "      <td>71.847731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     83.504459  86.399597  79.080492  87.102449  86.771802  89.147780   \n",
       "1     83.519259  86.593020  79.101701  86.823706  86.758531  89.178741   \n",
       "2     83.537091  86.782692  79.118162  86.546519  86.749143  89.213281   \n",
       "3     83.557419  86.968268  79.130002  86.270870  86.743626  89.251556   \n",
       "4     83.579760  87.149185  79.137413  85.996588  86.742103  89.293636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  92.020713  87.437522  78.460299  75.652257  91.864426  93.264508   \n",
       "2439  92.240183  87.534982  78.470896  75.771393  91.834236  93.460502   \n",
       "2440  92.461814  87.630229  78.486618  75.894408  91.807209  93.653126   \n",
       "2441  92.685277  87.722960  78.507761  76.021213  91.783334  93.842537   \n",
       "2442  92.909981  87.812998  78.534559  76.151518  91.762590  94.029071   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     85.664094  91.059360  83.770213  88.197770  ...  81.239405  78.048687   \n",
       "1     85.666877  90.877329  83.697448  88.085323  ...  81.155170  78.370631   \n",
       "2     85.665636  90.703549  83.626904  87.974585  ...  81.075011  78.688196   \n",
       "3     85.660263  90.537891  83.558588  87.865334  ...  80.998904  79.000898   \n",
       "4     85.650554  90.379940  83.492659  87.757402  ...  80.927080  79.308376   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.532459  84.719985  82.388790  83.227223  ...  78.864388  80.495710   \n",
       "2439  87.600872  84.600752  82.230454  83.128281  ...  78.829764  80.388767   \n",
       "2440  87.672303  84.480406  82.073045  83.028264  ...  78.797137  80.284453   \n",
       "2441  87.746920  84.359183  81.916747  82.926738  ...  78.767266  80.182617   \n",
       "2442  87.824930  84.237114  81.761408  82.823075  ...  78.740971  80.082791   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     84.186534  79.617342  88.756527  87.301170  84.126977  87.795904   \n",
       "1     84.187540  79.793881  88.724417  87.359682  84.163713  87.768586   \n",
       "2     84.187046  79.965533  88.695040  87.420345  84.200228  87.739953   \n",
       "3     84.184876  80.132052  88.668364  87.483563  84.236669  87.710200   \n",
       "4     84.181101  80.293286  88.644123  87.549464  84.272948  87.679623   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  80.470719  81.113167  91.693359  91.156410  84.697309  85.748841   \n",
       "2439  80.310960  81.125860  91.847410  91.294340  84.606032  85.729828   \n",
       "2440  80.149074  81.137419  92.003863  91.430279  84.514645  85.711048   \n",
       "2441  79.985102  81.147198  92.162897  91.563879  84.422441  85.691773   \n",
       "2442  79.819085  81.154675  92.324398  91.694908  84.328841  85.671334   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     67.633187  78.108970  \n",
       "1     67.625241  78.155214  \n",
       "2     67.616693  78.199453  \n",
       "3     67.608228  78.241218  \n",
       "4     67.600649  78.280300  \n",
       "...         ...        ...  \n",
       "2438  76.423079  71.173074  \n",
       "2439  76.590800  71.336844  \n",
       "2440  76.759045  71.503928  \n",
       "2441  76.928062  71.674240  \n",
       "2442  77.098275  71.847731  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 19ms/step - loss: 1387.7358 - val_loss: 1273.1666\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1221.8301 - val_loss: 1168.1187\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1131.9352 - val_loss: 1091.5991\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1066.4022 - val_loss: 1035.0720\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1018.0903 - val_loss: 993.5817\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 983.0817 - val_loss: 964.0346\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 958.6274 - val_loss: 943.6902\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 942.2598 - val_loss: 930.4304\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 932.0497 - val_loss: 922.5802\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1046 - val_loss: 918.1130\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.9598 - val_loss: 915.8443\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.4820 - val_loss: 914.8015\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.8214 - val_loss: 914.3928\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.6091 - val_loss: 914.2676\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.5881 - val_loss: 914.2036\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.5500 - val_loss: 914.2110\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.5406 - val_loss: 914.2126\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 920.5225 - val_loss: 914.2352\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 920.5547 - val_loss: 914.2335\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5470 - val_loss: 914.1946\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5652 - val_loss: 914.1982\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5580 - val_loss: 914.1976\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5733 - val_loss: 914.1970\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5240 - val_loss: 914.1850\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5585 - val_loss: 914.1958\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 920.5354 - val_loss: 914.1956\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5585 - val_loss: 914.1962\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5275 - val_loss: 914.2095\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5479 - val_loss: 914.1937\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 920.5436 - val_loss: 914.1866\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5306 - val_loss: 914.2107\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5341 - val_loss: 914.2002\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5715 - val_loss: 914.2211\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5756 - val_loss: 914.2125\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5746 - val_loss: 914.1952\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 920.5414 - val_loss: 914.1919\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5480 - val_loss: 914.2043\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 920.5178 - val_loss: 914.1960\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5363 - val_loss: 914.1941\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5430 - val_loss: 914.1957\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5399 - val_loss: 914.2114\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5471 - val_loss: 914.2073\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5175 - val_loss: 914.2173\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 920.5737 - val_loss: 914.2056\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5325 - val_loss: 914.2052\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5406 - val_loss: 914.1929\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5434 - val_loss: 914.2151\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5717 - val_loss: 914.2015\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5234 - val_loss: 914.2012\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5530 - val_loss: 914.1927\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5271 - val_loss: 914.2001\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5344 - val_loss: 914.1879\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 920.5308 - val_loss: 914.1937\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 920.5604 - val_loss: 914.1766\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 920.5302 - val_loss: 914.2069\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5315 - val_loss: 915.8953\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 921.1480 - val_loss: 914.5013\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.6182 - val_loss: 914.4136\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5948 - val_loss: 914.3209\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5472 - val_loss: 914.2936\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.5693 - val_loss: 914.2819\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 920.2260 - val_loss: 912.3983\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 904.9323 - val_loss: 877.6077\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 866.6263 - val_loss: 841.5909\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 842.2641 - val_loss: 819.1900\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 817.4914 - val_loss: 797.0874\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 796.1078 - val_loss: 774.8881\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 771.6299 - val_loss: 749.3665\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 746.9088 - val_loss: 724.8906\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 714.8053 - val_loss: 694.1980\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 686.3088 - val_loss: 669.2387\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 656.0122 - val_loss: 646.0975\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 624.6972 - val_loss: 608.2399\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 594.9480 - val_loss: 577.4403\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 563.6899 - val_loss: 552.4839\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 539.4142 - val_loss: 530.5872\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 50ms/step - loss: 514.5934 - val_loss: 504.5957\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 495.4873 - val_loss: 480.8666\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 467.0229 - val_loss: 461.6689\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 441.6105 - val_loss: 428.1759\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 412.4131 - val_loss: 396.7756\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 384.6859 - val_loss: 375.5223\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 358.9195 - val_loss: 348.8347\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 336.8644 - val_loss: 327.6611\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 315.9473 - val_loss: 311.7830\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 295.6714 - val_loss: 300.7787\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 277.4130 - val_loss: 270.8168\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 258.5374 - val_loss: 251.9257\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 241.9040 - val_loss: 239.1046\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 227.4362 - val_loss: 219.6659\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 212.6869 - val_loss: 207.5735\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 198.7079 - val_loss: 195.5596\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 186.3398 - val_loss: 193.4351\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 173.7619 - val_loss: 167.1387\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 162.7763 - val_loss: 163.2985\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 152.7741 - val_loss: 162.1591\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 143.7230 - val_loss: 141.7795\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 133.9240 - val_loss: 132.0918\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 123.8864 - val_loss: 141.4790\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 118.8980 - val_loss: 114.0280\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 107.1191 - val_loss: 115.8259\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 100.1451 - val_loss: 97.0717\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 91.4808 - val_loss: 87.2079\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 84.8329 - val_loss: 82.2288\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 77.5039 - val_loss: 77.0083\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 75.2703 - val_loss: 79.1256\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 68.7670 - val_loss: 66.5497\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 60.1723 - val_loss: 65.5272\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 56.0245 - val_loss: 56.9320\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 52.8617 - val_loss: 53.3839\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 52.4667 - val_loss: 93.2615\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 48.9802 - val_loss: 47.9100\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 44.4467 - val_loss: 42.6130\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 39.6063 - val_loss: 40.4749\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 36.7650 - val_loss: 37.9837\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 35.7505 - val_loss: 35.7621\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 32.5467 - val_loss: 33.3153\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 29.6155 - val_loss: 33.9647\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 29.9208 - val_loss: 32.1155\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 26.5689 - val_loss: 29.8367\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 25.2233 - val_loss: 25.3741\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 23.3568 - val_loss: 39.1374\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 24.6248 - val_loss: 28.5857\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 22.6158 - val_loss: 23.2372\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.5663 - val_loss: 24.0999\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 18.5448 - val_loss: 17.4519\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 18.5122 - val_loss: 16.8601\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 17.5757 - val_loss: 18.8109\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 15.4452 - val_loss: 21.6519\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 14.2364 - val_loss: 13.9359\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.2920 - val_loss: 14.5438\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.2294 - val_loss: 18.4383\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 13.4436 - val_loss: 12.1817\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 12.6916 - val_loss: 14.2588\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.7110 - val_loss: 11.4063\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.9358 - val_loss: 12.2470\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.5898 - val_loss: 9.2747\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 8.6998 - val_loss: 10.9883\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 10.7649 - val_loss: 12.5528\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 10.7450 - val_loss: 8.1643\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 8.5675 - val_loss: 7.9801\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.9563 - val_loss: 8.1671\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 7.9312 - val_loss: 7.4542\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 7.0523 - val_loss: 8.8299\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 8.4079 - val_loss: 7.5179\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 5.6760 - val_loss: 6.5681\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 7.5104 - val_loss: 7.5981\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 4.8236 - val_loss: 6.8093\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 7.0410 - val_loss: 6.5112\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 6.0942 - val_loss: 5.2663\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 4.7508 - val_loss: 5.8672\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 5.9567 - val_loss: 6.7203\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 4.3009 - val_loss: 4.3740\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1244 - val_loss: 5.3724\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.9637 - val_loss: 3.9630\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 5.7150 - val_loss: 7.2634\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.4616 - val_loss: 3.4790\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 4.8762 - val_loss: 4.9608\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.2428 - val_loss: 3.5678\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.2380 - val_loss: 3.3474\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3.3835 - val_loss: 2.7437\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 5.3068 - val_loss: 3.9509\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2.9854 - val_loss: 3.4688\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3.0906 - val_loss: 4.1835\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.2498 - val_loss: 2.6535\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 6.5763 - val_loss: 22.4767\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.2192 - val_loss: 2.5320\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2.2932 - val_loss: 3.7734\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2220 - val_loss: 3.5159\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3.4967 - val_loss: 2.1912\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2.1327 - val_loss: 3.1073\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9075 - val_loss: 4.1366\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9747 - val_loss: 2.1291\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3.6170 - val_loss: 2.9592\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2.4836 - val_loss: 2.0307\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6031 - val_loss: 2.2316\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.9409 - val_loss: 1.7537\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 3.9041 - val_loss: 2.5759\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.6879 - val_loss: 2.1733\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.5228 - val_loss: 2.5635\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4462 - val_loss: 1.7084\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 2.1150 - val_loss: 2.2923\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 4.0063 - val_loss: 2.5720\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.6031 - val_loss: 1.3610\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7944 - val_loss: 2.2081\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5091 - val_loss: 1.9614\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7669 - val_loss: 4.8090\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5906 - val_loss: 1.7745\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1935 - val_loss: 1.7859\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3259 - val_loss: 1.2850\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9752 - val_loss: 1.9266\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2362 - val_loss: 1.6712\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3037 - val_loss: 1.8778\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.3245 - val_loss: 4.0629\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4584 - val_loss: 1.1748\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8847 - val_loss: 0.9450\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7300 - val_loss: 0.7903\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0221 - val_loss: 1.1074\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1371 - val_loss: 2.0799\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4175 - val_loss: 1.1138\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 1.1137772636136816\n",
      "Mean Absolute Error (MAE): 0.7898916499264484\n",
      "Root Mean Squared Error (RMSE): 1.0553564628189291\n",
      "Time taken: 3661.4439215660095\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 1358.4365 - val_loss: 1250.8331\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1202.5083 - val_loss: 1155.9220\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1119.7156 - val_loss: 1086.0787\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1057.5898 - val_loss: 1033.1731\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1011.3044 - val_loss: 994.6372\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 977.5389 - val_loss: 966.6028\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 953.6476 - val_loss: 948.1426\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 938.2027 - val_loss: 936.6080\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 928.6747 - val_loss: 929.8042\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 923.1830 - val_loss: 926.2191\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 920.4324 - val_loss: 924.7256\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.0793 - val_loss: 923.9532\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.5076 - val_loss: 923.8192\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2789 - val_loss: 923.8073\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2151 - val_loss: 923.7832\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2486 - val_loss: 923.7568\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2251 - val_loss: 923.8253\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2327 - val_loss: 923.8327\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2535 - val_loss: 923.7411\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1831 - val_loss: 923.7925\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1889 - val_loss: 923.8027\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2473 - val_loss: 923.7414\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2082 - val_loss: 923.7907\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2393 - val_loss: 923.7412\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2059 - val_loss: 923.7900\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1827 - val_loss: 923.7485\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2186 - val_loss: 923.7779\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2004 - val_loss: 923.7468\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2089 - val_loss: 923.7484\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1890 - val_loss: 923.7675\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2021 - val_loss: 923.7863\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2034 - val_loss: 923.7762\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2077 - val_loss: 923.7997\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1998 - val_loss: 923.7683\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1646 - val_loss: 923.8174\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2286 - val_loss: 923.7784\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2067 - val_loss: 923.7090\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2152 - val_loss: 923.7100\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2179 - val_loss: 923.7335\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1730 - val_loss: 923.7593\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2296 - val_loss: 923.7631\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2119 - val_loss: 923.7635\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1987 - val_loss: 923.8085\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2046 - val_loss: 923.7036\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2036 - val_loss: 923.7613\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1891 - val_loss: 923.7766\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2552 - val_loss: 923.8238\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2210 - val_loss: 923.8313\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2123 - val_loss: 923.8856\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1958 - val_loss: 923.8307\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2404 - val_loss: 923.8088\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2399 - val_loss: 923.8026\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1870 - val_loss: 923.7853\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.2481 - val_loss: 923.7569\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.1938 - val_loss: 923.8700\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 907.3522 - val_loss: 887.9545\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 871.3123 - val_loss: 866.5303\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 853.1152 - val_loss: 852.2661\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 838.5909 - val_loss: 840.0046\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 827.0438 - val_loss: 830.5424\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 817.1386 - val_loss: 820.5554\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 806.3102 - val_loss: 813.3540\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 794.6860 - val_loss: 800.5109\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 776.5854 - val_loss: 767.8163\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 761.6183 - val_loss: 760.2021\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 740.6491 - val_loss: 734.8260\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 716.1548 - val_loss: 713.7620\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 696.9245 - val_loss: 697.5085\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 675.7038 - val_loss: 672.5377\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 672.3413 - val_loss: 673.1100\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 668.7038 - val_loss: 736.2540\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 672.8098 - val_loss: 627.3796\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 610.4544 - val_loss: 606.6073\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 593.5336 - val_loss: 588.0002\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 570.5919 - val_loss: 563.1181\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 553.3000 - val_loss: 570.5801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 526.2625 - val_loss: 519.0414\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 528.0695 - val_loss: 767.2874\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 665.1414 - val_loss: 617.1722\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 498.5055 - val_loss: 478.0634\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 463.7644 - val_loss: 456.9340\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 444.4158 - val_loss: 439.9793\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 424.9982 - val_loss: 418.8586\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 407.3493 - val_loss: 398.2010\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 389.2022 - val_loss: 381.1042\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 373.5776 - val_loss: 366.6501\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 357.1581 - val_loss: 351.5724\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 338.5739 - val_loss: 330.8404\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 320.6483 - val_loss: 312.6384\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 304.2883 - val_loss: 295.3178\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 288.9413 - val_loss: 279.8348\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 273.1847 - val_loss: 263.9359\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 253.9601 - val_loss: 243.0754\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 237.4783 - val_loss: 234.6356\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 226.0015 - val_loss: 213.2825\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 210.9770 - val_loss: 203.2731\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 198.3184 - val_loss: 185.2154\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 185.5234 - val_loss: 174.5077\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 166.7231 - val_loss: 156.5932\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 153.3628 - val_loss: 145.0004\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 140.7221 - val_loss: 142.7431\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 128.4872 - val_loss: 133.2614\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 119.6585 - val_loss: 110.4666\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 108.2079 - val_loss: 104.0711\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 99.6802 - val_loss: 95.7241\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 93.1979 - val_loss: 88.6463\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 84.4553 - val_loss: 77.5707\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 78.6608 - val_loss: 78.3780\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 73.6627 - val_loss: 69.7239\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 69.1051 - val_loss: 66.2077\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 62.0808 - val_loss: 64.3088\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 63.0193 - val_loss: 53.7994\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 54.2035 - val_loss: 54.8294\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 50.5326 - val_loss: 49.8661\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 47.3667 - val_loss: 42.7059\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 45.6987 - val_loss: 41.0039\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 39.1081 - val_loss: 37.7512\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 36.3534 - val_loss: 35.4438\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 37.3655 - val_loss: 32.2925\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 31.4521 - val_loss: 31.1319\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 30.1242 - val_loss: 37.6948\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 29.7493 - val_loss: 26.0324\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 25.4824 - val_loss: 28.6874\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 25.0482 - val_loss: 22.5049\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 30.4649 - val_loss: 21.8813\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 20.9926 - val_loss: 24.6412\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.7204 - val_loss: 20.7806\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.7528 - val_loss: 18.5360\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 17.4341 - val_loss: 17.0531\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 18.8636 - val_loss: 19.4929\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 16.5147 - val_loss: 19.4024\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 16.9329 - val_loss: 16.9797\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.5217 - val_loss: 14.1478\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.9598 - val_loss: 16.7634\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.8651 - val_loss: 13.3600\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.1188 - val_loss: 14.8097\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.0474 - val_loss: 13.3698\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4815 - val_loss: 11.2224\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.3693 - val_loss: 11.8899\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.1720 - val_loss: 9.0943\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.9500 - val_loss: 13.4558\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.3282 - val_loss: 8.6220\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.5537 - val_loss: 8.3553\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.6445 - val_loss: 8.3872\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.4505 - val_loss: 8.9361\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.0009 - val_loss: 9.0412\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.7526 - val_loss: 9.5922\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.4884 - val_loss: 7.1049\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.9794 - val_loss: 12.1474\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.0190 - val_loss: 6.4123\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.0341 - val_loss: 7.6968\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.5264 - val_loss: 5.8334\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 6.4023 - val_loss: 6.0060\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.3189 - val_loss: 5.8925\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.4967 - val_loss: 7.1763\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.0159 - val_loss: 4.6245\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.9299 - val_loss: 6.3568\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.4929 - val_loss: 4.7350\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.7166 - val_loss: 16.0343\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.3377 - val_loss: 4.4435\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6385 - val_loss: 4.0842\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.4285 - val_loss: 7.1732\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.7741 - val_loss: 4.1015\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.0457 - val_loss: 4.2522\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.7556 - val_loss: 3.8454\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.3651 - val_loss: 3.2415\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.3961 - val_loss: 8.5866\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1425 - val_loss: 3.4696\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8775 - val_loss: 2.6684\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6445 - val_loss: 3.5664\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.3482 - val_loss: 2.7897\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.1189 - val_loss: 24.3039\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.4179 - val_loss: 2.8712\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1878 - val_loss: 2.6611\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1423 - val_loss: 2.2400\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3520 - val_loss: 4.0241\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.1826 - val_loss: 2.1055\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4202 - val_loss: 4.6873\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.5496 - val_loss: 2.3874\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.8769 - val_loss: 4.5579\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1213 - val_loss: 1.7851\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9069 - val_loss: 3.9628\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8605 - val_loss: 2.5754\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.1506 - val_loss: 2.5006\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8615 - val_loss: 1.8890\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3664 - val_loss: 1.5767\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8322 - val_loss: 1.8578\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.7522 - val_loss: 2.8973\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6078 - val_loss: 1.4651\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8156 - val_loss: 1.6114\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5848 - val_loss: 1.5817\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.9682 - val_loss: 4.5533\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6400 - val_loss: 1.9180\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1848 - val_loss: 1.4980\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8195 - val_loss: 2.4735\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9840 - val_loss: 10.5437\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.0538 - val_loss: 1.0661\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2376 - val_loss: 1.1565\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9367 - val_loss: 0.8820\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3254 - val_loss: 2.6150\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 2.6150050539901444\n",
      "Mean Absolute Error (MAE): 1.249604816404544\n",
      "Root Mean Squared Error (RMSE): 1.617097725553451\n",
      "Time taken: 3836.1366889476776\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 1367.5070 - val_loss: 1269.4797\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1207.9045 - val_loss: 1174.0580\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1122.4766 - val_loss: 1103.0979\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1057.9871 - val_loss: 1050.6742\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1010.4407 - val_loss: 1012.4914\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 975.7283 - val_loss: 985.4597\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 951.2578 - val_loss: 967.1611\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 934.9698 - val_loss: 955.7898\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 924.6747 - val_loss: 949.1152\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 918.6977 - val_loss: 945.7473\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 915.4980 - val_loss: 944.3474\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.9974 - val_loss: 943.9748\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.4442 - val_loss: 943.9162\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1455 - val_loss: 944.0140\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0721 - val_loss: 944.0570\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1270 - val_loss: 944.1609\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0688 - val_loss: 944.1833\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0604 - val_loss: 944.1430\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0537 - val_loss: 944.1482\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0654 - val_loss: 944.1648\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0587 - val_loss: 944.1377\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0723 - val_loss: 944.1857\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1006 - val_loss: 944.1931\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0729 - val_loss: 944.1829\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0335 - val_loss: 944.1902\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0563 - val_loss: 944.1706\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0561 - val_loss: 944.1597\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0542 - val_loss: 944.1879\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0543 - val_loss: 944.1830\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0665 - val_loss: 944.2050\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0980 - val_loss: 944.2052\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0740 - val_loss: 944.2261\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0739 - val_loss: 944.1923\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0662 - val_loss: 944.2661\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0780 - val_loss: 944.1960\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0619 - val_loss: 944.2452\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0568 - val_loss: 944.2164\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1132 - val_loss: 944.1909\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0313 - val_loss: 944.2025\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0470 - val_loss: 944.1673\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0656 - val_loss: 944.1938\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1136 - val_loss: 944.1537\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1049 - val_loss: 944.1681\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.0829 - val_loss: 944.1901\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 913.1653 - val_loss: 943.8019\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 906.1019 - val_loss: 913.0376\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 876.0715 - val_loss: 887.8239\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 857.6526 - val_loss: 873.7094\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 843.9866 - val_loss: 862.7079\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 833.5782 - val_loss: 852.6893\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 824.9152 - val_loss: 848.0106\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 814.8223 - val_loss: 826.5611\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 790.4785 - val_loss: 806.1880\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 763.2636 - val_loss: 769.2361\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 747.5471 - val_loss: 752.6060\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 719.0009 - val_loss: 726.2275\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 697.2059 - val_loss: 699.4799\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 670.3892 - val_loss: 676.6493\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 652.7458 - val_loss: 653.1547\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 625.1664 - val_loss: 627.7319\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 643.9075 - val_loss: 624.9788\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 593.9008 - val_loss: 600.8394\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 607.7462 - val_loss: 788.2001\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 742.9291 - val_loss: 757.5270\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 724.3038 - val_loss: 745.3525\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 710.8824 - val_loss: 733.3922\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 699.2847 - val_loss: 721.9628\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 686.1021 - val_loss: 705.7910\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 664.1771 - val_loss: 675.1523\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 629.9224 - val_loss: 636.1018\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 595.4063 - val_loss: 601.7678\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 564.9790 - val_loss: 574.9468\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 542.1971 - val_loss: 554.8280\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 518.8095 - val_loss: 526.6321\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 494.5530 - val_loss: 501.3055\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 470.5863 - val_loss: 477.3349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 449.5161 - val_loss: 454.8531\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 428.4187 - val_loss: 435.0010\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 409.5076 - val_loss: 416.3365\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 390.1475 - val_loss: 404.4913\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 366.8056 - val_loss: 370.7063\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 346.9030 - val_loss: 350.3392\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 326.4484 - val_loss: 329.2901\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 307.5497 - val_loss: 309.5032\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 291.9766 - val_loss: 291.0625\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 275.1733 - val_loss: 275.4058\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 257.1421 - val_loss: 261.1504\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 243.0829 - val_loss: 244.4283\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 228.3398 - val_loss: 224.3970\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 209.2910 - val_loss: 213.0342\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 196.5106 - val_loss: 198.6496\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 186.1631 - val_loss: 188.5624\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 175.7381 - val_loss: 182.2463\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 166.0656 - val_loss: 167.5254\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 156.2362 - val_loss: 158.7648\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 147.9036 - val_loss: 147.4597\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 136.9960 - val_loss: 137.1458\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 131.2867 - val_loss: 132.3102\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 123.1680 - val_loss: 122.4740\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 116.9316 - val_loss: 129.5838\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 108.1935 - val_loss: 116.0519\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 105.0833 - val_loss: 103.5833\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 96.9615 - val_loss: 96.0430\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 92.6296 - val_loss: 93.9853\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 85.6448 - val_loss: 83.8500\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 78.1149 - val_loss: 77.6914\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 73.7048 - val_loss: 70.3713\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 67.9743 - val_loss: 67.7286\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 63.5975 - val_loss: 67.7485\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 57.4612 - val_loss: 60.3756\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 56.1949 - val_loss: 53.1064\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 49.7843 - val_loss: 48.5127\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 45.3198 - val_loss: 43.9552\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 42.7711 - val_loss: 39.8633\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 40.9165 - val_loss: 42.8261\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 36.4240 - val_loss: 42.6867\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 34.9343 - val_loss: 33.7719\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 32.9442 - val_loss: 36.8801\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 31.4389 - val_loss: 33.0756\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 27.0749 - val_loss: 26.3332\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 26.5646 - val_loss: 32.3853\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 25.4764 - val_loss: 28.5270\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 23.9758 - val_loss: 25.7163\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 21.2048 - val_loss: 21.5278\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.8084 - val_loss: 20.2897\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.7591 - val_loss: 18.9872\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 17.5629 - val_loss: 16.6237\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.6381 - val_loss: 15.7725\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.9787 - val_loss: 26.1875\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.2249 - val_loss: 13.0268\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.4517 - val_loss: 13.2709\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 12.8632 - val_loss: 13.3045\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 12.5342 - val_loss: 11.6487\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 18.0835 - val_loss: 12.1608\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4307 - val_loss: 9.4928\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.0520 - val_loss: 9.7175\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.5619 - val_loss: 9.2507\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.7757 - val_loss: 11.7979\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.1568 - val_loss: 8.1672\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.8222 - val_loss: 7.7559\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.9581 - val_loss: 7.0270\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.8611 - val_loss: 9.0426\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.9632 - val_loss: 5.5968\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.0596 - val_loss: 6.2898\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.1197 - val_loss: 9.9000\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.2797 - val_loss: 5.2842\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.0795 - val_loss: 5.5022\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.7760 - val_loss: 5.9668\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.3290 - val_loss: 4.4596\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.5713 - val_loss: 4.9153\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.9012 - val_loss: 8.7998\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.0421 - val_loss: 4.0552\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 3.8893 - val_loss: 3.3069\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.7260 - val_loss: 4.5900\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.3256 - val_loss: 9.0999\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.9024 - val_loss: 3.6723\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.5705 - val_loss: 2.8089\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.0782 - val_loss: 5.9662\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.9465 - val_loss: 3.7499\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8602 - val_loss: 3.6850\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8344 - val_loss: 3.5410\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8232 - val_loss: 2.5044\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8001 - val_loss: 2.4751\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.2538 - val_loss: 3.4670\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.1393 - val_loss: 3.8391\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2981 - val_loss: 2.2011\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1783 - val_loss: 2.7333\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.2556 - val_loss: 14.4670\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.0674 - val_loss: 2.0707\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5824 - val_loss: 1.5731\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3612 - val_loss: 1.3477\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7278 - val_loss: 1.3442\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6058 - val_loss: 1.8470\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.0544 - val_loss: 1.8726\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6503 - val_loss: 1.7055\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.1573 - val_loss: 1.4515\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3567 - val_loss: 1.3745\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2532 - val_loss: 1.6865\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9869 - val_loss: 3.6069\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.7771 - val_loss: 1.5495\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2245 - val_loss: 1.1286\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9437 - val_loss: 0.9722\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0672 - val_loss: 1.6731\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2514 - val_loss: 1.1877\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6340 - val_loss: 1.2672\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6103 - val_loss: 0.8735\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0265 - val_loss: 0.7519\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1418 - val_loss: 1.0959\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1802 - val_loss: 1.5717\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.8724 - val_loss: 3.0021\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2751 - val_loss: 0.7765\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7229 - val_loss: 0.7789\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7785 - val_loss: 0.9002\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9515 - val_loss: 0.6910\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8435 - val_loss: 0.7526\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.3842 - val_loss: 1.6522\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0675 - val_loss: 0.8903\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7363 - val_loss: 1.0736\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8309 - val_loss: 0.8891\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8989 - val_loss: 0.5879\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.587948476033279\n",
      "Mean Absolute Error (MAE): 0.5968503311299953\n",
      "Root Mean Squared Error (RMSE): 0.7667779835345294\n",
      "Time taken: 3837.9082210063934\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 1398.5339 - val_loss: 1281.3704\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1237.8833 - val_loss: 1176.6667\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1147.8523 - val_loss: 1095.4958\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1080.5055 - val_loss: 1035.4806\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1030.3809 - val_loss: 990.4733\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 993.6859 - val_loss: 957.6628\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 967.6617 - val_loss: 934.4010\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 949.9825 - val_loss: 918.7692\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 938.6676 - val_loss: 908.7877\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 931.8064 - val_loss: 902.7120\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 928.1504 - val_loss: 899.3755\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 926.3683 - val_loss: 897.7231\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.5877 - val_loss: 896.8197\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.2704 - val_loss: 896.4960\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1617 - val_loss: 896.2419\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0989 - val_loss: 896.1748\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1184 - val_loss: 896.0873\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0740 - val_loss: 896.0666\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0414 - val_loss: 896.0132\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0814 - val_loss: 896.0452\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0935 - val_loss: 895.9786\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0923 - val_loss: 896.0346\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0833 - val_loss: 896.0035\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1035 - val_loss: 896.0758\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0640 - val_loss: 896.1130\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1042 - val_loss: 896.1416\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0758 - val_loss: 896.1072\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0769 - val_loss: 896.0712\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0804 - val_loss: 896.0118\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0596 - val_loss: 896.0400\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0870 - val_loss: 896.0470\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0958 - val_loss: 896.0388\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0679 - val_loss: 896.0898\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1057 - val_loss: 896.0759\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0653 - val_loss: 896.0556\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0905 - val_loss: 896.0916\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0760 - val_loss: 896.0443\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0698 - val_loss: 896.0184\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1133 - val_loss: 896.0406\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0808 - val_loss: 895.9904\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0890 - val_loss: 896.0387\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1422 - val_loss: 896.0659\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0838 - val_loss: 896.0663\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0818 - val_loss: 896.0535\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1019 - val_loss: 896.0315\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0651 - val_loss: 896.0845\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0864 - val_loss: 896.0984\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0816 - val_loss: 896.0862\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1102 - val_loss: 896.0731\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0597 - val_loss: 896.0490\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0850 - val_loss: 896.0272\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0901 - val_loss: 896.0149\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0890 - val_loss: 896.0804\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1155 - val_loss: 896.0437\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0668 - val_loss: 896.0253\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.1014 - val_loss: 896.0195\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.0816 - val_loss: 896.0510\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.7955 - val_loss: 897.4838\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.2430 - val_loss: 896.0063\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 917.8146 - val_loss: 870.6033\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 892.7289 - val_loss: 862.7289\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 873.1747 - val_loss: 828.5955\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 845.7642 - val_loss: 806.6887\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 826.1241 - val_loss: 787.8007\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 808.3581 - val_loss: 770.2473\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 786.5069 - val_loss: 748.0920\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 760.7772 - val_loss: 722.3279\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 733.8979 - val_loss: 696.4482\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 706.2249 - val_loss: 667.6545\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 678.6587 - val_loss: 640.4044\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 650.4313 - val_loss: 613.4801\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 620.7048 - val_loss: 584.2944\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 592.5414 - val_loss: 558.9113\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 566.1824 - val_loss: 534.2919\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 540.1130 - val_loss: 509.0544\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 515.5630 - val_loss: 483.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 491.2227 - val_loss: 463.4274\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 466.6842 - val_loss: 435.9052\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 443.0758 - val_loss: 415.2494\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 421.7971 - val_loss: 396.2349\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 399.4550 - val_loss: 370.7495\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 378.1884 - val_loss: 355.2572\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 359.0506 - val_loss: 336.7847\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 341.8721 - val_loss: 337.7970\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 323.2394 - val_loss: 303.8422\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 303.8646 - val_loss: 284.3910\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 288.2237 - val_loss: 269.2936\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 270.9390 - val_loss: 249.4759\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 253.4398 - val_loss: 239.9360\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 235.9888 - val_loss: 217.4873\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 221.1418 - val_loss: 208.3829\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 205.3491 - val_loss: 196.1426\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 189.8692 - val_loss: 175.8107\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 175.9522 - val_loss: 163.7688\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 164.1698 - val_loss: 151.4841\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 151.8229 - val_loss: 142.8664\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 139.9789 - val_loss: 128.8960\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 128.0632 - val_loss: 119.2391\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 118.3250 - val_loss: 112.7730\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 109.7347 - val_loss: 102.1822\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 100.4319 - val_loss: 93.4847\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 91.7007 - val_loss: 87.4302\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 84.3865 - val_loss: 83.2849\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 77.7784 - val_loss: 74.9095\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 72.3694 - val_loss: 70.6885\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 66.6920 - val_loss: 63.7539\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 61.3479 - val_loss: 59.9722\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 57.5970 - val_loss: 54.9729\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 52.6357 - val_loss: 53.7246\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 54.3336 - val_loss: 51.9647\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 48.5905 - val_loss: 46.9662\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 45.4000 - val_loss: 47.8329\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 43.7098 - val_loss: 42.3967\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 40.9522 - val_loss: 48.4493\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 38.1063 - val_loss: 37.4934\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 35.6553 - val_loss: 36.0810\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 34.0928 - val_loss: 32.6925\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 32.3014 - val_loss: 33.0399\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 29.5761 - val_loss: 30.7222\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 28.8291 - val_loss: 30.0604\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 27.3324 - val_loss: 27.1077\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 25.9079 - val_loss: 25.9383\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 24.7580 - val_loss: 24.9636\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 23.3854 - val_loss: 22.2248\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 23.1225 - val_loss: 23.0737\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 20.6431 - val_loss: 23.8815\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 20.8277 - val_loss: 21.5492\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 19.7714 - val_loss: 18.7090\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 23.3120 - val_loss: 20.4420\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 17.5747 - val_loss: 18.4515\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 18.2098 - val_loss: 16.4166\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 16.6410 - val_loss: 19.7585\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 16.6763 - val_loss: 18.8499\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.7499 - val_loss: 18.0083\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 15.6232 - val_loss: 14.1152\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.6239 - val_loss: 13.7584\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.1085 - val_loss: 14.8563\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.7805 - val_loss: 12.4186\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 12.3850 - val_loss: 14.2106\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 12.2793 - val_loss: 12.5138\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.8237 - val_loss: 11.8097\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.5115 - val_loss: 11.1011\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.9029 - val_loss: 10.4894\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.9872 - val_loss: 11.5056\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4233 - val_loss: 10.5974\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4332 - val_loss: 10.5591\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4009 - val_loss: 9.1313\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.8361 - val_loss: 8.9769\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.3734 - val_loss: 10.3795\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.4445 - val_loss: 8.3742\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.9129 - val_loss: 8.1518\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.1294 - val_loss: 8.6309\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 8.0368 - val_loss: 7.3769\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.2080 - val_loss: 8.5544\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.2840 - val_loss: 7.4918\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.5623 - val_loss: 7.5408\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.7034 - val_loss: 6.4517\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.4059 - val_loss: 6.3557\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.1361 - val_loss: 6.7273\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.0279 - val_loss: 6.5606\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.1674 - val_loss: 5.4833\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1762 - val_loss: 7.4143\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.3326 - val_loss: 7.1021\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1652 - val_loss: 6.4550\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.2204 - val_loss: 4.3371\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.5034 - val_loss: 4.6773\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.4772 - val_loss: 9.4091\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1621 - val_loss: 4.5909\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.3422 - val_loss: 5.5783\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.4203 - val_loss: 5.7757\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.5830 - val_loss: 6.5642\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.8295 - val_loss: 6.7854\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.9110 - val_loss: 4.5881\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6140 - val_loss: 3.5671\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.5951 - val_loss: 4.2241\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.4335 - val_loss: 3.2273\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6764 - val_loss: 3.6911\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.4957 - val_loss: 9.9827\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6958 - val_loss: 3.3581\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8288 - val_loss: 2.8986\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6599 - val_loss: 2.6295\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9959 - val_loss: 3.3068\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9453 - val_loss: 3.2485\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.0609 - val_loss: 4.8077\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8441 - val_loss: 3.0015\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.4990 - val_loss: 12.1537\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.5398 - val_loss: 2.7692\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.5396 - val_loss: 2.4541\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3749 - val_loss: 2.5819\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9904 - val_loss: 3.0570\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.1534 - val_loss: 2.5187\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4174 - val_loss: 2.4200\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4207 - val_loss: 2.3886\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2321 - val_loss: 2.1987\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8922 - val_loss: 2.5917\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3759 - val_loss: 3.4708\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4647 - val_loss: 2.3807\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8527 - val_loss: 2.7884\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.0010 - val_loss: 2.0207\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8731 - val_loss: 1.8576\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 1.857634724429845\n",
      "Mean Absolute Error (MAE): 1.02018796281685\n",
      "Root Mean Squared Error (RMSE): 1.362950741747421\n",
      "Time taken: 3819.470855474472\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 1375.1873 - val_loss: 1271.3191\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1211.6782 - val_loss: 1168.9277\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1125.5868 - val_loss: 1094.3733\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1062.0850 - val_loss: 1038.7960\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1014.9757 - val_loss: 997.7516\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 980.5829 - val_loss: 968.3839\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 956.5783 - val_loss: 948.2197\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 940.5510 - val_loss: 934.9979\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 930.5121 - val_loss: 927.0050\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 924.7693 - val_loss: 922.6378\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.7653 - val_loss: 920.4176\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 920.3109 - val_loss: 919.4320\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.7692 - val_loss: 919.0543\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.6051 - val_loss: 918.8906\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.5228 - val_loss: 918.8103\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4654 - val_loss: 918.7426\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4368 - val_loss: 918.6701\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4801 - val_loss: 918.6830\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4802 - val_loss: 918.6942\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4417 - val_loss: 918.6760\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4250 - val_loss: 918.6658\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4458 - val_loss: 918.6969\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 919.4700 - val_loss: 918.6764\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4706 - val_loss: 918.6945\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4594 - val_loss: 918.6885\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4448 - val_loss: 918.6436\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4960 - val_loss: 918.6667\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4412 - val_loss: 918.6752\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4686 - val_loss: 918.6762\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.5082 - val_loss: 918.6479\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4987 - val_loss: 918.6909\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4622 - val_loss: 918.6525\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4435 - val_loss: 918.6775\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4860 - val_loss: 918.6921\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4302 - val_loss: 918.6544\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4566 - val_loss: 918.6916\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4488 - val_loss: 918.6983\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4727 - val_loss: 918.7678\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4405 - val_loss: 918.6709\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4475 - val_loss: 918.7000\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4373 - val_loss: 918.6642\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4645 - val_loss: 918.6791\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4496 - val_loss: 918.7054\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4465 - val_loss: 918.6943\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4681 - val_loss: 918.7546\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4337 - val_loss: 918.7023\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4577 - val_loss: 918.6613\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4661 - val_loss: 918.6496\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4506 - val_loss: 918.7137\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4568 - val_loss: 918.6686\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4677 - val_loss: 918.7115\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4587 - val_loss: 918.6425\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4301 - val_loss: 918.7136\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4559 - val_loss: 918.7347\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4438 - val_loss: 918.7126\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4409 - val_loss: 918.6947\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4402 - val_loss: 918.7365\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4273 - val_loss: 918.7186\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4393 - val_loss: 918.7269\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4689 - val_loss: 918.7200\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.4259 - val_loss: 918.7447\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 920.1960 - val_loss: 919.7154\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 909.2097 - val_loss: 887.5392\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 868.9345 - val_loss: 860.4216\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 849.0992 - val_loss: 839.6533\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 831.4656 - val_loss: 822.7182\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 816.4194 - val_loss: 806.9198\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 800.3813 - val_loss: 794.4125\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 785.3442 - val_loss: 777.0250\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 769.3645 - val_loss: 760.0647\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 756.3514 - val_loss: 745.8402\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 739.7405 - val_loss: 730.0253\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 727.1996 - val_loss: 721.8163\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 714.8876 - val_loss: 710.1102\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 704.3733 - val_loss: 696.6220\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 694.4745 - val_loss: 684.0984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 684.0839 - val_loss: 671.6426\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 669.2363 - val_loss: 658.9782\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 652.1522 - val_loss: 637.0933\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 632.5976 - val_loss: 618.0079\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 610.4632 - val_loss: 590.3521\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 578.1442 - val_loss: 557.3210\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 538.4097 - val_loss: 520.6433\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 503.5008 - val_loss: 480.4185\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 469.9556 - val_loss: 461.0714\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 440.2282 - val_loss: 420.2589\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 411.4620 - val_loss: 391.9613\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 382.0748 - val_loss: 363.4670\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 354.4753 - val_loss: 338.4178\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 329.1819 - val_loss: 320.8787\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 306.8857 - val_loss: 289.6833\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 282.5928 - val_loss: 275.9112\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 259.5506 - val_loss: 245.2006\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 240.2228 - val_loss: 225.5877\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 219.9583 - val_loss: 209.7250\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 201.8177 - val_loss: 188.4040\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 184.7615 - val_loss: 171.4984\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 168.6327 - val_loss: 157.0757\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 153.7790 - val_loss: 149.7809\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 141.0658 - val_loss: 130.2778\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 126.5934 - val_loss: 119.1626\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 114.4186 - val_loss: 104.3650\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 101.7001 - val_loss: 92.7836\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 91.1821 - val_loss: 85.6302\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 80.9174 - val_loss: 77.6596\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 72.5029 - val_loss: 66.9920\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 63.9914 - val_loss: 61.5673\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 56.8180 - val_loss: 52.3393\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 53.2439 - val_loss: 54.6172\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 44.6369 - val_loss: 40.8435\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 39.1673 - val_loss: 37.3686\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 36.1206 - val_loss: 31.8067\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 31.5906 - val_loss: 29.0074\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 27.7111 - val_loss: 24.3447\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 24.0638 - val_loss: 23.4260\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 21.5746 - val_loss: 21.2695\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 21.1295 - val_loss: 22.4277\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 17.4579 - val_loss: 15.4869\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 15.3189 - val_loss: 14.2743\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.4168 - val_loss: 19.7525\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 13.0686 - val_loss: 11.3472\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.1206 - val_loss: 9.7118\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.7433 - val_loss: 9.4083\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.2280 - val_loss: 9.1560\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.8355 - val_loss: 8.1165\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.3616 - val_loss: 8.2137\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.2998 - val_loss: 6.7987\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.4019 - val_loss: 12.4017\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.1970 - val_loss: 6.5837\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.8319 - val_loss: 5.3211\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.1151 - val_loss: 5.2922\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.5101 - val_loss: 7.8356\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.8998 - val_loss: 5.1989\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.7470 - val_loss: 3.7498\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.2407 - val_loss: 3.7635\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 6.8019 - val_loss: 3.3963\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.3669 - val_loss: 3.4315\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.7388 - val_loss: 4.1159\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6181 - val_loss: 3.3623\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.0942 - val_loss: 4.8405\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.4637 - val_loss: 3.8561\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.0002 - val_loss: 2.6028\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.1804 - val_loss: 4.3603\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6517 - val_loss: 2.6665\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3336 - val_loss: 2.1327\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4791 - val_loss: 2.6006\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.7003 - val_loss: 4.3405\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.0847 - val_loss: 1.4437\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2296 - val_loss: 1.8355\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7616 - val_loss: 1.6616\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9498 - val_loss: 5.2656\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4170 - val_loss: 1.6536\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1837 - val_loss: 2.2420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.8002 - val_loss: 1.5806\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9668 - val_loss: 27.1090\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.5180 - val_loss: 2.2682\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4994 - val_loss: 1.4534\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4989 - val_loss: 1.8302\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1987 - val_loss: 1.3984\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2790 - val_loss: 2.0533\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5648 - val_loss: 1.2699\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1315 - val_loss: 1.1995\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2308 - val_loss: 1.2921\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2680 - val_loss: 1.5559\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9812 - val_loss: 1.7062\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3422 - val_loss: 0.9406\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2539 - val_loss: 1.9316\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6765 - val_loss: 3.2166\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 8.2923 - val_loss: 1.5656\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0210 - val_loss: 0.9172\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7766 - val_loss: 0.7681\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7157 - val_loss: 0.7301\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6875 - val_loss: 0.8088\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0766 - val_loss: 0.7887\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0943 - val_loss: 1.3037\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3374 - val_loss: 1.2844\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8992 - val_loss: 0.8910\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7778 - val_loss: 0.8929\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.0321 - val_loss: 1.2325\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6714 - val_loss: 0.6082\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5061 - val_loss: 0.5216\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5961 - val_loss: 0.8407\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7092 - val_loss: 0.8411\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4016 - val_loss: 1.7359\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8532 - val_loss: 0.5289\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5917 - val_loss: 0.7703\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5815 - val_loss: 5.4848\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.5707 - val_loss: 0.7852\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5423 - val_loss: 0.6276\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4910 - val_loss: 0.8004\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6227 - val_loss: 0.6110\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5806 - val_loss: 0.6468\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9827 - val_loss: 1.2722\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1851 - val_loss: 0.5492\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7444 - val_loss: 1.0472\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7565 - val_loss: 0.4476\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.3621 - val_loss: 11.0696\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2123 - val_loss: 0.6109\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4268 - val_loss: 0.5107\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4119 - val_loss: 0.4507\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.45069769069104987\n",
      "Mean Absolute Error (MAE): 0.5222370958844761\n",
      "Root Mean Squared Error (RMSE): 0.6713402197776102\n",
      "Time taken: 3818.947678089142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_7720\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  1.113777  0.789892  1.055356  3661.443922\n",
      "1        2  2.615005  1.249605  1.617098  3836.136689\n",
      "2        3  0.587948  0.596850  0.766778  3837.908221\n",
      "3        4  1.857635  1.020188  1.362951  3819.470855\n",
      "4        5  0.450698  0.522237  0.671340  3818.947678\n",
      "5  Average  1.325013  0.835754  1.094705  3794.781473\n",
      "Results saved to 'DL_Result_PL_model_2_smoothing2_iReg_f_obese.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_smoothing2_iReg_f_obese.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_2_smoothing2_iReg_f_obese.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
