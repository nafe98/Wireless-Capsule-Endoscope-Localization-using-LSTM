{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f_obese.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225.406990</td>\n",
       "      <td>239.008790</td>\n",
       "      <td>158.191094</td>\n",
       "      <td>187.189908</td>\n",
       "      <td>275.467386</td>\n",
       "      <td>281.229012</td>\n",
       "      <td>211.308257</td>\n",
       "      <td>229.358293</td>\n",
       "      <td>201.582876</td>\n",
       "      <td>225.230211</td>\n",
       "      <td>...</td>\n",
       "      <td>169.531659</td>\n",
       "      <td>172.666548</td>\n",
       "      <td>192.453077</td>\n",
       "      <td>187.047372</td>\n",
       "      <td>242.967384</td>\n",
       "      <td>237.534152</td>\n",
       "      <td>198.792993</td>\n",
       "      <td>224.891271</td>\n",
       "      <td>107.486538</td>\n",
       "      <td>148.012190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228.633781</td>\n",
       "      <td>245.151589</td>\n",
       "      <td>156.800392</td>\n",
       "      <td>184.587306</td>\n",
       "      <td>264.466138</td>\n",
       "      <td>277.713108</td>\n",
       "      <td>213.270706</td>\n",
       "      <td>234.364622</td>\n",
       "      <td>207.477214</td>\n",
       "      <td>217.770303</td>\n",
       "      <td>...</td>\n",
       "      <td>166.147794</td>\n",
       "      <td>170.038327</td>\n",
       "      <td>196.866198</td>\n",
       "      <td>187.848953</td>\n",
       "      <td>245.784763</td>\n",
       "      <td>248.073511</td>\n",
       "      <td>198.882069</td>\n",
       "      <td>221.364753</td>\n",
       "      <td>105.383493</td>\n",
       "      <td>139.161556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.153794</td>\n",
       "      <td>249.810077</td>\n",
       "      <td>153.038031</td>\n",
       "      <td>187.365600</td>\n",
       "      <td>261.717666</td>\n",
       "      <td>283.899837</td>\n",
       "      <td>217.474753</td>\n",
       "      <td>225.270731</td>\n",
       "      <td>200.043037</td>\n",
       "      <td>207.928725</td>\n",
       "      <td>...</td>\n",
       "      <td>169.214058</td>\n",
       "      <td>185.371351</td>\n",
       "      <td>193.593967</td>\n",
       "      <td>188.513661</td>\n",
       "      <td>247.270057</td>\n",
       "      <td>237.878404</td>\n",
       "      <td>200.563666</td>\n",
       "      <td>230.747023</td>\n",
       "      <td>107.781494</td>\n",
       "      <td>150.405524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215.946864</td>\n",
       "      <td>243.204526</td>\n",
       "      <td>150.392309</td>\n",
       "      <td>192.410319</td>\n",
       "      <td>265.338779</td>\n",
       "      <td>275.743540</td>\n",
       "      <td>209.657333</td>\n",
       "      <td>236.181002</td>\n",
       "      <td>202.469535</td>\n",
       "      <td>219.065106</td>\n",
       "      <td>...</td>\n",
       "      <td>170.679691</td>\n",
       "      <td>182.101000</td>\n",
       "      <td>195.548369</td>\n",
       "      <td>189.891649</td>\n",
       "      <td>237.982636</td>\n",
       "      <td>252.286560</td>\n",
       "      <td>202.535132</td>\n",
       "      <td>220.895367</td>\n",
       "      <td>110.265810</td>\n",
       "      <td>149.756557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226.184948</td>\n",
       "      <td>249.814628</td>\n",
       "      <td>162.709835</td>\n",
       "      <td>186.328688</td>\n",
       "      <td>264.637581</td>\n",
       "      <td>277.555411</td>\n",
       "      <td>218.032813</td>\n",
       "      <td>232.644672</td>\n",
       "      <td>209.252621</td>\n",
       "      <td>215.162594</td>\n",
       "      <td>...</td>\n",
       "      <td>175.716059</td>\n",
       "      <td>171.187222</td>\n",
       "      <td>200.664477</td>\n",
       "      <td>184.740323</td>\n",
       "      <td>243.157506</td>\n",
       "      <td>239.385167</td>\n",
       "      <td>198.765299</td>\n",
       "      <td>224.987574</td>\n",
       "      <td>108.316042</td>\n",
       "      <td>147.834524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>244.631935</td>\n",
       "      <td>227.486924</td>\n",
       "      <td>152.860450</td>\n",
       "      <td>141.746816</td>\n",
       "      <td>288.640369</td>\n",
       "      <td>283.967031</td>\n",
       "      <td>230.864109</td>\n",
       "      <td>223.355707</td>\n",
       "      <td>217.206802</td>\n",
       "      <td>213.323681</td>\n",
       "      <td>...</td>\n",
       "      <td>173.571829</td>\n",
       "      <td>193.956862</td>\n",
       "      <td>189.695903</td>\n",
       "      <td>177.222097</td>\n",
       "      <td>259.449935</td>\n",
       "      <td>235.875875</td>\n",
       "      <td>221.579495</td>\n",
       "      <td>208.014293</td>\n",
       "      <td>144.366279</td>\n",
       "      <td>115.315693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>241.439417</td>\n",
       "      <td>227.313179</td>\n",
       "      <td>160.417181</td>\n",
       "      <td>135.108209</td>\n",
       "      <td>292.941557</td>\n",
       "      <td>274.486373</td>\n",
       "      <td>232.084329</td>\n",
       "      <td>213.084474</td>\n",
       "      <td>204.916336</td>\n",
       "      <td>202.388167</td>\n",
       "      <td>...</td>\n",
       "      <td>180.144556</td>\n",
       "      <td>189.998904</td>\n",
       "      <td>192.092449</td>\n",
       "      <td>166.192675</td>\n",
       "      <td>244.962685</td>\n",
       "      <td>247.866220</td>\n",
       "      <td>225.432035</td>\n",
       "      <td>212.803153</td>\n",
       "      <td>145.580688</td>\n",
       "      <td>107.107362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>243.485176</td>\n",
       "      <td>228.839487</td>\n",
       "      <td>167.040019</td>\n",
       "      <td>139.364266</td>\n",
       "      <td>288.030906</td>\n",
       "      <td>276.184358</td>\n",
       "      <td>236.803816</td>\n",
       "      <td>216.808270</td>\n",
       "      <td>216.363399</td>\n",
       "      <td>200.870898</td>\n",
       "      <td>...</td>\n",
       "      <td>171.256528</td>\n",
       "      <td>196.366081</td>\n",
       "      <td>194.788796</td>\n",
       "      <td>171.981077</td>\n",
       "      <td>259.054124</td>\n",
       "      <td>237.921634</td>\n",
       "      <td>215.723724</td>\n",
       "      <td>212.874373</td>\n",
       "      <td>142.529206</td>\n",
       "      <td>107.861245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>238.026661</td>\n",
       "      <td>236.849963</td>\n",
       "      <td>164.718777</td>\n",
       "      <td>143.433579</td>\n",
       "      <td>291.108831</td>\n",
       "      <td>282.412229</td>\n",
       "      <td>240.958178</td>\n",
       "      <td>214.110993</td>\n",
       "      <td>213.230860</td>\n",
       "      <td>189.934369</td>\n",
       "      <td>...</td>\n",
       "      <td>171.995866</td>\n",
       "      <td>194.416994</td>\n",
       "      <td>199.046725</td>\n",
       "      <td>166.954828</td>\n",
       "      <td>253.129921</td>\n",
       "      <td>233.517925</td>\n",
       "      <td>220.636955</td>\n",
       "      <td>216.644809</td>\n",
       "      <td>142.204405</td>\n",
       "      <td>111.443804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.660196</td>\n",
       "      <td>229.393753</td>\n",
       "      <td>165.773564</td>\n",
       "      <td>151.618905</td>\n",
       "      <td>282.858214</td>\n",
       "      <td>280.936428</td>\n",
       "      <td>238.669200</td>\n",
       "      <td>225.183727</td>\n",
       "      <td>204.387444</td>\n",
       "      <td>210.230827</td>\n",
       "      <td>...</td>\n",
       "      <td>176.587346</td>\n",
       "      <td>192.341783</td>\n",
       "      <td>200.027368</td>\n",
       "      <td>171.198233</td>\n",
       "      <td>252.700679</td>\n",
       "      <td>243.504570</td>\n",
       "      <td>226.870974</td>\n",
       "      <td>212.239032</td>\n",
       "      <td>139.960394</td>\n",
       "      <td>115.935347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     225.406990  239.008790  158.191094  187.189908  275.467386  281.229012   \n",
       "1     228.633781  245.151589  156.800392  184.587306  264.466138  277.713108   \n",
       "2     224.153794  249.810077  153.038031  187.365600  261.717666  283.899837   \n",
       "3     215.946864  243.204526  150.392309  192.410319  265.338779  275.743540   \n",
       "4     226.184948  249.814628  162.709835  186.328688  264.637581  277.555411   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  244.631935  227.486924  152.860450  141.746816  288.640369  283.967031   \n",
       "2439  241.439417  227.313179  160.417181  135.108209  292.941557  274.486373   \n",
       "2440  243.485176  228.839487  167.040019  139.364266  288.030906  276.184358   \n",
       "2441  238.026661  236.849963  164.718777  143.433579  291.108831  282.412229   \n",
       "2442  240.660196  229.393753  165.773564  151.618905  282.858214  280.936428   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     211.308257  229.358293  201.582876  225.230211  ...  169.531659   \n",
       "1     213.270706  234.364622  207.477214  217.770303  ...  166.147794   \n",
       "2     217.474753  225.270731  200.043037  207.928725  ...  169.214058   \n",
       "3     209.657333  236.181002  202.469535  219.065106  ...  170.679691   \n",
       "4     218.032813  232.644672  209.252621  215.162594  ...  175.716059   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  230.864109  223.355707  217.206802  213.323681  ...  173.571829   \n",
       "2439  232.084329  213.084474  204.916336  202.388167  ...  180.144556   \n",
       "2440  236.803816  216.808270  216.363399  200.870898  ...  171.256528   \n",
       "2441  240.958178  214.110993  213.230860  189.934369  ...  171.995866   \n",
       "2442  238.669200  225.183727  204.387444  210.230827  ...  176.587346   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     172.666548  192.453077  187.047372  242.967384  237.534152  198.792993   \n",
       "1     170.038327  196.866198  187.848953  245.784763  248.073511  198.882069   \n",
       "2     185.371351  193.593967  188.513661  247.270057  237.878404  200.563666   \n",
       "3     182.101000  195.548369  189.891649  237.982636  252.286560  202.535132   \n",
       "4     171.187222  200.664477  184.740323  243.157506  239.385167  198.765299   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  193.956862  189.695903  177.222097  259.449935  235.875875  221.579495   \n",
       "2439  189.998904  192.092449  166.192675  244.962685  247.866220  225.432035   \n",
       "2440  196.366081  194.788796  171.981077  259.054124  237.921634  215.723724   \n",
       "2441  194.416994  199.046725  166.954828  253.129921  233.517925  220.636955   \n",
       "2442  192.341783  200.027368  171.198233  252.700679  243.504570  226.870974   \n",
       "\n",
       "              45          46          47  \n",
       "0     224.891271  107.486538  148.012190  \n",
       "1     221.364753  105.383493  139.161556  \n",
       "2     230.747023  107.781494  150.405524  \n",
       "3     220.895367  110.265810  149.756557  \n",
       "4     224.987574  108.316042  147.834524  \n",
       "...          ...         ...         ...  \n",
       "2438  208.014293  144.366279  115.315693  \n",
       "2439  212.803153  145.580688  107.107362  \n",
       "2440  212.874373  142.529206  107.861245  \n",
       "2441  216.644809  142.204405  111.443804  \n",
       "2442  212.239032  139.960394  115.935347  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225.406990</td>\n",
       "      <td>239.008790</td>\n",
       "      <td>158.191094</td>\n",
       "      <td>187.189908</td>\n",
       "      <td>275.467386</td>\n",
       "      <td>281.229012</td>\n",
       "      <td>211.308257</td>\n",
       "      <td>229.358293</td>\n",
       "      <td>201.582876</td>\n",
       "      <td>225.230211</td>\n",
       "      <td>...</td>\n",
       "      <td>169.531659</td>\n",
       "      <td>172.666548</td>\n",
       "      <td>192.453077</td>\n",
       "      <td>187.047372</td>\n",
       "      <td>242.967384</td>\n",
       "      <td>237.534152</td>\n",
       "      <td>198.792993</td>\n",
       "      <td>224.891271</td>\n",
       "      <td>107.486538</td>\n",
       "      <td>148.012190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228.633781</td>\n",
       "      <td>245.151589</td>\n",
       "      <td>156.800392</td>\n",
       "      <td>184.587306</td>\n",
       "      <td>264.466138</td>\n",
       "      <td>277.713108</td>\n",
       "      <td>213.270706</td>\n",
       "      <td>234.364622</td>\n",
       "      <td>207.477214</td>\n",
       "      <td>217.770303</td>\n",
       "      <td>...</td>\n",
       "      <td>166.147794</td>\n",
       "      <td>170.038327</td>\n",
       "      <td>196.866198</td>\n",
       "      <td>187.848953</td>\n",
       "      <td>245.784763</td>\n",
       "      <td>248.073511</td>\n",
       "      <td>198.882069</td>\n",
       "      <td>221.364753</td>\n",
       "      <td>105.383493</td>\n",
       "      <td>139.161556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.153794</td>\n",
       "      <td>249.810077</td>\n",
       "      <td>153.038031</td>\n",
       "      <td>187.365600</td>\n",
       "      <td>261.717666</td>\n",
       "      <td>283.899837</td>\n",
       "      <td>217.474753</td>\n",
       "      <td>225.270731</td>\n",
       "      <td>200.043037</td>\n",
       "      <td>207.928725</td>\n",
       "      <td>...</td>\n",
       "      <td>169.214058</td>\n",
       "      <td>185.371351</td>\n",
       "      <td>193.593967</td>\n",
       "      <td>188.513661</td>\n",
       "      <td>247.270057</td>\n",
       "      <td>237.878404</td>\n",
       "      <td>200.563666</td>\n",
       "      <td>230.747023</td>\n",
       "      <td>107.781494</td>\n",
       "      <td>150.405524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215.946864</td>\n",
       "      <td>243.204526</td>\n",
       "      <td>150.392309</td>\n",
       "      <td>192.410319</td>\n",
       "      <td>265.338779</td>\n",
       "      <td>275.743540</td>\n",
       "      <td>209.657333</td>\n",
       "      <td>236.181002</td>\n",
       "      <td>202.469535</td>\n",
       "      <td>219.065106</td>\n",
       "      <td>...</td>\n",
       "      <td>170.679691</td>\n",
       "      <td>182.101000</td>\n",
       "      <td>195.548369</td>\n",
       "      <td>189.891649</td>\n",
       "      <td>237.982636</td>\n",
       "      <td>252.286560</td>\n",
       "      <td>202.535132</td>\n",
       "      <td>220.895367</td>\n",
       "      <td>110.265810</td>\n",
       "      <td>149.756557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226.184948</td>\n",
       "      <td>249.814628</td>\n",
       "      <td>162.709835</td>\n",
       "      <td>186.328688</td>\n",
       "      <td>264.637581</td>\n",
       "      <td>277.555411</td>\n",
       "      <td>218.032813</td>\n",
       "      <td>232.644672</td>\n",
       "      <td>209.252621</td>\n",
       "      <td>215.162594</td>\n",
       "      <td>...</td>\n",
       "      <td>175.716059</td>\n",
       "      <td>171.187222</td>\n",
       "      <td>200.664477</td>\n",
       "      <td>184.740323</td>\n",
       "      <td>243.157506</td>\n",
       "      <td>239.385167</td>\n",
       "      <td>198.765299</td>\n",
       "      <td>224.987574</td>\n",
       "      <td>108.316042</td>\n",
       "      <td>147.834524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>244.631935</td>\n",
       "      <td>227.486924</td>\n",
       "      <td>152.860450</td>\n",
       "      <td>141.746816</td>\n",
       "      <td>288.640369</td>\n",
       "      <td>283.967031</td>\n",
       "      <td>230.864109</td>\n",
       "      <td>223.355707</td>\n",
       "      <td>217.206802</td>\n",
       "      <td>213.323681</td>\n",
       "      <td>...</td>\n",
       "      <td>173.571829</td>\n",
       "      <td>193.956862</td>\n",
       "      <td>189.695903</td>\n",
       "      <td>177.222097</td>\n",
       "      <td>259.449935</td>\n",
       "      <td>235.875875</td>\n",
       "      <td>221.579495</td>\n",
       "      <td>208.014293</td>\n",
       "      <td>144.366279</td>\n",
       "      <td>115.315693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>241.439417</td>\n",
       "      <td>227.313179</td>\n",
       "      <td>160.417181</td>\n",
       "      <td>135.108209</td>\n",
       "      <td>292.941557</td>\n",
       "      <td>274.486373</td>\n",
       "      <td>232.084329</td>\n",
       "      <td>213.084474</td>\n",
       "      <td>204.916336</td>\n",
       "      <td>202.388167</td>\n",
       "      <td>...</td>\n",
       "      <td>180.144556</td>\n",
       "      <td>189.998904</td>\n",
       "      <td>192.092449</td>\n",
       "      <td>166.192675</td>\n",
       "      <td>244.962685</td>\n",
       "      <td>247.866220</td>\n",
       "      <td>225.432035</td>\n",
       "      <td>212.803153</td>\n",
       "      <td>145.580688</td>\n",
       "      <td>107.107362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>243.485176</td>\n",
       "      <td>228.839487</td>\n",
       "      <td>167.040019</td>\n",
       "      <td>139.364266</td>\n",
       "      <td>288.030906</td>\n",
       "      <td>276.184358</td>\n",
       "      <td>236.803816</td>\n",
       "      <td>216.808270</td>\n",
       "      <td>216.363399</td>\n",
       "      <td>200.870898</td>\n",
       "      <td>...</td>\n",
       "      <td>171.256528</td>\n",
       "      <td>196.366081</td>\n",
       "      <td>194.788796</td>\n",
       "      <td>171.981077</td>\n",
       "      <td>259.054124</td>\n",
       "      <td>237.921634</td>\n",
       "      <td>215.723724</td>\n",
       "      <td>212.874373</td>\n",
       "      <td>142.529206</td>\n",
       "      <td>107.861245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>238.026661</td>\n",
       "      <td>236.849963</td>\n",
       "      <td>164.718777</td>\n",
       "      <td>143.433579</td>\n",
       "      <td>291.108831</td>\n",
       "      <td>282.412229</td>\n",
       "      <td>240.958178</td>\n",
       "      <td>214.110993</td>\n",
       "      <td>213.230860</td>\n",
       "      <td>189.934369</td>\n",
       "      <td>...</td>\n",
       "      <td>171.995866</td>\n",
       "      <td>194.416994</td>\n",
       "      <td>199.046725</td>\n",
       "      <td>166.954828</td>\n",
       "      <td>253.129921</td>\n",
       "      <td>233.517925</td>\n",
       "      <td>220.636955</td>\n",
       "      <td>216.644809</td>\n",
       "      <td>142.204405</td>\n",
       "      <td>111.443804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.660196</td>\n",
       "      <td>229.393753</td>\n",
       "      <td>165.773564</td>\n",
       "      <td>151.618905</td>\n",
       "      <td>282.858214</td>\n",
       "      <td>280.936428</td>\n",
       "      <td>238.669200</td>\n",
       "      <td>225.183727</td>\n",
       "      <td>204.387444</td>\n",
       "      <td>210.230827</td>\n",
       "      <td>...</td>\n",
       "      <td>176.587346</td>\n",
       "      <td>192.341783</td>\n",
       "      <td>200.027368</td>\n",
       "      <td>171.198233</td>\n",
       "      <td>252.700679</td>\n",
       "      <td>243.504570</td>\n",
       "      <td>226.870974</td>\n",
       "      <td>212.239032</td>\n",
       "      <td>139.960394</td>\n",
       "      <td>115.935347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     225.406990  239.008790  158.191094  187.189908  275.467386  281.229012   \n",
       "1     228.633781  245.151589  156.800392  184.587306  264.466138  277.713108   \n",
       "2     224.153794  249.810077  153.038031  187.365600  261.717666  283.899837   \n",
       "3     215.946864  243.204526  150.392309  192.410319  265.338779  275.743540   \n",
       "4     226.184948  249.814628  162.709835  186.328688  264.637581  277.555411   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  244.631935  227.486924  152.860450  141.746816  288.640369  283.967031   \n",
       "2439  241.439417  227.313179  160.417181  135.108209  292.941557  274.486373   \n",
       "2440  243.485176  228.839487  167.040019  139.364266  288.030906  276.184358   \n",
       "2441  238.026661  236.849963  164.718777  143.433579  291.108831  282.412229   \n",
       "2442  240.660196  229.393753  165.773564  151.618905  282.858214  280.936428   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     211.308257  229.358293  201.582876  225.230211  ...  169.531659   \n",
       "1     213.270706  234.364622  207.477214  217.770303  ...  166.147794   \n",
       "2     217.474753  225.270731  200.043037  207.928725  ...  169.214058   \n",
       "3     209.657333  236.181002  202.469535  219.065106  ...  170.679691   \n",
       "4     218.032813  232.644672  209.252621  215.162594  ...  175.716059   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  230.864109  223.355707  217.206802  213.323681  ...  173.571829   \n",
       "2439  232.084329  213.084474  204.916336  202.388167  ...  180.144556   \n",
       "2440  236.803816  216.808270  216.363399  200.870898  ...  171.256528   \n",
       "2441  240.958178  214.110993  213.230860  189.934369  ...  171.995866   \n",
       "2442  238.669200  225.183727  204.387444  210.230827  ...  176.587346   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     172.666548  192.453077  187.047372  242.967384  237.534152  198.792993   \n",
       "1     170.038327  196.866198  187.848953  245.784763  248.073511  198.882069   \n",
       "2     185.371351  193.593967  188.513661  247.270057  237.878404  200.563666   \n",
       "3     182.101000  195.548369  189.891649  237.982636  252.286560  202.535132   \n",
       "4     171.187222  200.664477  184.740323  243.157506  239.385167  198.765299   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  193.956862  189.695903  177.222097  259.449935  235.875875  221.579495   \n",
       "2439  189.998904  192.092449  166.192675  244.962685  247.866220  225.432035   \n",
       "2440  196.366081  194.788796  171.981077  259.054124  237.921634  215.723724   \n",
       "2441  194.416994  199.046725  166.954828  253.129921  233.517925  220.636955   \n",
       "2442  192.341783  200.027368  171.198233  252.700679  243.504570  226.870974   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     224.891271  107.486538  148.012190  \n",
       "1     221.364753  105.383493  139.161556  \n",
       "2     230.747023  107.781494  150.405524  \n",
       "3     220.895367  110.265810  149.756557  \n",
       "4     224.987574  108.316042  147.834524  \n",
       "...          ...         ...         ...  \n",
       "2438  208.014293  144.366279  115.315693  \n",
       "2439  212.803153  145.580688  107.107362  \n",
       "2440  212.874373  142.529206  107.861245  \n",
       "2441  216.644809  142.204405  111.443804  \n",
       "2442  212.239032  139.960394  115.935347  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 17s 19ms/step - loss: 1379.5153 - val_loss: 1281.2739\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1213.3892 - val_loss: 1177.7155\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1124.5935 - val_loss: 1103.3689\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1059.9353 - val_loss: 1049.0533\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1012.1555 - val_loss: 1009.5879\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 977.3710 - val_loss: 981.5217\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 952.7808 - val_loss: 962.5671\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 936.4138 - val_loss: 950.5073\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.1930 - val_loss: 943.6417\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.2665 - val_loss: 940.0729\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1725 - val_loss: 938.4215\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.6573 - val_loss: 937.8406\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.0489 - val_loss: 937.7457\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.8633 - val_loss: 937.7822\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7758 - val_loss: 937.8080\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7771 - val_loss: 937.8472\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7814 - val_loss: 937.8492\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7629 - val_loss: 937.7762\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7537 - val_loss: 937.8080\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7789 - val_loss: 937.8883\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.8016 - val_loss: 937.9684\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7766 - val_loss: 937.8243\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7537 - val_loss: 937.7881\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.8060 - val_loss: 937.8366\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7590 - val_loss: 937.8948\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.6368 - val_loss: 939.2880\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2385 - val_loss: 938.6356\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.8791 - val_loss: 938.4715\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.8106 - val_loss: 938.3698\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.7772 - val_loss: 938.2515\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7736 - val_loss: 938.1284\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.6617 - val_loss: 937.6122\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 910.1447 - val_loss: 919.5331\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 879.2770 - val_loss: 886.5261\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 844.8992 - val_loss: 849.7286\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 809.8192 - val_loss: 816.6381\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 781.9769 - val_loss: 785.1254\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 739.2718 - val_loss: 726.3813\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 685.2444 - val_loss: 675.4657\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 626.4890 - val_loss: 612.4002\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 570.7435 - val_loss: 559.4871\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 525.8162 - val_loss: 516.3204\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 485.7119 - val_loss: 477.0148\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 447.6077 - val_loss: 439.7237\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 413.5863 - val_loss: 405.5083\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 381.7301 - val_loss: 376.5454\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 350.9113 - val_loss: 347.4869\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 321.7857 - val_loss: 316.2095\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 296.2395 - val_loss: 290.3683\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 270.4404 - val_loss: 263.3517\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 247.0503 - val_loss: 240.2339\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 224.2705 - val_loss: 218.6942\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 204.0726 - val_loss: 199.2112\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 186.4139 - val_loss: 184.1133\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 167.6741 - val_loss: 164.0290\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 151.5246 - val_loss: 147.2521\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 136.7967 - val_loss: 131.8215\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 123.6255 - val_loss: 121.9045\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 111.2825 - val_loss: 110.6014\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 101.0618 - val_loss: 102.3347\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 91.1861 - val_loss: 89.9047\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 81.7230 - val_loss: 81.3141\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 73.7378 - val_loss: 70.1694\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.8322 - val_loss: 68.2649\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 60.1779 - val_loss: 56.4769\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.9586 - val_loss: 53.1408\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.0537 - val_loss: 46.6180\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.6835 - val_loss: 42.7491\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.3427 - val_loss: 39.4192\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.8268 - val_loss: 35.5515\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.3770 - val_loss: 32.0736\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.2159 - val_loss: 31.0962\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.8711 - val_loss: 27.2895\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8153 - val_loss: 24.1941\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.6975 - val_loss: 27.4712\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.5915 - val_loss: 22.2438\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.8132 - val_loss: 19.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.1627 - val_loss: 21.5646\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.0457 - val_loss: 16.9817\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.8311 - val_loss: 24.0158\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.9441 - val_loss: 15.4300\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.4725 - val_loss: 14.0128\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5685 - val_loss: 13.8830\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.0336 - val_loss: 15.6688\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.8217 - val_loss: 12.3161\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.7331 - val_loss: 13.6427\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.8663 - val_loss: 13.4130\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.4583 - val_loss: 12.7326\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0854 - val_loss: 11.6653\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.2974 - val_loss: 10.9773\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1887 - val_loss: 9.6022\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1197 - val_loss: 10.0224\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5984 - val_loss: 10.7751\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0675 - val_loss: 9.7155\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5275 - val_loss: 10.9780\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4432 - val_loss: 8.9458\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0597 - val_loss: 8.7002\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0390 - val_loss: 10.1156\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6961 - val_loss: 8.0557\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0250 - val_loss: 8.6075\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8761 - val_loss: 8.9375\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9230 - val_loss: 6.5801\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6055 - val_loss: 7.4336\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6404 - val_loss: 8.5995\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1055 - val_loss: 13.3368\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1231 - val_loss: 7.6600\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6562 - val_loss: 7.6541\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6490 - val_loss: 7.7607\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4360 - val_loss: 7.3555\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4845 - val_loss: 6.0337\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4715 - val_loss: 7.1882\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2196 - val_loss: 6.5649\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8874 - val_loss: 6.5678\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9325 - val_loss: 7.8432\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7602 - val_loss: 5.3747\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9659 - val_loss: 6.1841\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7444 - val_loss: 7.4182\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4356 - val_loss: 5.7275\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1306 - val_loss: 7.4324\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7451 - val_loss: 5.9468\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9079 - val_loss: 6.6795\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2406 - val_loss: 6.3428\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0583 - val_loss: 5.0928\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4801 - val_loss: 5.6105\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9801 - val_loss: 5.7702\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6237 - val_loss: 6.9129\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6955 - val_loss: 5.3613\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3943 - val_loss: 4.9358\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4494 - val_loss: 8.4754\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4250 - val_loss: 5.5465\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1299 - val_loss: 5.9471\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3889 - val_loss: 5.8145\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4445 - val_loss: 6.9703\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1049 - val_loss: 5.2055\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2939 - val_loss: 6.3292\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2987 - val_loss: 7.0237\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9939 - val_loss: 4.5164\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3456 - val_loss: 4.9823\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9734 - val_loss: 4.8579\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9039 - val_loss: 4.5917\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7459 - val_loss: 5.4633\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.8674 - val_loss: 5.0397\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1166 - val_loss: 6.8698\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5342 - val_loss: 5.0651\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9619 - val_loss: 4.6435\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3646 - val_loss: 5.8786\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4162 - val_loss: 6.3908\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9700 - val_loss: 5.3956\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5653 - val_loss: 5.4385\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5190 - val_loss: 4.2739\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7188 - val_loss: 6.7142\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4142 - val_loss: 4.4713\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3878 - val_loss: 4.8139\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2418 - val_loss: 4.7096\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1074 - val_loss: 5.8493\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4754 - val_loss: 7.5344\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1328 - val_loss: 3.9258\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5308 - val_loss: 4.9197\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1272 - val_loss: 3.7516\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9968 - val_loss: 4.2822\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3248 - val_loss: 5.3373\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7264 - val_loss: 5.2638\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9301 - val_loss: 4.9804\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2026 - val_loss: 4.3473\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9558 - val_loss: 5.3100\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2567 - val_loss: 3.8152\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1421 - val_loss: 5.7565\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8303 - val_loss: 4.6110\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9497 - val_loss: 4.6258\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8561 - val_loss: 4.5573\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7209 - val_loss: 3.8523\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9152 - val_loss: 4.7757\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2897 - val_loss: 4.0629\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7087 - val_loss: 4.1782\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4927 - val_loss: 4.6657\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7474 - val_loss: 4.9504\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5714 - val_loss: 4.2650\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6387 - val_loss: 4.2660\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4778 - val_loss: 4.2173\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5227 - val_loss: 3.8583\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7731 - val_loss: 4.1103\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5230 - val_loss: 3.4861\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7116 - val_loss: 5.0700\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7325 - val_loss: 4.4623\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5888 - val_loss: 3.8081\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4751 - val_loss: 4.2656\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0270 - val_loss: 3.4176\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8776 - val_loss: 4.0714\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3452 - val_loss: 3.6924\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1059 - val_loss: 3.6928\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5281 - val_loss: 3.7222\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3569 - val_loss: 3.6790\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3068 - val_loss: 3.6166\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0702 - val_loss: 4.7552\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6249 - val_loss: 4.7683\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0485 - val_loss: 3.3868\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2489 - val_loss: 3.7362\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3428 - val_loss: 3.3562\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0608 - val_loss: 14.1837\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7816 - val_loss: 3.7562\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 3.756212375149134\n",
      "Mean Absolute Error (MAE): 1.2699512226675413\n",
      "Root Mean Squared Error (RMSE): 1.9380950376978767\n",
      "Time taken: 1219.6034030914307\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 18ms/step - loss: 1414.0242 - val_loss: 1282.1470\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1243.4536 - val_loss: 1174.6198\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1149.6953 - val_loss: 1095.7850\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1081.0419 - val_loss: 1037.7351\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1029.8239 - val_loss: 994.7078\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 992.2234 - val_loss: 964.1010\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 965.3795 - val_loss: 942.8121\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 946.9225 - val_loss: 928.7723\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 934.9942 - val_loss: 920.3966\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.9154 - val_loss: 915.8807\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.0746 - val_loss: 913.7028\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.2035 - val_loss: 912.8621\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.4230 - val_loss: 912.5768\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.1315 - val_loss: 912.5107\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9825 - val_loss: 912.6196\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9891 - val_loss: 912.5994\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9651 - val_loss: 912.6420\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9702 - val_loss: 912.7145\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9559 - val_loss: 912.6589\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9811 - val_loss: 912.6600\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9610 - val_loss: 912.7270\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9481 - val_loss: 912.6627\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9851 - val_loss: 912.6736\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9767 - val_loss: 912.6722\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9609 - val_loss: 912.6888\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9529 - val_loss: 912.6885\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9620 - val_loss: 912.6584\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.9612 - val_loss: 912.6309\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.5399 - val_loss: 913.1913\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2841 - val_loss: 912.7379\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.5376 - val_loss: 912.3428\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.5239 - val_loss: 890.1963\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 869.1315 - val_loss: 847.5179\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 825.8391 - val_loss: 794.8953\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 773.5392 - val_loss: 746.7501\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 727.7739 - val_loss: 699.4992\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 680.2529 - val_loss: 652.7132\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 633.2499 - val_loss: 608.0173\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 585.9583 - val_loss: 553.6076\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 538.2788 - val_loss: 510.2868\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 490.2110 - val_loss: 466.9407\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 449.2355 - val_loss: 425.7057\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 409.5590 - val_loss: 392.6795\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 374.1033 - val_loss: 354.3387\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 340.0506 - val_loss: 321.6729\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 308.2709 - val_loss: 291.5742\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 277.6681 - val_loss: 262.1122\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 249.9142 - val_loss: 239.8266\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 225.9974 - val_loss: 212.7664\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.9207 - val_loss: 191.6022\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 181.9630 - val_loss: 172.8592\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 162.2703 - val_loss: 153.4163\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 144.4532 - val_loss: 136.5763\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 128.6906 - val_loss: 122.3812\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 112.6938 - val_loss: 109.7952\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.8680 - val_loss: 96.1850\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 89.8024 - val_loss: 88.5627\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 79.5160 - val_loss: 77.2999\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 71.1029 - val_loss: 69.0976\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.8529 - val_loss: 61.0906\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.2306 - val_loss: 60.5352\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 50.4388 - val_loss: 46.5865\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 45.2950 - val_loss: 45.4644\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.6047 - val_loss: 37.2158\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.8446 - val_loss: 33.3889\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.5704 - val_loss: 37.7960\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.3037 - val_loss: 28.6212\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.7495 - val_loss: 27.3601\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.3083 - val_loss: 23.6624\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.5205 - val_loss: 22.3126\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.5341 - val_loss: 19.3159\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.5459 - val_loss: 23.6101\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.4716 - val_loss: 18.6657\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.3557 - val_loss: 14.5364\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7504 - val_loss: 15.2707\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.6148 - val_loss: 13.0777\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.4414 - val_loss: 16.0876\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 12.8204 - val_loss: 12.7922\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2750 - val_loss: 12.6406\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.6488 - val_loss: 13.2264\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.4230 - val_loss: 10.4407\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.4332 - val_loss: 13.3173\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.8493 - val_loss: 10.7360\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0833 - val_loss: 10.6843\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3931 - val_loss: 8.9642\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1338 - val_loss: 12.5910\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4025 - val_loss: 12.2907\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3830 - val_loss: 11.1531\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9502 - val_loss: 11.4661\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2797 - val_loss: 12.8811\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7071 - val_loss: 17.8116\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1639 - val_loss: 9.1957\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8499 - val_loss: 8.0446\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4580 - val_loss: 6.7429\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6460 - val_loss: 7.1790\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5953 - val_loss: 7.4290\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1127 - val_loss: 7.8972\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5957 - val_loss: 7.3370\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7219 - val_loss: 7.8373\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.1072 - val_loss: 8.3634\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2849 - val_loss: 7.8063\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4434 - val_loss: 9.1300\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9505 - val_loss: 6.6932\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2516 - val_loss: 7.3070\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3980 - val_loss: 6.7497\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7977 - val_loss: 8.8132\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8164 - val_loss: 10.8894\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1916 - val_loss: 7.8493\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5565 - val_loss: 6.5835\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8611 - val_loss: 14.7912\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3969 - val_loss: 5.7784\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7890 - val_loss: 5.7342\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9852 - val_loss: 8.0387\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1474 - val_loss: 7.6326\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0302 - val_loss: 7.1415\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.6061 - val_loss: 6.2079\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2728 - val_loss: 6.0014\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2196 - val_loss: 10.5675\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4735 - val_loss: 6.7385\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7080 - val_loss: 7.5640\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3255 - val_loss: 6.0774\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3894 - val_loss: 5.2896\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1313 - val_loss: 5.1405\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4325 - val_loss: 6.4703\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0583 - val_loss: 5.3956\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1265 - val_loss: 6.2032\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7681 - val_loss: 5.7990\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8376 - val_loss: 7.5351\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1048 - val_loss: 5.4873\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8301 - val_loss: 6.7420\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7245 - val_loss: 7.4970\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2167 - val_loss: 5.4771\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0849 - val_loss: 5.7219\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2457 - val_loss: 4.6829\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3772 - val_loss: 5.1795\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7159 - val_loss: 5.9717\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0199 - val_loss: 5.0282\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9931 - val_loss: 4.8219\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9979 - val_loss: 5.6890\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3776 - val_loss: 4.8991\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6115 - val_loss: 4.1533\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4986 - val_loss: 4.6671\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9530 - val_loss: 5.2754\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2104 - val_loss: 4.6365\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9187 - val_loss: 4.6714\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2418 - val_loss: 5.5600\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0944 - val_loss: 5.3226\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6646 - val_loss: 4.2230\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7545 - val_loss: 4.5641\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7613 - val_loss: 9.0814\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7930 - val_loss: 4.2742\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4435 - val_loss: 4.7413\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2822 - val_loss: 4.4267\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2977 - val_loss: 5.2884\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6657 - val_loss: 6.5507\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5119 - val_loss: 4.3484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1212 - val_loss: 5.3821\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8048 - val_loss: 4.1149\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3682 - val_loss: 5.2915\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4727 - val_loss: 4.5144\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0699 - val_loss: 5.0525\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0513 - val_loss: 7.5983\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1612 - val_loss: 7.5074\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1524 - val_loss: 5.5362\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0638 - val_loss: 14.3017\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6630 - val_loss: 4.1034\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6963 - val_loss: 4.1841\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8637 - val_loss: 4.2472\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9522 - val_loss: 4.8372\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2053 - val_loss: 3.9542\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8602 - val_loss: 3.8040\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9120 - val_loss: 4.2113\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6122 - val_loss: 4.4393\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7199 - val_loss: 5.5143\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0259 - val_loss: 4.1637\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0831 - val_loss: 4.0458\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6360 - val_loss: 6.1174\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2326 - val_loss: 7.3938\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2003 - val_loss: 4.7046\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7715 - val_loss: 4.4207\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1339 - val_loss: 4.6614\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4611 - val_loss: 4.2964\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8430 - val_loss: 5.5408\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8490 - val_loss: 3.7321\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3008 - val_loss: 4.4112\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2615 - val_loss: 3.6394\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6119 - val_loss: 3.8131\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5515 - val_loss: 4.2977\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9365 - val_loss: 5.2800\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4870 - val_loss: 4.1266\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3882 - val_loss: 3.6663\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2929 - val_loss: 3.7920\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1719 - val_loss: 3.9555\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3943 - val_loss: 3.6479\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8834 - val_loss: 5.3721\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1646 - val_loss: 12.2016\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3670 - val_loss: 4.1529\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1828 - val_loss: 3.9752\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1051 - val_loss: 3.7957\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5770 - val_loss: 4.6220\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 4.622001236049557\n",
      "Mean Absolute Error (MAE): 1.4426957488033707\n",
      "Root Mean Squared Error (RMSE): 2.149884005254599\n",
      "Time taken: 1210.093076467514\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 18ms/step - loss: 1409.4338 - val_loss: 1296.7592\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1240.9832 - val_loss: 1193.7682\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1148.8953 - val_loss: 1118.2273\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1079.9022 - val_loss: 1061.3690\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1027.8580 - val_loss: 1018.8964\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 989.4802 - val_loss: 988.1905\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 961.9141 - val_loss: 966.7835\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 942.8802 - val_loss: 952.6766\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.4875 - val_loss: 944.0504\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.0177 - val_loss: 939.1128\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.8505 - val_loss: 936.7960\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.7749 - val_loss: 935.9021\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.8608 - val_loss: 935.5930\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.4486 - val_loss: 935.6097\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.2976 - val_loss: 935.6708\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.2739 - val_loss: 935.6297\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.3132 - val_loss: 935.7248\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.2807 - val_loss: 935.6547\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2539 - val_loss: 935.6309\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2617 - val_loss: 935.6410\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2677 - val_loss: 935.7581\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.2703 - val_loss: 935.7236\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2457 - val_loss: 935.6282\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2857 - val_loss: 935.4091\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3090 - val_loss: 935.5573\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2823 - val_loss: 935.6005\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.2606 - val_loss: 935.6281\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.1425 - val_loss: 935.6191\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3021 - val_loss: 935.6847\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.9492 - val_loss: 934.7375\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3585 - val_loss: 932.2643\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 890.1936 - val_loss: 881.7118\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 847.5347 - val_loss: 859.9925\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 820.1359 - val_loss: 820.1130\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 771.0319 - val_loss: 755.7672\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 718.8561 - val_loss: 699.8961\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 662.4011 - val_loss: 641.6362\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 614.5519 - val_loss: 600.6235\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 573.9517 - val_loss: 560.9729\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 538.2047 - val_loss: 524.7258\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 498.8959 - val_loss: 487.5889\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 459.4095 - val_loss: 444.1013\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 420.5472 - val_loss: 408.5989\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 386.7790 - val_loss: 399.8141\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 355.2401 - val_loss: 344.6183\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 325.5580 - val_loss: 314.2544\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 297.4814 - val_loss: 287.6665\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 270.9714 - val_loss: 261.5456\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 247.5684 - val_loss: 239.4716\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 226.4644 - val_loss: 218.8376\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 205.5938 - val_loss: 200.0058\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 187.1009 - val_loss: 180.0860\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 169.5659 - val_loss: 164.7278\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 154.3995 - val_loss: 148.0787\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 139.0551 - val_loss: 132.9776\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 125.6700 - val_loss: 121.8502\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 113.6431 - val_loss: 108.1785\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 102.0570 - val_loss: 96.5680\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 92.3536 - val_loss: 89.9660\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 83.2455 - val_loss: 77.9316\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.4866 - val_loss: 79.9380\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.8475 - val_loss: 64.3795\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 60.1208 - val_loss: 57.5051\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.6563 - val_loss: 50.8410\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.8332 - val_loss: 47.6470\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.2904 - val_loss: 43.2152\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.8300 - val_loss: 39.2067\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.4851 - val_loss: 36.9045\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.8145 - val_loss: 34.2399\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.9177 - val_loss: 29.2394\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.9952 - val_loss: 35.5222\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.9161 - val_loss: 30.1170\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.2703 - val_loss: 23.6466\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.9592 - val_loss: 22.7639\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.0947 - val_loss: 22.5660\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.8103 - val_loss: 18.8195\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.5509 - val_loss: 25.4235\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 17.3162 - val_loss: 18.1625\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.1738 - val_loss: 16.5988\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3125 - val_loss: 14.7567\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.8971 - val_loss: 13.7668\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.3457 - val_loss: 13.4746\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.3662 - val_loss: 15.4096\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.1200 - val_loss: 13.8624\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.6139 - val_loss: 14.2156\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2527 - val_loss: 14.0158\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5400 - val_loss: 14.2501\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3194 - val_loss: 14.1037\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7097 - val_loss: 10.9198\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.0167 - val_loss: 13.2887\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5107 - val_loss: 9.5964\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 9.3479 - val_loss: 9.3128\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.8516 - val_loss: 10.2996\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1267 - val_loss: 13.2644\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0870 - val_loss: 8.8802\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2656 - val_loss: 10.5983\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8992 - val_loss: 9.2705\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4940 - val_loss: 10.8115\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9919 - val_loss: 8.7908\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8857 - val_loss: 11.0727\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5864 - val_loss: 9.0464\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1226 - val_loss: 8.4414\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0017 - val_loss: 9.7456\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5020 - val_loss: 10.5979\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3980 - val_loss: 7.7843\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4025 - val_loss: 10.4943\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6802 - val_loss: 7.1732\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4004 - val_loss: 9.6162\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.3466 - val_loss: 7.0104\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3859 - val_loss: 7.3467\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.0493 - val_loss: 8.5350\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8694 - val_loss: 6.6323\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1668 - val_loss: 8.0352\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9384 - val_loss: 6.2931\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6603 - val_loss: 6.0142\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0430 - val_loss: 6.8076\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2191 - val_loss: 6.2183\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1442 - val_loss: 7.1123\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1575 - val_loss: 5.9350\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7934 - val_loss: 10.7035\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6658 - val_loss: 6.4141\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1152 - val_loss: 7.5623\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1658 - val_loss: 6.2402\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2715 - val_loss: 5.9044\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.4270 - val_loss: 5.7641\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8603 - val_loss: 11.9631\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.0335 - val_loss: 5.8020\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.5942 - val_loss: 5.1766\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1966 - val_loss: 5.7759\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.1659 - val_loss: 6.0881\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.0212 - val_loss: 6.8115\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.7631 - val_loss: 6.5038\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3332 - val_loss: 5.5202\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.3647 - val_loss: 7.9845\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7661 - val_loss: 4.8339\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0277 - val_loss: 5.6397\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8967 - val_loss: 5.3833\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1597 - val_loss: 7.1648\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7466 - val_loss: 5.6660\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.6015 - val_loss: 9.4792\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4677 - val_loss: 7.0234\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0346 - val_loss: 6.4746\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7856 - val_loss: 6.0662\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8985 - val_loss: 5.2465\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7594 - val_loss: 4.7153\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7206 - val_loss: 6.6771\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.0859 - val_loss: 7.9628\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5481 - val_loss: 5.6926\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3046 - val_loss: 6.0106\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4566 - val_loss: 5.5897\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4256 - val_loss: 13.3131\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0159 - val_loss: 4.7973\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5378 - val_loss: 5.6937\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5691 - val_loss: 5.5672\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1431 - val_loss: 4.7558\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3190 - val_loss: 9.1226\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.9555 - val_loss: 4.5987\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3834 - val_loss: 5.7375\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1054 - val_loss: 6.5694\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0566 - val_loss: 5.8454\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1682 - val_loss: 5.0981\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1498 - val_loss: 5.3438\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3311 - val_loss: 4.3193\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0193 - val_loss: 6.3906\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1508 - val_loss: 4.3658\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9078 - val_loss: 6.9067\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0991 - val_loss: 5.0976\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9620 - val_loss: 6.2306\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3601 - val_loss: 7.0442\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7599 - val_loss: 4.4358\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.7755 - val_loss: 4.8918\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8056 - val_loss: 4.4580\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.7656 - val_loss: 14.3648\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2934 - val_loss: 5.0640\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9660 - val_loss: 4.9643\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2679 - val_loss: 5.2131\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6356 - val_loss: 5.8927\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9025 - val_loss: 5.2327\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1150 - val_loss: 5.0547\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2769 - val_loss: 4.6763\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.7232 - val_loss: 4.1313\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5390 - val_loss: 4.7794\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.9324 - val_loss: 4.4572\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4889 - val_loss: 4.0608\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.3841 - val_loss: 8.0749\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2983 - val_loss: 4.0901\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5059 - val_loss: 3.9166\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2342 - val_loss: 3.9692\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8413 - val_loss: 10.3986\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6736 - val_loss: 3.8230\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2369 - val_loss: 3.8074\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2802 - val_loss: 4.8218\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1209 - val_loss: 5.6239\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 3.6881 - val_loss: 3.7757\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0151 - val_loss: 5.2712\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2992 - val_loss: 5.0185\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2512 - val_loss: 3.7127\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1943 - val_loss: 4.4233\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2653 - val_loss: 4.0865\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2373 - val_loss: 6.3986\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 6.398646440547835\n",
      "Mean Absolute Error (MAE): 1.5647943870807086\n",
      "Root Mean Squared Error (RMSE): 2.529554593312395\n",
      "Time taken: 1189.602865934372\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1402.3439 - val_loss: 1323.5334\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1237.2625 - val_loss: 1215.0844\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1146.7646 - val_loss: 1135.3770\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1079.0768 - val_loss: 1074.7108\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1027.7948 - val_loss: 1028.3013\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 988.9716 - val_loss: 994.4548\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 961.5311 - val_loss: 970.1833\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 942.2388 - val_loss: 953.7444\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.2803 - val_loss: 943.7604\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.2726 - val_loss: 937.8704\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 919.3584 - val_loss: 934.6423\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.4764 - val_loss: 932.9656\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6107 - val_loss: 932.2327\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.3281 - val_loss: 931.9438\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.2316 - val_loss: 931.7420\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.1548 - val_loss: 931.6929\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.9308 - val_loss: 928.7969\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 865.0724 - val_loss: 828.1442\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 785.5862 - val_loss: 768.3627\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 726.1425 - val_loss: 715.2658\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 678.9302 - val_loss: 666.6249\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 636.5059 - val_loss: 626.2056\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 595.6215 - val_loss: 583.0693\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 553.6318 - val_loss: 545.2518\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 516.7831 - val_loss: 507.4211\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 481.4967 - val_loss: 474.7299\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 448.1285 - val_loss: 439.7639\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 414.6013 - val_loss: 406.0483\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 383.9164 - val_loss: 374.6991\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 353.2101 - val_loss: 344.2954\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 325.9244 - val_loss: 316.6842\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 298.8033 - val_loss: 290.3589\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 275.0641 - val_loss: 269.4455\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 251.2462 - val_loss: 243.4554\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 230.5308 - val_loss: 223.4683\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 210.0181 - val_loss: 202.8452\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 191.7348 - val_loss: 187.6500\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 174.9160 - val_loss: 170.1178\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 159.6347 - val_loss: 154.5185\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 144.8955 - val_loss: 138.9956\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 130.4439 - val_loss: 124.4271\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 118.0397 - val_loss: 114.9297\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 106.4915 - val_loss: 109.2461\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 96.2278 - val_loss: 95.2399\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 86.1512 - val_loss: 84.4192\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 77.8719 - val_loss: 76.6017\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.5887 - val_loss: 66.5891\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 62.5900 - val_loss: 63.7715\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.7035 - val_loss: 53.1011\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 51.2465 - val_loss: 56.8721\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.6618 - val_loss: 44.6234\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 42.1528 - val_loss: 42.4044\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 38.0451 - val_loss: 36.8072\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 35.3534 - val_loss: 33.9937\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.8388 - val_loss: 33.6370\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 29.1478 - val_loss: 33.6778\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.7317 - val_loss: 29.4109\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.4830 - val_loss: 27.1664\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.6475 - val_loss: 24.5571\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.9323 - val_loss: 20.6174\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.7096 - val_loss: 20.3453\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.7301 - val_loss: 21.6825\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.1628 - val_loss: 17.5413\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.8459 - val_loss: 18.1513\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.9573 - val_loss: 17.1814\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.5619 - val_loss: 17.3134\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.0409 - val_loss: 17.6178\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5112 - val_loss: 15.1719\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6544 - val_loss: 15.2282\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 13.4880 - val_loss: 12.7527\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.2657 - val_loss: 12.9411\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 11.9698 - val_loss: 21.3792\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 11.9953 - val_loss: 11.8180\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 11.6870 - val_loss: 11.1887\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6430 - val_loss: 11.3696\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 11.2119 - val_loss: 11.8118\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7339 - val_loss: 10.4301\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0651 - val_loss: 9.4419\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3968 - val_loss: 14.7757\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.4104 - val_loss: 11.0255\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.6469 - val_loss: 8.7677\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9982 - val_loss: 9.3870\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9079 - val_loss: 8.1304\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2973 - val_loss: 8.7208\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2340 - val_loss: 9.9270\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9674 - val_loss: 8.3687\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.5987 - val_loss: 15.3156\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0017 - val_loss: 11.7744\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4506 - val_loss: 8.6356\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7193 - val_loss: 9.5184\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.9956 - val_loss: 7.3912\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7671 - val_loss: 7.7133\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2036 - val_loss: 8.6156\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0075 - val_loss: 6.2139\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.9642 - val_loss: 9.2116\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9517 - val_loss: 9.1536\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.9740 - val_loss: 8.8613\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4394 - val_loss: 7.4589\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2247 - val_loss: 9.3546\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1790 - val_loss: 7.3150\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7476 - val_loss: 9.5889\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7180 - val_loss: 6.2095\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0013 - val_loss: 6.3364\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5598 - val_loss: 7.3103\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7225 - val_loss: 9.3671\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2102 - val_loss: 5.7267\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3531 - val_loss: 7.9782\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5282 - val_loss: 7.6362\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9992 - val_loss: 7.4398\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1825 - val_loss: 6.0086\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.4980 - val_loss: 4.8435\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.7187 - val_loss: 5.4199\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.7002 - val_loss: 5.6743\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1662 - val_loss: 6.8568\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5755 - val_loss: 6.0851\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.5076 - val_loss: 5.5875\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3845 - val_loss: 6.1438\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.6952 - val_loss: 5.0244\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1943 - val_loss: 4.8883\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3214 - val_loss: 5.2595\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1164 - val_loss: 4.9925\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2258 - val_loss: 5.3749\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2971 - val_loss: 7.6696\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3783 - val_loss: 4.8878\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0306 - val_loss: 5.2420\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1870 - val_loss: 4.3257\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8974 - val_loss: 5.4736\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.6159 - val_loss: 5.0688\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.9489 - val_loss: 4.5592\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4856 - val_loss: 4.9223\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5410 - val_loss: 5.9656\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5712 - val_loss: 5.4736\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1186 - val_loss: 5.9810\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5481 - val_loss: 8.0789\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6380 - val_loss: 5.1363\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4691 - val_loss: 4.3616\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2965 - val_loss: 4.5070\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.4963 - val_loss: 5.5131\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3858 - val_loss: 4.3258\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4074 - val_loss: 3.9535\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0287 - val_loss: 5.1373\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4481 - val_loss: 4.3770\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5891 - val_loss: 6.2252\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1671 - val_loss: 3.9884\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9774 - val_loss: 4.6036\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9284 - val_loss: 4.6248\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8617 - val_loss: 4.6985\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9526 - val_loss: 3.7206\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8205 - val_loss: 4.1179\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8715 - val_loss: 4.3450\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.6883 - val_loss: 4.1753\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8445 - val_loss: 6.2834\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3204 - val_loss: 4.6141\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2401 - val_loss: 4.8274\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5902 - val_loss: 3.4116\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9746 - val_loss: 3.9824\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 2.8919 - val_loss: 3.7718\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1508 - val_loss: 6.9745\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6196 - val_loss: 5.2134\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5597 - val_loss: 6.9815\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4517 - val_loss: 4.5912\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1300 - val_loss: 4.5866\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6091 - val_loss: 4.7010\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8108 - val_loss: 4.1859\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6921 - val_loss: 4.3864\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5613 - val_loss: 3.7410\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5338 - val_loss: 4.0737\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3631 - val_loss: 3.3524\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3601 - val_loss: 3.2502\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6486 - val_loss: 3.2664\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5189 - val_loss: 3.5906\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5169 - val_loss: 3.4616\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9455 - val_loss: 11.9746\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.1991 - val_loss: 3.1679\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2483 - val_loss: 3.4268\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2170 - val_loss: 3.5070\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1757 - val_loss: 3.4484\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.8777 - val_loss: 4.1185\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1754 - val_loss: 5.0807\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0988 - val_loss: 3.7000\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1492 - val_loss: 3.2788\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2917 - val_loss: 3.6462\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0057 - val_loss: 4.9043\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5360 - val_loss: 3.3401\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0575 - val_loss: 3.0856\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1544 - val_loss: 3.1894\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0296 - val_loss: 4.3055\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4931 - val_loss: 3.0994\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8951 - val_loss: 3.7085\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4118 - val_loss: 5.7263\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.9907 - val_loss: 3.3245\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5458 - val_loss: 3.3546\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7283 - val_loss: 6.9039\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1721 - val_loss: 3.1314\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.9152 - val_loss: 3.2754\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0371 - val_loss: 3.3028\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8414 - val_loss: 5.6331\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2059 - val_loss: 3.0743\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6486 - val_loss: 3.0612\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7517 - val_loss: 3.1675\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 3.1675072768290202\n",
      "Mean Absolute Error (MAE): 1.2130289544799913\n",
      "Root Mean Squared Error (RMSE): 1.7797492173980638\n",
      "Time taken: 1160.1314496994019\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1384.2045 - val_loss: 1252.5477\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1225.3352 - val_loss: 1151.0856\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1140.6815 - val_loss: 1075.6868\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1076.9954 - val_loss: 1017.2288\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1028.5646 - val_loss: 973.2384\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 993.4883 - val_loss: 941.0960\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 968.6793 - val_loss: 918.3961\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 951.9210 - val_loss: 903.0292\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 941.2200 - val_loss: 893.1309\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 934.9611 - val_loss: 887.3042\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 931.7140 - val_loss: 883.9827\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.1144 - val_loss: 882.1573\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 929.3541 - val_loss: 881.2999\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 929.1231 - val_loss: 880.8557\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 929.0263 - val_loss: 880.6011\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 929.0004 - val_loss: 880.5161\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 928.9177 - val_loss: 880.2906\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.2043 - val_loss: 845.6279\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 874.9493 - val_loss: 819.9739\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 847.0511 - val_loss: 790.1733\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 803.2550 - val_loss: 732.4026\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 739.1958 - val_loss: 681.0995\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 697.2780 - val_loss: 645.8062\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 657.2420 - val_loss: 603.2642\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 619.5126 - val_loss: 569.3196\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 583.8498 - val_loss: 537.9164\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 549.4306 - val_loss: 502.9033\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 513.0583 - val_loss: 466.2426\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 477.9445 - val_loss: 434.0873\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 439.7347 - val_loss: 396.3898\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 403.9934 - val_loss: 363.4072\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 368.6082 - val_loss: 331.5072\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 336.7745 - val_loss: 301.5883\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 306.6562 - val_loss: 277.9095\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 281.8016 - val_loss: 253.8619\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 256.6197 - val_loss: 228.1478\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 232.3880 - val_loss: 206.1048\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 210.4612 - val_loss: 190.3799\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 190.6520 - val_loss: 168.0828\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 171.4726 - val_loss: 151.9327\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 155.2394 - val_loss: 143.3638\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 138.1958 - val_loss: 122.8386\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 126.5731 - val_loss: 119.8350\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 112.8216 - val_loss: 102.9816\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 99.7180 - val_loss: 90.8057\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 90.6979 - val_loss: 79.5067\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 80.5766 - val_loss: 73.3001\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 73.0403 - val_loss: 67.0501\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.6171 - val_loss: 65.8805\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.6626 - val_loss: 55.0130\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.7399 - val_loss: 52.7865\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 47.7594 - val_loss: 46.3447\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.5680 - val_loss: 39.5398\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.3376 - val_loss: 37.5929\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.8910 - val_loss: 32.7889\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 33.7190 - val_loss: 32.9394\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.2768 - val_loss: 29.5650\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.3061 - val_loss: 28.8342\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8622 - val_loss: 23.7939\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.4035 - val_loss: 21.9566\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.4135 - val_loss: 20.1585\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.3433 - val_loss: 20.2130\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.7893 - val_loss: 20.4770\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.4740 - val_loss: 25.0774\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.3364 - val_loss: 19.3987\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.9846 - val_loss: 16.6071\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3310 - val_loss: 20.1342\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.6842 - val_loss: 14.9544\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.8160 - val_loss: 18.3598\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.5541 - val_loss: 15.4387\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.0870 - val_loss: 15.5630\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0442 - val_loss: 15.9699\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0332 - val_loss: 12.4004\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.0831 - val_loss: 12.8375\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7057 - val_loss: 11.0265\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7791 - val_loss: 13.4061\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 12.5808 - val_loss: 14.1219\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 11.5940 - val_loss: 22.3738\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2676 - val_loss: 11.3170\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 10.6371 - val_loss: 13.7338\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2293 - val_loss: 11.1270\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.8728 - val_loss: 9.9549\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9462 - val_loss: 10.4970\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0758 - val_loss: 9.9015\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5539 - val_loss: 10.3724\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4781 - val_loss: 9.9588\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9119 - val_loss: 10.2258\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4437 - val_loss: 9.4622\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.8769 - val_loss: 11.3803\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7433 - val_loss: 10.2388\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4074 - val_loss: 9.2909\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7104 - val_loss: 10.5416\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.0407 - val_loss: 8.5218\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2764 - val_loss: 10.4912\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.5013 - val_loss: 8.2990\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6399 - val_loss: 9.6415\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.9227 - val_loss: 12.0098\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.3598 - val_loss: 9.3492\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2938 - val_loss: 8.6355\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.3694 - val_loss: 9.2651\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4192 - val_loss: 9.6970\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.1150 - val_loss: 8.4721\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.4356 - val_loss: 9.3226\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9415 - val_loss: 10.7813\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3186 - val_loss: 7.7575\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1208 - val_loss: 7.5785\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3828 - val_loss: 9.7866\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3442 - val_loss: 13.8423\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9180 - val_loss: 10.6280\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2070 - val_loss: 12.9599\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.3438 - val_loss: 7.3959\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1315 - val_loss: 6.4643\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8457 - val_loss: 7.9221\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2908 - val_loss: 12.2041\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6574 - val_loss: 9.1778\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6719 - val_loss: 11.4517\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.8901 - val_loss: 7.6019\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.1658 - val_loss: 7.3189\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1577 - val_loss: 6.2140\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3705 - val_loss: 7.9176\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7634 - val_loss: 8.4083\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0664 - val_loss: 7.5010\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0292 - val_loss: 8.4104\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4137 - val_loss: 7.1156\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0748 - val_loss: 6.3028\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6714 - val_loss: 7.1028\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0209 - val_loss: 5.8662\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4393 - val_loss: 6.9018\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3597 - val_loss: 13.4098\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4097 - val_loss: 5.8997\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3277 - val_loss: 6.9444\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0304 - val_loss: 6.8596\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1374 - val_loss: 5.4720\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3992 - val_loss: 6.0720\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7812 - val_loss: 6.5873\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3560 - val_loss: 7.3429\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6517 - val_loss: 7.6452\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7780 - val_loss: 7.1223\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.3672 - val_loss: 9.6727\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8708 - val_loss: 8.4458\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9585 - val_loss: 9.5521\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.9578 - val_loss: 8.9063\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7826 - val_loss: 6.5927\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0342 - val_loss: 9.5193\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0107 - val_loss: 19.4049\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4993 - val_loss: 5.5916\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6155 - val_loss: 5.7948\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4616 - val_loss: 6.2432\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5940 - val_loss: 6.6065\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4705 - val_loss: 7.2939\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0151 - val_loss: 6.0677\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0729 - val_loss: 5.9329\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.1283 - val_loss: 6.2659\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0298 - val_loss: 5.7571\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.3196 - val_loss: 6.2571\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.6782 - val_loss: 6.1039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9862 - val_loss: 5.1464\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.8996 - val_loss: 6.7363\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.2317 - val_loss: 6.1827\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7230 - val_loss: 5.3714\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.8028 - val_loss: 6.7621\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.6894 - val_loss: 8.5883\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.0904 - val_loss: 5.2795\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.6195 - val_loss: 6.0013\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.4782 - val_loss: 5.7074\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9646 - val_loss: 4.9414\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9034 - val_loss: 5.2410\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7716 - val_loss: 9.2516\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5966 - val_loss: 7.1468\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4502 - val_loss: 6.3279\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6378 - val_loss: 4.7523\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5200 - val_loss: 9.0406\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.0246 - val_loss: 5.7512\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4123 - val_loss: 4.7431\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3817 - val_loss: 7.6044\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.4968 - val_loss: 5.1972\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3922 - val_loss: 5.8454\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0971 - val_loss: 5.4632\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2084 - val_loss: 5.0801\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9469 - val_loss: 6.8651\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3296 - val_loss: 4.5426\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8800 - val_loss: 4.8194\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5725 - val_loss: 4.5324\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.2634 - val_loss: 15.5451\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5431 - val_loss: 9.5061\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5628 - val_loss: 4.9014\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4841 - val_loss: 7.9330\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5899 - val_loss: 5.3496\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8513 - val_loss: 4.2426\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1622 - val_loss: 4.4870\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2731 - val_loss: 6.9798\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4335 - val_loss: 4.8519\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6318 - val_loss: 4.9828\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0722 - val_loss: 11.0185\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8396 - val_loss: 5.3786\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8570 - val_loss: 5.8558\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8080 - val_loss: 8.8710\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5904 - val_loss: 4.6970\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8334 - val_loss: 5.3353\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5430 - val_loss: 5.1059\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 5.10594567198317\n",
      "Mean Absolute Error (MAE): 1.5562234255877272\n",
      "Root Mean Squared Error (RMSE): 2.259633968584994\n",
      "Time taken: 1170.1702325344086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_20716\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  3.756212  1.269951  1.938095  1219.603403\n",
      "1        2  4.622001  1.442696  2.149884  1210.093076\n",
      "2        3  6.398646  1.564794  2.529555  1189.602866\n",
      "3        4  3.167507  1.213029  1.779749  1160.131450\n",
      "4        5  5.105946  1.556223  2.259634  1170.170233\n",
      "5  Average  4.610063  1.409339  2.131383  1189.920206\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f_obese.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f_obese.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f_obese.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
