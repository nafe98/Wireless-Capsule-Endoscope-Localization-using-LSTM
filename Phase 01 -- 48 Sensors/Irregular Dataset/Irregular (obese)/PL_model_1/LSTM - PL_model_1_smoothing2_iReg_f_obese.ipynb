{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f_obese.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.743085</td>\n",
       "      <td>245.587438</td>\n",
       "      <td>154.771840</td>\n",
       "      <td>186.323260</td>\n",
       "      <td>267.056984</td>\n",
       "      <td>281.794220</td>\n",
       "      <td>213.136940</td>\n",
       "      <td>233.416212</td>\n",
       "      <td>202.821461</td>\n",
       "      <td>215.055444</td>\n",
       "      <td>...</td>\n",
       "      <td>169.147656</td>\n",
       "      <td>175.680869</td>\n",
       "      <td>195.231478</td>\n",
       "      <td>187.637399</td>\n",
       "      <td>244.296045</td>\n",
       "      <td>240.707677</td>\n",
       "      <td>198.439601</td>\n",
       "      <td>223.215212</td>\n",
       "      <td>108.578469</td>\n",
       "      <td>150.342019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.875959</td>\n",
       "      <td>245.628403</td>\n",
       "      <td>154.990000</td>\n",
       "      <td>186.452474</td>\n",
       "      <td>266.774556</td>\n",
       "      <td>281.682030</td>\n",
       "      <td>212.844259</td>\n",
       "      <td>233.292498</td>\n",
       "      <td>203.094102</td>\n",
       "      <td>215.228612</td>\n",
       "      <td>...</td>\n",
       "      <td>169.469071</td>\n",
       "      <td>175.677374</td>\n",
       "      <td>195.428247</td>\n",
       "      <td>187.875116</td>\n",
       "      <td>243.940958</td>\n",
       "      <td>240.313042</td>\n",
       "      <td>198.337504</td>\n",
       "      <td>223.071305</td>\n",
       "      <td>108.490446</td>\n",
       "      <td>150.358478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.011181</td>\n",
       "      <td>245.670578</td>\n",
       "      <td>155.208309</td>\n",
       "      <td>186.581710</td>\n",
       "      <td>266.493200</td>\n",
       "      <td>281.567210</td>\n",
       "      <td>212.552198</td>\n",
       "      <td>233.165031</td>\n",
       "      <td>203.367043</td>\n",
       "      <td>215.404428</td>\n",
       "      <td>...</td>\n",
       "      <td>169.790500</td>\n",
       "      <td>175.672988</td>\n",
       "      <td>195.625443</td>\n",
       "      <td>188.111875</td>\n",
       "      <td>243.586884</td>\n",
       "      <td>239.920057</td>\n",
       "      <td>198.240067</td>\n",
       "      <td>222.930539</td>\n",
       "      <td>108.401175</td>\n",
       "      <td>150.371969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224.148595</td>\n",
       "      <td>245.714444</td>\n",
       "      <td>155.426454</td>\n",
       "      <td>186.711006</td>\n",
       "      <td>266.212869</td>\n",
       "      <td>281.449838</td>\n",
       "      <td>212.260816</td>\n",
       "      <td>233.033756</td>\n",
       "      <td>203.640209</td>\n",
       "      <td>215.582574</td>\n",
       "      <td>...</td>\n",
       "      <td>170.112037</td>\n",
       "      <td>175.668061</td>\n",
       "      <td>195.823263</td>\n",
       "      <td>188.347696</td>\n",
       "      <td>243.233929</td>\n",
       "      <td>239.528739</td>\n",
       "      <td>198.147536</td>\n",
       "      <td>222.793100</td>\n",
       "      <td>108.310803</td>\n",
       "      <td>150.382686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224.288038</td>\n",
       "      <td>245.760404</td>\n",
       "      <td>155.644190</td>\n",
       "      <td>186.840358</td>\n",
       "      <td>265.933706</td>\n",
       "      <td>281.329994</td>\n",
       "      <td>211.970027</td>\n",
       "      <td>232.898766</td>\n",
       "      <td>203.913603</td>\n",
       "      <td>215.762856</td>\n",
       "      <td>...</td>\n",
       "      <td>170.433821</td>\n",
       "      <td>175.662771</td>\n",
       "      <td>196.021985</td>\n",
       "      <td>188.582581</td>\n",
       "      <td>242.882111</td>\n",
       "      <td>239.139085</td>\n",
       "      <td>198.060040</td>\n",
       "      <td>222.659032</td>\n",
       "      <td>108.219520</td>\n",
       "      <td>150.390880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.491893</td>\n",
       "      <td>228.347280</td>\n",
       "      <td>163.325436</td>\n",
       "      <td>140.725997</td>\n",
       "      <td>289.325607</td>\n",
       "      <td>279.420575</td>\n",
       "      <td>235.391196</td>\n",
       "      <td>220.497413</td>\n",
       "      <td>211.044658</td>\n",
       "      <td>205.695511</td>\n",
       "      <td>...</td>\n",
       "      <td>174.913838</td>\n",
       "      <td>195.123629</td>\n",
       "      <td>194.102154</td>\n",
       "      <td>170.352117</td>\n",
       "      <td>253.804827</td>\n",
       "      <td>238.780862</td>\n",
       "      <td>223.348777</td>\n",
       "      <td>213.205734</td>\n",
       "      <td>141.658729</td>\n",
       "      <td>109.645137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.356194</td>\n",
       "      <td>228.299296</td>\n",
       "      <td>163.396140</td>\n",
       "      <td>140.781240</td>\n",
       "      <td>289.263115</td>\n",
       "      <td>279.352105</td>\n",
       "      <td>235.529317</td>\n",
       "      <td>220.497198</td>\n",
       "      <td>210.910581</td>\n",
       "      <td>205.530370</td>\n",
       "      <td>...</td>\n",
       "      <td>174.881507</td>\n",
       "      <td>194.997915</td>\n",
       "      <td>194.137131</td>\n",
       "      <td>170.297434</td>\n",
       "      <td>253.794552</td>\n",
       "      <td>238.893328</td>\n",
       "      <td>223.134036</td>\n",
       "      <td>213.184004</td>\n",
       "      <td>141.996996</td>\n",
       "      <td>109.875721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.221940</td>\n",
       "      <td>228.253433</td>\n",
       "      <td>163.467314</td>\n",
       "      <td>140.838753</td>\n",
       "      <td>289.200562</td>\n",
       "      <td>279.283755</td>\n",
       "      <td>235.669179</td>\n",
       "      <td>220.493546</td>\n",
       "      <td>210.775845</td>\n",
       "      <td>205.365708</td>\n",
       "      <td>...</td>\n",
       "      <td>174.851815</td>\n",
       "      <td>194.869596</td>\n",
       "      <td>194.174712</td>\n",
       "      <td>170.241666</td>\n",
       "      <td>253.784674</td>\n",
       "      <td>239.004938</td>\n",
       "      <td>222.919171</td>\n",
       "      <td>213.161838</td>\n",
       "      <td>142.336676</td>\n",
       "      <td>110.108562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.089132</td>\n",
       "      <td>228.209953</td>\n",
       "      <td>163.539226</td>\n",
       "      <td>140.898725</td>\n",
       "      <td>289.137701</td>\n",
       "      <td>279.215639</td>\n",
       "      <td>235.811103</td>\n",
       "      <td>220.486423</td>\n",
       "      <td>210.640521</td>\n",
       "      <td>205.201545</td>\n",
       "      <td>...</td>\n",
       "      <td>174.824357</td>\n",
       "      <td>194.738693</td>\n",
       "      <td>194.215230</td>\n",
       "      <td>170.184822</td>\n",
       "      <td>253.775163</td>\n",
       "      <td>239.115735</td>\n",
       "      <td>222.704098</td>\n",
       "      <td>213.139343</td>\n",
       "      <td>142.677461</td>\n",
       "      <td>110.343690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>239.957675</td>\n",
       "      <td>228.169041</td>\n",
       "      <td>163.612031</td>\n",
       "      <td>140.961407</td>\n",
       "      <td>289.074300</td>\n",
       "      <td>279.147979</td>\n",
       "      <td>235.955342</td>\n",
       "      <td>220.476014</td>\n",
       "      <td>210.504516</td>\n",
       "      <td>205.038096</td>\n",
       "      <td>...</td>\n",
       "      <td>174.798854</td>\n",
       "      <td>194.605222</td>\n",
       "      <td>194.258957</td>\n",
       "      <td>170.126913</td>\n",
       "      <td>253.765884</td>\n",
       "      <td>239.225797</td>\n",
       "      <td>222.488912</td>\n",
       "      <td>213.116608</td>\n",
       "      <td>143.019033</td>\n",
       "      <td>110.581220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     223.743085  245.587438  154.771840  186.323260  267.056984  281.794220   \n",
       "1     223.875959  245.628403  154.990000  186.452474  266.774556  281.682030   \n",
       "2     224.011181  245.670578  155.208309  186.581710  266.493200  281.567210   \n",
       "3     224.148595  245.714444  155.426454  186.711006  266.212869  281.449838   \n",
       "4     224.288038  245.760404  155.644190  186.840358  265.933706  281.329994   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.491893  228.347280  163.325436  140.725997  289.325607  279.420575   \n",
       "2439  240.356194  228.299296  163.396140  140.781240  289.263115  279.352105   \n",
       "2440  240.221940  228.253433  163.467314  140.838753  289.200562  279.283755   \n",
       "2441  240.089132  228.209953  163.539226  140.898725  289.137701  279.215639   \n",
       "2442  239.957675  228.169041  163.612031  140.961407  289.074300  279.147979   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     213.136940  233.416212  202.821461  215.055444  ...  169.147656   \n",
       "1     212.844259  233.292498  203.094102  215.228612  ...  169.469071   \n",
       "2     212.552198  233.165031  203.367043  215.404428  ...  169.790500   \n",
       "3     212.260816  233.033756  203.640209  215.582574  ...  170.112037   \n",
       "4     211.970027  232.898766  203.913603  215.762856  ...  170.433821   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  235.391196  220.497413  211.044658  205.695511  ...  174.913838   \n",
       "2439  235.529317  220.497198  210.910581  205.530370  ...  174.881507   \n",
       "2440  235.669179  220.493546  210.775845  205.365708  ...  174.851815   \n",
       "2441  235.811103  220.486423  210.640521  205.201545  ...  174.824357   \n",
       "2442  235.955342  220.476014  210.504516  205.038096  ...  174.798854   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     175.680869  195.231478  187.637399  244.296045  240.707677  198.439601   \n",
       "1     175.677374  195.428247  187.875116  243.940958  240.313042  198.337504   \n",
       "2     175.672988  195.625443  188.111875  243.586884  239.920057  198.240067   \n",
       "3     175.668061  195.823263  188.347696  243.233929  239.528739  198.147536   \n",
       "4     175.662771  196.021985  188.582581  242.882111  239.139085  198.060040   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  195.123629  194.102154  170.352117  253.804827  238.780862  223.348777   \n",
       "2439  194.997915  194.137131  170.297434  253.794552  238.893328  223.134036   \n",
       "2440  194.869596  194.174712  170.241666  253.784674  239.004938  222.919171   \n",
       "2441  194.738693  194.215230  170.184822  253.775163  239.115735  222.704098   \n",
       "2442  194.605222  194.258957  170.126913  253.765884  239.225797  222.488912   \n",
       "\n",
       "              45          46          47  \n",
       "0     223.215212  108.578469  150.342019  \n",
       "1     223.071305  108.490446  150.358478  \n",
       "2     222.930539  108.401175  150.371969  \n",
       "3     222.793100  108.310803  150.382686  \n",
       "4     222.659032  108.219520  150.390880  \n",
       "...          ...         ...         ...  \n",
       "2438  213.205734  141.658729  109.645137  \n",
       "2439  213.184004  141.996996  109.875721  \n",
       "2440  213.161838  142.336676  110.108562  \n",
       "2441  213.139343  142.677461  110.343690  \n",
       "2442  213.116608  143.019033  110.581220  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.743085</td>\n",
       "      <td>245.587438</td>\n",
       "      <td>154.771840</td>\n",
       "      <td>186.323260</td>\n",
       "      <td>267.056984</td>\n",
       "      <td>281.794220</td>\n",
       "      <td>213.136940</td>\n",
       "      <td>233.416212</td>\n",
       "      <td>202.821461</td>\n",
       "      <td>215.055444</td>\n",
       "      <td>...</td>\n",
       "      <td>169.147656</td>\n",
       "      <td>175.680869</td>\n",
       "      <td>195.231478</td>\n",
       "      <td>187.637399</td>\n",
       "      <td>244.296045</td>\n",
       "      <td>240.707677</td>\n",
       "      <td>198.439601</td>\n",
       "      <td>223.215212</td>\n",
       "      <td>108.578469</td>\n",
       "      <td>150.342019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.875959</td>\n",
       "      <td>245.628403</td>\n",
       "      <td>154.990000</td>\n",
       "      <td>186.452474</td>\n",
       "      <td>266.774556</td>\n",
       "      <td>281.682030</td>\n",
       "      <td>212.844259</td>\n",
       "      <td>233.292498</td>\n",
       "      <td>203.094102</td>\n",
       "      <td>215.228612</td>\n",
       "      <td>...</td>\n",
       "      <td>169.469071</td>\n",
       "      <td>175.677374</td>\n",
       "      <td>195.428247</td>\n",
       "      <td>187.875116</td>\n",
       "      <td>243.940958</td>\n",
       "      <td>240.313042</td>\n",
       "      <td>198.337504</td>\n",
       "      <td>223.071305</td>\n",
       "      <td>108.490446</td>\n",
       "      <td>150.358478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.011181</td>\n",
       "      <td>245.670578</td>\n",
       "      <td>155.208309</td>\n",
       "      <td>186.581710</td>\n",
       "      <td>266.493200</td>\n",
       "      <td>281.567210</td>\n",
       "      <td>212.552198</td>\n",
       "      <td>233.165031</td>\n",
       "      <td>203.367043</td>\n",
       "      <td>215.404428</td>\n",
       "      <td>...</td>\n",
       "      <td>169.790500</td>\n",
       "      <td>175.672988</td>\n",
       "      <td>195.625443</td>\n",
       "      <td>188.111875</td>\n",
       "      <td>243.586884</td>\n",
       "      <td>239.920057</td>\n",
       "      <td>198.240067</td>\n",
       "      <td>222.930539</td>\n",
       "      <td>108.401175</td>\n",
       "      <td>150.371969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224.148595</td>\n",
       "      <td>245.714444</td>\n",
       "      <td>155.426454</td>\n",
       "      <td>186.711006</td>\n",
       "      <td>266.212869</td>\n",
       "      <td>281.449838</td>\n",
       "      <td>212.260816</td>\n",
       "      <td>233.033756</td>\n",
       "      <td>203.640209</td>\n",
       "      <td>215.582574</td>\n",
       "      <td>...</td>\n",
       "      <td>170.112037</td>\n",
       "      <td>175.668061</td>\n",
       "      <td>195.823263</td>\n",
       "      <td>188.347696</td>\n",
       "      <td>243.233929</td>\n",
       "      <td>239.528739</td>\n",
       "      <td>198.147536</td>\n",
       "      <td>222.793100</td>\n",
       "      <td>108.310803</td>\n",
       "      <td>150.382686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224.288038</td>\n",
       "      <td>245.760404</td>\n",
       "      <td>155.644190</td>\n",
       "      <td>186.840358</td>\n",
       "      <td>265.933706</td>\n",
       "      <td>281.329994</td>\n",
       "      <td>211.970027</td>\n",
       "      <td>232.898766</td>\n",
       "      <td>203.913603</td>\n",
       "      <td>215.762856</td>\n",
       "      <td>...</td>\n",
       "      <td>170.433821</td>\n",
       "      <td>175.662771</td>\n",
       "      <td>196.021985</td>\n",
       "      <td>188.582581</td>\n",
       "      <td>242.882111</td>\n",
       "      <td>239.139085</td>\n",
       "      <td>198.060040</td>\n",
       "      <td>222.659032</td>\n",
       "      <td>108.219520</td>\n",
       "      <td>150.390880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.491893</td>\n",
       "      <td>228.347280</td>\n",
       "      <td>163.325436</td>\n",
       "      <td>140.725997</td>\n",
       "      <td>289.325607</td>\n",
       "      <td>279.420575</td>\n",
       "      <td>235.391196</td>\n",
       "      <td>220.497413</td>\n",
       "      <td>211.044658</td>\n",
       "      <td>205.695511</td>\n",
       "      <td>...</td>\n",
       "      <td>174.913838</td>\n",
       "      <td>195.123629</td>\n",
       "      <td>194.102154</td>\n",
       "      <td>170.352117</td>\n",
       "      <td>253.804827</td>\n",
       "      <td>238.780862</td>\n",
       "      <td>223.348777</td>\n",
       "      <td>213.205734</td>\n",
       "      <td>141.658729</td>\n",
       "      <td>109.645137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.356194</td>\n",
       "      <td>228.299296</td>\n",
       "      <td>163.396140</td>\n",
       "      <td>140.781240</td>\n",
       "      <td>289.263115</td>\n",
       "      <td>279.352105</td>\n",
       "      <td>235.529317</td>\n",
       "      <td>220.497198</td>\n",
       "      <td>210.910581</td>\n",
       "      <td>205.530370</td>\n",
       "      <td>...</td>\n",
       "      <td>174.881507</td>\n",
       "      <td>194.997915</td>\n",
       "      <td>194.137131</td>\n",
       "      <td>170.297434</td>\n",
       "      <td>253.794552</td>\n",
       "      <td>238.893328</td>\n",
       "      <td>223.134036</td>\n",
       "      <td>213.184004</td>\n",
       "      <td>141.996996</td>\n",
       "      <td>109.875721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.221940</td>\n",
       "      <td>228.253433</td>\n",
       "      <td>163.467314</td>\n",
       "      <td>140.838753</td>\n",
       "      <td>289.200562</td>\n",
       "      <td>279.283755</td>\n",
       "      <td>235.669179</td>\n",
       "      <td>220.493546</td>\n",
       "      <td>210.775845</td>\n",
       "      <td>205.365708</td>\n",
       "      <td>...</td>\n",
       "      <td>174.851815</td>\n",
       "      <td>194.869596</td>\n",
       "      <td>194.174712</td>\n",
       "      <td>170.241666</td>\n",
       "      <td>253.784674</td>\n",
       "      <td>239.004938</td>\n",
       "      <td>222.919171</td>\n",
       "      <td>213.161838</td>\n",
       "      <td>142.336676</td>\n",
       "      <td>110.108562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.089132</td>\n",
       "      <td>228.209953</td>\n",
       "      <td>163.539226</td>\n",
       "      <td>140.898725</td>\n",
       "      <td>289.137701</td>\n",
       "      <td>279.215639</td>\n",
       "      <td>235.811103</td>\n",
       "      <td>220.486423</td>\n",
       "      <td>210.640521</td>\n",
       "      <td>205.201545</td>\n",
       "      <td>...</td>\n",
       "      <td>174.824357</td>\n",
       "      <td>194.738693</td>\n",
       "      <td>194.215230</td>\n",
       "      <td>170.184822</td>\n",
       "      <td>253.775163</td>\n",
       "      <td>239.115735</td>\n",
       "      <td>222.704098</td>\n",
       "      <td>213.139343</td>\n",
       "      <td>142.677461</td>\n",
       "      <td>110.343690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>239.957675</td>\n",
       "      <td>228.169041</td>\n",
       "      <td>163.612031</td>\n",
       "      <td>140.961407</td>\n",
       "      <td>289.074300</td>\n",
       "      <td>279.147979</td>\n",
       "      <td>235.955342</td>\n",
       "      <td>220.476014</td>\n",
       "      <td>210.504516</td>\n",
       "      <td>205.038096</td>\n",
       "      <td>...</td>\n",
       "      <td>174.798854</td>\n",
       "      <td>194.605222</td>\n",
       "      <td>194.258957</td>\n",
       "      <td>170.126913</td>\n",
       "      <td>253.765884</td>\n",
       "      <td>239.225797</td>\n",
       "      <td>222.488912</td>\n",
       "      <td>213.116608</td>\n",
       "      <td>143.019033</td>\n",
       "      <td>110.581220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     223.743085  245.587438  154.771840  186.323260  267.056984  281.794220   \n",
       "1     223.875959  245.628403  154.990000  186.452474  266.774556  281.682030   \n",
       "2     224.011181  245.670578  155.208309  186.581710  266.493200  281.567210   \n",
       "3     224.148595  245.714444  155.426454  186.711006  266.212869  281.449838   \n",
       "4     224.288038  245.760404  155.644190  186.840358  265.933706  281.329994   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.491893  228.347280  163.325436  140.725997  289.325607  279.420575   \n",
       "2439  240.356194  228.299296  163.396140  140.781240  289.263115  279.352105   \n",
       "2440  240.221940  228.253433  163.467314  140.838753  289.200562  279.283755   \n",
       "2441  240.089132  228.209953  163.539226  140.898725  289.137701  279.215639   \n",
       "2442  239.957675  228.169041  163.612031  140.961407  289.074300  279.147979   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     213.136940  233.416212  202.821461  215.055444  ...  169.147656   \n",
       "1     212.844259  233.292498  203.094102  215.228612  ...  169.469071   \n",
       "2     212.552198  233.165031  203.367043  215.404428  ...  169.790500   \n",
       "3     212.260816  233.033756  203.640209  215.582574  ...  170.112037   \n",
       "4     211.970027  232.898766  203.913603  215.762856  ...  170.433821   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  235.391196  220.497413  211.044658  205.695511  ...  174.913838   \n",
       "2439  235.529317  220.497198  210.910581  205.530370  ...  174.881507   \n",
       "2440  235.669179  220.493546  210.775845  205.365708  ...  174.851815   \n",
       "2441  235.811103  220.486423  210.640521  205.201545  ...  174.824357   \n",
       "2442  235.955342  220.476014  210.504516  205.038096  ...  174.798854   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     175.680869  195.231478  187.637399  244.296045  240.707677  198.439601   \n",
       "1     175.677374  195.428247  187.875116  243.940958  240.313042  198.337504   \n",
       "2     175.672988  195.625443  188.111875  243.586884  239.920057  198.240067   \n",
       "3     175.668061  195.823263  188.347696  243.233929  239.528739  198.147536   \n",
       "4     175.662771  196.021985  188.582581  242.882111  239.139085  198.060040   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  195.123629  194.102154  170.352117  253.804827  238.780862  223.348777   \n",
       "2439  194.997915  194.137131  170.297434  253.794552  238.893328  223.134036   \n",
       "2440  194.869596  194.174712  170.241666  253.784674  239.004938  222.919171   \n",
       "2441  194.738693  194.215230  170.184822  253.775163  239.115735  222.704098   \n",
       "2442  194.605222  194.258957  170.126913  253.765884  239.225797  222.488912   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     223.215212  108.578469  150.342019  \n",
       "1     223.071305  108.490446  150.358478  \n",
       "2     222.930539  108.401175  150.371969  \n",
       "3     222.793100  108.310803  150.382686  \n",
       "4     222.659032  108.219520  150.390880  \n",
       "...          ...         ...         ...  \n",
       "2438  213.205734  141.658729  109.645137  \n",
       "2439  213.184004  141.996996  109.875721  \n",
       "2440  213.161838  142.336676  110.108562  \n",
       "2441  213.139343  142.677461  110.343690  \n",
       "2442  213.116608  143.019033  110.581220  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 18ms/step - loss: 1386.5312 - val_loss: 1312.8201\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1218.0367 - val_loss: 1205.0276\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1127.0342 - val_loss: 1126.9795\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1060.5298 - val_loss: 1069.8969\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1011.4174 - val_loss: 1027.9327\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 975.8353 - val_loss: 998.0154\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 950.9371 - val_loss: 977.4741\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 934.3171 - val_loss: 964.1470\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.8744 - val_loss: 956.2537\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.7798 - val_loss: 951.8035\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.6039 - val_loss: 949.6640\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0926 - val_loss: 948.7654\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.4750 - val_loss: 948.5369\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.2304 - val_loss: 948.3496\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1734 - val_loss: 948.2505\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1238 - val_loss: 948.2794\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0978 - val_loss: 948.2606\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1295 - val_loss: 948.2242\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1547 - val_loss: 948.3480\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1154 - val_loss: 948.2729\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1950 - val_loss: 948.1425\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1241 - val_loss: 948.1777\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1340 - val_loss: 948.2355\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1206 - val_loss: 948.1756\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1085 - val_loss: 948.2286\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1378 - val_loss: 948.1913\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.1402 - val_loss: 948.2472\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.1141 - val_loss: 948.2635\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0875 - val_loss: 948.2761\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.0874 - val_loss: 948.5180\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 912.2989 - val_loss: 948.2811\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4885 - val_loss: 948.0072\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 871.1686 - val_loss: 882.4346\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 815.7846 - val_loss: 817.8560\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 755.7356 - val_loss: 757.7162\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 707.5869 - val_loss: 708.6677\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 663.0129 - val_loss: 663.2794\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 620.3661 - val_loss: 620.2689\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 582.2337 - val_loss: 583.9330\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 544.6153 - val_loss: 540.4588\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 504.4295 - val_loss: 503.5322\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 465.8223 - val_loss: 458.9977\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 425.2953 - val_loss: 419.1289\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 391.5997 - val_loss: 388.6067\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 357.3433 - val_loss: 348.8958\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 323.8554 - val_loss: 315.8936\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 293.2285 - val_loss: 286.0113\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 264.9610 - val_loss: 256.9494\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 238.9420 - val_loss: 234.5698\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 214.5805 - val_loss: 207.3396\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 192.1276 - val_loss: 185.5656\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 169.8006 - val_loss: 165.8860\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 149.4898 - val_loss: 142.3492\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 131.2370 - val_loss: 127.1506\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 115.1495 - val_loss: 108.7939\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.4177 - val_loss: 99.6565\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 86.9093 - val_loss: 82.5723\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.5893 - val_loss: 72.9693\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.9536 - val_loss: 63.9289\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.9242 - val_loss: 53.2715\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 48.5625 - val_loss: 45.3321\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.9701 - val_loss: 41.0372\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.2664 - val_loss: 33.6998\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.8485 - val_loss: 29.7329\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.8128 - val_loss: 26.0652\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.5032 - val_loss: 21.7783\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.6066 - val_loss: 17.2494\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.1017 - val_loss: 14.7226\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.1280 - val_loss: 12.1072\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.8439 - val_loss: 12.2971\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.7608 - val_loss: 9.0985\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.2202 - val_loss: 7.5875\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 7.1301 - val_loss: 6.7851\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0716 - val_loss: 5.2832\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0869 - val_loss: 4.3835\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3559 - val_loss: 4.4131\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9725 - val_loss: 5.7555\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6258 - val_loss: 3.0983\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7850 - val_loss: 2.2975\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4511 - val_loss: 4.0668\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3069 - val_loss: 1.9404\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7328 - val_loss: 1.2901\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5039 - val_loss: 1.1950\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7209 - val_loss: 1.3971\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5195 - val_loss: 1.0956\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3563 - val_loss: 1.9818\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5308 - val_loss: 1.4796\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2785 - val_loss: 0.7415\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6920 - val_loss: 0.5755\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8826 - val_loss: 0.7323\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7751 - val_loss: 0.7066\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1492 - val_loss: 0.7694\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5530 - val_loss: 9.5362\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2654 - val_loss: 0.4422\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4188 - val_loss: 0.4257\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4745 - val_loss: 0.3124\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6206 - val_loss: 0.7531\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8387 - val_loss: 0.5443\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6197 - val_loss: 0.6768\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 0.4308 - val_loss: 0.3028\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4948 - val_loss: 0.4052\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9582 - val_loss: 1.5278\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4270 - val_loss: 0.2297\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2786 - val_loss: 0.2332\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3170 - val_loss: 0.3427\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5709 - val_loss: 0.3348\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8640 - val_loss: 1.2194\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4736 - val_loss: 1.4659\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4808 - val_loss: 0.3810\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3386 - val_loss: 0.1987\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4316 - val_loss: 0.4945\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4395 - val_loss: 0.6467\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4782 - val_loss: 0.8196\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9586 - val_loss: 0.3979\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2396 - val_loss: 0.4338\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4575 - val_loss: 1.0131\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4280 - val_loss: 0.4332\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6849 - val_loss: 4.0207\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1826 - val_loss: 0.1464\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1538 - val_loss: 0.2418\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1645 - val_loss: 0.1693\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2287 - val_loss: 0.1853\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1852 - val_loss: 0.2038\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1440 - val_loss: 0.0967\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.0991 - val_loss: 0.1207\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1110 - val_loss: 0.1145\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2689 - val_loss: 0.9298\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4366 - val_loss: 0.2944\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2436 - val_loss: 0.4709\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3699 - val_loss: 1.1806\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7583 - val_loss: 0.3366\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1809 - val_loss: 0.1584\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2066 - val_loss: 0.1976\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6478 - val_loss: 0.3260\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2499 - val_loss: 0.2222\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2830 - val_loss: 0.1895\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3300 - val_loss: 0.3589\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3271 - val_loss: 0.6256\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3432 - val_loss: 1.2398\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2452 - val_loss: 0.2344\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2660 - val_loss: 0.6802\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7422 - val_loss: 0.0812\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0993 - val_loss: 0.1114\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2309 - val_loss: 0.2998\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3529 - val_loss: 0.1790\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3887 - val_loss: 0.3463\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2194 - val_loss: 0.1769\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2493 - val_loss: 0.6975\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4901 - val_loss: 0.2268\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2847 - val_loss: 0.2239\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3032 - val_loss: 0.2033\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3327 - val_loss: 0.1941\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1591 - val_loss: 0.0735\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3403 - val_loss: 0.3807\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7950 - val_loss: 0.1498\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1263 - val_loss: 0.0811\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1394 - val_loss: 0.0899\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2755 - val_loss: 0.0821\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2344 - val_loss: 0.1914\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4398 - val_loss: 0.1465\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1945 - val_loss: 0.1080\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4193 - val_loss: 0.3820\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2047 - val_loss: 0.1447\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2440 - val_loss: 0.2614\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1992 - val_loss: 0.6342\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5458 - val_loss: 0.0681\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1257 - val_loss: 0.1902\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2282 - val_loss: 0.2840\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2616 - val_loss: 0.2620\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4080 - val_loss: 0.1846\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1352 - val_loss: 0.0678\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2894 - val_loss: 0.4841\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2629 - val_loss: 0.0577\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3496 - val_loss: 0.0962\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1725 - val_loss: 0.1484\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4101 - val_loss: 0.2209\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1210 - val_loss: 0.0540\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1324 - val_loss: 0.2689\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4823 - val_loss: 0.0576\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1055 - val_loss: 0.1257\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4238 - val_loss: 0.3475\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1260 - val_loss: 0.2926\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2251 - val_loss: 0.3745\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3062 - val_loss: 0.1058\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2070 - val_loss: 0.1510\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2049 - val_loss: 0.6346\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1973 - val_loss: 0.1966\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2896 - val_loss: 0.5001\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2438 - val_loss: 0.0972\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1453 - val_loss: 0.0919\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2624 - val_loss: 0.1854\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2529 - val_loss: 0.2122\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1895 - val_loss: 0.4972\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2944 - val_loss: 0.7881\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1757 - val_loss: 0.0955\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1770 - val_loss: 0.1406\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1815 - val_loss: 1.2711\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4084 - val_loss: 0.0566\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1140 - val_loss: 0.0556\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1101 - val_loss: 0.0682\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.068152703508831\n",
      "Mean Absolute Error (MAE): 0.19451385877616115\n",
      "Root Mean Squared Error (RMSE): 0.2610607276264107\n",
      "Time taken: 1206.1287648677826\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1385.5574 - val_loss: 1279.3789\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1220.3402 - val_loss: 1176.1299\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1132.8518 - val_loss: 1099.3673\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1067.8857 - val_loss: 1040.4883\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1019.0143 - val_loss: 996.9984\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 983.8048 - val_loss: 965.5996\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 959.3848 - val_loss: 943.6873\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 943.2787 - val_loss: 929.3822\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 933.1550 - val_loss: 920.2165\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 927.2873 - val_loss: 914.7796\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.1974 - val_loss: 911.7958\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 922.7523 - val_loss: 910.2443\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 922.2013 - val_loss: 909.4891\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 921.9888 - val_loss: 909.1853\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 921.9702 - val_loss: 909.0209\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9417 - val_loss: 908.9109\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9401 - val_loss: 908.9768\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9211 - val_loss: 908.9070\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9310 - val_loss: 908.9249\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9256 - val_loss: 908.9525\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9223 - val_loss: 908.9427\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9193 - val_loss: 908.9753\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9445 - val_loss: 908.8931\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.9503 - val_loss: 908.9084\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9014 - val_loss: 908.9316\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.1678 - val_loss: 909.2350\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9362 - val_loss: 909.0167\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.2230 - val_loss: 884.1373\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 864.1790 - val_loss: 828.5231\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 807.3539 - val_loss: 770.8100\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 750.4458 - val_loss: 719.5963\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 697.0538 - val_loss: 674.8170\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 652.1865 - val_loss: 629.6652\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 612.7199 - val_loss: 591.2757\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 572.5477 - val_loss: 553.8885\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 531.7184 - val_loss: 510.8729\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 489.2327 - val_loss: 469.7742\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 447.5395 - val_loss: 424.9458\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 404.3165 - val_loss: 385.0339\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 364.2897 - val_loss: 350.0088\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 331.7779 - val_loss: 320.8011\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 299.7768 - val_loss: 286.7296\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 270.6815 - val_loss: 259.8712\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 245.5843 - val_loss: 234.2690\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 220.1349 - val_loss: 210.3491\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 198.0640 - val_loss: 189.7328\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 178.4953 - val_loss: 170.2115\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 158.1934 - val_loss: 152.1155\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 140.3425 - val_loss: 133.6592\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 123.8939 - val_loss: 118.9740\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 110.6661 - val_loss: 108.9727\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 97.3195 - val_loss: 91.8312\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 83.1962 - val_loss: 77.9578\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 70.7639 - val_loss: 67.1589\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.3979 - val_loss: 57.2556\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.7570 - val_loss: 49.6055\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.6473 - val_loss: 42.1271\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.7966 - val_loss: 36.3855\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.2688 - val_loss: 30.1047\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.0472 - val_loss: 28.3815\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 25.9630 - val_loss: 23.2738\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.6418 - val_loss: 22.2763\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.7388 - val_loss: 16.0779\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.6611 - val_loss: 13.1113\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 13.3163 - val_loss: 15.1827\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 11.0768 - val_loss: 9.8145\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.6776 - val_loss: 8.7534\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 8.1539 - val_loss: 6.9399\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7729 - val_loss: 8.2464\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.5303 - val_loss: 5.2686\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8158 - val_loss: 3.9866\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0678 - val_loss: 3.2152\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.8374 - val_loss: 3.1732\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4091 - val_loss: 2.7540\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.3774 - val_loss: 3.5063\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3480 - val_loss: 1.6519\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9712 - val_loss: 1.5801\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1488 - val_loss: 2.0630\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6862 - val_loss: 2.1711\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.1626 - val_loss: 1.3633\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3232 - val_loss: 4.2330\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7915 - val_loss: 1.8770\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1938 - val_loss: 0.7846\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5392 - val_loss: 1.5115\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7673 - val_loss: 1.3214\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.0169 - val_loss: 1.0334\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1584 - val_loss: 0.8169\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5238 - val_loss: 3.4398\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9891 - val_loss: 0.4094\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8402 - val_loss: 3.2212\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8924 - val_loss: 0.5753\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5115 - val_loss: 0.4034\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5633 - val_loss: 0.4250\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4353 - val_loss: 0.4938\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5780 - val_loss: 0.7481\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8302 - val_loss: 1.8741\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5680 - val_loss: 0.3413\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4412 - val_loss: 0.5536\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8937 - val_loss: 0.8610\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0160 - val_loss: 0.2355\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7636 - val_loss: 1.1846\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5651 - val_loss: 0.7246\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0155 - val_loss: 2.3385\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7648 - val_loss: 0.3141\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4981 - val_loss: 0.4967\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6756 - val_loss: 1.6149\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7793 - val_loss: 0.5084\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3769 - val_loss: 0.3571\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5818 - val_loss: 0.5568\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9637 - val_loss: 0.5593\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2130 - val_loss: 0.1698\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1852 - val_loss: 0.1724\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2958 - val_loss: 0.6776\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3180 - val_loss: 0.2461\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1730 - val_loss: 0.1445\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2334 - val_loss: 0.2714\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1533 - val_loss: 0.5362\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3116 - val_loss: 0.5410\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3628 - val_loss: 0.8153\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.9751 - val_loss: 0.2916\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2070 - val_loss: 0.2115\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2083 - val_loss: 0.2595\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3054 - val_loss: 0.9997\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.8660 - val_loss: 0.6447\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2400 - val_loss: 0.2522\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2144 - val_loss: 0.3091\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1622 - val_loss: 0.0979\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7362 - val_loss: 0.4506\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2147 - val_loss: 0.1116\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2712 - val_loss: 0.1708\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4473 - val_loss: 0.9680\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3846 - val_loss: 0.4272\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3802 - val_loss: 0.5100\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5065 - val_loss: 0.4057\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7454 - val_loss: 0.9638\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3234 - val_loss: 0.3876\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3195 - val_loss: 0.3661\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0077 - val_loss: 1.0580\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2212 - val_loss: 0.1387\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1542 - val_loss: 0.1472\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4918 - val_loss: 0.8224\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3303 - val_loss: 0.4206\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2651 - val_loss: 0.3289\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5162 - val_loss: 2.1045\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7397 - val_loss: 0.1371\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2434 - val_loss: 0.1214\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6479 - val_loss: 0.9735\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2744 - val_loss: 0.0918\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1017 - val_loss: 0.1224\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1034 - val_loss: 0.0838\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1387 - val_loss: 0.1305\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1266 - val_loss: 0.1237\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5746 - val_loss: 0.2115\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3474 - val_loss: 0.4685\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2370 - val_loss: 0.2157\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3249 - val_loss: 0.3456\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7066 - val_loss: 0.9057\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2702 - val_loss: 0.1304\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2981 - val_loss: 0.3649\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2497 - val_loss: 0.1281\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7868 - val_loss: 0.7859\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1215 - val_loss: 0.0421\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0640 - val_loss: 0.1145\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1005 - val_loss: 0.1174\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1356 - val_loss: 0.3634\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4255 - val_loss: 0.6419\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5730 - val_loss: 0.8204\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2537 - val_loss: 0.0685\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3143 - val_loss: 0.9744\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2657 - val_loss: 0.2581\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4492 - val_loss: 0.6320\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3380 - val_loss: 0.0875\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3743 - val_loss: 0.1578\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2349 - val_loss: 0.4761\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4210 - val_loss: 0.1452\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1683 - val_loss: 0.3259\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4738 - val_loss: 0.8184\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3362 - val_loss: 0.1467\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1963 - val_loss: 0.3150\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3341 - val_loss: 0.2035\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4907 - val_loss: 0.6092\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2490 - val_loss: 0.0906\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4956 - val_loss: 0.7162\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1397 - val_loss: 0.0977\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1062 - val_loss: 0.1533\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5698 - val_loss: 1.4973\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4285 - val_loss: 0.0499\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1220 - val_loss: 0.1251\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4409 - val_loss: 0.1843\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1524 - val_loss: 0.1188\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3966 - val_loss: 0.1031\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0934 - val_loss: 0.0891\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.3993 - val_loss: 0.3520\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1450 - val_loss: 0.0863\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0752 - val_loss: 0.0559\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0660 - val_loss: 0.0646\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1270 - val_loss: 0.3887\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2523 - val_loss: 0.3123\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2183 - val_loss: 0.0914\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.3734 - val_loss: 0.3224\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.3224325762824413\n",
      "Mean Absolute Error (MAE): 0.4228815106216471\n",
      "Root Mean Squared Error (RMSE): 0.5678314682037632\n",
      "Time taken: 1192.3196957111359\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1391.7333 - val_loss: 1274.9707\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1225.7472 - val_loss: 1175.9913\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1137.1544 - val_loss: 1104.8859\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1071.2565 - val_loss: 1052.1167\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1021.6076 - val_loss: 1013.2582\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 984.6065 - val_loss: 985.5619\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 957.8746 - val_loss: 966.6081\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 939.5919 - val_loss: 954.8671\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.6313 - val_loss: 948.0374\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4432 - val_loss: 944.7404\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.4712 - val_loss: 943.4426\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 914.5442 - val_loss: 943.2876\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.6188 - val_loss: 943.5083\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.2270 - val_loss: 943.8442\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 913.1528 - val_loss: 944.0926\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0627 - val_loss: 944.2449\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0585 - val_loss: 944.2759\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 913.1040 - val_loss: 944.3640\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0850 - val_loss: 944.3312\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0536 - val_loss: 944.2864\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0996 - val_loss: 944.3940\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0594 - val_loss: 944.3948\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0796 - val_loss: 944.4189\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 913.0635 - val_loss: 944.2764\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0708 - val_loss: 944.3231\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 913.0544 - val_loss: 944.2676\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 913.1343 - val_loss: 944.3375\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 913.1109 - val_loss: 944.2979\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.0826 - val_loss: 944.3519\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.0810 - val_loss: 944.3805\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.0996 - val_loss: 944.3379\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 913.4626 - val_loss: 944.0682\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.3118 - val_loss: 944.3309\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 913.1436 - val_loss: 944.4828\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 913.0756 - val_loss: 944.5703\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 898.4546 - val_loss: 876.6300\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 826.2996 - val_loss: 821.8258\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 772.2621 - val_loss: 768.9684\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 719.0502 - val_loss: 705.8423\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 660.1334 - val_loss: 648.4283\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 606.6260 - val_loss: 600.7744\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 562.7417 - val_loss: 559.2852\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 521.7190 - val_loss: 524.1523\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 483.7462 - val_loss: 479.2581\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 448.2225 - val_loss: 445.9328\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 414.0289 - val_loss: 409.3367\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 381.5072 - val_loss: 377.2132\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 349.9758 - val_loss: 347.2679\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 321.8790 - val_loss: 320.6494\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 295.9217 - val_loss: 293.6452\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 271.3774 - val_loss: 270.3559\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 249.4115 - val_loss: 248.9784\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 227.5684 - val_loss: 226.7338\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 207.6303 - val_loss: 207.5387\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 189.1406 - val_loss: 188.9971\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 171.7631 - val_loss: 172.1924\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 155.5080 - val_loss: 156.2390\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 141.5263 - val_loss: 142.4950\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 126.6846 - val_loss: 127.2426\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 112.8587 - val_loss: 115.7362\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 100.7293 - val_loss: 102.3640\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 90.5447 - val_loss: 92.9552\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 80.2480 - val_loss: 82.5145\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 72.3058 - val_loss: 73.5406\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 62.4852 - val_loss: 64.4351\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 54.6067 - val_loss: 57.8272\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 48.4067 - val_loss: 50.2716\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 42.8405 - val_loss: 44.4641\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 37.2881 - val_loss: 39.1856\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 34.3094 - val_loss: 34.5378\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 28.5175 - val_loss: 30.6507\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 24.9905 - val_loss: 27.1137\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 21.9769 - val_loss: 24.3055\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 18.9396 - val_loss: 20.2043\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 16.4372 - val_loss: 17.7919\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 15.0635 - val_loss: 17.2303\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 12.3430 - val_loss: 13.6084\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 10.6768 - val_loss: 12.6096\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 9.4133 - val_loss: 10.2784\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 8.3457 - val_loss: 8.7565\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 8.1637 - val_loss: 8.5900\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 6.0788 - val_loss: 8.7008\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 5.5800 - val_loss: 5.8233\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 5.3565 - val_loss: 5.2210\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.0142 - val_loss: 4.5000\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.6442 - val_loss: 5.3187\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.8001 - val_loss: 3.6455\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.0827 - val_loss: 3.2485\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.4561 - val_loss: 2.7028\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.0364 - val_loss: 2.5974\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.8748 - val_loss: 2.1235\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1.8024 - val_loss: 1.8370\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 2.0520 - val_loss: 13.2313\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.3421 - val_loss: 2.2296\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.5296 - val_loss: 1.8757\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.3407 - val_loss: 3.2431\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.5529 - val_loss: 1.6466\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2800 - val_loss: 1.5194\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2943 - val_loss: 1.1968\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0471 - val_loss: 1.2802\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0719 - val_loss: 0.9542\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9452 - val_loss: 0.9821\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9438 - val_loss: 1.0829\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9762 - val_loss: 0.8304\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7410 - val_loss: 0.7156\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7168 - val_loss: 0.7351\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.1429 - val_loss: 0.6668\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5452 - val_loss: 1.8199\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6916 - val_loss: 0.3848\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5403 - val_loss: 1.3838\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0141 - val_loss: 1.1274\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6340 - val_loss: 0.5707\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4361 - val_loss: 0.2715\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4320 - val_loss: 0.4358\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.8841 - val_loss: 1.0369\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5805 - val_loss: 0.3982\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2905 - val_loss: 0.4826\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4800 - val_loss: 1.9334\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7780 - val_loss: 0.3551\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.3885 - val_loss: 0.7587\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4458 - val_loss: 0.2380\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4352 - val_loss: 0.4217\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4553 - val_loss: 2.1041\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5553 - val_loss: 0.5553\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2519 - val_loss: 0.2254\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3629 - val_loss: 0.2115\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8650 - val_loss: 3.1752\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3227 - val_loss: 0.4270\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.2727 - val_loss: 0.2134\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3559 - val_loss: 0.3043\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2660 - val_loss: 0.3832\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3885 - val_loss: 0.6379\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.5218 - val_loss: 0.4091\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1661 - val_loss: 0.1110\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1595 - val_loss: 0.1458\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2819 - val_loss: 0.1646\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5474 - val_loss: 0.9873\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2740 - val_loss: 0.2126\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3305 - val_loss: 1.4141\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2022 - val_loss: 0.5574\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6877 - val_loss: 1.7557\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4068 - val_loss: 0.0979\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1267 - val_loss: 0.4797\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3668 - val_loss: 0.7219\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5097 - val_loss: 1.3470\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2907 - val_loss: 0.1196\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.2000 - val_loss: 0.1267\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1778 - val_loss: 0.3338\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3902 - val_loss: 0.1924\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3502 - val_loss: 0.2252\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5308 - val_loss: 0.2572\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2132 - val_loss: 0.1481\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1624 - val_loss: 0.0836\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4241 - val_loss: 0.7652\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 21s 54ms/step - loss: 0.2684 - val_loss: 0.2271\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6020 - val_loss: 1.3417\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7985 - val_loss: 0.1354\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0848 - val_loss: 0.0635\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0977 - val_loss: 0.1211\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2345 - val_loss: 0.1839\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2759 - val_loss: 0.9232\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3383 - val_loss: 0.1103\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3334 - val_loss: 0.1055\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1268 - val_loss: 0.1560\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4337 - val_loss: 0.1668\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1398 - val_loss: 0.7779\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3251 - val_loss: 0.2735\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3370 - val_loss: 0.0889\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1251 - val_loss: 0.1388\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4854 - val_loss: 0.8363\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1877 - val_loss: 0.2386\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1745 - val_loss: 0.1148\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2880 - val_loss: 0.8418\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2186 - val_loss: 0.1056\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3522 - val_loss: 0.1585\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2055 - val_loss: 0.0966\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1183 - val_loss: 0.1586\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2795 - val_loss: 0.2302\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8566 - val_loss: 0.0503\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0467 - val_loss: 0.0505\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1033 - val_loss: 0.1170\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1570 - val_loss: 0.2235\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5255 - val_loss: 0.6380\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.2689 - val_loss: 0.0881\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.0637 - val_loss: 0.0854\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3351 - val_loss: 0.4121\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1824 - val_loss: 0.2289\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1691 - val_loss: 0.0868\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2125 - val_loss: 0.3504\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1899 - val_loss: 0.0854\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5063 - val_loss: 0.2881\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1300 - val_loss: 0.1384\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0815 - val_loss: 0.1604\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1983 - val_loss: 0.2633\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0320 - val_loss: 0.2513\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0974 - val_loss: 0.0619\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.0611 - val_loss: 0.0445\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0723 - val_loss: 0.0688\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1841 - val_loss: 0.1371\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1364 - val_loss: 0.2776\n",
      "16/16 [==============================] - 1s 33ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.2776447265879497\n",
      "Mean Absolute Error (MAE): 0.409800012742536\n",
      "Root Mean Squared Error (RMSE): 0.5269200381347721\n",
      "Time taken: 3405.9270746707916\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 25ms/step - loss: 1403.8969 - val_loss: 1261.4612\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1232.8719 - val_loss: 1150.2805\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1141.8844 - val_loss: 1072.1794\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1075.6289 - val_loss: 1013.8188\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1026.9164 - val_loss: 970.5543\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 991.7353 - val_loss: 939.3154\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 966.9611 - val_loss: 917.5713\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 950.4145 - val_loss: 903.0267\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 940.1020 - val_loss: 894.1695\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 934.1622 - val_loss: 889.0359\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 931.0253 - val_loss: 886.0825\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 929.5382 - val_loss: 884.7537\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 928.9215 - val_loss: 884.0005\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.6883 - val_loss: 883.9783\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.6033 - val_loss: 883.8194\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5519 - val_loss: 883.5656\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5610 - val_loss: 883.4376\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5455 - val_loss: 883.6248\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5421 - val_loss: 883.5518\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5400 - val_loss: 883.6075\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 928.5584 - val_loss: 883.5065\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 928.5287 - val_loss: 883.5782\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5306 - val_loss: 883.4771\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.5891 - val_loss: 883.4193\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 923.6522 - val_loss: 852.0457\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 871.8480 - val_loss: 817.0157\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 825.7244 - val_loss: 759.5736\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 763.3890 - val_loss: 703.2995\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 705.0729 - val_loss: 651.0284\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 653.5652 - val_loss: 603.8514\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 605.8802 - val_loss: 561.6763\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 565.4817 - val_loss: 524.8423\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 524.3175 - val_loss: 487.6329\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 479.8279 - val_loss: 442.3991\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 437.7284 - val_loss: 401.4604\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 397.2582 - val_loss: 368.2541\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 362.4646 - val_loss: 333.7805\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 328.0149 - val_loss: 311.4457\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 295.5114 - val_loss: 271.8173\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 265.9859 - val_loss: 246.6351\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 240.2564 - val_loss: 220.4481\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 213.5544 - val_loss: 195.9531\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 189.5089 - val_loss: 177.3110\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 168.8916 - val_loss: 153.2691\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 146.8960 - val_loss: 137.7796\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 129.6200 - val_loss: 121.8222\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 112.5990 - val_loss: 103.7946\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 98.0233 - val_loss: 89.9199\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 86.1129 - val_loss: 81.1399\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 75.9631 - val_loss: 68.8082\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 64.9256 - val_loss: 59.8859\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.2588 - val_loss: 55.7744\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.2717 - val_loss: 46.7096\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.9814 - val_loss: 37.6265\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 34.9879 - val_loss: 33.8530\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.3822 - val_loss: 28.4805\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.3016 - val_loss: 23.2977\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.3720 - val_loss: 19.3237\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.9036 - val_loss: 16.9101\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.9966 - val_loss: 15.2301\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.9794 - val_loss: 13.1703\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.8998 - val_loss: 10.2195\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0037 - val_loss: 8.7733\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3715 - val_loss: 7.5238\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2943 - val_loss: 7.2631\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3284 - val_loss: 5.4390\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9375 - val_loss: 4.9984\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8500 - val_loss: 4.0195\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7865 - val_loss: 3.5519\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4640 - val_loss: 3.0611\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5959 - val_loss: 2.9479\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3877 - val_loss: 1.9795\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.9787 - val_loss: 1.7905\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.6425 - val_loss: 2.6245\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5711 - val_loss: 1.4220\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4237 - val_loss: 1.4913\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0285 - val_loss: 1.5253\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2766 - val_loss: 1.3353\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0647 - val_loss: 0.9109\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1386 - val_loss: 1.5180\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0897 - val_loss: 0.8058\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4880 - val_loss: 0.6236\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0149 - val_loss: 0.6801\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8781 - val_loss: 0.6431\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5844 - val_loss: 0.6007\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9413 - val_loss: 1.0644\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7541 - val_loss: 0.4296\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1542 - val_loss: 0.7611\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3726 - val_loss: 0.2851\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3819 - val_loss: 0.3797\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4048 - val_loss: 0.3186\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1037 - val_loss: 0.7654\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4135 - val_loss: 0.3919\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1875 - val_loss: 0.3624\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3441 - val_loss: 0.2390\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3427 - val_loss: 0.3479\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5549 - val_loss: 0.9457\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8874 - val_loss: 0.2199\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3096 - val_loss: 0.1884\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9051 - val_loss: 2.5632\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2594 - val_loss: 0.1700\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2646 - val_loss: 0.3107\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2588 - val_loss: 0.1444\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3723 - val_loss: 0.3958\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7193 - val_loss: 1.2801\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6657 - val_loss: 0.1541\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2313 - val_loss: 0.1047\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4948 - val_loss: 1.0554\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6190 - val_loss: 0.3982\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5201 - val_loss: 0.3005\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4006 - val_loss: 0.2323\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2185 - val_loss: 0.3410\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4587 - val_loss: 0.1933\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1365 - val_loss: 0.1583\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1410 - val_loss: 0.2081\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4719 - val_loss: 0.5668\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5244 - val_loss: 0.8538\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5124 - val_loss: 0.4994\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2404 - val_loss: 0.5441\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2715 - val_loss: 0.5956\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0451 - val_loss: 0.2849\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2040 - val_loss: 0.0868\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1686 - val_loss: 0.1589\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3342 - val_loss: 0.7974\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6723 - val_loss: 0.0916\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2818 - val_loss: 1.2742\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9226 - val_loss: 0.0814\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1162 - val_loss: 0.4065\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6345 - val_loss: 3.8686\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5900 - val_loss: 0.1804\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1035 - val_loss: 0.1093\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1591 - val_loss: 0.2419\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1409 - val_loss: 0.2722\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3145 - val_loss: 0.6911\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4236 - val_loss: 0.2440\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1095 - val_loss: 0.1576\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6595 - val_loss: 0.2028\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2043 - val_loss: 0.2134\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4806 - val_loss: 0.6900\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2241 - val_loss: 0.1121\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2076 - val_loss: 0.3137\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4245 - val_loss: 0.3321\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3778 - val_loss: 0.4819\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2372 - val_loss: 0.7669\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6504 - val_loss: 1.0232\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2534 - val_loss: 0.1737\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1720 - val_loss: 0.1996\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1502 - val_loss: 0.4822\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7855 - val_loss: 0.3740\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1425 - val_loss: 0.1012\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1955 - val_loss: 1.2877\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5739 - val_loss: 0.0938\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1084 - val_loss: 0.2194\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2442 - val_loss: 0.2181\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9265 - val_loss: 0.3078\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0734 - val_loss: 0.1230\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0549 - val_loss: 0.0457\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2128 - val_loss: 1.9259\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5138 - val_loss: 0.1411\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4713 - val_loss: 0.1328\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1691 - val_loss: 0.0618\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1247 - val_loss: 0.3895\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6578 - val_loss: 0.1936\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1160 - val_loss: 0.0433\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0883 - val_loss: 0.3181\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3039 - val_loss: 0.2297\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5637 - val_loss: 0.1214\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1883 - val_loss: 0.1776\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1275 - val_loss: 0.0565\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1802 - val_loss: 0.2719\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6483 - val_loss: 0.1631\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0750 - val_loss: 0.0417\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1561 - val_loss: 0.7706\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5789 - val_loss: 0.0845\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1080 - val_loss: 0.4635\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3092 - val_loss: 0.1123\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1090 - val_loss: 0.1991\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4379 - val_loss: 0.1055\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0576 - val_loss: 0.0408\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0356 - val_loss: 0.0320\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0554 - val_loss: 0.0268\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0768 - val_loss: 0.1299\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3670 - val_loss: 0.2394\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1950 - val_loss: 0.0459\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0992 - val_loss: 0.2781\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3227 - val_loss: 0.1058\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4350 - val_loss: 0.1116\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0764 - val_loss: 0.0569\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1727 - val_loss: 0.2125\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2284 - val_loss: 0.2802\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2962 - val_loss: 0.1913\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1091 - val_loss: 0.1811\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2937 - val_loss: 0.1566\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1515 - val_loss: 0.4192\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2340 - val_loss: 0.0987\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1274 - val_loss: 0.2085\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3251 - val_loss: 0.2001\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1428 - val_loss: 0.1216\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3004 - val_loss: 0.1181\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1790 - val_loss: 0.1649\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.16496572500549608\n",
      "Mean Absolute Error (MAE): 0.3091680102997537\n",
      "Root Mean Squared Error (RMSE): 0.4061597284388201\n",
      "Time taken: 1182.1543238162994\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1372.0371 - val_loss: 1249.7678\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1204.5022 - val_loss: 1150.1635\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1120.1582 - val_loss: 1078.9658\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1057.8312 - val_loss: 1026.1844\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1011.8275 - val_loss: 987.6031\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 978.6282 - val_loss: 960.2495\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 955.3779 - val_loss: 941.8010\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 940.0743 - val_loss: 930.2147\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.6926 - val_loss: 923.5303\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.2173 - val_loss: 919.7760\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 922.3112 - val_loss: 917.9556\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.9930 - val_loss: 917.3328\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 920.3781 - val_loss: 916.9580\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1685 - val_loss: 917.0333\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 920.1240 - val_loss: 916.9536\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1005 - val_loss: 916.9168\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.0980 - val_loss: 916.9583\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.0896 - val_loss: 916.9191\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.1234 - val_loss: 916.8560\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.2432 - val_loss: 917.2340\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 920.1154 - val_loss: 917.1000\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.1185 - val_loss: 917.2316\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.0991 - val_loss: 917.1265\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.0802 - val_loss: 917.0467\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.2573 - val_loss: 917.0066\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6194 - val_loss: 888.5679\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 872.4487 - val_loss: 867.3729\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 822.6025 - val_loss: 783.5051\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 763.7206 - val_loss: 724.4877\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 708.6521 - val_loss: 670.2330\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 656.7087 - val_loss: 616.3204\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 604.9868 - val_loss: 566.9388\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 561.0197 - val_loss: 524.1469\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 517.2099 - val_loss: 492.7886\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 474.2914 - val_loss: 442.6433\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 431.8930 - val_loss: 397.6707\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 392.5146 - val_loss: 358.6353\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 355.5207 - val_loss: 323.3597\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 320.3053 - val_loss: 289.2265\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 288.4473 - val_loss: 259.3457\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 260.9950 - val_loss: 232.2076\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 234.4472 - val_loss: 206.8338\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 208.6084 - val_loss: 184.7796\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 188.8648 - val_loss: 165.4054\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 165.4490 - val_loss: 147.1694\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 145.2714 - val_loss: 123.9947\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 128.3718 - val_loss: 107.4002\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 112.3725 - val_loss: 93.3887\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 97.6233 - val_loss: 80.4117\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 83.6189 - val_loss: 69.8312\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 75.4520 - val_loss: 59.1769\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.2233 - val_loss: 52.6338\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.5798 - val_loss: 43.2868\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.9254 - val_loss: 36.5365\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.6603 - val_loss: 32.5001\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.5630 - val_loss: 26.7429\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.1792 - val_loss: 22.8864\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.7710 - val_loss: 18.5005\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.3279 - val_loss: 17.3611\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.1667 - val_loss: 13.9418\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.5327 - val_loss: 11.4963\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.7068 - val_loss: 10.0069\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6961 - val_loss: 7.8963\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.6395 - val_loss: 7.3061\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2058 - val_loss: 5.5090\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0689 - val_loss: 4.5782\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 5.3414 - val_loss: 3.8971\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4838 - val_loss: 3.9776\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1191 - val_loss: 3.3331\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5034 - val_loss: 3.0893\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7011 - val_loss: 3.3424\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8003 - val_loss: 1.8013\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4574 - val_loss: 3.0729\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5812 - val_loss: 1.6507\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4656 - val_loss: 2.9492\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6293 - val_loss: 1.3769\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2168 - val_loss: 1.1365\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3371 - val_loss: 1.2848\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9447 - val_loss: 0.7309\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1279 - val_loss: 2.0372\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0120 - val_loss: 0.6596\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7064 - val_loss: 0.7580\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8414 - val_loss: 1.3021\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5001 - val_loss: 0.4362\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4891 - val_loss: 0.4244\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7374 - val_loss: 0.6600\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6592 - val_loss: 0.7547\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8529 - val_loss: 1.2467\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8541 - val_loss: 0.6598\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.9855 - val_loss: 3.5772\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0392 - val_loss: 0.6307\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5602 - val_loss: 1.0001\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5373 - val_loss: 0.8260\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4604 - val_loss: 0.5781\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7740 - val_loss: 0.7246\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5687 - val_loss: 1.3785\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0251 - val_loss: 0.6951\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4562 - val_loss: 0.2399\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9022 - val_loss: 1.8722\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1047 - val_loss: 1.4385\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4448 - val_loss: 0.2333\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3508 - val_loss: 0.4558\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3604 - val_loss: 0.8585\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7801 - val_loss: 0.2393\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6276 - val_loss: 1.4547\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1056 - val_loss: 0.5216\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2744 - val_loss: 0.1831\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2836 - val_loss: 0.3949\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4383 - val_loss: 0.7756\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3760 - val_loss: 0.6628\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2853 - val_loss: 0.4509\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2609 - val_loss: 1.2628\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0646 - val_loss: 0.1312\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1499 - val_loss: 0.1818\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3873 - val_loss: 1.0272\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3859 - val_loss: 0.3432\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4361 - val_loss: 0.5193\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0955 - val_loss: 0.8867\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4679 - val_loss: 0.1574\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1666 - val_loss: 0.1734\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3666 - val_loss: 0.4068\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7176 - val_loss: 1.7113\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4514 - val_loss: 0.1458\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0861 - val_loss: 0.0587\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0998 - val_loss: 0.3265\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1159 - val_loss: 0.1020\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1831 - val_loss: 0.1626\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5348 - val_loss: 0.9255\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3323 - val_loss: 0.5612\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6699 - val_loss: 0.3302\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3974 - val_loss: 1.1856\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5746 - val_loss: 0.3633\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4014 - val_loss: 0.4835\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3518 - val_loss: 0.1846\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2696 - val_loss: 0.3190\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6104 - val_loss: 0.9850\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4275 - val_loss: 0.2410\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1994 - val_loss: 0.2166\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6789 - val_loss: 0.3538\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1913 - val_loss: 0.1223\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1405 - val_loss: 0.0998\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1214 - val_loss: 0.1082\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1443 - val_loss: 0.2551\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2839 - val_loss: 0.3257\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3079 - val_loss: 0.3319\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4714 - val_loss: 0.2753\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5509 - val_loss: 2.9197\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7953 - val_loss: 0.0654\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0859 - val_loss: 0.0671\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1181 - val_loss: 0.2095\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4386 - val_loss: 0.3968\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2267 - val_loss: 0.1281\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3733 - val_loss: 2.3211\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5603 - val_loss: 0.0637\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0674 - val_loss: 0.0520\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0624 - val_loss: 0.0648\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0968 - val_loss: 0.0930\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3835 - val_loss: 0.1894\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2365 - val_loss: 0.2837\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4042 - val_loss: 0.6289\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2310 - val_loss: 0.2108\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5319 - val_loss: 0.1680\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1952 - val_loss: 0.3368\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4405 - val_loss: 0.6122\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2789 - val_loss: 0.4541\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3677 - val_loss: 0.9028\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2775 - val_loss: 0.1235\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5263 - val_loss: 0.4899\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2871 - val_loss: 0.2121\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1782 - val_loss: 0.1030\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2063 - val_loss: 0.3630\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3006 - val_loss: 0.1182\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3271 - val_loss: 0.3419\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2421 - val_loss: 0.1649\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4938 - val_loss: 0.1224\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1149 - val_loss: 0.1919\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3050 - val_loss: 0.5117\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1212 - val_loss: 0.0480\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0399 - val_loss: 0.0552\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1004 - val_loss: 0.0541\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1177 - val_loss: 0.4595\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2592 - val_loss: 0.3430\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2611 - val_loss: 0.2667\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7543 - val_loss: 0.1123\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0561 - val_loss: 0.0422\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0616 - val_loss: 0.0585\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6339 - val_loss: 1.6518\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5297 - val_loss: 0.1203\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1227 - val_loss: 0.0407\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0530 - val_loss: 0.0939\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1105 - val_loss: 0.1755\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5225 - val_loss: 0.3991\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2334 - val_loss: 0.0594\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.1705 - val_loss: 0.4598\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2788 - val_loss: 0.9367\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4556 - val_loss: 0.0913\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1009 - val_loss: 0.1169\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6921 - val_loss: 7.6136\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8067 - val_loss: 0.0898\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0702 - val_loss: 0.0736\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.07353605983468699\n",
      "Mean Absolute Error (MAE): 0.20204792826996767\n",
      "Root Mean Squared Error (RMSE): 0.27117533043159914\n",
      "Time taken: 1177.5772767066956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_15372\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.068153  0.194514  0.261061  1206.128765\n",
      "1        2  0.322433  0.422882  0.567831  1192.319696\n",
      "2        3  0.277645  0.409800  0.526920  3405.927075\n",
      "3        4  0.164966  0.309168  0.406160  1182.154324\n",
      "4        5  0.073536  0.202048  0.271175  1177.577277\n",
      "5  Average  0.181346  0.307682  0.406629  1632.821427\n",
      "Results saved to 'DL_Result_PL_model_1_smoothing2_iReg_f_obese.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f_obese.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_smoothing2_iReg_f_obese.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
