{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f_over.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.843921</td>\n",
       "      <td>85.117574</td>\n",
       "      <td>66.199429</td>\n",
       "      <td>76.899627</td>\n",
       "      <td>87.788196</td>\n",
       "      <td>91.089447</td>\n",
       "      <td>75.743961</td>\n",
       "      <td>86.525597</td>\n",
       "      <td>83.432111</td>\n",
       "      <td>81.778161</td>\n",
       "      <td>...</td>\n",
       "      <td>72.865606</td>\n",
       "      <td>78.729236</td>\n",
       "      <td>75.034019</td>\n",
       "      <td>74.528333</td>\n",
       "      <td>85.553399</td>\n",
       "      <td>79.249138</td>\n",
       "      <td>81.185208</td>\n",
       "      <td>80.612998</td>\n",
       "      <td>59.175003</td>\n",
       "      <td>75.983292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.939019</td>\n",
       "      <td>85.132249</td>\n",
       "      <td>66.232727</td>\n",
       "      <td>76.899817</td>\n",
       "      <td>87.565384</td>\n",
       "      <td>90.878656</td>\n",
       "      <td>75.826668</td>\n",
       "      <td>86.203180</td>\n",
       "      <td>83.052069</td>\n",
       "      <td>81.563300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868093</td>\n",
       "      <td>78.454510</td>\n",
       "      <td>74.949960</td>\n",
       "      <td>74.650483</td>\n",
       "      <td>85.387279</td>\n",
       "      <td>79.340137</td>\n",
       "      <td>81.006439</td>\n",
       "      <td>80.654394</td>\n",
       "      <td>59.009440</td>\n",
       "      <td>75.998586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.033485</td>\n",
       "      <td>85.142924</td>\n",
       "      <td>66.267055</td>\n",
       "      <td>76.899800</td>\n",
       "      <td>87.342381</td>\n",
       "      <td>90.668924</td>\n",
       "      <td>75.914190</td>\n",
       "      <td>85.885511</td>\n",
       "      <td>82.676273</td>\n",
       "      <td>81.348898</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868292</td>\n",
       "      <td>78.182075</td>\n",
       "      <td>74.869758</td>\n",
       "      <td>74.768909</td>\n",
       "      <td>85.223174</td>\n",
       "      <td>79.432720</td>\n",
       "      <td>80.829889</td>\n",
       "      <td>80.697306</td>\n",
       "      <td>58.841687</td>\n",
       "      <td>76.015002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.127072</td>\n",
       "      <td>85.149434</td>\n",
       "      <td>66.302453</td>\n",
       "      <td>76.899253</td>\n",
       "      <td>87.119457</td>\n",
       "      <td>90.460149</td>\n",
       "      <td>76.006324</td>\n",
       "      <td>85.573394</td>\n",
       "      <td>82.304584</td>\n",
       "      <td>81.135511</td>\n",
       "      <td>...</td>\n",
       "      <td>72.866288</td>\n",
       "      <td>77.911985</td>\n",
       "      <td>74.793979</td>\n",
       "      <td>74.883125</td>\n",
       "      <td>85.061654</td>\n",
       "      <td>79.526583</td>\n",
       "      <td>80.656280</td>\n",
       "      <td>80.741616</td>\n",
       "      <td>58.672093</td>\n",
       "      <td>76.031822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.219727</td>\n",
       "      <td>85.151660</td>\n",
       "      <td>66.338765</td>\n",
       "      <td>76.897856</td>\n",
       "      <td>86.896708</td>\n",
       "      <td>90.252426</td>\n",
       "      <td>76.102647</td>\n",
       "      <td>85.267343</td>\n",
       "      <td>81.936555</td>\n",
       "      <td>80.923941</td>\n",
       "      <td>...</td>\n",
       "      <td>72.862138</td>\n",
       "      <td>77.644118</td>\n",
       "      <td>74.723442</td>\n",
       "      <td>74.992960</td>\n",
       "      <td>84.903421</td>\n",
       "      <td>79.621167</td>\n",
       "      <td>80.486152</td>\n",
       "      <td>80.787184</td>\n",
       "      <td>58.501165</td>\n",
       "      <td>76.048494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>83.327226</td>\n",
       "      <td>83.518546</td>\n",
       "      <td>72.426351</td>\n",
       "      <td>62.659755</td>\n",
       "      <td>88.956235</td>\n",
       "      <td>88.942345</td>\n",
       "      <td>82.268363</td>\n",
       "      <td>81.250052</td>\n",
       "      <td>82.160443</td>\n",
       "      <td>74.882290</td>\n",
       "      <td>...</td>\n",
       "      <td>76.776115</td>\n",
       "      <td>80.941986</td>\n",
       "      <td>74.631486</td>\n",
       "      <td>66.530311</td>\n",
       "      <td>85.707722</td>\n",
       "      <td>80.212064</td>\n",
       "      <td>85.543311</td>\n",
       "      <td>80.703155</td>\n",
       "      <td>73.788238</td>\n",
       "      <td>62.641024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>83.286758</td>\n",
       "      <td>83.560974</td>\n",
       "      <td>72.373649</td>\n",
       "      <td>62.609847</td>\n",
       "      <td>88.863257</td>\n",
       "      <td>88.942589</td>\n",
       "      <td>82.404968</td>\n",
       "      <td>81.322865</td>\n",
       "      <td>82.338203</td>\n",
       "      <td>74.808569</td>\n",
       "      <td>...</td>\n",
       "      <td>77.006760</td>\n",
       "      <td>80.864543</td>\n",
       "      <td>74.649267</td>\n",
       "      <td>66.347601</td>\n",
       "      <td>85.896138</td>\n",
       "      <td>80.159673</td>\n",
       "      <td>85.633353</td>\n",
       "      <td>80.670093</td>\n",
       "      <td>73.846209</td>\n",
       "      <td>62.712693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>83.246854</td>\n",
       "      <td>83.603087</td>\n",
       "      <td>72.324180</td>\n",
       "      <td>62.557213</td>\n",
       "      <td>88.768335</td>\n",
       "      <td>88.942075</td>\n",
       "      <td>82.543950</td>\n",
       "      <td>81.400108</td>\n",
       "      <td>82.515614</td>\n",
       "      <td>74.734698</td>\n",
       "      <td>...</td>\n",
       "      <td>77.236622</td>\n",
       "      <td>80.784010</td>\n",
       "      <td>74.664822</td>\n",
       "      <td>66.162730</td>\n",
       "      <td>86.087450</td>\n",
       "      <td>80.109937</td>\n",
       "      <td>85.722839</td>\n",
       "      <td>80.639276</td>\n",
       "      <td>73.899692</td>\n",
       "      <td>62.785040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>83.207049</td>\n",
       "      <td>83.645151</td>\n",
       "      <td>72.278027</td>\n",
       "      <td>62.501762</td>\n",
       "      <td>88.671491</td>\n",
       "      <td>88.940424</td>\n",
       "      <td>82.685667</td>\n",
       "      <td>81.482106</td>\n",
       "      <td>82.692290</td>\n",
       "      <td>74.661157</td>\n",
       "      <td>...</td>\n",
       "      <td>77.465150</td>\n",
       "      <td>80.700478</td>\n",
       "      <td>74.678199</td>\n",
       "      <td>65.976653</td>\n",
       "      <td>86.281472</td>\n",
       "      <td>80.062785</td>\n",
       "      <td>85.812210</td>\n",
       "      <td>80.610958</td>\n",
       "      <td>73.948257</td>\n",
       "      <td>62.858862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>83.166811</td>\n",
       "      <td>83.687639</td>\n",
       "      <td>72.234952</td>\n",
       "      <td>62.443628</td>\n",
       "      <td>88.572647</td>\n",
       "      <td>88.937398</td>\n",
       "      <td>82.830291</td>\n",
       "      <td>81.568810</td>\n",
       "      <td>82.868067</td>\n",
       "      <td>74.588255</td>\n",
       "      <td>...</td>\n",
       "      <td>77.691951</td>\n",
       "      <td>80.613928</td>\n",
       "      <td>74.689688</td>\n",
       "      <td>65.790084</td>\n",
       "      <td>86.478064</td>\n",
       "      <td>80.018261</td>\n",
       "      <td>85.901580</td>\n",
       "      <td>80.585607</td>\n",
       "      <td>73.991807</td>\n",
       "      <td>62.934768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     76.843921  85.117574  66.199429  76.899627  87.788196  91.089447   \n",
       "1     76.939019  85.132249  66.232727  76.899817  87.565384  90.878656   \n",
       "2     77.033485  85.142924  66.267055  76.899800  87.342381  90.668924   \n",
       "3     77.127072  85.149434  66.302453  76.899253  87.119457  90.460149   \n",
       "4     77.219727  85.151660  66.338765  76.897856  86.896708  90.252426   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  83.327226  83.518546  72.426351  62.659755  88.956235  88.942345   \n",
       "2439  83.286758  83.560974  72.373649  62.609847  88.863257  88.942589   \n",
       "2440  83.246854  83.603087  72.324180  62.557213  88.768335  88.942075   \n",
       "2441  83.207049  83.645151  72.278027  62.501762  88.671491  88.940424   \n",
       "2442  83.166811  83.687639  72.234952  62.443628  88.572647  88.937398   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     75.743961  86.525597  83.432111  81.778161  ...  72.865606  78.729236   \n",
       "1     75.826668  86.203180  83.052069  81.563300  ...  72.868093  78.454510   \n",
       "2     75.914190  85.885511  82.676273  81.348898  ...  72.868292  78.182075   \n",
       "3     76.006324  85.573394  82.304584  81.135511  ...  72.866288  77.911985   \n",
       "4     76.102647  85.267343  81.936555  80.923941  ...  72.862138  77.644118   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  82.268363  81.250052  82.160443  74.882290  ...  76.776115  80.941986   \n",
       "2439  82.404968  81.322865  82.338203  74.808569  ...  77.006760  80.864543   \n",
       "2440  82.543950  81.400108  82.515614  74.734698  ...  77.236622  80.784010   \n",
       "2441  82.685667  81.482106  82.692290  74.661157  ...  77.465150  80.700478   \n",
       "2442  82.830291  81.568810  82.868067  74.588255  ...  77.691951  80.613928   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     75.034019  74.528333  85.553399  79.249138  81.185208  80.612998   \n",
       "1     74.949960  74.650483  85.387279  79.340137  81.006439  80.654394   \n",
       "2     74.869758  74.768909  85.223174  79.432720  80.829889  80.697306   \n",
       "3     74.793979  74.883125  85.061654  79.526583  80.656280  80.741616   \n",
       "4     74.723442  74.992960  84.903421  79.621167  80.486152  80.787184   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  74.631486  66.530311  85.707722  80.212064  85.543311  80.703155   \n",
       "2439  74.649267  66.347601  85.896138  80.159673  85.633353  80.670093   \n",
       "2440  74.664822  66.162730  86.087450  80.109937  85.722839  80.639276   \n",
       "2441  74.678199  65.976653  86.281472  80.062785  85.812210  80.610958   \n",
       "2442  74.689688  65.790084  86.478064  80.018261  85.901580  80.585607   \n",
       "\n",
       "             46         47  \n",
       "0     59.175003  75.983292  \n",
       "1     59.009440  75.998586  \n",
       "2     58.841687  76.015002  \n",
       "3     58.672093  76.031822  \n",
       "4     58.501165  76.048494  \n",
       "...         ...        ...  \n",
       "2438  73.788238  62.641024  \n",
       "2439  73.846209  62.712693  \n",
       "2440  73.899692  62.785040  \n",
       "2441  73.948257  62.858862  \n",
       "2442  73.991807  62.934768  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.843921</td>\n",
       "      <td>85.117574</td>\n",
       "      <td>66.199429</td>\n",
       "      <td>76.899627</td>\n",
       "      <td>87.788196</td>\n",
       "      <td>91.089447</td>\n",
       "      <td>75.743961</td>\n",
       "      <td>86.525597</td>\n",
       "      <td>83.432111</td>\n",
       "      <td>81.778161</td>\n",
       "      <td>...</td>\n",
       "      <td>72.865606</td>\n",
       "      <td>78.729236</td>\n",
       "      <td>75.034019</td>\n",
       "      <td>74.528333</td>\n",
       "      <td>85.553399</td>\n",
       "      <td>79.249138</td>\n",
       "      <td>81.185208</td>\n",
       "      <td>80.612998</td>\n",
       "      <td>59.175003</td>\n",
       "      <td>75.983292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.939019</td>\n",
       "      <td>85.132249</td>\n",
       "      <td>66.232727</td>\n",
       "      <td>76.899817</td>\n",
       "      <td>87.565384</td>\n",
       "      <td>90.878656</td>\n",
       "      <td>75.826668</td>\n",
       "      <td>86.203180</td>\n",
       "      <td>83.052069</td>\n",
       "      <td>81.563300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868093</td>\n",
       "      <td>78.454510</td>\n",
       "      <td>74.949960</td>\n",
       "      <td>74.650483</td>\n",
       "      <td>85.387279</td>\n",
       "      <td>79.340137</td>\n",
       "      <td>81.006439</td>\n",
       "      <td>80.654394</td>\n",
       "      <td>59.009440</td>\n",
       "      <td>75.998586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.033485</td>\n",
       "      <td>85.142924</td>\n",
       "      <td>66.267055</td>\n",
       "      <td>76.899800</td>\n",
       "      <td>87.342381</td>\n",
       "      <td>90.668924</td>\n",
       "      <td>75.914190</td>\n",
       "      <td>85.885511</td>\n",
       "      <td>82.676273</td>\n",
       "      <td>81.348898</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868292</td>\n",
       "      <td>78.182075</td>\n",
       "      <td>74.869758</td>\n",
       "      <td>74.768909</td>\n",
       "      <td>85.223174</td>\n",
       "      <td>79.432720</td>\n",
       "      <td>80.829889</td>\n",
       "      <td>80.697306</td>\n",
       "      <td>58.841687</td>\n",
       "      <td>76.015002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.127072</td>\n",
       "      <td>85.149434</td>\n",
       "      <td>66.302453</td>\n",
       "      <td>76.899253</td>\n",
       "      <td>87.119457</td>\n",
       "      <td>90.460149</td>\n",
       "      <td>76.006324</td>\n",
       "      <td>85.573394</td>\n",
       "      <td>82.304584</td>\n",
       "      <td>81.135511</td>\n",
       "      <td>...</td>\n",
       "      <td>72.866288</td>\n",
       "      <td>77.911985</td>\n",
       "      <td>74.793979</td>\n",
       "      <td>74.883125</td>\n",
       "      <td>85.061654</td>\n",
       "      <td>79.526583</td>\n",
       "      <td>80.656280</td>\n",
       "      <td>80.741616</td>\n",
       "      <td>58.672093</td>\n",
       "      <td>76.031822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.219727</td>\n",
       "      <td>85.151660</td>\n",
       "      <td>66.338765</td>\n",
       "      <td>76.897856</td>\n",
       "      <td>86.896708</td>\n",
       "      <td>90.252426</td>\n",
       "      <td>76.102647</td>\n",
       "      <td>85.267343</td>\n",
       "      <td>81.936555</td>\n",
       "      <td>80.923941</td>\n",
       "      <td>...</td>\n",
       "      <td>72.862138</td>\n",
       "      <td>77.644118</td>\n",
       "      <td>74.723442</td>\n",
       "      <td>74.992960</td>\n",
       "      <td>84.903421</td>\n",
       "      <td>79.621167</td>\n",
       "      <td>80.486152</td>\n",
       "      <td>80.787184</td>\n",
       "      <td>58.501165</td>\n",
       "      <td>76.048494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>83.327226</td>\n",
       "      <td>83.518546</td>\n",
       "      <td>72.426351</td>\n",
       "      <td>62.659755</td>\n",
       "      <td>88.956235</td>\n",
       "      <td>88.942345</td>\n",
       "      <td>82.268363</td>\n",
       "      <td>81.250052</td>\n",
       "      <td>82.160443</td>\n",
       "      <td>74.882290</td>\n",
       "      <td>...</td>\n",
       "      <td>76.776115</td>\n",
       "      <td>80.941986</td>\n",
       "      <td>74.631486</td>\n",
       "      <td>66.530311</td>\n",
       "      <td>85.707722</td>\n",
       "      <td>80.212064</td>\n",
       "      <td>85.543311</td>\n",
       "      <td>80.703155</td>\n",
       "      <td>73.788238</td>\n",
       "      <td>62.641024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>83.286758</td>\n",
       "      <td>83.560974</td>\n",
       "      <td>72.373649</td>\n",
       "      <td>62.609847</td>\n",
       "      <td>88.863257</td>\n",
       "      <td>88.942589</td>\n",
       "      <td>82.404968</td>\n",
       "      <td>81.322865</td>\n",
       "      <td>82.338203</td>\n",
       "      <td>74.808569</td>\n",
       "      <td>...</td>\n",
       "      <td>77.006760</td>\n",
       "      <td>80.864543</td>\n",
       "      <td>74.649267</td>\n",
       "      <td>66.347601</td>\n",
       "      <td>85.896138</td>\n",
       "      <td>80.159673</td>\n",
       "      <td>85.633353</td>\n",
       "      <td>80.670093</td>\n",
       "      <td>73.846209</td>\n",
       "      <td>62.712693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>83.246854</td>\n",
       "      <td>83.603087</td>\n",
       "      <td>72.324180</td>\n",
       "      <td>62.557213</td>\n",
       "      <td>88.768335</td>\n",
       "      <td>88.942075</td>\n",
       "      <td>82.543950</td>\n",
       "      <td>81.400108</td>\n",
       "      <td>82.515614</td>\n",
       "      <td>74.734698</td>\n",
       "      <td>...</td>\n",
       "      <td>77.236622</td>\n",
       "      <td>80.784010</td>\n",
       "      <td>74.664822</td>\n",
       "      <td>66.162730</td>\n",
       "      <td>86.087450</td>\n",
       "      <td>80.109937</td>\n",
       "      <td>85.722839</td>\n",
       "      <td>80.639276</td>\n",
       "      <td>73.899692</td>\n",
       "      <td>62.785040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>83.207049</td>\n",
       "      <td>83.645151</td>\n",
       "      <td>72.278027</td>\n",
       "      <td>62.501762</td>\n",
       "      <td>88.671491</td>\n",
       "      <td>88.940424</td>\n",
       "      <td>82.685667</td>\n",
       "      <td>81.482106</td>\n",
       "      <td>82.692290</td>\n",
       "      <td>74.661157</td>\n",
       "      <td>...</td>\n",
       "      <td>77.465150</td>\n",
       "      <td>80.700478</td>\n",
       "      <td>74.678199</td>\n",
       "      <td>65.976653</td>\n",
       "      <td>86.281472</td>\n",
       "      <td>80.062785</td>\n",
       "      <td>85.812210</td>\n",
       "      <td>80.610958</td>\n",
       "      <td>73.948257</td>\n",
       "      <td>62.858862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>83.166811</td>\n",
       "      <td>83.687639</td>\n",
       "      <td>72.234952</td>\n",
       "      <td>62.443628</td>\n",
       "      <td>88.572647</td>\n",
       "      <td>88.937398</td>\n",
       "      <td>82.830291</td>\n",
       "      <td>81.568810</td>\n",
       "      <td>82.868067</td>\n",
       "      <td>74.588255</td>\n",
       "      <td>...</td>\n",
       "      <td>77.691951</td>\n",
       "      <td>80.613928</td>\n",
       "      <td>74.689688</td>\n",
       "      <td>65.790084</td>\n",
       "      <td>86.478064</td>\n",
       "      <td>80.018261</td>\n",
       "      <td>85.901580</td>\n",
       "      <td>80.585607</td>\n",
       "      <td>73.991807</td>\n",
       "      <td>62.934768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     76.843921  85.117574  66.199429  76.899627  87.788196  91.089447   \n",
       "1     76.939019  85.132249  66.232727  76.899817  87.565384  90.878656   \n",
       "2     77.033485  85.142924  66.267055  76.899800  87.342381  90.668924   \n",
       "3     77.127072  85.149434  66.302453  76.899253  87.119457  90.460149   \n",
       "4     77.219727  85.151660  66.338765  76.897856  86.896708  90.252426   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  83.327226  83.518546  72.426351  62.659755  88.956235  88.942345   \n",
       "2439  83.286758  83.560974  72.373649  62.609847  88.863257  88.942589   \n",
       "2440  83.246854  83.603087  72.324180  62.557213  88.768335  88.942075   \n",
       "2441  83.207049  83.645151  72.278027  62.501762  88.671491  88.940424   \n",
       "2442  83.166811  83.687639  72.234952  62.443628  88.572647  88.937398   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     75.743961  86.525597  83.432111  81.778161  ...  72.865606  78.729236   \n",
       "1     75.826668  86.203180  83.052069  81.563300  ...  72.868093  78.454510   \n",
       "2     75.914190  85.885511  82.676273  81.348898  ...  72.868292  78.182075   \n",
       "3     76.006324  85.573394  82.304584  81.135511  ...  72.866288  77.911985   \n",
       "4     76.102647  85.267343  81.936555  80.923941  ...  72.862138  77.644118   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  82.268363  81.250052  82.160443  74.882290  ...  76.776115  80.941986   \n",
       "2439  82.404968  81.322865  82.338203  74.808569  ...  77.006760  80.864543   \n",
       "2440  82.543950  81.400108  82.515614  74.734698  ...  77.236622  80.784010   \n",
       "2441  82.685667  81.482106  82.692290  74.661157  ...  77.465150  80.700478   \n",
       "2442  82.830291  81.568810  82.868067  74.588255  ...  77.691951  80.613928   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     75.034019  74.528333  85.553399  79.249138  81.185208  80.612998   \n",
       "1     74.949960  74.650483  85.387279  79.340137  81.006439  80.654394   \n",
       "2     74.869758  74.768909  85.223174  79.432720  80.829889  80.697306   \n",
       "3     74.793979  74.883125  85.061654  79.526583  80.656280  80.741616   \n",
       "4     74.723442  74.992960  84.903421  79.621167  80.486152  80.787184   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  74.631486  66.530311  85.707722  80.212064  85.543311  80.703155   \n",
       "2439  74.649267  66.347601  85.896138  80.159673  85.633353  80.670093   \n",
       "2440  74.664822  66.162730  86.087450  80.109937  85.722839  80.639276   \n",
       "2441  74.678199  65.976653  86.281472  80.062785  85.812210  80.610958   \n",
       "2442  74.689688  65.790084  86.478064  80.018261  85.901580  80.585607   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     59.175003  75.983292  \n",
       "1     59.009440  75.998586  \n",
       "2     58.841687  76.015002  \n",
       "3     58.672093  76.031822  \n",
       "4     58.501165  76.048494  \n",
       "...         ...        ...  \n",
       "2438  73.788238  62.641024  \n",
       "2439  73.846209  62.712693  \n",
       "2440  73.899692  62.785040  \n",
       "2441  73.948257  62.858862  \n",
       "2442  73.991807  62.934768  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 15s 20ms/step - loss: 1384.9006 - val_loss: 1242.6863\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1223.3180 - val_loss: 1143.8877\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1136.2094 - val_loss: 1069.8901\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1071.5507 - val_loss: 1015.2911\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1023.8298 - val_loss: 974.8812\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 988.9529 - val_loss: 945.7223\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 964.3766 - val_loss: 925.7329\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 947.8203 - val_loss: 912.6863\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 937.3299 - val_loss: 904.5738\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 931.1985 - val_loss: 900.1514\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.8672 - val_loss: 897.8430\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.2829 - val_loss: 896.6655\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.5667 - val_loss: 896.2772\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.3575 - val_loss: 896.1865\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 925.2815 - val_loss: 896.1421\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 925.2393 - val_loss: 895.9115\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 925.2609 - val_loss: 895.9965\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.2603 - val_loss: 895.9940\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.2466 - val_loss: 895.9455\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 925.2280 - val_loss: 895.9017\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 925.2366 - val_loss: 895.9377\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 925.2344 - val_loss: 895.9596\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 925.2772 - val_loss: 896.0005\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.2668 - val_loss: 896.0112\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.2476 - val_loss: 896.0154\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 925.2576 - val_loss: 896.0060\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.8518 - val_loss: 881.9872\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 892.8506 - val_loss: 850.7376\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 878.9867 - val_loss: 840.8409\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 869.4914 - val_loss: 831.9078\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 860.2141 - val_loss: 823.1832\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 849.9871 - val_loss: 814.9551\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 840.2897 - val_loss: 805.6075\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 832.4106 - val_loss: 796.9841\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 824.5796 - val_loss: 789.4693\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 818.0913 - val_loss: 783.2138\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 812.0032 - val_loss: 777.9407\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 806.2379 - val_loss: 773.0751\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 801.0776 - val_loss: 769.0718\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 786.6702 - val_loss: 757.0604\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 781.3366 - val_loss: 753.9378\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 778.3367 - val_loss: 750.6685\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 776.0629 - val_loss: 748.2783\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 773.2741 - val_loss: 745.2049\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 769.9197 - val_loss: 743.6655\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 770.6913 - val_loss: 743.4506\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 769.9606 - val_loss: 742.2430\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 769.0604 - val_loss: 739.9643\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 767.3785 - val_loss: 741.6606\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 766.1595 - val_loss: 739.8452\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 763.5474 - val_loss: 739.0940\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 763.5235 - val_loss: 739.8352\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 765.0212 - val_loss: 739.4006\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 764.2858 - val_loss: 736.3502\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 772.2845 - val_loss: 729.4224\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 831.9963 - val_loss: 796.7694\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 752.7336 - val_loss: 711.1043\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 725.5813 - val_loss: 694.0811\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 786.2672 - val_loss: 742.2085\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 941.8785 - val_loss: 878.6783\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 758.3734 - val_loss: 698.3320\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 716.7887 - val_loss: 682.0973\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 703.7415 - val_loss: 671.7913\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 692.8923 - val_loss: 661.3961\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 678.6768 - val_loss: 656.0814\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 661.5615 - val_loss: 621.5513\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 643.7810 - val_loss: 617.2255\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 635.0550 - val_loss: 610.2308\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 623.9638 - val_loss: 597.9669\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 607.6957 - val_loss: 569.2180\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 579.9017 - val_loss: 533.1565\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 537.4341 - val_loss: 495.5398\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 496.5335 - val_loss: 456.1158\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 461.0636 - val_loss: 413.6637\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 437.2337 - val_loss: 385.5157\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 391.9066 - val_loss: 354.0814\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 361.9359 - val_loss: 325.5060\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 331.0293 - val_loss: 301.2884\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 310.7766 - val_loss: 280.5262\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 280.2976 - val_loss: 251.1969\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 277.5104 - val_loss: 248.6294\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 241.1312 - val_loss: 212.6692\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 216.5102 - val_loss: 195.3023\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 199.7716 - val_loss: 177.6447\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 180.0184 - val_loss: 159.5709\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 165.7357 - val_loss: 145.9344\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 147.8012 - val_loss: 129.8528\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 135.1365 - val_loss: 121.3458\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 121.3468 - val_loss: 106.1535\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 107.6809 - val_loss: 123.5233\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 98.6483 - val_loss: 94.5232\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 86.2883 - val_loss: 76.3333\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.3674 - val_loss: 67.8782\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 67.5967 - val_loss: 57.8283\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 60.2635 - val_loss: 50.8278\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 54.3160 - val_loss: 46.9549\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 46.8074 - val_loss: 39.1638\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 40.7961 - val_loss: 34.7904\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 42.8186 - val_loss: 31.6816\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 31.5588 - val_loss: 34.8625\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.1680 - val_loss: 23.3113\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.2864 - val_loss: 26.5513\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.1852 - val_loss: 28.6962\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.5350 - val_loss: 17.0990\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.1530 - val_loss: 14.8323\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.4866 - val_loss: 13.5617\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3851 - val_loss: 11.7964\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.5699 - val_loss: 14.9137\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.5328 - val_loss: 9.3512\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1635 - val_loss: 7.9402\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5157 - val_loss: 8.3653\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.2185 - val_loss: 8.5809\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2355 - val_loss: 8.3492\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4510 - val_loss: 8.0970\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.3538 - val_loss: 9.3718\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3568 - val_loss: 5.4306\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.9675 - val_loss: 7.4871\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3144 - val_loss: 4.6742\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0961 - val_loss: 5.2069\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5859 - val_loss: 3.6905\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9813 - val_loss: 8.2833\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1399 - val_loss: 3.7990\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4338 - val_loss: 4.6340\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2468 - val_loss: 2.9857\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0112 - val_loss: 4.4143\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8062 - val_loss: 4.7183\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8035 - val_loss: 3.4024\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2545 - val_loss: 4.0756\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9105 - val_loss: 3.4529\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1251 - val_loss: 2.9058\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7097 - val_loss: 2.5882\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4126 - val_loss: 2.6394\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8841 - val_loss: 3.5877\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1803 - val_loss: 4.1063\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.8430 - val_loss: 4.8474\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.2720 - val_loss: 1.8972\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9972 - val_loss: 2.4107\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2982 - val_loss: 1.9667\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2838 - val_loss: 2.1445\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8526 - val_loss: 1.2415\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2685 - val_loss: 1.7959\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4063 - val_loss: 1.8051\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6699 - val_loss: 1.2533\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0420 - val_loss: 1.6281\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6324 - val_loss: 1.4574\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4432 - val_loss: 1.5538\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6066 - val_loss: 7.9674\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0062 - val_loss: 1.5041\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2483 - val_loss: 1.1600\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8297 - val_loss: 1.7578\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1881 - val_loss: 1.2632\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4924 - val_loss: 1.4947\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8014 - val_loss: 1.3582\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0438 - val_loss: 1.4948\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6004 - val_loss: 0.8588\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1806 - val_loss: 8.7347\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3575 - val_loss: 1.6848\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0482 - val_loss: 1.4987\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2642 - val_loss: 1.9002\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0260 - val_loss: 0.7541\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1841 - val_loss: 1.1224\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6941 - val_loss: 6.1414\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5913 - val_loss: 0.8726\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8222 - val_loss: 0.7401\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6617 - val_loss: 1.4873\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1285 - val_loss: 0.6563\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7157 - val_loss: 2.1528\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8717 - val_loss: 0.8673\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8471 - val_loss: 0.8231\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8257 - val_loss: 0.8970\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7089 - val_loss: 0.7961\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7564 - val_loss: 0.7345\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7763 - val_loss: 0.7795\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0424 - val_loss: 0.6619\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.0784 - val_loss: 2.2088\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4549 - val_loss: 0.9961\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7511 - val_loss: 0.9283\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6535 - val_loss: 0.6860\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8430 - val_loss: 1.2992\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8362 - val_loss: 0.7701\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0996 - val_loss: 1.6599\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8745 - val_loss: 0.4536\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6608 - val_loss: 1.0865\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6093 - val_loss: 0.7367\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6690 - val_loss: 0.6109\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6808 - val_loss: 0.9187\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8658 - val_loss: 1.3705\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1098 - val_loss: 1.0994\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8193 - val_loss: 0.5723\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6289 - val_loss: 0.9087\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8447 - val_loss: 0.6108\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6632 - val_loss: 0.5550\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6726 - val_loss: 0.5889\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0304 - val_loss: 0.5041\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5744 - val_loss: 0.9845\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7551 - val_loss: 0.4892\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8947 - val_loss: 2.1903\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8621 - val_loss: 0.4685\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6541 - val_loss: 3.1895\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2428 - val_loss: 0.5643\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.5642696581596834\n",
      "Mean Absolute Error (MAE): 0.5652842764618232\n",
      "Root Mean Squared Error (RMSE): 0.75117884565507\n",
      "Time taken: 1263.019216299057\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 19ms/step - loss: 1364.9884 - val_loss: 1338.8851\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1205.6912 - val_loss: 1240.9515\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1119.4674 - val_loss: 1165.6766\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1054.1072 - val_loss: 1109.5194\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1005.9664 - val_loss: 1067.7180\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 970.6846 - val_loss: 1036.8262\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 945.5517 - val_loss: 1014.9341\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.4940 - val_loss: 1000.4149\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.1487 - val_loss: 989.9959\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 910.9340 - val_loss: 985.0785\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 907.7823 - val_loss: 982.5074\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 906.2631 - val_loss: 980.8061\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 905.5898 - val_loss: 980.2567\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.3439 - val_loss: 979.9692\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2645 - val_loss: 980.2309\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2883 - val_loss: 979.7958\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2419 - val_loss: 980.0300\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2650 - val_loss: 980.1643\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 905.2259 - val_loss: 979.9654\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 905.2734 - val_loss: 979.7550\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2278 - val_loss: 980.1260\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2263 - val_loss: 980.2715\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 905.2520 - val_loss: 980.2575\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2374 - val_loss: 979.9916\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2748 - val_loss: 980.0712\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2867 - val_loss: 980.0026\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2418 - val_loss: 980.1263\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2738 - val_loss: 980.2857\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2458 - val_loss: 980.0891\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 905.2782 - val_loss: 980.1541\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2245 - val_loss: 979.9875\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.2368 - val_loss: 980.2181\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.7163 - val_loss: 980.7812\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 905.7524 - val_loss: 978.6126\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 904.7158 - val_loss: 974.8284\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 874.7620 - val_loss: 925.7395\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 843.9923 - val_loss: 901.8445\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 825.5756 - val_loss: 887.0823\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 809.2902 - val_loss: 879.0569\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 786.4883 - val_loss: 839.8842\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 746.1378 - val_loss: 795.7445\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 704.9482 - val_loss: 760.1252\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 663.9974 - val_loss: 721.6763\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 625.1794 - val_loss: 671.5040\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 587.7063 - val_loss: 635.0447\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 549.2803 - val_loss: 593.6412\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 514.8378 - val_loss: 558.1489\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 482.0726 - val_loss: 525.1735\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 450.1003 - val_loss: 492.4437\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 422.2263 - val_loss: 460.4859\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 397.3327 - val_loss: 432.4627\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 369.2552 - val_loss: 404.6800\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 345.4165 - val_loss: 415.5869\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 325.2974 - val_loss: 352.7670\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 298.1740 - val_loss: 328.2695\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 275.1207 - val_loss: 305.4223\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 255.1947 - val_loss: 283.3788\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 236.4992 - val_loss: 269.8498\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 219.1328 - val_loss: 242.5426\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 201.2489 - val_loss: 226.7247\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 190.1396 - val_loss: 221.0091\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 172.5123 - val_loss: 190.6546\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 154.5687 - val_loss: 172.5066\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 141.9468 - val_loss: 158.2502\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 128.6377 - val_loss: 141.8647\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 116.7990 - val_loss: 132.6999\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 106.1575 - val_loss: 120.3102\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 96.5083 - val_loss: 107.2117\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 87.7260 - val_loss: 101.3206\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 80.3116 - val_loss: 94.6414\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.1361 - val_loss: 83.1364\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 65.4698 - val_loss: 73.7768\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 59.8076 - val_loss: 65.2254\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.8122 - val_loss: 60.0727\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.5349 - val_loss: 57.2994\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.5594 - val_loss: 53.8152\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.9101 - val_loss: 44.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.8973 - val_loss: 42.2242\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.3507 - val_loss: 37.4420\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.9553 - val_loss: 31.6709\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.3776 - val_loss: 28.7665\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.3696 - val_loss: 25.9475\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.0906 - val_loss: 25.1461\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.2703 - val_loss: 20.7415\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.6594 - val_loss: 18.4087\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.5034 - val_loss: 18.3024\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5359 - val_loss: 14.7916\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.3310 - val_loss: 14.2025\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.9249 - val_loss: 11.4441\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 9.9652 - val_loss: 10.5112\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.8413 - val_loss: 12.6717\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5231 - val_loss: 8.7927\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0151 - val_loss: 9.7298\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8838 - val_loss: 8.7702\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7864 - val_loss: 7.2551\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3775 - val_loss: 6.7590\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0096 - val_loss: 5.8712\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7825 - val_loss: 7.6500\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7920 - val_loss: 4.2655\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5111 - val_loss: 4.3642\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1543 - val_loss: 4.5912\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7376 - val_loss: 4.9489\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1075 - val_loss: 17.8248\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8670 - val_loss: 4.0475\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0533 - val_loss: 3.2651\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8577 - val_loss: 3.6585\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6356 - val_loss: 3.8166\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2510 - val_loss: 3.4031\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5907 - val_loss: 3.0781\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5915 - val_loss: 2.7364\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.1163 - val_loss: 3.7425\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4041 - val_loss: 4.6720\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1746 - val_loss: 1.9982\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5502 - val_loss: 1.5687\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6846 - val_loss: 1.4455\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1906 - val_loss: 1.4678\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4793 - val_loss: 3.8753\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3521 - val_loss: 1.2083\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3599 - val_loss: 1.8997\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5070 - val_loss: 1.4051\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1760 - val_loss: 2.3959\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3189 - val_loss: 1.0708\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9972 - val_loss: 1.1209\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1194 - val_loss: 1.5723\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1554 - val_loss: 0.9189\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3491 - val_loss: 0.8870\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6790 - val_loss: 1.6712\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1324 - val_loss: 1.3523\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2453 - val_loss: 1.7822\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3651 - val_loss: 1.6337\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2065 - val_loss: 1.3291\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4523 - val_loss: 1.6262\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7917 - val_loss: 0.7958\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6982 - val_loss: 0.8899\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7504 - val_loss: 1.4613\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0323 - val_loss: 0.7522\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9052 - val_loss: 1.0266\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2922 - val_loss: 3.6920\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5031 - val_loss: 0.6749\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7708 - val_loss: 1.0820\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6783 - val_loss: 1.0719\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9863 - val_loss: 0.8571\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8335 - val_loss: 1.2224\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5638 - val_loss: 0.7623\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5763 - val_loss: 0.6057\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8648 - val_loss: 0.7977\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6710 - val_loss: 0.7949\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9396 - val_loss: 0.5739\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8575 - val_loss: 0.9537\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8532 - val_loss: 0.7726\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7496 - val_loss: 0.4319\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7231 - val_loss: 0.6541\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9267 - val_loss: 0.7793\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5335 - val_loss: 0.5908\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2976 - val_loss: 5.8672\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9263 - val_loss: 0.3732\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4158 - val_loss: 0.3776\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3545 - val_loss: 0.4678\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4841 - val_loss: 0.4989\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9945 - val_loss: 0.6595\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5758 - val_loss: 0.7704\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6744 - val_loss: 0.5279\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5771 - val_loss: 1.7561\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5522 - val_loss: 0.3862\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3219 - val_loss: 0.3993\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5286 - val_loss: 1.3396\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7968 - val_loss: 0.7975\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6140 - val_loss: 0.3679\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5495 - val_loss: 0.7698\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7511 - val_loss: 1.2526\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7907 - val_loss: 0.6384\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4210 - val_loss: 0.5808\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5121 - val_loss: 0.4884\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5699 - val_loss: 2.4293\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7363 - val_loss: 0.8843\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6772 - val_loss: 0.7202\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4440 - val_loss: 0.5007\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5862 - val_loss: 0.4494\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5163 - val_loss: 0.6172\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4193 - val_loss: 0.2859\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7120 - val_loss: 1.5175\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8875 - val_loss: 0.5894\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2544 - val_loss: 0.2171\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3893 - val_loss: 0.7310\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.0513 - val_loss: 0.2723\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2674 - val_loss: 0.2991\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2908 - val_loss: 0.6821\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4073 - val_loss: 0.2587\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2068 - val_loss: 0.1975\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2746 - val_loss: 0.3762\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3071 - val_loss: 0.2867\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4719 - val_loss: 0.2913\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3248 - val_loss: 0.2570\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4060 - val_loss: 0.7427\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6342 - val_loss: 0.3590\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4355 - val_loss: 0.5090\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4181 - val_loss: 0.7284\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3155 - val_loss: 0.2281\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3756 - val_loss: 0.3193\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7042 - val_loss: 1.0791\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 1.0791210283484292\n",
      "Mean Absolute Error (MAE): 0.7792532208754821\n",
      "Root Mean Squared Error (RMSE): 1.0388075030285588\n",
      "Time taken: 1247.730548620224\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1395.6975 - val_loss: 1267.3431\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1233.6094 - val_loss: 1163.2150\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1142.5050 - val_loss: 1087.1295\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1075.8197 - val_loss: 1030.8882\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1026.0847 - val_loss: 989.2922\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 989.5466 - val_loss: 959.2873\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 963.5393 - val_loss: 939.0524\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 945.9888 - val_loss: 925.6209\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 934.7032 - val_loss: 917.6931\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 928.1191 - val_loss: 913.2479\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.6467 - val_loss: 911.1833\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.9136 - val_loss: 910.5591\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.1761 - val_loss: 910.3360\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9202 - val_loss: 910.2787\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8552 - val_loss: 910.4299\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7859 - val_loss: 910.4311\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8016 - val_loss: 910.5956\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8046 - val_loss: 910.5513\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8135 - val_loss: 910.6605\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8014 - val_loss: 910.6487\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7916 - val_loss: 910.4982\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7878 - val_loss: 910.4550\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8257 - val_loss: 910.3641\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7831 - val_loss: 910.4385\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8153 - val_loss: 910.4011\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8102 - val_loss: 910.4470\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8286 - val_loss: 910.3785\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7911 - val_loss: 910.3233\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7998 - val_loss: 910.2553\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7836 - val_loss: 910.3422\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7891 - val_loss: 910.3422\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8247 - val_loss: 910.4922\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.8093 - val_loss: 910.6041\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 914.7756 - val_loss: 881.3044\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 879.2295 - val_loss: 855.7955\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 865.3364 - val_loss: 851.9618\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 854.3364 - val_loss: 836.8275\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 842.2198 - val_loss: 825.0479\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 831.3932 - val_loss: 811.2018\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 816.7955 - val_loss: 789.1813\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 788.7446 - val_loss: 750.6680\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 752.7066 - val_loss: 720.0693\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 719.9297 - val_loss: 699.8408\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 690.2924 - val_loss: 657.5544\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 660.7219 - val_loss: 632.4309\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 632.1613 - val_loss: 600.6161\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 601.3587 - val_loss: 571.9589\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 569.1723 - val_loss: 538.5204\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 542.4374 - val_loss: 516.6236\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 510.6383 - val_loss: 479.0096\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 480.1279 - val_loss: 450.4517\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 456.0879 - val_loss: 428.7799\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 430.4527 - val_loss: 400.5879\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 404.5708 - val_loss: 390.9944\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 379.9053 - val_loss: 352.7379\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 355.5300 - val_loss: 335.2589\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 335.3014 - val_loss: 309.6256\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 312.1355 - val_loss: 296.8652\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 294.1276 - val_loss: 277.2804\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 277.0966 - val_loss: 255.4304\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 254.9638 - val_loss: 236.4163\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 235.9691 - val_loss: 217.3338\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 220.3412 - val_loss: 201.7286\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 201.8474 - val_loss: 190.3410\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 190.2938 - val_loss: 175.7935\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 175.2767 - val_loss: 192.8488\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 165.6729 - val_loss: 151.4785\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 148.7469 - val_loss: 137.7364\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 133.0900 - val_loss: 123.2499\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 122.9763 - val_loss: 108.4895\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 111.1863 - val_loss: 100.2931\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 100.7686 - val_loss: 91.5918\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 92.8053 - val_loss: 92.9431\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 82.3933 - val_loss: 71.0803\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 73.1516 - val_loss: 66.6461\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.1269 - val_loss: 58.5257\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 61.7352 - val_loss: 60.2121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 55.1748 - val_loss: 48.1235\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.9464 - val_loss: 45.4804\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.4803 - val_loss: 39.6501\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.1460 - val_loss: 36.4662\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.3302 - val_loss: 32.7379\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.7552 - val_loss: 31.7570\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.4483 - val_loss: 27.7141\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.5145 - val_loss: 23.9455\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.0579 - val_loss: 24.3558\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.4568 - val_loss: 22.8572\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.8116 - val_loss: 16.7170\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.5071 - val_loss: 15.9295\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.0780 - val_loss: 18.4000\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6163 - val_loss: 14.7970\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.9038 - val_loss: 15.5519\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.9926 - val_loss: 9.5875\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.1515 - val_loss: 9.6830\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.4591 - val_loss: 8.7876\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5530 - val_loss: 8.2511\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.6908 - val_loss: 6.9818\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0943 - val_loss: 7.9732\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4141 - val_loss: 7.0002\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1784 - val_loss: 8.9541\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.7794 - val_loss: 6.9753\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9957 - val_loss: 5.6464\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8096 - val_loss: 5.2892\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0468 - val_loss: 5.4444\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0063 - val_loss: 4.7983\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9325 - val_loss: 5.0299\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0839 - val_loss: 5.4484\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3719 - val_loss: 8.4620\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1225 - val_loss: 3.0557\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4576 - val_loss: 2.8042\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3911 - val_loss: 2.7757\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7903 - val_loss: 3.5096\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7718 - val_loss: 2.3872\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0003 - val_loss: 3.1059\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3342 - val_loss: 2.9993\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6847 - val_loss: 3.6321\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1562 - val_loss: 8.9953\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.0371 - val_loss: 1.6051\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.1509 - val_loss: 3.4347\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.2097 - val_loss: 1.7009\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6657 - val_loss: 2.3038\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.9098 - val_loss: 1.3711\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.0565 - val_loss: 1.5858\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7181 - val_loss: 1.4957\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7020 - val_loss: 1.7947\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3285 - val_loss: 3.7532\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8957 - val_loss: 1.2116\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4737 - val_loss: 1.3948\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9620 - val_loss: 3.0825\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7698 - val_loss: 1.3825\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2836 - val_loss: 1.6949\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4365 - val_loss: 1.3737\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2732 - val_loss: 9.8554\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2496 - val_loss: 1.0657\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9400 - val_loss: 1.0644\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8937 - val_loss: 1.3378\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0022 - val_loss: 1.6644\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2769 - val_loss: 0.9099\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.0018 - val_loss: 0.9931\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2816 - val_loss: 1.1701\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3297 - val_loss: 0.8252\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1983 - val_loss: 1.7246\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4659 - val_loss: 1.7042\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4402 - val_loss: 1.0609\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1537 - val_loss: 1.6765\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7168 - val_loss: 0.6482\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8922 - val_loss: 2.2979\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1638 - val_loss: 0.9179\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9103 - val_loss: 0.5665\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8633 - val_loss: 1.8497\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9865 - val_loss: 1.1497\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5676 - val_loss: 0.9153\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7252 - val_loss: 0.9387\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5235 - val_loss: 0.4081\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4631 - val_loss: 0.3500\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5735 - val_loss: 0.9799\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8289 - val_loss: 0.5140\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7459 - val_loss: 0.5564\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6372 - val_loss: 0.6839\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3214 - val_loss: 0.4670\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5545 - val_loss: 0.5316\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4798 - val_loss: 0.3297\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9395 - val_loss: 0.8527\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9542 - val_loss: 0.3571\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6138 - val_loss: 0.7086\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7278 - val_loss: 0.7984\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4614 - val_loss: 0.4243\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3341 - val_loss: 0.2729\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6016 - val_loss: 0.6646\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4220 - val_loss: 0.7013\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7791 - val_loss: 0.4094\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5099 - val_loss: 0.4192\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4806 - val_loss: 0.4035\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7936 - val_loss: 1.2114\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6728 - val_loss: 1.3612\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2767 - val_loss: 0.4944\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3782 - val_loss: 0.2762\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5314 - val_loss: 0.7317\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4481 - val_loss: 0.7104\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6767 - val_loss: 0.4664\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4022 - val_loss: 0.2182\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3962 - val_loss: 0.3257\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4474 - val_loss: 0.6388\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5340 - val_loss: 0.3353\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7234 - val_loss: 2.7673\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5274 - val_loss: 0.1718\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5405 - val_loss: 0.3212\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7373 - val_loss: 4.8706\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8940 - val_loss: 0.2829\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2702 - val_loss: 0.7003\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3211 - val_loss: 0.4533\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9193 - val_loss: 0.5288\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3673 - val_loss: 0.2984\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2101 - val_loss: 2.0211\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6168 - val_loss: 0.3316\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2550 - val_loss: 0.2463\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2005 - val_loss: 0.1933\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2465 - val_loss: 0.3378\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2779 - val_loss: 0.2611\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2491 - val_loss: 0.2301\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.23012228843508484\n",
      "Mean Absolute Error (MAE): 0.3583427081868377\n",
      "Root Mean Squared Error (RMSE): 0.47971062989586216\n",
      "Time taken: 1250.7088220119476\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1370.5743 - val_loss: 1261.9594\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1209.3710 - val_loss: 1163.4338\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1125.4377 - val_loss: 1091.2872\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1063.0764 - val_loss: 1037.6042\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1016.0626 - val_loss: 996.2547\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 980.6672 - val_loss: 967.0716\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 956.3854 - val_loss: 947.4951\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 940.3231 - val_loss: 935.0724\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 930.3782 - val_loss: 927.5603\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.5808 - val_loss: 923.4592\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.5748 - val_loss: 921.4029\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1852 - val_loss: 920.4515\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.5662 - val_loss: 920.3082\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.3444 - val_loss: 920.1394\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2800 - val_loss: 920.1458\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.2262 - val_loss: 920.0950\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2869 - val_loss: 920.0568\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2576 - val_loss: 920.1875\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2303 - val_loss: 920.2109\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2480 - val_loss: 920.2163\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2378 - val_loss: 920.2618\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2356 - val_loss: 920.1912\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.3082 - val_loss: 920.4014\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2424 - val_loss: 920.2144\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2074 - val_loss: 920.2421\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2432 - val_loss: 920.1959\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2505 - val_loss: 920.1463\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 919.2697 - val_loss: 920.2055\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 919.2551 - val_loss: 920.1256\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2266 - val_loss: 920.2985\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2432 - val_loss: 920.2711\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.8960 - val_loss: 911.5753\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 882.9753 - val_loss: 866.3987\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 856.8669 - val_loss: 846.3381\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 835.7638 - val_loss: 820.0797\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 808.2942 - val_loss: 786.3016\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 765.2967 - val_loss: 734.4402\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 715.6710 - val_loss: 690.1279\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 669.4967 - val_loss: 651.1579\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 626.5929 - val_loss: 604.3978\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 590.6565 - val_loss: 569.7296\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 553.0839 - val_loss: 535.1573\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 517.5995 - val_loss: 499.0706\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 481.7663 - val_loss: 470.1187\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 449.3869 - val_loss: 437.2095\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 420.3741 - val_loss: 404.1863\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 391.1406 - val_loss: 378.1299\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 366.0901 - val_loss: 351.5874\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 340.3893 - val_loss: 331.6459\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 319.2205 - val_loss: 309.3315\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 293.9721 - val_loss: 285.0338\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 272.5584 - val_loss: 264.0847\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 253.5190 - val_loss: 244.1181\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 232.5776 - val_loss: 227.6757\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 213.8003 - val_loss: 208.8676\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 194.9012 - val_loss: 191.2375\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 179.0048 - val_loss: 172.3487\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 162.5756 - val_loss: 169.5056\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 147.2085 - val_loss: 144.0225\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 133.8887 - val_loss: 132.2999\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 122.7041 - val_loss: 120.6745\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 111.4013 - val_loss: 110.1330\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 99.1346 - val_loss: 99.1981\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.8836 - val_loss: 87.9889\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 81.9161 - val_loss: 79.8974\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 72.6343 - val_loss: 75.0485\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.3790 - val_loss: 67.0088\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.3984 - val_loss: 63.5644\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 56.2716 - val_loss: 53.0782\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 47.8481 - val_loss: 48.1617\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.1084 - val_loss: 47.6734\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.0825 - val_loss: 42.4714\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.1203 - val_loss: 46.8488\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.5655 - val_loss: 31.3479\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.5238 - val_loss: 29.8445\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.5763 - val_loss: 28.8018\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.8160 - val_loss: 28.5410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.4972 - val_loss: 21.9738\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.5632 - val_loss: 21.8572\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.0531 - val_loss: 18.6502\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.8374 - val_loss: 16.6772\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.6584 - val_loss: 14.8162\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.6515 - val_loss: 15.5100\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7901 - val_loss: 11.4097\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1521 - val_loss: 10.1976\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3059 - val_loss: 10.9532\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5029 - val_loss: 8.8185\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7289 - val_loss: 10.4883\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9585 - val_loss: 8.5195\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3383 - val_loss: 8.4330\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.6353 - val_loss: 8.1510\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5645 - val_loss: 6.0077\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2640 - val_loss: 5.4942\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4495 - val_loss: 4.9659\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5975 - val_loss: 11.8099\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8611 - val_loss: 5.0997\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5565 - val_loss: 4.5945\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1979 - val_loss: 4.3557\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3573 - val_loss: 41.4748\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.4081 - val_loss: 4.6160\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3406 - val_loss: 3.3422\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1427 - val_loss: 3.7265\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1804 - val_loss: 2.8939\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0806 - val_loss: 3.3863\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0581 - val_loss: 3.2643\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7663 - val_loss: 3.4414\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4051 - val_loss: 6.8597\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7314 - val_loss: 2.6502\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2999 - val_loss: 2.6943\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5156 - val_loss: 2.2769\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3980 - val_loss: 7.8469\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6728 - val_loss: 2.0166\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6450 - val_loss: 1.7003\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3090 - val_loss: 3.8211\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2686 - val_loss: 1.7347\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2279 - val_loss: 1.6659\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1018 - val_loss: 2.2661\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1016 - val_loss: 2.4616\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6452 - val_loss: 1.9834\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1069 - val_loss: 2.8581\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8445 - val_loss: 1.8733\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7385 - val_loss: 3.6749\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8040 - val_loss: 1.3364\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1929 - val_loss: 1.5654\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8404 - val_loss: 1.4960\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4646 - val_loss: 1.0734\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6099 - val_loss: 1.8593\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3717 - val_loss: 1.2382\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1182 - val_loss: 1.0984\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2559 - val_loss: 1.3176\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5367 - val_loss: 3.6525\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2102 - val_loss: 1.2129\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8377 - val_loss: 0.9103\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6963 - val_loss: 0.8228\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6837 - val_loss: 0.7813\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6918 - val_loss: 0.7491\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9287 - val_loss: 1.0200\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1385 - val_loss: 0.9958\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7614 - val_loss: 0.8006\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1883 - val_loss: 1.6726\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7800 - val_loss: 5.3600\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9460 - val_loss: 0.6826\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6205 - val_loss: 0.6184\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7699 - val_loss: 0.9570\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8126 - val_loss: 1.0886\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2814 - val_loss: 1.9012\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8021 - val_loss: 1.5605\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0556 - val_loss: 0.9332\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7840 - val_loss: 0.9044\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1484 - val_loss: 1.0215\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5495 - val_loss: 0.7258\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7013 - val_loss: 0.5371\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7324 - val_loss: 0.7147\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7071 - val_loss: 0.8768\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2081 - val_loss: 0.6262\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4732 - val_loss: 0.4797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4320 - val_loss: 0.4781\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5168 - val_loss: 1.0815\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7115 - val_loss: 0.9528\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1101 - val_loss: 2.2876\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1271 - val_loss: 0.5516\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5010 - val_loss: 0.4576\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4731 - val_loss: 0.8093\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9116 - val_loss: 3.7125\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9922 - val_loss: 0.4310\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 0.6746 - val_loss: 0.4983\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8392 - val_loss: 0.6204\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4716 - val_loss: 0.4053\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5527 - val_loss: 0.7252\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4104 - val_loss: 0.3179\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.3500 - val_loss: 0.3762\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.4991 - val_loss: 0.4130\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.7140 - val_loss: 1.0141\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7288 - val_loss: 1.2183\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8789 - val_loss: 0.4656\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3747 - val_loss: 0.3913\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.5097 - val_loss: 0.7970\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8065 - val_loss: 0.6991\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4084 - val_loss: 0.4540\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5678 - val_loss: 1.5784\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.7479 - val_loss: 0.4711\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5763 - val_loss: 0.4367\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4654 - val_loss: 1.0029\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6830 - val_loss: 0.7328\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5763 - val_loss: 0.3212\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4436 - val_loss: 0.8385\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6877 - val_loss: 0.3162\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3374 - val_loss: 0.2592\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6212 - val_loss: 0.2680\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4697 - val_loss: 0.5028\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6715 - val_loss: 0.5701\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4041 - val_loss: 0.5889\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.3164 - val_loss: 0.4288\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.5633 - val_loss: 1.1542\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4257 - val_loss: 0.5979\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.4232 - val_loss: 0.3225\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5169 - val_loss: 0.5065\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3489 - val_loss: 0.8145\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3946 - val_loss: 0.4612\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3855 - val_loss: 0.5641\n",
      "16/16 [==============================] - 1s 16ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.5634545436298172\n",
      "Mean Absolute Error (MAE): 0.5273094110825763\n",
      "Root Mean Squared Error (RMSE): 0.750636092677282\n",
      "Time taken: 1436.8331835269928\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 22s 48ms/step - loss: 1391.7877 - val_loss: 1268.1538\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1230.6776 - val_loss: 1162.9681\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1141.6339 - val_loss: 1085.4991\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1075.6488 - val_loss: 1028.0826\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1026.4397 - val_loss: 985.6698\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 990.4354 - val_loss: 955.3124\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 964.9167 - val_loss: 934.1652\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 947.5471 - val_loss: 920.6155\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 936.4016 - val_loss: 912.1363\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 929.7730 - val_loss: 907.4791\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 926.1692 - val_loss: 905.1337\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 924.4957 - val_loss: 904.1713\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.7078 - val_loss: 903.8246\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.4709 - val_loss: 903.6057\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3648 - val_loss: 903.6140\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3129 - val_loss: 903.5750\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3369 - val_loss: 903.4912\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3423 - val_loss: 903.5160\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3442 - val_loss: 903.6083\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3280 - val_loss: 903.5692\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3494 - val_loss: 903.6065\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3051 - val_loss: 903.7409\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3288 - val_loss: 903.7070\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.2914 - val_loss: 903.6376\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3378 - val_loss: 903.7006\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3261 - val_loss: 903.6040\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3480 - val_loss: 903.7070\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3301 - val_loss: 903.6268\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3033 - val_loss: 903.6534\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.2853 - val_loss: 903.6029\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3196 - val_loss: 903.6052\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3052 - val_loss: 903.6177\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3441 - val_loss: 903.5528\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3422 - val_loss: 903.6455\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3212 - val_loss: 903.6874\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.2811 - val_loss: 903.7136\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3229 - val_loss: 903.7632\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3148 - val_loss: 903.6667\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3385 - val_loss: 903.6644\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3228 - val_loss: 903.5854\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3215 - val_loss: 903.6143\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3496 - val_loss: 903.5810\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3287 - val_loss: 903.5880\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3287 - val_loss: 903.5402\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 923.3176 - val_loss: 903.6306\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 923.3238 - val_loss: 903.5369\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 921.6238 - val_loss: 891.2531\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 888.6862 - val_loss: 863.6967\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 866.3318 - val_loss: 840.2009\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 838.9891 - val_loss: 814.4859\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 802.3539 - val_loss: 779.4138\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 766.1889 - val_loss: 729.5243\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 733.3776 - val_loss: 699.1124\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 701.7318 - val_loss: 667.9788\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 671.0096 - val_loss: 636.5853\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 638.2849 - val_loss: 607.8869\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 607.6493 - val_loss: 577.3002\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 575.2101 - val_loss: 545.2972\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 546.4233 - val_loss: 515.7217\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 517.5970 - val_loss: 490.2447\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 489.3080 - val_loss: 461.4963\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 464.0497 - val_loss: 436.8006\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 437.7023 - val_loss: 410.7970\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 413.0432 - val_loss: 388.1194\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 388.7279 - val_loss: 365.0103\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 366.2124 - val_loss: 344.2964\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 345.3533 - val_loss: 322.4110\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 324.1049 - val_loss: 304.7992\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 303.7645 - val_loss: 289.0149\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 287.4096 - val_loss: 267.4281\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 265.7798 - val_loss: 245.2772\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 248.0408 - val_loss: 230.8149\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 230.2793 - val_loss: 212.3332\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 214.6751 - val_loss: 196.7509\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 198.8909 - val_loss: 182.0639\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 185.7336 - val_loss: 168.3075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 170.0593 - val_loss: 154.1342\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 155.7534 - val_loss: 140.9777\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 141.3982 - val_loss: 127.5970\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 129.3808 - val_loss: 113.5849\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 116.1276 - val_loss: 102.4495\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 105.2795 - val_loss: 90.9257\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 94.1767 - val_loss: 86.0389\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 86.5724 - val_loss: 77.0545\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 78.1913 - val_loss: 71.5277\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 71.4425 - val_loss: 62.2753\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 64.6329 - val_loss: 57.2769\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 59.3865 - val_loss: 51.0136\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 54.4007 - val_loss: 47.3877\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 49.3015 - val_loss: 46.2739\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 45.2180 - val_loss: 42.2815\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 40.8833 - val_loss: 37.6221\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 37.2921 - val_loss: 31.8661\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 33.0251 - val_loss: 29.8164\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 31.0101 - val_loss: 27.5263\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 27.7514 - val_loss: 24.3706\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 25.2018 - val_loss: 21.1575\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 22.6537 - val_loss: 19.6204\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 21.4032 - val_loss: 17.5457\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 19.6172 - val_loss: 15.8052\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 17.2006 - val_loss: 14.9161\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 16.2120 - val_loss: 13.6952\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 14.7534 - val_loss: 12.1872\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 14.4422 - val_loss: 11.2896\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 12.9960 - val_loss: 11.1981\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 11.6756 - val_loss: 11.3217\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 10.8011 - val_loss: 10.9144\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 10.0489 - val_loss: 10.8466\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 9.9794 - val_loss: 7.6660\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 8.5517 - val_loss: 6.9083\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 7.8069 - val_loss: 7.5995\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 8.2575 - val_loss: 6.8194\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 6.6735 - val_loss: 5.8505\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 6.8797 - val_loss: 5.9592\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 5.6964 - val_loss: 4.5045\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 6.1570 - val_loss: 5.0521\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 5.1676 - val_loss: 4.2604\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 5.1356 - val_loss: 5.0254\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 4.3485 - val_loss: 3.6594\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.9510 - val_loss: 4.9776\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.7598 - val_loss: 3.8055\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.3939 - val_loss: 3.2657\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.5992 - val_loss: 2.6036\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.3829 - val_loss: 4.0811\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.4797 - val_loss: 2.6222\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.9885 - val_loss: 2.7205\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.5696 - val_loss: 3.5666\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.0194 - val_loss: 2.5868\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.3726 - val_loss: 2.6113\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.6570 - val_loss: 2.3290\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.5663 - val_loss: 2.3487\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.1825 - val_loss: 1.9486\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.4352 - val_loss: 4.3561\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.2549 - val_loss: 2.1128\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.0003 - val_loss: 2.0817\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.0441 - val_loss: 2.0051\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.0067 - val_loss: 1.6376\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.5850 - val_loss: 1.5597\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.8130 - val_loss: 4.3520\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.1024 - val_loss: 1.5954\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.8365 - val_loss: 3.7160\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.7999 - val_loss: 1.2011\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.5957 - val_loss: 1.1466\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.5618 - val_loss: 2.4225\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.4355 - val_loss: 1.2593\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.3801 - val_loss: 0.9463\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.0774 - val_loss: 1.1964\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.1982 - val_loss: 1.5179\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.2544 - val_loss: 2.9691\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.5809 - val_loss: 1.2430\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.0030 - val_loss: 1.2654\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.7243 - val_loss: 1.0812\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.2033 - val_loss: 1.0337\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 18s 45ms/step - loss: 1.3558 - val_loss: 1.0936\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 0.9496 - val_loss: 1.0035\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.3205 - val_loss: 1.1641\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.1510 - val_loss: 0.9091\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.1398 - val_loss: 0.8581\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.1982 - val_loss: 1.3442\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.1580 - val_loss: 0.8370\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.7505 - val_loss: 0.8059\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.0940 - val_loss: 1.0480\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.2440 - val_loss: 0.9378\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.0770 - val_loss: 0.9576\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.7506 - val_loss: 0.6107\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.8309 - val_loss: 0.6031\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1846 - val_loss: 1.3074\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9155 - val_loss: 0.6024\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6291 - val_loss: 0.6436\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7502 - val_loss: 0.5954\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4355 - val_loss: 0.8490\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6420 - val_loss: 0.8642\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4134 - val_loss: 0.7484\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4636 - val_loss: 0.5094\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.7181 - val_loss: 0.6874\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7042 - val_loss: 0.7373\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6454 - val_loss: 0.6446\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.2055 - val_loss: 2.6205\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7959 - val_loss: 0.4847\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.4966 - val_loss: 0.6913\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.4979 - val_loss: 0.4259\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6292 - val_loss: 1.3133\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7762 - val_loss: 0.5269\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.5603 - val_loss: 0.3853\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.0632 - val_loss: 1.2080\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.5356 - val_loss: 0.5642\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.5954 - val_loss: 0.4576\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7331 - val_loss: 1.1132\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.5867 - val_loss: 0.5284\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6366 - val_loss: 0.5440\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6084 - val_loss: 0.4994\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6666 - val_loss: 0.6748\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.5165 - val_loss: 0.3788\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5929 - val_loss: 0.7240\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7352 - val_loss: 0.4062\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5441 - val_loss: 0.6094\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6618 - val_loss: 0.7582\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5545 - val_loss: 0.3939\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6031 - val_loss: 0.6767\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.4081 - val_loss: 0.4338\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.43377232682053796\n",
      "Mean Absolute Error (MAE): 0.48613840123818397\n",
      "Root Mean Squared Error (RMSE): 0.658613943688211\n",
      "Time taken: 3616.6104140281677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_2136\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.564270  0.565284  0.751179  1263.019216\n",
      "1        2  1.079121  0.779253  1.038808  1247.730549\n",
      "2        3  0.230122  0.358343  0.479711  1250.708822\n",
      "3        4  0.563455  0.527309  0.750636  1436.833184\n",
      "4        5  0.433772  0.486138  0.658614  3616.610414\n",
      "5  Average  0.574148  0.543266  0.735789  1762.980437\n",
      "Results saved to 'DL_Result_PL_model_2_smoothing2_iReg_f_over.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_smoothing2_iReg_f_over.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_2_smoothing2_iReg_f_over.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
