{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f_over.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.476682</td>\n",
       "      <td>193.330550</td>\n",
       "      <td>96.162106</td>\n",
       "      <td>142.324408</td>\n",
       "      <td>204.069137</td>\n",
       "      <td>226.487294</td>\n",
       "      <td>159.507560</td>\n",
       "      <td>188.621279</td>\n",
       "      <td>145.304518</td>\n",
       "      <td>162.856305</td>\n",
       "      <td>...</td>\n",
       "      <td>123.466814</td>\n",
       "      <td>137.342275</td>\n",
       "      <td>136.770446</td>\n",
       "      <td>131.359859</td>\n",
       "      <td>186.077649</td>\n",
       "      <td>181.399217</td>\n",
       "      <td>154.185528</td>\n",
       "      <td>189.427030</td>\n",
       "      <td>78.635342</td>\n",
       "      <td>137.662134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.640865</td>\n",
       "      <td>193.421432</td>\n",
       "      <td>96.473737</td>\n",
       "      <td>142.538333</td>\n",
       "      <td>203.821631</td>\n",
       "      <td>226.279209</td>\n",
       "      <td>159.292625</td>\n",
       "      <td>188.367463</td>\n",
       "      <td>145.516914</td>\n",
       "      <td>163.023106</td>\n",
       "      <td>...</td>\n",
       "      <td>123.548958</td>\n",
       "      <td>137.168631</td>\n",
       "      <td>137.034249</td>\n",
       "      <td>131.572686</td>\n",
       "      <td>185.877640</td>\n",
       "      <td>181.117865</td>\n",
       "      <td>154.137553</td>\n",
       "      <td>189.195777</td>\n",
       "      <td>78.208883</td>\n",
       "      <td>137.446677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.804876</td>\n",
       "      <td>193.513963</td>\n",
       "      <td>96.785755</td>\n",
       "      <td>142.754701</td>\n",
       "      <td>203.574562</td>\n",
       "      <td>226.072615</td>\n",
       "      <td>159.074802</td>\n",
       "      <td>188.115384</td>\n",
       "      <td>145.723935</td>\n",
       "      <td>163.189982</td>\n",
       "      <td>...</td>\n",
       "      <td>123.632229</td>\n",
       "      <td>136.999740</td>\n",
       "      <td>137.298953</td>\n",
       "      <td>131.785763</td>\n",
       "      <td>185.675631</td>\n",
       "      <td>180.833507</td>\n",
       "      <td>154.094457</td>\n",
       "      <td>188.969105</td>\n",
       "      <td>77.784021</td>\n",
       "      <td>137.231304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.969023</td>\n",
       "      <td>193.608218</td>\n",
       "      <td>97.098013</td>\n",
       "      <td>142.973684</td>\n",
       "      <td>203.327923</td>\n",
       "      <td>225.867702</td>\n",
       "      <td>158.854006</td>\n",
       "      <td>187.865118</td>\n",
       "      <td>145.925562</td>\n",
       "      <td>163.356997</td>\n",
       "      <td>...</td>\n",
       "      <td>123.716187</td>\n",
       "      <td>136.835852</td>\n",
       "      <td>137.564418</td>\n",
       "      <td>131.999238</td>\n",
       "      <td>185.471432</td>\n",
       "      <td>180.546277</td>\n",
       "      <td>154.056215</td>\n",
       "      <td>188.747092</td>\n",
       "      <td>77.360992</td>\n",
       "      <td>137.016240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.133541</td>\n",
       "      <td>193.704239</td>\n",
       "      <td>97.410161</td>\n",
       "      <td>143.195258</td>\n",
       "      <td>203.081681</td>\n",
       "      <td>225.664618</td>\n",
       "      <td>158.630091</td>\n",
       "      <td>187.616890</td>\n",
       "      <td>146.121871</td>\n",
       "      <td>163.524314</td>\n",
       "      <td>...</td>\n",
       "      <td>123.800229</td>\n",
       "      <td>136.677121</td>\n",
       "      <td>137.830676</td>\n",
       "      <td>132.213338</td>\n",
       "      <td>185.265040</td>\n",
       "      <td>180.256474</td>\n",
       "      <td>154.022577</td>\n",
       "      <td>188.529873</td>\n",
       "      <td>76.940078</td>\n",
       "      <td>136.801560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>186.890133</td>\n",
       "      <td>167.136498</td>\n",
       "      <td>124.028245</td>\n",
       "      <td>85.043260</td>\n",
       "      <td>231.545990</td>\n",
       "      <td>216.365295</td>\n",
       "      <td>189.006691</td>\n",
       "      <td>169.742748</td>\n",
       "      <td>155.697636</td>\n",
       "      <td>143.211613</td>\n",
       "      <td>...</td>\n",
       "      <td>122.724440</td>\n",
       "      <td>157.900833</td>\n",
       "      <td>134.673921</td>\n",
       "      <td>111.367070</td>\n",
       "      <td>196.520345</td>\n",
       "      <td>182.752655</td>\n",
       "      <td>187.431706</td>\n",
       "      <td>171.551963</td>\n",
       "      <td>125.922063</td>\n",
       "      <td>87.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>186.702775</td>\n",
       "      <td>166.895061</td>\n",
       "      <td>123.939206</td>\n",
       "      <td>85.009440</td>\n",
       "      <td>231.401355</td>\n",
       "      <td>216.246492</td>\n",
       "      <td>189.010895</td>\n",
       "      <td>169.791443</td>\n",
       "      <td>155.558933</td>\n",
       "      <td>142.925403</td>\n",
       "      <td>...</td>\n",
       "      <td>122.509028</td>\n",
       "      <td>157.845609</td>\n",
       "      <td>134.558118</td>\n",
       "      <td>111.428789</td>\n",
       "      <td>196.649728</td>\n",
       "      <td>182.907850</td>\n",
       "      <td>187.353642</td>\n",
       "      <td>171.458225</td>\n",
       "      <td>126.062032</td>\n",
       "      <td>87.723378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>186.514267</td>\n",
       "      <td>166.652393</td>\n",
       "      <td>123.846834</td>\n",
       "      <td>84.976688</td>\n",
       "      <td>231.254031</td>\n",
       "      <td>216.126424</td>\n",
       "      <td>189.009812</td>\n",
       "      <td>169.838965</td>\n",
       "      <td>155.421963</td>\n",
       "      <td>142.636550</td>\n",
       "      <td>...</td>\n",
       "      <td>122.292300</td>\n",
       "      <td>157.786215</td>\n",
       "      <td>134.441261</td>\n",
       "      <td>111.491544</td>\n",
       "      <td>196.781185</td>\n",
       "      <td>183.064300</td>\n",
       "      <td>187.272978</td>\n",
       "      <td>171.364782</td>\n",
       "      <td>126.203656</td>\n",
       "      <td>87.886959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>186.323815</td>\n",
       "      <td>166.408256</td>\n",
       "      <td>123.751067</td>\n",
       "      <td>84.945188</td>\n",
       "      <td>231.103987</td>\n",
       "      <td>216.005000</td>\n",
       "      <td>189.003431</td>\n",
       "      <td>169.885537</td>\n",
       "      <td>155.286893</td>\n",
       "      <td>142.345518</td>\n",
       "      <td>...</td>\n",
       "      <td>122.074546</td>\n",
       "      <td>157.723039</td>\n",
       "      <td>134.323109</td>\n",
       "      <td>111.555569</td>\n",
       "      <td>196.914248</td>\n",
       "      <td>183.222101</td>\n",
       "      <td>187.189564</td>\n",
       "      <td>171.271735</td>\n",
       "      <td>126.346672</td>\n",
       "      <td>88.051739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>186.130811</td>\n",
       "      <td>166.162262</td>\n",
       "      <td>123.652019</td>\n",
       "      <td>84.915006</td>\n",
       "      <td>230.951197</td>\n",
       "      <td>215.882337</td>\n",
       "      <td>188.991762</td>\n",
       "      <td>169.931555</td>\n",
       "      <td>155.153987</td>\n",
       "      <td>142.052720</td>\n",
       "      <td>...</td>\n",
       "      <td>121.855963</td>\n",
       "      <td>157.656580</td>\n",
       "      <td>134.203592</td>\n",
       "      <td>111.621060</td>\n",
       "      <td>197.048575</td>\n",
       "      <td>183.381282</td>\n",
       "      <td>187.103513</td>\n",
       "      <td>171.179020</td>\n",
       "      <td>126.490913</td>\n",
       "      <td>88.217522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     165.476682  193.330550   96.162106  142.324408  204.069137  226.487294   \n",
       "1     165.640865  193.421432   96.473737  142.538333  203.821631  226.279209   \n",
       "2     165.804876  193.513963   96.785755  142.754701  203.574562  226.072615   \n",
       "3     165.969023  193.608218   97.098013  142.973684  203.327923  225.867702   \n",
       "4     166.133541  193.704239   97.410161  143.195258  203.081681  225.664618   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  186.890133  167.136498  124.028245   85.043260  231.545990  216.365295   \n",
       "2439  186.702775  166.895061  123.939206   85.009440  231.401355  216.246492   \n",
       "2440  186.514267  166.652393  123.846834   84.976688  231.254031  216.126424   \n",
       "2441  186.323815  166.408256  123.751067   84.945188  231.103987  216.005000   \n",
       "2442  186.130811  166.162262  123.652019   84.915006  230.951197  215.882337   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     159.507560  188.621279  145.304518  162.856305  ...  123.466814   \n",
       "1     159.292625  188.367463  145.516914  163.023106  ...  123.548958   \n",
       "2     159.074802  188.115384  145.723935  163.189982  ...  123.632229   \n",
       "3     158.854006  187.865118  145.925562  163.356997  ...  123.716187   \n",
       "4     158.630091  187.616890  146.121871  163.524314  ...  123.800229   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  189.006691  169.742748  155.697636  143.211613  ...  122.724440   \n",
       "2439  189.010895  169.791443  155.558933  142.925403  ...  122.509028   \n",
       "2440  189.009812  169.838965  155.421963  142.636550  ...  122.292300   \n",
       "2441  189.003431  169.885537  155.286893  142.345518  ...  122.074546   \n",
       "2442  188.991762  169.931555  155.153987  142.052720  ...  121.855963   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     137.342275  136.770446  131.359859  186.077649  181.399217  154.185528   \n",
       "1     137.168631  137.034249  131.572686  185.877640  181.117865  154.137553   \n",
       "2     136.999740  137.298953  131.785763  185.675631  180.833507  154.094457   \n",
       "3     136.835852  137.564418  131.999238  185.471432  180.546277  154.056215   \n",
       "4     136.677121  137.830676  132.213338  185.265040  180.256474  154.022577   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  157.900833  134.673921  111.367070  196.520345  182.752655  187.431706   \n",
       "2439  157.845609  134.558118  111.428789  196.649728  182.907850  187.353642   \n",
       "2440  157.786215  134.441261  111.491544  196.781185  183.064300  187.272978   \n",
       "2441  157.723039  134.323109  111.555569  196.914248  183.222101  187.189564   \n",
       "2442  157.656580  134.203592  111.621060  197.048575  183.381282  187.103513   \n",
       "\n",
       "              45          46          47  \n",
       "0     189.427030   78.635342  137.662134  \n",
       "1     189.195777   78.208883  137.446677  \n",
       "2     188.969105   77.784021  137.231304  \n",
       "3     188.747092   77.360992  137.016240  \n",
       "4     188.529873   76.940078  136.801560  \n",
       "...          ...         ...         ...  \n",
       "2438  171.551963  125.922063   87.561227  \n",
       "2439  171.458225  126.062032   87.723378  \n",
       "2440  171.364782  126.203656   87.886959  \n",
       "2441  171.271735  126.346672   88.051739  \n",
       "2442  171.179020  126.490913   88.217522  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.476682</td>\n",
       "      <td>193.330550</td>\n",
       "      <td>96.162106</td>\n",
       "      <td>142.324408</td>\n",
       "      <td>204.069137</td>\n",
       "      <td>226.487294</td>\n",
       "      <td>159.507560</td>\n",
       "      <td>188.621279</td>\n",
       "      <td>145.304518</td>\n",
       "      <td>162.856305</td>\n",
       "      <td>...</td>\n",
       "      <td>123.466814</td>\n",
       "      <td>137.342275</td>\n",
       "      <td>136.770446</td>\n",
       "      <td>131.359859</td>\n",
       "      <td>186.077649</td>\n",
       "      <td>181.399217</td>\n",
       "      <td>154.185528</td>\n",
       "      <td>189.427030</td>\n",
       "      <td>78.635342</td>\n",
       "      <td>137.662134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.640865</td>\n",
       "      <td>193.421432</td>\n",
       "      <td>96.473737</td>\n",
       "      <td>142.538333</td>\n",
       "      <td>203.821631</td>\n",
       "      <td>226.279209</td>\n",
       "      <td>159.292625</td>\n",
       "      <td>188.367463</td>\n",
       "      <td>145.516914</td>\n",
       "      <td>163.023106</td>\n",
       "      <td>...</td>\n",
       "      <td>123.548958</td>\n",
       "      <td>137.168631</td>\n",
       "      <td>137.034249</td>\n",
       "      <td>131.572686</td>\n",
       "      <td>185.877640</td>\n",
       "      <td>181.117865</td>\n",
       "      <td>154.137553</td>\n",
       "      <td>189.195777</td>\n",
       "      <td>78.208883</td>\n",
       "      <td>137.446677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.804876</td>\n",
       "      <td>193.513963</td>\n",
       "      <td>96.785755</td>\n",
       "      <td>142.754701</td>\n",
       "      <td>203.574562</td>\n",
       "      <td>226.072615</td>\n",
       "      <td>159.074802</td>\n",
       "      <td>188.115384</td>\n",
       "      <td>145.723935</td>\n",
       "      <td>163.189982</td>\n",
       "      <td>...</td>\n",
       "      <td>123.632229</td>\n",
       "      <td>136.999740</td>\n",
       "      <td>137.298953</td>\n",
       "      <td>131.785763</td>\n",
       "      <td>185.675631</td>\n",
       "      <td>180.833507</td>\n",
       "      <td>154.094457</td>\n",
       "      <td>188.969105</td>\n",
       "      <td>77.784021</td>\n",
       "      <td>137.231304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.969023</td>\n",
       "      <td>193.608218</td>\n",
       "      <td>97.098013</td>\n",
       "      <td>142.973684</td>\n",
       "      <td>203.327923</td>\n",
       "      <td>225.867702</td>\n",
       "      <td>158.854006</td>\n",
       "      <td>187.865118</td>\n",
       "      <td>145.925562</td>\n",
       "      <td>163.356997</td>\n",
       "      <td>...</td>\n",
       "      <td>123.716187</td>\n",
       "      <td>136.835852</td>\n",
       "      <td>137.564418</td>\n",
       "      <td>131.999238</td>\n",
       "      <td>185.471432</td>\n",
       "      <td>180.546277</td>\n",
       "      <td>154.056215</td>\n",
       "      <td>188.747092</td>\n",
       "      <td>77.360992</td>\n",
       "      <td>137.016240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.133541</td>\n",
       "      <td>193.704239</td>\n",
       "      <td>97.410161</td>\n",
       "      <td>143.195258</td>\n",
       "      <td>203.081681</td>\n",
       "      <td>225.664618</td>\n",
       "      <td>158.630091</td>\n",
       "      <td>187.616890</td>\n",
       "      <td>146.121871</td>\n",
       "      <td>163.524314</td>\n",
       "      <td>...</td>\n",
       "      <td>123.800229</td>\n",
       "      <td>136.677121</td>\n",
       "      <td>137.830676</td>\n",
       "      <td>132.213338</td>\n",
       "      <td>185.265040</td>\n",
       "      <td>180.256474</td>\n",
       "      <td>154.022577</td>\n",
       "      <td>188.529873</td>\n",
       "      <td>76.940078</td>\n",
       "      <td>136.801560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>186.890133</td>\n",
       "      <td>167.136498</td>\n",
       "      <td>124.028245</td>\n",
       "      <td>85.043260</td>\n",
       "      <td>231.545990</td>\n",
       "      <td>216.365295</td>\n",
       "      <td>189.006691</td>\n",
       "      <td>169.742748</td>\n",
       "      <td>155.697636</td>\n",
       "      <td>143.211613</td>\n",
       "      <td>...</td>\n",
       "      <td>122.724440</td>\n",
       "      <td>157.900833</td>\n",
       "      <td>134.673921</td>\n",
       "      <td>111.367070</td>\n",
       "      <td>196.520345</td>\n",
       "      <td>182.752655</td>\n",
       "      <td>187.431706</td>\n",
       "      <td>171.551963</td>\n",
       "      <td>125.922063</td>\n",
       "      <td>87.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>186.702775</td>\n",
       "      <td>166.895061</td>\n",
       "      <td>123.939206</td>\n",
       "      <td>85.009440</td>\n",
       "      <td>231.401355</td>\n",
       "      <td>216.246492</td>\n",
       "      <td>189.010895</td>\n",
       "      <td>169.791443</td>\n",
       "      <td>155.558933</td>\n",
       "      <td>142.925403</td>\n",
       "      <td>...</td>\n",
       "      <td>122.509028</td>\n",
       "      <td>157.845609</td>\n",
       "      <td>134.558118</td>\n",
       "      <td>111.428789</td>\n",
       "      <td>196.649728</td>\n",
       "      <td>182.907850</td>\n",
       "      <td>187.353642</td>\n",
       "      <td>171.458225</td>\n",
       "      <td>126.062032</td>\n",
       "      <td>87.723378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>186.514267</td>\n",
       "      <td>166.652393</td>\n",
       "      <td>123.846834</td>\n",
       "      <td>84.976688</td>\n",
       "      <td>231.254031</td>\n",
       "      <td>216.126424</td>\n",
       "      <td>189.009812</td>\n",
       "      <td>169.838965</td>\n",
       "      <td>155.421963</td>\n",
       "      <td>142.636550</td>\n",
       "      <td>...</td>\n",
       "      <td>122.292300</td>\n",
       "      <td>157.786215</td>\n",
       "      <td>134.441261</td>\n",
       "      <td>111.491544</td>\n",
       "      <td>196.781185</td>\n",
       "      <td>183.064300</td>\n",
       "      <td>187.272978</td>\n",
       "      <td>171.364782</td>\n",
       "      <td>126.203656</td>\n",
       "      <td>87.886959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>186.323815</td>\n",
       "      <td>166.408256</td>\n",
       "      <td>123.751067</td>\n",
       "      <td>84.945188</td>\n",
       "      <td>231.103987</td>\n",
       "      <td>216.005000</td>\n",
       "      <td>189.003431</td>\n",
       "      <td>169.885537</td>\n",
       "      <td>155.286893</td>\n",
       "      <td>142.345518</td>\n",
       "      <td>...</td>\n",
       "      <td>122.074546</td>\n",
       "      <td>157.723039</td>\n",
       "      <td>134.323109</td>\n",
       "      <td>111.555569</td>\n",
       "      <td>196.914248</td>\n",
       "      <td>183.222101</td>\n",
       "      <td>187.189564</td>\n",
       "      <td>171.271735</td>\n",
       "      <td>126.346672</td>\n",
       "      <td>88.051739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>186.130811</td>\n",
       "      <td>166.162262</td>\n",
       "      <td>123.652019</td>\n",
       "      <td>84.915006</td>\n",
       "      <td>230.951197</td>\n",
       "      <td>215.882337</td>\n",
       "      <td>188.991762</td>\n",
       "      <td>169.931555</td>\n",
       "      <td>155.153987</td>\n",
       "      <td>142.052720</td>\n",
       "      <td>...</td>\n",
       "      <td>121.855963</td>\n",
       "      <td>157.656580</td>\n",
       "      <td>134.203592</td>\n",
       "      <td>111.621060</td>\n",
       "      <td>197.048575</td>\n",
       "      <td>183.381282</td>\n",
       "      <td>187.103513</td>\n",
       "      <td>171.179020</td>\n",
       "      <td>126.490913</td>\n",
       "      <td>88.217522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.476682  193.330550   96.162106  142.324408  204.069137  226.487294   \n",
       "1     165.640865  193.421432   96.473737  142.538333  203.821631  226.279209   \n",
       "2     165.804876  193.513963   96.785755  142.754701  203.574562  226.072615   \n",
       "3     165.969023  193.608218   97.098013  142.973684  203.327923  225.867702   \n",
       "4     166.133541  193.704239   97.410161  143.195258  203.081681  225.664618   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  186.890133  167.136498  124.028245   85.043260  231.545990  216.365295   \n",
       "2439  186.702775  166.895061  123.939206   85.009440  231.401355  216.246492   \n",
       "2440  186.514267  166.652393  123.846834   84.976688  231.254031  216.126424   \n",
       "2441  186.323815  166.408256  123.751067   84.945188  231.103987  216.005000   \n",
       "2442  186.130811  166.162262  123.652019   84.915006  230.951197  215.882337   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     159.507560  188.621279  145.304518  162.856305  ...  123.466814   \n",
       "1     159.292625  188.367463  145.516914  163.023106  ...  123.548958   \n",
       "2     159.074802  188.115384  145.723935  163.189982  ...  123.632229   \n",
       "3     158.854006  187.865118  145.925562  163.356997  ...  123.716187   \n",
       "4     158.630091  187.616890  146.121871  163.524314  ...  123.800229   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  189.006691  169.742748  155.697636  143.211613  ...  122.724440   \n",
       "2439  189.010895  169.791443  155.558933  142.925403  ...  122.509028   \n",
       "2440  189.009812  169.838965  155.421963  142.636550  ...  122.292300   \n",
       "2441  189.003431  169.885537  155.286893  142.345518  ...  122.074546   \n",
       "2442  188.991762  169.931555  155.153987  142.052720  ...  121.855963   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     137.342275  136.770446  131.359859  186.077649  181.399217  154.185528   \n",
       "1     137.168631  137.034249  131.572686  185.877640  181.117865  154.137553   \n",
       "2     136.999740  137.298953  131.785763  185.675631  180.833507  154.094457   \n",
       "3     136.835852  137.564418  131.999238  185.471432  180.546277  154.056215   \n",
       "4     136.677121  137.830676  132.213338  185.265040  180.256474  154.022577   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  157.900833  134.673921  111.367070  196.520345  182.752655  187.431706   \n",
       "2439  157.845609  134.558118  111.428789  196.649728  182.907850  187.353642   \n",
       "2440  157.786215  134.441261  111.491544  196.781185  183.064300  187.272978   \n",
       "2441  157.723039  134.323109  111.555569  196.914248  183.222101  187.189564   \n",
       "2442  157.656580  134.203592  111.621060  197.048575  183.381282  187.103513   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     189.427030   78.635342  137.662134  \n",
       "1     189.195777   78.208883  137.446677  \n",
       "2     188.969105   77.784021  137.231304  \n",
       "3     188.747092   77.360992  137.016240  \n",
       "4     188.529873   76.940078  136.801560  \n",
       "...          ...         ...         ...  \n",
       "2438  171.551963  125.922063   87.561227  \n",
       "2439  171.458225  126.062032   87.723378  \n",
       "2440  171.364782  126.203656   87.886959  \n",
       "2441  171.271735  126.346672   88.051739  \n",
       "2442  171.179020  126.490913   88.217522  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 14s 18ms/step - loss: 1389.7557 - val_loss: 1297.8170\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1222.6417 - val_loss: 1191.2804\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1133.6106 - val_loss: 1113.8385\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1067.8781 - val_loss: 1055.7587\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1018.8220 - val_loss: 1013.0328\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 982.9766 - val_loss: 981.9000\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 957.6127 - val_loss: 960.3475\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 940.5450 - val_loss: 946.2149\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 929.7196 - val_loss: 937.5370\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 923.3168 - val_loss: 932.6134\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 919.9330 - val_loss: 930.0991\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.3651 - val_loss: 928.7867\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.6212 - val_loss: 928.3754\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.3739 - val_loss: 928.2493\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.2726 - val_loss: 928.0294\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.3006 - val_loss: 927.9695\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.2545 - val_loss: 927.9446\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.2590 - val_loss: 928.0073\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.2299 - val_loss: 927.9769\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.2853 - val_loss: 927.9775\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.2651 - val_loss: 927.9934\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.2259 - val_loss: 927.8668\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 898.0909 - val_loss: 880.7364\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 843.4312 - val_loss: 812.2759\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 768.6101 - val_loss: 747.7007\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 711.6047 - val_loss: 699.8479\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 658.9442 - val_loss: 641.1510\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 605.4070 - val_loss: 586.5180\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 552.9695 - val_loss: 540.5281\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 504.6503 - val_loss: 488.1578\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 461.3623 - val_loss: 446.0149\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 418.5448 - val_loss: 406.4966\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 381.5478 - val_loss: 369.7944\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 346.9808 - val_loss: 337.3795\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 313.1707 - val_loss: 306.3606\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 282.0480 - val_loss: 272.3977\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 253.8040 - val_loss: 245.7778\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 226.9685 - val_loss: 220.4293\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.8827 - val_loss: 200.3243\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 183.5628 - val_loss: 174.6406\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 160.3538 - val_loss: 157.0856\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 143.2184 - val_loss: 138.3200\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 125.8215 - val_loss: 120.5644\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 110.6264 - val_loss: 107.8843\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 97.2566 - val_loss: 94.0800\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 82.4127 - val_loss: 80.0182\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 72.1801 - val_loss: 69.4375\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 62.5047 - val_loss: 67.3616\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 54.8841 - val_loss: 51.2289\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 45.2286 - val_loss: 44.2964\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 39.5903 - val_loss: 40.2157\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 35.3801 - val_loss: 33.0128\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 28.7073 - val_loss: 27.8535\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.8085 - val_loss: 28.2938\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.7060 - val_loss: 21.3428\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.4656 - val_loss: 17.3376\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.9741 - val_loss: 14.4793\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6252 - val_loss: 13.3528\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.3443 - val_loss: 14.5965\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4765 - val_loss: 8.9628\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6815 - val_loss: 7.7774\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8174 - val_loss: 6.4909\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5730 - val_loss: 5.8105\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0867 - val_loss: 5.3191\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3390 - val_loss: 4.4195\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3393 - val_loss: 5.3686\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2722 - val_loss: 3.1856\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9047 - val_loss: 2.6171\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7882 - val_loss: 2.1879\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.9746 - val_loss: 2.3459\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7476 - val_loss: 1.8985\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2164 - val_loss: 1.8877\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7032 - val_loss: 1.5241\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2892 - val_loss: 1.0730\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4879 - val_loss: 1.3782\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6390 - val_loss: 0.9928\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8553 - val_loss: 1.3340\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7560 - val_loss: 0.8555\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8679 - val_loss: 1.1796\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7341 - val_loss: 0.8674\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2094 - val_loss: 12.2740\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9347 - val_loss: 0.3742\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3533 - val_loss: 0.4035\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5991 - val_loss: 1.0350\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6319 - val_loss: 1.0474\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5862 - val_loss: 1.4390\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0011 - val_loss: 0.5123\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6437 - val_loss: 0.3173\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.6168 - val_loss: 0.3935\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.6316 - val_loss: 1.2326\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7566 - val_loss: 0.2753\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3412 - val_loss: 0.3647\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5009 - val_loss: 0.3480\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9496 - val_loss: 0.4677\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3453 - val_loss: 0.4040\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6619 - val_loss: 1.1485\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6431 - val_loss: 0.4549\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2737 - val_loss: 0.3436\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5098 - val_loss: 0.4375\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1611 - val_loss: 0.1068\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3191 - val_loss: 0.3651\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5501 - val_loss: 0.3364\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3056 - val_loss: 0.6605\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6873 - val_loss: 0.2608\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3223 - val_loss: 0.5213\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4375 - val_loss: 0.2525\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.2959 - val_loss: 0.2597\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2.2281 - val_loss: 0.4968\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1676 - val_loss: 0.1051\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1038 - val_loss: 0.0853\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1748 - val_loss: 0.1272\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3336 - val_loss: 0.1939\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3022 - val_loss: 0.1711\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7433 - val_loss: 0.6335\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3752 - val_loss: 0.1654\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2183 - val_loss: 0.2041\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4979 - val_loss: 0.6206\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4943 - val_loss: 0.3235\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4233 - val_loss: 0.4170\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2835 - val_loss: 0.2945\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1714 - val_loss: 0.2736\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6525 - val_loss: 0.6705\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3351 - val_loss: 0.3252\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2896 - val_loss: 0.5951\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4509 - val_loss: 0.1998\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.3400 - val_loss: 1.2960\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7941 - val_loss: 0.1751\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1239 - val_loss: 0.3174\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2477 - val_loss: 0.2479\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4843 - val_loss: 0.6009\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2435 - val_loss: 0.1818\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2153 - val_loss: 0.8731\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5094 - val_loss: 1.3447\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6091 - val_loss: 0.1570\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2288 - val_loss: 0.0874\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1765 - val_loss: 0.2656\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5775 - val_loss: 0.4488\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2091 - val_loss: 0.1902\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6045 - val_loss: 11.1990\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7609 - val_loss: 0.0543\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0616 - val_loss: 0.1226\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0811 - val_loss: 0.0847\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6290 - val_loss: 0.9606\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.2313 - val_loss: 0.0965\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.1584 - val_loss: 0.6340\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4493 - val_loss: 0.5286\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2911 - val_loss: 0.1270\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2377 - val_loss: 0.6947\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2685 - val_loss: 0.2307\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4531 - val_loss: 0.3311\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1598 - val_loss: 0.0806\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1545 - val_loss: 0.8298\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7838 - val_loss: 0.1893\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1347 - val_loss: 0.1644\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2457 - val_loss: 0.2549\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1515 - val_loss: 0.1775\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2967 - val_loss: 0.1916\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6873 - val_loss: 0.2600\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1031 - val_loss: 0.0740\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0486 - val_loss: 0.0411\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.0455 - val_loss: 0.0768\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0472 - val_loss: 0.0679\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0797 - val_loss: 0.1264\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2244 - val_loss: 0.0993\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4233 - val_loss: 0.4995\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1402 - val_loss: 0.0857\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2751 - val_loss: 0.0637\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1277 - val_loss: 0.6181\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2515 - val_loss: 0.3021\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7501 - val_loss: 0.1581\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0721 - val_loss: 0.0513\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0453 - val_loss: 0.0477\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0415 - val_loss: 0.0736\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1216 - val_loss: 0.0753\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2003 - val_loss: 1.0442\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4673 - val_loss: 0.0547\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1045 - val_loss: 0.0599\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1414 - val_loss: 0.3446\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2494 - val_loss: 0.4557\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.4302 - val_loss: 0.1385\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2004 - val_loss: 0.1879\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2532 - val_loss: 0.2319\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1691 - val_loss: 0.5837\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3114 - val_loss: 0.1906\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2355 - val_loss: 0.1811\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1386 - val_loss: 0.1750\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2275 - val_loss: 0.4256\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2404 - val_loss: 0.4896\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5303 - val_loss: 0.1402\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1655 - val_loss: 0.0549\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0852 - val_loss: 0.0971\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2790 - val_loss: 0.6670\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2792 - val_loss: 0.0744\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0756 - val_loss: 0.0906\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.5339 - val_loss: 0.4880\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1696 - val_loss: 0.0620\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0427 - val_loss: 0.0387\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0367 - val_loss: 0.0413\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0422 - val_loss: 0.0631\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0430 - val_loss: 0.0512\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.051232366102318044\n",
      "Mean Absolute Error (MAE): 0.17287734834597904\n",
      "Root Mean Squared Error (RMSE): 0.22634567833806335\n",
      "Time taken: 2564.7393927574158\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 50ms/step - loss: 1408.0320 - val_loss: 1265.1228\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1246.4175 - val_loss: 1166.4786\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1155.7384 - val_loss: 1092.8016\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1087.2465 - val_loss: 1037.5863\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1035.3357 - val_loss: 996.7423\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 996.5171 - val_loss: 967.0173\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 968.3791 - val_loss: 946.5120\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 948.7831 - val_loss: 933.1819\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 935.5287 - val_loss: 924.5078\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 927.0975 - val_loss: 920.5730\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 922.8353 - val_loss: 919.0372\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 920.7266 - val_loss: 918.6824\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.8369 - val_loss: 918.7967\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.4359 - val_loss: 918.9959\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.3334 - val_loss: 919.1907\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.3155 - val_loss: 919.3400\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2631 - val_loss: 919.3849\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2636 - val_loss: 919.4490\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2691 - val_loss: 919.4962\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.4616 - val_loss: 919.4547\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.3185 - val_loss: 919.5635\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2714 - val_loss: 919.6620\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 919.2755 - val_loss: 919.6159\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 919.2468 - val_loss: 919.6140\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2610 - val_loss: 919.6663\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2814 - val_loss: 919.5852\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.2830 - val_loss: 919.6008\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 902.5458 - val_loss: 859.1960\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 836.4650 - val_loss: 783.0206\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 749.9094 - val_loss: 699.5264\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 682.4650 - val_loss: 641.4229\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 630.3071 - val_loss: 593.6643\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 583.9960 - val_loss: 547.9073\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 537.7842 - val_loss: 510.1472\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 492.9639 - val_loss: 458.5847\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 444.8727 - val_loss: 412.5409\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 402.5587 - val_loss: 371.0391\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 362.8883 - val_loss: 337.3052\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 328.7487 - val_loss: 298.3785\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 293.0334 - val_loss: 266.7303\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 262.7307 - val_loss: 236.9752\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 234.9100 - val_loss: 210.9045\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 208.1597 - val_loss: 186.2856\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 184.9409 - val_loss: 163.9196\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 163.4243 - val_loss: 146.4129\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 144.0048 - val_loss: 125.7809\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 125.3444 - val_loss: 107.5423\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 108.5030 - val_loss: 94.6436\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 95.6360 - val_loss: 84.4010\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 81.9462 - val_loss: 69.5177\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 71.2507 - val_loss: 59.0712\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 60.6797 - val_loss: 49.8141\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 54.4593 - val_loss: 42.7557\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 44.0431 - val_loss: 36.4316\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 38.9367 - val_loss: 30.6774\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 31.7450 - val_loss: 27.2059\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 28.0981 - val_loss: 21.9395\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 22.6953 - val_loss: 18.1465\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 20.2865 - val_loss: 16.0280\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 16.3987 - val_loss: 12.8817\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 13.3751 - val_loss: 10.5526\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 12.1545 - val_loss: 19.6909\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 10.1807 - val_loss: 8.5030\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 8.1786 - val_loss: 5.9696\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 7.3855 - val_loss: 4.7537\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.2858 - val_loss: 4.3856\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.9944 - val_loss: 4.7894\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.1987 - val_loss: 3.0036\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.3495 - val_loss: 17.4103\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.6939 - val_loss: 1.8116\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1137 - val_loss: 1.5333\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.1090 - val_loss: 1.6276\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.6917 - val_loss: 0.9740\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.5513 - val_loss: 1.0226\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.6044 - val_loss: 1.7182\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9637 - val_loss: 0.8485\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4093 - val_loss: 1.7079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7331 - val_loss: 0.7049\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0145 - val_loss: 1.1782\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9384 - val_loss: 0.5195\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7719 - val_loss: 0.7401\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.7822 - val_loss: 0.8104\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 1.1926 - val_loss: 0.9868\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0636 - val_loss: 0.5452\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7172 - val_loss: 0.9604\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.8485 - val_loss: 0.7302\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8505 - val_loss: 0.4419\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8768 - val_loss: 1.2681\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2703 - val_loss: 1.0157\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.5280 - val_loss: 0.2642\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.9592 - val_loss: 0.5077\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.5183 - val_loss: 0.5313\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.4824 - val_loss: 0.5494\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.7261 - val_loss: 0.3397\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.6477 - val_loss: 0.6897\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.5555 - val_loss: 0.2771\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.6919 - val_loss: 0.4397\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.3248 - val_loss: 0.5385\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.2254 - val_loss: 1.3523\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.3371 - val_loss: 0.1586\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.6637 - val_loss: 0.5998\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3159 - val_loss: 0.1658\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2313 - val_loss: 0.2011\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2722 - val_loss: 0.2627\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3204 - val_loss: 0.6681\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3152 - val_loss: 0.1959\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4362 - val_loss: 0.5023\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5011 - val_loss: 0.3181\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5002 - val_loss: 0.2842\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6024 - val_loss: 0.4914\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6261 - val_loss: 0.2473\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2158 - val_loss: 0.2952\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3724 - val_loss: 1.5462\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1.8050 - val_loss: 0.3690\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1914 - val_loss: 0.0666\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.2024 - val_loss: 0.1696\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2699 - val_loss: 0.2136\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3019 - val_loss: 0.3027\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4335 - val_loss: 1.3517\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8031 - val_loss: 0.0926\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.1208 - val_loss: 0.2233\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4187 - val_loss: 0.6493\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4491 - val_loss: 0.3257\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2909 - val_loss: 0.2768\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4251 - val_loss: 0.3360\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4430 - val_loss: 0.2565\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3540 - val_loss: 0.3653\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4094 - val_loss: 0.7871\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.2741 - val_loss: 0.3253\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.3344 - val_loss: 0.3242\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.4967 - val_loss: 0.3894\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3067 - val_loss: 0.3084\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3364 - val_loss: 0.1609\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3037 - val_loss: 0.1849\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.0877 - val_loss: 0.3699\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1090 - val_loss: 0.0520\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0503 - val_loss: 0.0366\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.0569 - val_loss: 0.0438\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1108 - val_loss: 0.2674\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4050 - val_loss: 0.3553\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4102 - val_loss: 0.3958\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1629 - val_loss: 0.2584\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3318 - val_loss: 0.2519\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.2620 - val_loss: 0.1492\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.3894 - val_loss: 0.2695\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1917 - val_loss: 0.1375\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4479 - val_loss: 0.5479\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4561 - val_loss: 0.5572\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2260 - val_loss: 0.0487\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1702 - val_loss: 0.2885\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.3122 - val_loss: 2.2343\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3447 - val_loss: 0.0785\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0699 - val_loss: 0.0786\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0627 - val_loss: 0.0663\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2370 - val_loss: 0.1113\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0978 - val_loss: 0.1968\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2729 - val_loss: 0.2548\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5506 - val_loss: 0.2694\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1195 - val_loss: 0.0625\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0794 - val_loss: 0.1480\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5008 - val_loss: 1.1363\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2640 - val_loss: 0.0521\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1512 - val_loss: 0.2367\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4200 - val_loss: 0.5530\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2605 - val_loss: 0.0712\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1308 - val_loss: 0.1256\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2158 - val_loss: 0.2210\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5457 - val_loss: 0.1026\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1319 - val_loss: 0.1607\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2684 - val_loss: 0.2520\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1869 - val_loss: 0.1306\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4127 - val_loss: 0.1557\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1414 - val_loss: 0.1022\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3517 - val_loss: 0.4649\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1778 - val_loss: 0.0667\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2650 - val_loss: 0.5150\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2488 - val_loss: 0.1422\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1751 - val_loss: 0.3328\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.3093 - val_loss: 0.1622\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1445 - val_loss: 0.1354\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3065 - val_loss: 0.4061\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.2127 - val_loss: 0.1674\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1426 - val_loss: 0.3551\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.2762 - val_loss: 0.1461\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1543 - val_loss: 0.1717\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3642 - val_loss: 0.4242\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3335 - val_loss: 0.1338\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.0515 - val_loss: 0.0566\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2657 - val_loss: 1.0726\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6286 - val_loss: 0.0601\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0330 - val_loss: 0.0735\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0576 - val_loss: 0.1037\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3798 - val_loss: 0.1834\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2384 - val_loss: 0.0654\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1402 - val_loss: 0.1312\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1120 - val_loss: 0.1633\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.3151 - val_loss: 0.8385\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4294 - val_loss: 0.2467\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0856 - val_loss: 0.0574\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0840 - val_loss: 0.1885\n",
      "16/16 [==============================] - 2s 30ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.18852293801097786\n",
      "Mean Absolute Error (MAE): 0.32946743978988097\n",
      "Root Mean Squared Error (RMSE): 0.43419228230241247\n",
      "Time taken: 3877.939977645874\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 50ms/step - loss: 1367.2577 - val_loss: 1292.0518\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1210.2656 - val_loss: 1187.6375\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1124.1305 - val_loss: 1111.4187\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1060.6575 - val_loss: 1054.7684\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1013.3953 - val_loss: 1012.5796\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 978.8376 - val_loss: 982.2745\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 954.6055 - val_loss: 961.2584\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 938.2764 - val_loss: 947.3884\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 928.0720 - val_loss: 939.0049\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 922.1802 - val_loss: 934.3123\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.1452 - val_loss: 931.8102\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 917.6500 - val_loss: 930.6696\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 916.9677 - val_loss: 928.9962\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 902.9435 - val_loss: 899.9019\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 866.5137 - val_loss: 864.8361\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 842.9028 - val_loss: 844.1429\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 816.8489 - val_loss: 817.1791\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 791.4706 - val_loss: 800.6168\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 765.5322 - val_loss: 772.0262\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 752.8072 - val_loss: 759.2684\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 730.2102 - val_loss: 734.6736\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 708.3519 - val_loss: 715.5502\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 691.0210 - val_loss: 699.0760\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 668.3055 - val_loss: 671.2648\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 632.9034 - val_loss: 636.4142\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 600.6385 - val_loss: 596.0396\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 565.3441 - val_loss: 563.1548\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 529.2513 - val_loss: 518.1471\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 490.6948 - val_loss: 487.0084\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 448.3177 - val_loss: 434.6254\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 398.1206 - val_loss: 388.5016\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 348.9405 - val_loss: 344.6066\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 302.8052 - val_loss: 291.3940\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 262.3794 - val_loss: 251.1450\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 226.1971 - val_loss: 219.2324\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 199.1997 - val_loss: 201.7387\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 174.3157 - val_loss: 165.3203\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 148.8943 - val_loss: 144.6865\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 129.4777 - val_loss: 123.7672\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 113.4775 - val_loss: 108.4224\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 94.7900 - val_loss: 90.6329\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 80.7268 - val_loss: 76.7075\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 71.1864 - val_loss: 65.6986\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 58.4600 - val_loss: 55.8223\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 49.0830 - val_loss: 47.2504\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 42.2199 - val_loss: 38.2609\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 38.2276 - val_loss: 34.3284\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 29.0938 - val_loss: 27.3111\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 23.6765 - val_loss: 22.0528\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 20.0491 - val_loss: 25.3919\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 17.2874 - val_loss: 13.6480\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 13.6054 - val_loss: 11.8643\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 10.8614 - val_loss: 10.3612\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 9.7140 - val_loss: 7.7424\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 7.1676 - val_loss: 6.2914\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 9.4402 - val_loss: 6.7804\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 5.2245 - val_loss: 4.4426\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 6.7549 - val_loss: 10.7411\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 5.1717 - val_loss: 3.5636\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 3.4803 - val_loss: 8.7574\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.7845 - val_loss: 2.3117\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 5.3420 - val_loss: 2.5566\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.1832 - val_loss: 2.2746\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.8705 - val_loss: 2.7235\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.7628 - val_loss: 1.7347\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.4402 - val_loss: 2.8114\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.8599 - val_loss: 6.3680\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.2984 - val_loss: 1.0999\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9116 - val_loss: 0.7035\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2337 - val_loss: 1.6029\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2104 - val_loss: 8.5981\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.4185 - val_loss: 1.0423\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0096 - val_loss: 2.0340\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9175 - val_loss: 0.6112\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0756 - val_loss: 1.2150\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6133 - val_loss: 1.2992\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.9179 - val_loss: 6.0019\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2117 - val_loss: 0.6107\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6192 - val_loss: 0.7072\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2768 - val_loss: 1.8402\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1732 - val_loss: 0.6454\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6412 - val_loss: 0.5663\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6515 - val_loss: 0.7629\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2427 - val_loss: 2.1414\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2272 - val_loss: 0.3061\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8804 - val_loss: 1.0062\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9562 - val_loss: 1.5739\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1567 - val_loss: 0.3729\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5364 - val_loss: 0.7637\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8969 - val_loss: 1.1807\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7949 - val_loss: 0.5689\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6360 - val_loss: 0.4214\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4030 - val_loss: 0.5713\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.6086 - val_loss: 2.6730\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8333 - val_loss: 0.2544\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3387 - val_loss: 0.1921\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2904 - val_loss: 0.2657\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0237 - val_loss: 2.9765\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1623 - val_loss: 1.3115\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7402 - val_loss: 0.3163\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3501 - val_loss: 0.4872\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8021 - val_loss: 1.3145\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.4836 - val_loss: 0.3243\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2207 - val_loss: 0.1622\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1830 - val_loss: 0.3302\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4234 - val_loss: 0.4626\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4661 - val_loss: 1.0368\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6839 - val_loss: 0.6097\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4738 - val_loss: 1.0507\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.0036 - val_loss: 0.3312\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2081 - val_loss: 0.1833\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4041 - val_loss: 0.2826\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9198 - val_loss: 0.4251\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3539 - val_loss: 0.5157\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9438 - val_loss: 0.5474\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4015 - val_loss: 0.6628\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7655 - val_loss: 0.4282\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4325 - val_loss: 0.6312\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5778 - val_loss: 0.7924\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4262 - val_loss: 0.5897\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6914 - val_loss: 1.3502\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2386 - val_loss: 0.1168\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2288 - val_loss: 0.2231\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.5253 - val_loss: 0.5157\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.5696 - val_loss: 0.1964\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4120 - val_loss: 1.7970\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6925 - val_loss: 0.3565\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3517 - val_loss: 0.4161\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2206 - val_loss: 0.8229\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2743 - val_loss: 0.1495\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2266 - val_loss: 0.1763\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4949 - val_loss: 0.2185\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4069 - val_loss: 1.1294\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6448 - val_loss: 0.1544\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.4656 - val_loss: 0.4812\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2371 - val_loss: 0.0798\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1019 - val_loss: 0.0843\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1214 - val_loss: 0.1397\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3212 - val_loss: 0.2343\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2796 - val_loss: 0.6548\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7855 - val_loss: 0.1465\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1149 - val_loss: 0.0837\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7464 - val_loss: 2.4166\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.5167 - val_loss: 0.2753\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4574 - val_loss: 0.3889\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4318 - val_loss: 0.2215\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2940 - val_loss: 2.0451\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4416 - val_loss: 0.1626\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2117 - val_loss: 0.1603\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.4263 - val_loss: 2.6644\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.4830 - val_loss: 0.3472\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1702 - val_loss: 0.1151\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2313 - val_loss: 0.1354\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2964 - val_loss: 0.3156\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3365 - val_loss: 0.1822\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1808 - val_loss: 0.1554\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.3841 - val_loss: 18.9960\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3406 - val_loss: 0.1254\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1146 - val_loss: 0.0962\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1190 - val_loss: 0.1404\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1290 - val_loss: 0.1268\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1521 - val_loss: 0.1166\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2231 - val_loss: 0.1584\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5825 - val_loss: 0.2692\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1740 - val_loss: 0.3649\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4777 - val_loss: 0.9649\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5912 - val_loss: 0.4238\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1666 - val_loss: 0.1055\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2522 - val_loss: 0.2760\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.1902 - val_loss: 0.1575\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.3969 - val_loss: 0.2346\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4306 - val_loss: 0.3004\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2569 - val_loss: 0.1497\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4861 - val_loss: 0.4493\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2704 - val_loss: 0.0889\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1360 - val_loss: 0.3239\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5095 - val_loss: 0.5179\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.1018 - val_loss: 0.1765\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1044 - val_loss: 0.0705\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1005 - val_loss: 0.1308\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2089 - val_loss: 0.0833\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1914 - val_loss: 0.3849\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4598 - val_loss: 0.3372\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2446 - val_loss: 0.1964\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3553 - val_loss: 0.1931\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2221 - val_loss: 0.2427\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2013 - val_loss: 0.6550\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3710 - val_loss: 0.4385\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6630 - val_loss: 0.7987\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1896 - val_loss: 0.1453\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0756 - val_loss: 0.1261\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4035 - val_loss: 0.3779\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2577 - val_loss: 0.1493\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3666 - val_loss: 0.2968\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6910 - val_loss: 1.0176\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3426 - val_loss: 0.0367\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0761 - val_loss: 0.1525\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0890 - val_loss: 0.3269\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4153 - val_loss: 0.1803\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1439 - val_loss: 0.0901\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.0900778333501815\n",
      "Mean Absolute Error (MAE): 0.21605574023813398\n",
      "Root Mean Squared Error (RMSE): 0.3001296942159864\n",
      "Time taken: 3758.7754838466644\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 49ms/step - loss: 1398.4430 - val_loss: 1275.8959\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 1233.0640 - val_loss: 1173.6559\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 1145.0221 - val_loss: 1098.6952\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 1079.2380 - val_loss: 1041.8199\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 1029.5133 - val_loss: 999.0986\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 992.5379 - val_loss: 967.4271\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 965.7268 - val_loss: 944.9120\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 947.1923 - val_loss: 930.0845\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 935.4326 - val_loss: 920.7921\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 928.4776 - val_loss: 915.5316\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 924.6378 - val_loss: 912.6885\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 922.8070 - val_loss: 911.4696\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 921.9263 - val_loss: 909.8513\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 905.4730 - val_loss: 872.9833\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 872.6147 - val_loss: 847.5469\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 847.9883 - val_loss: 823.5048\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 826.0641 - val_loss: 806.3564\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 804.7291 - val_loss: 778.6734\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 776.0386 - val_loss: 748.2134\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 746.7375 - val_loss: 716.6557\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 708.1726 - val_loss: 673.7799\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 663.6263 - val_loss: 647.1520\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 607.7260 - val_loss: 570.6874\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 554.4377 - val_loss: 522.3269\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 501.0727 - val_loss: 477.0191\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 458.2502 - val_loss: 432.5547\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 413.4807 - val_loss: 388.0216\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 372.6299 - val_loss: 343.4757\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 328.7048 - val_loss: 330.9598\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 295.0892 - val_loss: 272.9114\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 258.9244 - val_loss: 241.6190\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 228.3562 - val_loss: 214.6394\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 204.1658 - val_loss: 185.0206\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 174.5582 - val_loss: 160.3878\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 151.3199 - val_loss: 138.3862\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 131.9483 - val_loss: 119.8520\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 112.8286 - val_loss: 103.9865\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 98.9700 - val_loss: 87.2708\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 82.7136 - val_loss: 73.8611\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 70.0526 - val_loss: 63.1446\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 60.4928 - val_loss: 60.5484\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 51.9203 - val_loss: 43.9680\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 41.1901 - val_loss: 36.3665\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 35.3493 - val_loss: 39.2629\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 32.1360 - val_loss: 24.3938\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 22.5474 - val_loss: 20.0314\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 20.8113 - val_loss: 16.7884\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 15.8895 - val_loss: 13.5634\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 13.9284 - val_loss: 20.5437\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 13.9785 - val_loss: 9.9696\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 9.5265 - val_loss: 8.8805\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 8.7773 - val_loss: 8.5938\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 7.8131 - val_loss: 8.3554\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 7.0791 - val_loss: 9.3518\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 6.3125 - val_loss: 6.1346\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.5000 - val_loss: 3.4143\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 3.3685 - val_loss: 2.7972\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 3.4566 - val_loss: 4.5598\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.1246 - val_loss: 4.4073\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.2431 - val_loss: 3.0354\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2299 - val_loss: 1.6982\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.9697 - val_loss: 2.2979\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.0575 - val_loss: 3.4516\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 11.6134 - val_loss: 4.4393\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.8401 - val_loss: 1.4198\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 1.2639 - val_loss: 1.1925\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.3650 - val_loss: 3.3120\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.0237 - val_loss: 1.1481\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5371 - val_loss: 1.2066\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.6382 - val_loss: 1.3670\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5903 - val_loss: 1.7596\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0810 - val_loss: 0.7482\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9775 - val_loss: 1.3101\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5088 - val_loss: 3.1700\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.5963 - val_loss: 1.8960\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2073 - val_loss: 1.7464\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 1.1960 - val_loss: 1.6968\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4499 - val_loss: 3.9223\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0382 - val_loss: 0.9549\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5652 - val_loss: 0.8179\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.8284 - val_loss: 0.7028\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8982 - val_loss: 0.9883\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.9065 - val_loss: 1.2840\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2786 - val_loss: 1.4497\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8718 - val_loss: 0.6479\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.3638 - val_loss: 9.2696\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.6918 - val_loss: 0.3725\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4248 - val_loss: 0.2682\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4605 - val_loss: 0.2878\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 0.5456 - val_loss: 0.6436\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6067 - val_loss: 0.8701\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3328 - val_loss: 0.8593\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9399 - val_loss: 0.3177\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4799 - val_loss: 0.6101\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2742 - val_loss: 1.2677\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1.1867 - val_loss: 0.5177\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8527 - val_loss: 1.1325\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9516 - val_loss: 1.9940\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7601 - val_loss: 0.3869\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3225 - val_loss: 0.7240\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6812 - val_loss: 0.5140\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.9816 - val_loss: 5.7840\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.8600 - val_loss: 0.2445\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1786 - val_loss: 0.1848\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1487 - val_loss: 0.1286\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3404 - val_loss: 1.3582\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7095 - val_loss: 0.5399\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4639 - val_loss: 0.3563\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.8611 - val_loss: 0.4848\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3815 - val_loss: 0.4726\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0654 - val_loss: 0.9013\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3990 - val_loss: 0.2999\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6336 - val_loss: 0.8956\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5080 - val_loss: 0.5106\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7556 - val_loss: 0.5969\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4119 - val_loss: 0.4575\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5785 - val_loss: 0.4346\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5248 - val_loss: 1.1631\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2059 - val_loss: 0.2811\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3461 - val_loss: 0.3036\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2533 - val_loss: 0.2629\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9814 - val_loss: 1.1294\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5090 - val_loss: 0.3221\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2517 - val_loss: 0.2614\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4277 - val_loss: 1.5615\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9539 - val_loss: 0.3609\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3355 - val_loss: 0.3567\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9691 - val_loss: 0.1585\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1540 - val_loss: 0.1099\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0970 - val_loss: 0.1431\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2248 - val_loss: 0.4678\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2425 - val_loss: 0.5586\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5555 - val_loss: 0.3018\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6080 - val_loss: 0.4384\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3727 - val_loss: 0.2171\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2764 - val_loss: 0.2598\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4994 - val_loss: 0.3685\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5072 - val_loss: 0.6572\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.7077 - val_loss: 0.1577\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.2394 - val_loss: 0.2466\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.4160 - val_loss: 0.4964\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3971 - val_loss: 0.4724\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4181 - val_loss: 0.6876\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.0492 - val_loss: 0.6047\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1545 - val_loss: 0.0839\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.0677 - val_loss: 0.0564\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.0556 - val_loss: 0.0540\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.1021 - val_loss: 0.1825\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3280 - val_loss: 0.5302\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 0.3217 - val_loss: 0.1901\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.2745 - val_loss: 0.3432\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.2739 - val_loss: 0.4125\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5997 - val_loss: 0.2996\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1988 - val_loss: 0.1223\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.7032 - val_loss: 1.3978\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4624 - val_loss: 0.0935\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1297 - val_loss: 0.1403\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2658 - val_loss: 0.6096\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6257 - val_loss: 0.2723\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3031 - val_loss: 0.6319\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2836 - val_loss: 0.1540\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9399 - val_loss: 1.6935\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3376 - val_loss: 0.0734\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0826 - val_loss: 0.0708\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1602 - val_loss: 0.5216\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3443 - val_loss: 0.2174\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3853 - val_loss: 0.1961\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3481 - val_loss: 0.2205\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1802 - val_loss: 0.2507\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4510 - val_loss: 0.1562\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3264 - val_loss: 0.1416\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3628 - val_loss: 0.5758\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3692 - val_loss: 0.3674\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4398 - val_loss: 0.3927\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1571 - val_loss: 0.0622\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3775 - val_loss: 1.5709\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5508 - val_loss: 0.2462\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2169 - val_loss: 0.0757\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1249 - val_loss: 0.1906\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4865 - val_loss: 0.7213\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4049 - val_loss: 0.2763\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1746 - val_loss: 0.0840\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3369 - val_loss: 0.4563\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2092 - val_loss: 0.3452\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2488 - val_loss: 0.2813\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1680 - val_loss: 0.3074\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3900 - val_loss: 0.7486\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3751 - val_loss: 0.1628\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1632 - val_loss: 0.1002\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1600 - val_loss: 0.1610\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4272 - val_loss: 0.3203\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2039 - val_loss: 0.1516\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2162 - val_loss: 1.0562\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4380 - val_loss: 0.1137\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2065 - val_loss: 0.1333\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2372 - val_loss: 0.2503\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2743 - val_loss: 0.2773\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1977 - val_loss: 0.2459\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1905 - val_loss: 0.2024\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2735 - val_loss: 0.2534\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.2533915621176424\n",
      "Mean Absolute Error (MAE): 0.35554478996407174\n",
      "Root Mean Squared Error (RMSE): 0.5033801367929037\n",
      "Time taken: 3791.069661617279\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 51ms/step - loss: 1385.2043 - val_loss: 1277.4807\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1221.6818 - val_loss: 1173.6937\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1133.9514 - val_loss: 1097.5012\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1068.9746 - val_loss: 1040.9336\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1020.6926 - val_loss: 998.3948\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 985.5032 - val_loss: 967.5051\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 960.7721 - val_loss: 946.0275\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 944.1264 - val_loss: 931.6906\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 933.4891 - val_loss: 922.4774\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 927.2304 - val_loss: 917.1916\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 923.8759 - val_loss: 914.3629\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 922.3133 - val_loss: 913.0366\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.6738 - val_loss: 912.2311\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.3793 - val_loss: 911.9764\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.3033 - val_loss: 911.7227\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.2951 - val_loss: 911.6645\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.2939 - val_loss: 911.6644\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.2637 - val_loss: 911.5516\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.2986 - val_loss: 911.5484\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.2900 - val_loss: 911.6630\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 921.3124 - val_loss: 911.6318\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 909.5914 - val_loss: 863.4380\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 851.9216 - val_loss: 811.8101\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 787.8524 - val_loss: 746.6929\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 724.3172 - val_loss: 690.1177\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 667.0288 - val_loss: 636.7845\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 617.6758 - val_loss: 588.4687\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 570.5435 - val_loss: 547.5279\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 526.1934 - val_loss: 499.0327\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 481.0594 - val_loss: 456.1674\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 438.6248 - val_loss: 414.0051\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 397.4115 - val_loss: 374.5256\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 358.2515 - val_loss: 335.7968\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 322.1844 - val_loss: 303.0786\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 291.6580 - val_loss: 277.5929\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 263.2028 - val_loss: 245.8947\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 234.2485 - val_loss: 231.5797\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 210.1442 - val_loss: 197.2015\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 187.2661 - val_loss: 174.6792\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 164.6398 - val_loss: 154.3848\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 145.9597 - val_loss: 137.3381\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 129.3510 - val_loss: 121.1084\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 112.5176 - val_loss: 107.5197\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 99.0609 - val_loss: 94.2664\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 86.1290 - val_loss: 81.2921\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 75.5990 - val_loss: 69.7656\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 64.3383 - val_loss: 66.5557\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 55.5713 - val_loss: 52.8759\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 46.8797 - val_loss: 43.8475\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 39.9835 - val_loss: 38.2128\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 35.3434 - val_loss: 38.2497\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 28.9911 - val_loss: 26.6973\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 24.0623 - val_loss: 23.3367\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 20.5205 - val_loss: 18.9913\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 16.8615 - val_loss: 16.8302\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 14.5499 - val_loss: 13.4957\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 11.9145 - val_loss: 11.5445\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.4359 - val_loss: 9.4985\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 10.8460 - val_loss: 10.2108\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 7.1926 - val_loss: 6.5255\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.7233 - val_loss: 6.2614\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.2511 - val_loss: 4.7939\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.1023 - val_loss: 4.2269\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.7035 - val_loss: 4.4106\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 3.1887 - val_loss: 4.0854\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.6585 - val_loss: 3.5008\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.9558 - val_loss: 2.4337\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9435 - val_loss: 1.9133\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1197 - val_loss: 1.4935\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7658 - val_loss: 1.6924\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2211 - val_loss: 2.1559\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7105 - val_loss: 1.7961\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1897 - val_loss: 0.8795\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0967 - val_loss: 2.6543\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.8867 - val_loss: 1.5240\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7226 - val_loss: 0.5614\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5146 - val_loss: 0.6641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7949 - val_loss: 0.6939\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9265 - val_loss: 1.3023\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7046 - val_loss: 0.3049\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6572 - val_loss: 0.6489\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1240 - val_loss: 0.7228\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4499 - val_loss: 0.2883\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6191 - val_loss: 1.1008\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.4064 - val_loss: 0.2929\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5090 - val_loss: 0.5680\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5449 - val_loss: 0.1531\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1826 - val_loss: 1.8107\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4389 - val_loss: 0.1320\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2458 - val_loss: 0.3835\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6915 - val_loss: 0.8431\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3262 - val_loss: 0.4271\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5050 - val_loss: 0.6799\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1467 - val_loss: 0.5397\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2222 - val_loss: 0.1977\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4567 - val_loss: 0.3698\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4551 - val_loss: 0.3584\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5232 - val_loss: 1.6134\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9061 - val_loss: 0.3387\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2546 - val_loss: 0.6683\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2509 - val_loss: 0.3093\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6264 - val_loss: 0.4215\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5245 - val_loss: 0.6915\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3085 - val_loss: 0.4630\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3794 - val_loss: 0.3926\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1767 - val_loss: 0.2580\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2058 - val_loss: 0.1955\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1746 - val_loss: 0.1277\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5726 - val_loss: 1.3266\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5361 - val_loss: 0.2076\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1884 - val_loss: 0.2191\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0854 - val_loss: 0.4509\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1800 - val_loss: 0.1250\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1063 - val_loss: 0.3281\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1635 - val_loss: 0.1942\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7912 - val_loss: 0.1446\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3013 - val_loss: 0.1858\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3290 - val_loss: 0.5149\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4461 - val_loss: 0.1034\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2136 - val_loss: 0.7088\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4261 - val_loss: 0.1148\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3503 - val_loss: 0.3307\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4179 - val_loss: 0.4931\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5290 - val_loss: 0.0769\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0922 - val_loss: 0.1181\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3591 - val_loss: 0.4564\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7053 - val_loss: 0.7614\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1598 - val_loss: 0.0733\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3017 - val_loss: 0.0558\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2082 - val_loss: 0.3668\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4592 - val_loss: 0.0998\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1637 - val_loss: 0.2603\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3476 - val_loss: 0.4359\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3463 - val_loss: 0.3501\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2412 - val_loss: 0.2060\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3226 - val_loss: 0.6729\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2979 - val_loss: 0.3252\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7372 - val_loss: 3.4471\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3722 - val_loss: 0.0564\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0816 - val_loss: 0.2434\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3215 - val_loss: 0.2484\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1096 - val_loss: 0.1626\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1701 - val_loss: 0.3383\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3396 - val_loss: 0.6424\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2863 - val_loss: 0.1820\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2801 - val_loss: 0.2058\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3060 - val_loss: 0.1331\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1397 - val_loss: 0.1500\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1721 - val_loss: 0.1201\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4229 - val_loss: 0.5016\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3070 - val_loss: 0.1428\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1293 - val_loss: 0.1161\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.7763 - val_loss: 0.2651\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0612 - val_loss: 0.0218\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0858 - val_loss: 0.2987\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1371 - val_loss: 0.3394\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6677 - val_loss: 0.5046\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0799 - val_loss: 0.0345\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0542 - val_loss: 0.1735\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3301 - val_loss: 0.1986\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3811 - val_loss: 0.7694\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3282 - val_loss: 0.0631\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0475 - val_loss: 0.0321\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0335 - val_loss: 0.0297\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0339 - val_loss: 0.0273\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0485 - val_loss: 0.0503\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.0631 - val_loss: 0.3622\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1650 - val_loss: 0.1395\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3553 - val_loss: 0.1027\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0599 - val_loss: 0.0451\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2329 - val_loss: 0.4066\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2083 - val_loss: 0.3799\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1807 - val_loss: 0.2560\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2125 - val_loss: 0.1252\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3040 - val_loss: 0.2541\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1933 - val_loss: 0.3380\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2632 - val_loss: 0.1056\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0778 - val_loss: 0.1411\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3755 - val_loss: 0.1934\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1215 - val_loss: 0.1051\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3136 - val_loss: 0.3109\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0805 - val_loss: 0.0732\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1743 - val_loss: 0.6707\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3513 - val_loss: 0.2537\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0870 - val_loss: 0.0390\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0702 - val_loss: 0.1073\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4966 - val_loss: 0.5546\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1802 - val_loss: 0.0526\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0742 - val_loss: 0.0761\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2411 - val_loss: 0.8049\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5498 - val_loss: 0.0513\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0427 - val_loss: 0.0368\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0241 - val_loss: 0.0278\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0293 - val_loss: 0.0627\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0356 - val_loss: 0.0899\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0730 - val_loss: 0.0976\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1226 - val_loss: 0.0897\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3907 - val_loss: 0.3802\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0891 - val_loss: 0.0510\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0747 - val_loss: 0.1251\n",
      "16/16 [==============================] - 1s 31ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.12503898537819028\n",
      "Mean Absolute Error (MAE): 0.2647561342989777\n",
      "Root Mean Squared Error (RMSE): 0.35360851994570247\n",
      "Time taken: 3811.5790424346924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_16876\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.051232  0.172877  0.226346  2564.739393\n",
      "1        2  0.188523  0.329467  0.434192  3877.939978\n",
      "2        3  0.090078  0.216056  0.300130  3758.775484\n",
      "3        4  0.253392  0.355545  0.503380  3791.069662\n",
      "4        5  0.125039  0.264756  0.353609  3811.579042\n",
      "5  Average  0.141653  0.267740  0.363531  3560.820712\n",
      "Results saved to 'DL_Result_PL_model_1_smoothing2_iReg_f_over.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f_over.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_smoothing2_iReg_f_over.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACyuUlEQVR4nOzde3gU1f0G8HdmN/c7BHIhAUJIwkUQBEEUKQoV8K54QSmoRakKWrzrT6VgtVRrWxVvtVbQFlu1rZcqoqgoKohcRAGRhCRAAgkQQjYkJNnszvz+CDtkSQKbfDe7M8v7eR4fN2cnu+e8Mxvyzcw5o+i6roOIiIiIiEhADXYHiIiIiIjI+lhYEBERERGRGAsLIiIiIiISY2FBRERERERiLCyIiIiIiEiMhQUREREREYmxsCAiIiIiIjEWFkREREREJMbCgoiIiIiIxFhYEBERERGRGAsLIqKT0OLFi6EoCtatWxfsrvhk48aN+MUvfoHMzExERESgS5cuGD9+PBYtWgS32x3s7hEREQB7sDtARER0PC+//DJuvvlmpKSkYNq0acjJycGhQ4fw6aefYsaMGSgrK8P//d//BbubREQnPRYWRERkWt988w1uvvlmjBo1CkuXLkVcXJzx3Jw5c7Bu3Tps3rzZL+9VW1uLmJgYv7wWEdHJiJdCERFRm7777jtMmjQJ8fHxiI2Nxbhx4/DNN994bdPY2Ij58+cjJycHkZGR6Nq1K0aPHo3ly5cb25SXl+OGG25ARkYGIiIikJaWhksuuQQ7duw47vvPnz8fiqJgyZIlXkWFx/Dhw3H99dcDAD7//HMoioLPP//ca5sdO3ZAURQsXrzYaLv++usRGxuLwsJCnH/++YiLi8PUqVMxe/ZsxMbG4vDhwy3e65prrkFqaqrXpVcffvghzj77bMTExCAuLg4XXHABtmzZctwxERGFKhYWRETUqi1btuDss8/G999/j3vvvRcPP/wwiouLMXbsWKxZs8bYbt68eZg/fz7OOeccPPvss3jwwQfRs2dPbNiwwdhm8uTJePvtt3HDDTfg+eefx+23345Dhw5h165dbb7/4cOH8emnn2LMmDHo2bOn38fncrkwYcIEdO/eHU8++SQmT56Mq6++GrW1tfjggw9a9OV///sfrrjiCthsNgDA3//+d1xwwQWIjY3F448/jocffhg//vgjRo8efcKCiYgoFPFSKCIiatVDDz2ExsZGfPXVV+jTpw8AYPr06cjLy8O9996LL774AgDwwQcf4Pzzz8dLL73U6utUVVVh1apV+MMf/oC7777baH/ggQeO+/7bt29HY2MjBg0a5KcReWtoaMCVV16JBQsWGG26rqNHjx544403cOWVVxrtH3zwAWpra3H11VcDAGpqanD77bfjxhtv9Br3ddddh7y8PPzud79rMw8iolDFMxZERNSC2+3Gxx9/jEsvvdQoKgAgLS0N1157Lb766itUV1cDABITE7FlyxYUFBS0+lpRUVEIDw/H559/joMHD/rcB8/rt3YJlL/ccsstXl8rioIrr7wSS5cuRU1NjdH+xhtvoEePHhg9ejQAYPny5aiqqsI111yDiooK4z+bzYaRI0dixYoVndZnIiKzYmFBREQt7N+/H4cPH0ZeXl6L5/r37w9N01BSUgIAeOSRR1BVVYXc3FwMGjQI99xzD3744Qdj+4iICDz++OP48MMPkZKSgjFjxuCJJ55AeXn5cfsQHx8PADh06JAfR3aU3W5HRkZGi/arr74adXV1eO+99wA0nZ1YunQprrzySiiKAgBGEXXuueeiW7duXv99/PHH2LdvX6f0mYjIzFhYEBGRyJgxY1BYWIhXXnkFp5xyCl5++WWcdtppePnll41t5syZg/z8fCxYsACRkZF4+OGH0b9/f3z33Xdtvm7fvn1ht9uxadMmn/rh+aX/WG3d5yIiIgKq2vKfwTPOOAO9e/fGm2++CQD43//+h7q6OuMyKADQNA1A0zyL5cuXt/jv3Xff9anPREShhIUFERG10K1bN0RHR2Pbtm0tnvvpp5+gqioyMzONti5duuCGG27AP//5T5SUlGDw4MGYN2+e1/dlZ2fjrrvuwscff4zNmzfD6XTij3/8Y5t9iI6OxrnnnouVK1caZ0eOJykpCUDTnI7mdu7cecLvPdZVV12FZcuWobq6Gm+88QZ69+6NM844w2ssANC9e3eMHz++xX9jx45t93sSEVkdCwsiImrBZrPhvPPOw7vvvuu1wtHevXvx+uuvY/To0calSgcOHPD63tjYWPTt2xcNDQ0AmlZUqq+v99omOzsbcXFxxjZt+c1vfgNd1zFt2jSvOQ8e69evx6uvvgoA6NWrF2w2G1auXOm1zfPPP+/boJu5+uqr0dDQgFdffRXLli3DVVdd5fX8hAkTEB8fj9/97ndobGxs8f379+9v93sSEVkdV4UiIjqJvfLKK1i2bFmL9l//+td49NFHsXz5cowePRq33nor7HY7/vKXv6ChoQFPPPGEse2AAQMwduxYDBs2DF26dMG6devw73//G7NnzwYA5OfnY9y4cbjqqqswYMAA2O12vP3229i7dy+mTJly3P6deeaZeO6553DrrbeiX79+Xnfe/vzzz/Hee+/h0UcfBQAkJCTgyiuvxMKFC6EoCrKzs/H+++93aL7Daaedhr59++LBBx9EQ0OD12VQQNP8jxdeeAHTpk3DaaedhilTpqBbt27YtWsXPvjgA5x11ll49tln2/2+RESWphMR0Uln0aJFOoA2/yspKdF1Xdc3bNigT5gwQY+NjdWjo6P1c845R1+1apXXaz366KP6iBEj9MTERD0qKkrv16+f/thjj+lOp1PXdV2vqKjQZ82apffr10+PiYnRExIS9JEjR+pvvvmmz/1dv369fu211+rp6el6WFiYnpSUpI8bN05/9dVXdbfbbWy3f/9+ffLkyXp0dLSelJSk/+pXv9I3b96sA9AXLVpkbHfdddfpMTExx33PBx98UAeg9+3bt81tVqxYoU+YMEFPSEjQIyMj9ezsbP3666/X161b5/PYiIhChaLruh60qoaIiIiIiEIC51gQEREREZEYCwsiIiIiIhJjYUFERERERGIsLIiIiIiISIyFBRERERERibGwICIiIiIiMd4gzweapmHPnj2Ii4uDoijB7g4RERERUUDouo5Dhw4hPT0dqnr8cxIsLHywZ88eZGZmBrsbRERERERBUVJSgoyMjONuw8LCB3FxcQCaAo2Pjw/4+7vdbhQWFiI7Oxs2my3g7x8KmKEcM5RhfnLMUIb5yTFDOWYoE4z8qqurkZmZafw+fDwsLHzgufwpPj4+aIVFbGws4uPj+SHsIGYoxwxlmJ8cM5RhfnLMUI4ZygQzP1+mA3DyNhERERERibGwsIgTTZahE2OGcsxQhvnJMUMZ5ifHDOWYoYyZ81N0XdeD3Qmzq66uRkJCAhwOR1AuhSIiIiIiCob2/B7MORYWoOs6amtrERMTw+VuO4gZyjFDGeYnxwxlmJ9csDPUNA1OpzPg7+tPuq7j8OHDiI6O5nHYAZ2RX1hYmN/ma7CwsABN01BaWoqcnBxOdOogZijHDGWYnxwzlGF+csHM0Ol0ori4GJqmBfR9/U3XdbhcLtjtdhYWHdBZ+SUmJiI1NVX8miwsiIiIiExM13WUlZXBZrMhMzPT1NfYn4iu62hoaEBERAQLiw7wd36eMyD79u0DAKSlpYlej4UFERERkYm5XC4cPnwY6enpiI6ODnZ3RDxTeyMjI1lYdEBn5BcVFQUA2LdvH7p37y46G2fdkvckoigKwsPD+QEUYIZyzFCG+ckxQxnmJxesDN1uNwAgPDw8oO/bWax8xsUMOiM/T8Ha2Ngoeh2esbAAVVXRp0+fYHfD0pihHDOUYX5yzFCG+ckFO8NQKAoVRUFERESwu2FZnZWfv44tlowWoOs6qqqqwJWBO44ZyjFDGeYnxwxlmJ8cM5TzTD5mhh1j9vxYWFiApmkoLy+3/EoQwcQM5ZihDPOTY4YyzE+OGfqH5HKb3r1746mnnvJ5+88//xyKoqCqqqrD72k20suVOlNQC4uVK1fioosuQnp6OhRFwTvvvNPmtjfffDMURWlxMFVWVmLq1KmIj49HYmIiZsyYgZqaGq9tfvjhB5x99tmIjIxEZmYmnnjiiU4YDREREREBTZfWtPafqqqIjo7GvHnzOvS6a9euxcyZM33e/swzz0RZWRkSEhI69H6+CsUCpiOCWljU1tbi1FNPxXPPPXfc7d5++2188803SE9Pb/Hc1KlTsWXLFixfvhzvv/8+Vq5c6XXAVVdX47zzzkOvXr2wfv16/OEPf8C8efPw0ksv+X08RERERASUlZUZ/z311FOIj49HWVkZ9uzZg6KiItx9993Gtp7Le3zRrVu3dq2MFR4e7pf7M5BvglpYTJo0CY8++iguu+yyNrfZvXs3brvtNixZsgRhYWFez23duhXLli3Dyy+/jJEjR2L06NFYuHAh/vWvf2HPnj0AgCVLlsDpdOKVV17BwIEDMWXKFNx+++3405/+1Klj8ydFUXinVCFmKMcMZZifHDOUYX5yzNB3qampxn8JCQlQFMX4evv27YiPj8eHH36IYcOGISIiAl999RUKCwtxySWXICUlBbGxsTj99NPxySefeL3usZdCKYqCl19+GZdddhmio6ORk5OD9957z3j+2DMJixcvRmJiIj766CP0798fsbGxmDhxIsrKyozvcblcuP3225GYmIiuXbvivvvuw3XXXYdLL720w3kcPHgQ06dPR1JSEqKjozFp0iQUFBQYz+/cuRMXXXQRkpKSEBMTg4EDB2Lp0qXG906dOtUoqgYNGoRFixZ1uC+dydSrQmmahmnTpuGee+7BwIEDWzy/evVqJCYmYvjw4Ubb+PHjoaoq1qxZg8suuwyrV6/GmDFjvJZomzBhAh5//HEcPHgQSUlJLV63oaEBDQ0NxtfV1dUAmpZ78yz55jmdp2ma1wSattpVVYWiKG22e163ebsnAwBIT0+HruvG9x57fafNZoOu617tnr601e5r3ztrTCdq9+eYmmfodrtDYkzB2E8ZGRnQNM3re6w+ptbaO2tMzc+6hsqYjtfu7zHpuu71OQ6FMQV6P2VmZrb4DFt9TIHeT+np6cfte2eMqXl/W5u0qyiKeDJvW6/hj3bP157/2+1Nv37ef//9ePLJJ5GVlYWkpCSUlJQYf3SOjIzEq6++iosuugg//fQTevbs6fV6zd9j/vz5ePzxx/HEE09g4cKFmDp1Knbs2IGuXbt6vbfnv8OHD+PJJ5/Ea6+9BlVVMW3aNNx9991YsmQJdF3H73//eyxZsgSvvPIK+vfvj2eeeQbvvPMOzjnnnBb76tixHft/j+uvvx4FBQV49913ER8fj/vvvx/nn38+tmzZgrCwMMyaNQtOpxNffPEFYmJi8OOPPyImJga6ruOhhx7Cjz/+iKVLl6Jbt24oKChAXV1dm33pyH469nfMY5/zlakLi8cffxx2ux233357q8+Xl5eje/fuXm12ux1dunRBeXm5sU1WVpbXNikpKcZzrRUWCxYswPz581u0FxYWIjY2FgCQkJCAtLQ07N27Fw6Hw9gmOTkZycnJ2L17N2pra4321NRUJCYmYseOHXA6nUZ7RkYGYmNjUVhY6PWDKCsrC3a7HQUFBdB1HXV1dYiKikJubi5cLheKi4uNbVVVRW5uLmpra1FaWmq0h4eHo0+fPnA4HEYeABATE4PMzExUVlaioqLCaA/kmJrLycnp9DGVl5ejvLwcUVFRUBQlJMYU6P2UnZ2N/fv3w+FwGH+ts/qYArmfPJ/jjIwMdO/ePSTGFOj9VFRUZPwstNlsITGmQO6nLl26QFVV1NbWoq6uLiTGFOj95Lnr8eDBg3H48OGAjan5L3pOpxOapuGKl9aiosaJph/HLX95bLtdAaDj2N8VW21XAAUKdOjAkfbk2HC8fcsZCA8Ph8vl8rqEyWazITw8HI2NjV7FkCdrT3t9fT0A4De/+Q1+/vOfo6GhAZqmIS8vD3l5eQgPD4fNZsNDDz2Et99+G//5z39wyy23GMusulwu4zUA4LrrrsOUKVPQ0NCAuXPnYuHChfj6669x8cUXG+9dX1+P+vp6uFwuNDY24tlnnzWKlZkzZ+L3v/+98doLFy7E3XffjUmTJsFms+HZZ5/F0qVLvfput9sRFhZmjMlznHnG7dlPALB9+3a89957+Prrr3HaaadB13W8/PLLyM3Nxdtvv42rrroKO3fuxCWXXIKcnBwAQJ8+faDrOurr67Fjxw4MGjQIgwYNQkREBNLT0736oqoqIiIi4Ha7vSZ2e/aHL/vJsw8AtPg8tefSM9MWFuvXr8fTTz+NDRs2BPyU4wMPPIA777zT+Lq6uhqZmZnIzs5GfHw8gKPr/aakpHgVN572Hj16tPgrCdB0Cq+19uzsbK8+eNpzcnLgdruxfft2ZGdnQ1VVhIeHGwdeczExMV7tnr4kJCQgLi6uRXuXLl28CqtAjunY9s4eU7du3VBVVYXs7GzYbLaQGFOg95Ou63A4HEaGoTCmQO4nz+fYs00ojOnY9s4eU3Z2tvGz0HMMWn1MgdxPmqahsLDQ+LckFMYU6P3k+Rzruh7QMdXX12PXrl0Ajt4k70BtI/YeOnp1RaAoimJcmm63242zD82FhYV5Xb7uyTcsLAx2u934esSIEQCOjqmmpgbz5s3D0qVLUVZWBpfLhbq6OpSVlSEyMtJ4Pbvd7vX14MGDoSgKIiMjERkZifj4eBw8eNDrvT3P2e12REdHIzc31/j+nj17Yt++fQCa5v/u27cPZ555pvEeiqJg2LBh0DTN632bj9UzBs/PpuZXyhQVFcFut2PkyJFGf3r06IG8vDz89NNPAIDbb78dt956K1asWIFx48Zh8uTJGDx4MCIjIzFr1ixcccUV+OGHH/Dzn/8c559/Pn72s5+1+P3YZrO1etfs9u6nYz9Pxy6KdDymLSy+/PJL7Nu3z+vUl9vtxl133YWnnnoKO3bsQGpqqnEgeLhcLlRWViI1NRVA018t9u7d67WN52vPNseKiIho9eYjre2w5j+cJe1t3T7d066qqtcvxK1tryhKu9r91feOjsmXdn+OyZNh8++z+pj80e5r3z2XkLX2ObDqmI7X3hlj8hyHvm5/oj62tz0U9tOxn+NQGNOxAjGm9ryOVcbUnnbJmDyvGcgxNX894w9mccG5yVy3uAijD2394ffY9ra291wF4mm/5557sHz5cjz55JPo27cvoqKicMUVV6CxsdHrexVF8fr62LuhN7/Mp/l7e/4LCwvz2t5zCVtr27d3bK2Nta3Hzd/npptuwsSJE/HBBx/g448/xu9//3v88Y9/xG233Ybzzz8fO3fuxNKlS7F8+XKcf/75uPXWW/HHP/7xuH1rT3vz8R57TLbnD/ymLSymTZuG8ePHe7VNmDAB06ZNww033AAAGDVqFKqqqrB+/XoMGzYMAPDZZ59B0zSMHDnS2ObBBx9EY2OjUZUtX74ceXl5rV4GZUYHDzuxp7oR9opa9E2JD3Z3iIiIKMj+d9voYHfB777++mtcf/31xqI+NTU12LFjR0D7kJCQgJSUFKxduxZjxowB0PSHtQ0bNmDIkCEdes3+/fvD5XJhzZo1OPPMMwEABw4cwLZt2zBgwABju8zMTNx88824+eab8cADD+Cvf/0rbrvtNgBNV15cd911mD59OkaOHIkHH3ywRWFhBkEtLGpqarB9+3bj6+LiYmzcuBFdunRBz5490bVrV6/tw8LCkJqairy8PABNO2rixIm46aab8OKLL6KxsRGzZ8/GlClTjEmS1157LebPn48ZM2bgvvvuw+bNm/H000/jz3/+c+AGKnTW45+jwaUhL/UgPpozJtjdsSRFUYxVKahjmKEM85NjhjLMT44Z+kdbZ2lycnLw3//+FxdddBEURcHDDz8clJsR3nbbbViwYAH69u2Lfv36YeHChTh48KBP+33Tpk0tLoU79dRTcckll+Cmm27CX/7yF8TFxeH+++9Hjx49cMkllwAA5syZg0mTJiE3NxcHDx7EihUr0L9/fwDA3LlzMWzYMAwcOBD19fVYtmyZ8ZzZBLWwWLduHc455xzja8+8huuuuw6LFy/26TWWLFmC2bNnY9y4cVBVFZMnT8YzzzxjPJ+QkICPP/4Ys2bNwrBhw5CcnIy5c+e26+YqwRYfFYb9hxpQU+/bGs/UkqqqSEtLC3Y3LI0ZyjA/OWYow/zkmKFc8zkax/rTn/6EX/7ylzjzzDORnJyM++67z1iZM5Duu+8+lJeXY/r06bDZbJg5cyYmTJjQ5qVyzXnOcnjYbDa4XC4sWrQIv/71r3HhhRfC6XRizJgxWLp0qZGF2+3GrFmzUFpaivj4eEycONH4I3h4eDgeeOAB7NixA1FRUTj77LPxr3/9y/8D9wNFl65PdhKorq5GQkICHA6HMXk7kM598nMUVdQiLtKOTfMmBPz9Q4Gmadi7dy9SUlLa/EsJHR8zlGF+csxQhvnJBSvD+vp6FBcXIysrq8XkYavRdd24PN0qZ340TUP//v1x1VVX4be//W1Q+9JZ+R3vGGvP78H8yWIBcZFNJ5ZqGlzQNNaBHeFZ0Yh1dMcxQxnmJ8cMZZifHDP0j2PvI2I2O3fuxF//+lfk5+dj06ZNuOWWW1BcXIxrr7022F0DYO78WFhYgKew0HWgxsnLoYiIiIg6i6qqWLx4MU4//XScddZZ2LRpEz755BPTzmswE9OuCkVHxUcevRbxUL3L62siIiIi8p/MzEx8/fXXwe6GJfGMhQV4zlgAwKH6xuNsSW3x3G3bKtdzmhEzlGF+csxQhvnJMUP/aO1mbeQ7M+dn3p6RIT7K+4wFtZ+qqkhOTg52NyyNGcowPzlmKMP85Jih3PFWhaITM3t+PGNhAbERR+u/6jqesegITdNQUlISlPWwQwUzlGF+csxQhvnJMUM5XdfhdDo5Ab6DzJ4fCwsLiI04um4yz1h0jK7rqK2tNe0H0QqYoQzzk2OGMsxPjhn6h5lXNbICM+fHwsICvC+F4hkLIiIiIjIfFhYWENf8UiiesSAiIiIiE2JhYQEJ0eHG42qesegQVVWRmprKu80KMEMZ5ifHDGWYnxwz9I/2TD4eO3Ys5syZY3zdu3dvPPXUU8f9HkVR8M4773Ssc53wOv7GydskEhfJVaGkFEVBYmIilwgUYIYyzE+OGcowPzlm6LuLLroIEydObNGuKApWr14NVVXxww8/tPt1165di5kzZ/qji4Z58+ZhyJAhLdrLysowadIkv77XsRYvXozExESft1cUBXa73bTHIAsLC4jj5G0xTdNQVFTElTwEmKEM85NjhjLMT44Z+m7GjBlYvnw5SktLvdp1XcfLL7+M4cOHY/Dgwe1+3W7duiE6Otpf3Tyu1NRUREREBOS9fKXrOhoaGky7gAALCwvwXhWKl0J1hNmXZ7MCZijD/OSYoQzzk2OGvrvwwgvRrVs3LF682Ku9pqYG//3vf/HLX/4SBw4cwDXXXIMePXogOjoagwYNwj//+c/jvu6xl0IVFBRgzJgxiIyMxIABA7B8+fIW33PfffchNzcX0dHR6NOnDx5++GE0Njb9PrV48WLMnz8f33//PRRFgaIoRp+PvRRq06ZNOPfccxEVFYWuXbti5syZqKmpMZ6//vrrcemll+LJJ59EWloaunbtilmzZhnv1RG7du3CJZdcgtjYWMTHx+Pqq69GWVmZ8fz333+Pc845B3FxcYiPj8ewYcOwbt06AMDOnTtx0UUXISkpCTExMRg4cCCWLl3a4b74gjfIswDex4KIiIisxG63Y/r06Vi8eDEefPBB49Kdt956C263G9dccw1qa2sxbNgw3HfffYiPj8cHH3yAadOmITs7GyNGjDjhe2iahssvvxwpKSlYs2YNHA6H13wMj7i4OCxevBjp6enYtGkTbrrpJsTFxeHee+/F1Vdfjc2bN2PZsmX45JNPAAAJCQktXqO2thYTJkzAqFGjsHbtWuzbtw833ngjZs+e7VU8rVixAmlpaVixYgW2b9+Oq6++GkOGDMFNN93U7gw1TTOKii+++AIulwuzZs3C9OnT8cUXXwAApk6diqFDh+KFF16AzWbDxo0bjTkYs2bNgtPpxMqVKxETE4Mff/wRsbGx7e5He7CwsAC7TUWkXUG9S+elUERERAT85WdAzb7Av29sd+BXX/i06S9/+Uv84Q9/wBdffIGxY8cCaDpDcOmllyIhIQGJiYm4++67je1vu+02fPTRR3jzzTd9Kiw++eQT/PTTT/joo4+Qnp4OAPjd737XYl7EQw89ZDzu3bs37r77bvzrX//Cvffei6ioKMTGxsJutyM1NbXN93r99ddRX1+P1157DTExMQCAZ599FhdddBEef/xxpKSkAACSkpLw7LPPwmazoV+/frjgggvw6aefdqiw+PTTT7Fp0yYUFxcjMzMTAPDqq6/ilFNOwdq1azFixAjs2rUL99xzD/r16wcAyMnJMb5/165dmDx5MgYNGgQA6NOnT7v70F4sLCxAVVUkRIWh/pCThUUHqaqKjIwMruQhwAxlmJ8cM5RhfnKmyrBmH3BoT7B7cVz9+vXDmWeeiVdeeQVjx47F9u3b8eWXXxpnBtxuN373u9/hzTffxO7du+F0OtHQ0ODzHIqtW7ciMzPTKCoAYNSoUS22e+ONN/DMM8+gsLAQNTU1cLlciI+Pb9dYtm7dilNPPdUoKgDgrLPOgqZp2LZtm1FYDBw4EDbb0UvY09LSsGnTpna9V/P3zMzMNIoKABgwYAASExOxdetWjBgxAnfeeSduvPFG/P3vf8f48eNx5ZVXIjs7GwBw++2345ZbbsHHH3+M8ePHY/LkyR2a19IeJvhk0IkoioL4qKYlZznHomMURUFsbKxpV1GwAmYow/zkmKEM85MzVYax3YG49MD/F9u9Xd2cMWMG/vOf/+DQoUNYtGgRsrOzce6550JRFPzhD3/A008/jfvuuw8rVqzAxo0bMWHCBDidTr/FtHr1akydOhXnn38+3n//fXz33Xd48MEH/foezR27FKyiKH6d7O859jz/nzdvHrZs2YILLrgAn332GQYMGIC3334bAHDjjTeiqKgI06ZNw6ZNmzB8+HAsXLjQb31pDc9YWIDb7UYYms5U1DrdcLk12G2sCdvD7XajsLAQ2dnZXn9JIN8xQxnmJ8cMZZifnKky9PFypGC76qqr8Otf/xqvv/46XnvtNdx8881oaGhAREQEvv76a1xyySX4xS9+AaBpTkF+fj4GDBjg02v3798fJSUlKCsrQ1paGgDgm2++8dpm1apV6NWrFx588EGjbefOnV7bhIeHw+12n/C9Fi9ejNraWuOsxddffw1VVZGXl+dTf9vLM76SkhLjrMWWLVtQVVWF/v37G9vl5uYiNzcXd9xxB6655hosWrQIl112GQAgMzMTN998M26++WY88MAD+Otf/4rbbrutU/oL8IyFZUSHHf3rSE0DL4fqCC4PKMcMZZifHDOUYX5yzLB9YmNjcfXVV+OBBx5AWVkZrr/+emNVrZycHCxfvhyrVq3C1q1b8atf/Qp79+71+bXHjx+P3NxcXHfddfj+++/x5ZdfehUQnvfYtWsX/vWvf6GwsBDPPPOM8Rd9j969e6O4uBgbN25ERUUFGhoaWrzX1KlTERkZieuuuw6bN2/GihUrcNttt2HatGnGZVAd5Xa7sXHjRq//tm7divHjx2PQoEGYOnUqNmzYgG+//RbXXXcdzj77bAwfPhx1dXWYPXs2Pv/8c+zcuRNff/011q5daxQdc+bMwUcffYTi4mJs2LABK1as8CpIOgMLC4uICeO9LIiIiMh6ZsyYgYMHD2LChAle8yEeeughnHbaaZgwYQLGjh2L1NRUXHrppT6/rqqqePvtt1FXV4cRI0bgxhtvxGOPPea1zcUXX4w77rgDs2fPxpAhQ7Bq1So8/PDDXttMnjwZEydOxDnnnINu3bq1uuRtdHQ0PvroI1RWVuL000/HFVdcgXHjxuHZZ59tXxitqKmpwdChQ73+u+iii6AoCt59910kJSVhzJgxGD9+PPr06YPXXnsNAGCz2XDgwAFMnz4dubm5uOqqqzBp0iTMnz8fQFPBMmvWLPTv3x8TJ05Ebm4unn/+eXF/j0fRuRjzCVVXVyMhIQEOh6Pdk338we1247ZXV2FpfjUA4IPbR2Ngesul0KhtbrcbBQUFyMnJCf7pa4tihjLMT44ZyjA/uWBlWF9fj+LiYmRlZSEyMjJg79sZdF1HfX09IiMjzTFXxWI6K7/jHWPt+T2YZywsQFVVpHdLMr6uruMZi/ZSVRVZWVnmWMnDopihDPOTY4YyzE+OGfqH2e5mbTVmzo+fDItIiA43HnNlqI6x27lWgRQzlGF+csxQhvnJMUM5nqmQMXN+LCwsQNM01FUfNL7mHIv20zQNBQUFnHQnwAxlmJ8cM5RhfnLM0D/q6+uD3QVLM3N+LCwsIjb86K7iGQsiIiIiMhsWFhYR06ywqOYZCyIiIiIyGRYWFhHDMxZEREQnNS7kSZ3FX5f3cQaSBaiqiv59swDsAcA5Fh2hqipycnK4kocAM5RhfnLMUIb5yQUrw7CwMCiKgv3796Nbt26mnrx7Ip7iqL6+3tLjCBZ/56frOpxOJ/bv3w9VVREeHn7ibzoOFhYWEdVsT7Gw6BiXyyX+wJzsmKEM85NjhjLMTy4YGdpsNmRkZKC0tBQ7duwI6Ht3Bl3XWVQIdEZ+0dHR6Nmzp7hoZmFhAZqm4eDe3cbX1bwUqt00TUNxcTFvDCXADGWYnxwzlGF+csHMMDY2Fjk5OWhstPbvAG63Gzt37kTPnj15HHZAZ+Rns9lgt9v9UqywsLCIqDAVigLoOidvExERnYxsNpvlfxl3u91QVRWRkZGWH0swmD0/XmhpEaqiICa8qQ7k5G0iIiIiMhsWFmanuaF893ck/7gYN4R9BIBzLDqKExblmKEM85NjhjLMT44ZyjFDGTPnp+hcu+yEqqurkZCQAIfDgfj4+MC+ua4Dj6UCrnrsUHti7OHfI8KuYtujkwLbDyIiIiI66bTn92DzljzURFGgx3YHAHTRqwAADS4NTpd/1hs+Wei6jpqaGq4BLsAMZZifHDOUYX5yzFCOGcqYPT8WFlYQmwIAiNerEYamy6A4z6J9NE1DaWmp324AczJihjLMT44ZyjA/OWYoxwxlzJ4fCwsriEkxHnaFAwDnWRARERGRubCwsADPpVAA0E1pKix4LwsiIiIiMhMWFlYQe/SMRTelCgDPWLSXoigIDw/nnT4FmKEM85NjhjLMT44ZyjFDGbPnxxvkWYAa17yw8FwKxTMW7aGqKvr06RPsblgaM5RhfnLMUIb5yTFDOWYoY/b8eMbCArwuhUIVAN59u710XUdVVZVpV1GwAmYow/zkmKEM85NjhnLMUMbs+bGwsAAtupvx2HMpVHUdz1i0h6ZpKC8vN+0qClbADGWYnxwzlGF+csxQjhnKmD0/FhZWENvapVA8Y0FERERE5sHCwgpiWp6xYGFBRERERGbCwsIClLBIuMObbqHeDZy83RGKoiAmJsa0qyhYATOUYX5yzFCG+ckxQzlmKGP2/LgqlAWoqgrEpwEV1UfnWLCwaBdVVZGZmRnsblgaM5RhfnLMUIb5yTFDOWYoY/b8eMbCAjRNgzMiCQAQozQgGvW8FKqdNE1DRUWFaSc7WQEzlGF+csxQhvnJMUM5Zihj9vxYWFiAruuos8UbX3dTqlhYtJOu66ioqDDt8mxWwAxlmJ8cM5RhfnLMUI4Zypg9PxYWFuGO6mo87oYqzrEgIiIiIlNhYWERrshmhYXi4A3yiIiIiMhUWFhYgKIoCO+SYXzddClUo2lPg5mRoihISEgw7SoKVsAMZZifHDOUYX5yzFCOGcqYPT+uCmUBqqoisUeu8XU3xYFGl44Gl4bIMFsQe2YdqqoiLS0t2N2wNGYow/zkmKEM85NjhnLMUMbs+fGMhQVomob99UcLiG6oAgAcPOwMUo+sR9M0lJWVmXYVBStghjLMT44ZyjA/OWYoxwxlzJ4fCwsL0HUdVa5w4+tuStNN8soc9cHqkuXoug6Hw8HLxwSYoQzzk2OGMsxPjhnKMUMZs+fHwsIi3OEJ0JWmsxaem+SVs7AgIiIiIpMIamGxcuVKXHTRRUhPT4eiKHjnnXeM5xobG3Hfffdh0KBBiImJQXp6OqZPn449e/Z4vUZlZSWmTp2K+Ph4JCYmYsaMGaipqfHa5ocffsDZZ5+NyMhIZGZm4oknngjE8PxLtQEx3QDwjAURERERmU9QC4va2lqceuqpeO6551o8d/jwYWzYsAEPP/wwNmzYgP/+97/Ytm0bLr74Yq/tpk6dii1btmD58uV4//33sXLlSsycOdN4vrq6Gueddx569eqF9evX4w9/+APmzZuHl156qdPH5y+KoiA5ORmI7Q4ASIYDCjSUVdUFuWfW4cnQrKsoWAEzlGF+csxQhvnJMUM5Zihj9vwU3SQXaSmKgrfffhuXXnppm9usXbsWI0aMwM6dO9GzZ09s3boVAwYMwNq1azF8+HAAwLJly3D++eejtLQU6enpeOGFF/Dggw+ivLwc4eFN8xTuv/9+vPPOO/jpp5986lt1dTUSEhLgcDgQHx9/4m/oLP+4Ati+HAAwtP5FnDk4D89de1rw+kNEREREIa09vwdbao6Fw+GAoihITEwEAKxevRqJiYlGUQEA48ePh6qqWLNmjbHNmDFjjKICACZMmIBt27bh4MGDAe1/R2mahpKSEuhHzlgATZdD8YyF7zwZmnUVBStghjLMT44ZyjA/OWYoxwxlzJ6fZe5jUV9fj/vuuw/XXHONUS2Vl5eje/fuXtvZ7XZ06dIF5eXlxjZZWVle26SkpBjPJSUltXivhoYGNDQ0GF9XV1cDANxuN9xuN4CmMyyqqkLTNK+Z+W21q6oKRVHabPe8bvN2oOkAcrvdOHToELToZHgWne2mVKHIUW98n81mg67rXgeapy9ttfva984Yky/t/hyTJ0OXywWbzRYSYwr0ftJ1HTU1NUaGoTCmQO4nzzHodrtDZkwnavf3mFwul9fnOBTGFMj9pGkaamtr4Xa7Q2ZMgd5Pns+xrushMyaPQO2n5p/jsLCwkBhTIPcTgBb/Fnf2mNpzcZMlCovGxkZcddVV0HUdL7zwQqe/34IFCzB//vwW7YWFhYiNjQUAJCQkIC0tDXv37oXD4TC2SU5ORnJyMnbv3o3a2lqjPTU1FYmJidixYweczqP3n8jIyEBsbCwKCwu9DoasrCzY7XYUFBRA0zRUVlZi/2EVqUee7wYHVlfX46dt+Qiz25Cbm4va2lqUlpYarxEeHo4+ffrA4XAYhRYAxMTEIDMzE5WVlaioqDDaAzmm5nJycuByuVBcXGy0qarq1zHt27cPlZWV2L59O1RVDYkxBXo/9enTB26328gwFMYUyP3k+RxXVlYiJSUlJMYU6P1UWFhofI7tdntIjCmQ+8nzh7Q9e/agru7oGW8rjynQ+0nTNONqh1AZExDY/XTo0CHjc5yenh4SYwrkfsrOzkZjY6PXv8WdPabo6Gj4yvRzLDxFRVFRET777DN07drVeO6VV17BXXfd5XVJk8vlQmRkJN566y1cdtllmD59Oqqrq71WnFqxYgXOPfdcVFZW+nzGwrNjPGdLAn3GYvv27ch1bob9nZsAAI81Xou/ui/E1/eORWpCZEhW5f4cU2NjIwoKCtC3b1+esejgmHRdR0FBAbKzs3nGooNnLLZv346cnByEhYWFxJhO1O7vMXn+MfV8jkNhTIE+Y1FYWIjs7Gzj/a0+pmCcsdi+fTvy8vKM97X6mDwCecbC8znmGYuOnbHIz8/3+re4s8dUU1ODxMREn+ZYmPqMhaeoKCgowIoVK7yKCgAYNWoUqqqqsH79egwbNgwA8Nlnn0HTNIwcOdLY5sEHH0RjYyPCwsIAAMuXL0deXl6rRQUAREREICIiokW75x+y5pr/cJa0H/u6zdtVVUV6ejpsVZVGu2fJ2X01TvToEgOg6YBo7XXaavdX3zsyJl/b/TUmu92O9PR044fYiba3wpgCvZ90XUdaWlqLDAHrjul47f4ek+dzbLfbfdpe0ve22q2+n8LCwlp8jq0+pkDuJ1VVkZqaCrvd3uIzfLzXMfOYOtre0TF5PseeXxJDYUzNBWJMrX2OrT6m9rRLx9SRf4ulfW/t50Vbgjp5u6amBhs3bsTGjRsBAMXFxdi4cSN27dqFxsZGXHHFFVi3bh2WLFkCt9uN8vJylJeXG6eW+vfvj4kTJ+Kmm27Ct99+i6+//hqzZ8/GlClTkJ6eDgC49tprER4ejhkzZmDLli1444038PTTT+POO+8M1rDbTVGaJqwrsalGm+cmebyXhW+MDNvx4SBvzFCG+ckxQxnmJ8cM5ZihjNnzC2phsW7dOgwdOhRDhw4FANx5550YOnQo5s6di927d+O9995DaWkphgwZgrS0NOO/VatWGa+xZMkS9OvXD+PGjcP555+P0aNHe92jIiEhAR9//DGKi4sxbNgw3HXXXZg7d67XvS7MTtM0FBUVQYtJNtq6oQoACwtfGRm2ckqRfMMMZZifHDOUYX5yzFCOGcqYPb+gXgo1duzY484092X6R5cuXfD6668fd5vBgwfjyy+/bHf/zELXdTidTuhhsYA9CnDVHb37Npec9YmRoTmmFFkSM5RhfnLMUIb5yTFDOWYoY/b8LHUfi5Oeohh33zYuharmGQsiIiIiCj4WFlYT23QPji5KDcLg4hkLIiIiIjIFFhYWoKoqMjIymmbpx6UY7SnKQZRzjoVPvDKkDmGGMsxPjhnKMD85ZijHDGXMnp85e0VeFEVBbGxs0woAib2M9kxlH/YeaoBbM+d1dmbilSF1CDOUYX5yzFCG+ckxQzlmKGP2/FhYWIDb7UZ+fn7TTVWaFRYZyn64NR37DzUc57sJOCZD6hBmKMP85JihDPOTY4ZyzFDG7PmxsLAIY1mxJO/CAgDKHJxn4QuzLs1mJcxQhvnJMUMZ5ifHDOWYoYyZ82NhYTWJPY2HmUZhwXkWRERERBRcLCysxquw2AeAhQURERERBR8LCwtQVRVZWVlNKwCExwAx3QA0O2PBJWdPyCtD6hBmKMP85JihDPOTY4ZyzFDG7PmZs1fUgt3e7CbpR85apCoHEY5G3iTPR14ZUocwQxnmJ8cMZZifHDOUY4YyZs6PhYUFaJqGgoKCo5N1mq0M1UOp4BkLH7TIkNqNGcowPzlmKMP85JihHDOUMXt+LCysKMn7Xha8SR4RERERBRsLCyvyukneft4kj4iIiIiCjoWFFTVbGYo3ySMiIiIiM2BhYQGqqiInJ+foCgBJvY3nMnmTPJ+0yJDajRnKMD85ZijD/OSYoRwzlDF7fubsFbXgcrmOfpGQAUABAGQcuZdF6UEWFifilSF1CDOUYX5yzFCG+ckxQzlmKGPm/FhYWICmaSguLj66AoA9AohPB3D0jEVxRW2wumcJLTKkdmOGMsxPjhnKMD85ZijHDGXMnh8LC6s6Ms+iq3II0ahnYUFEREREQcXCwqqarQyVoexHEQsLIiIiIgoiFhYW0WKSzjH3sijaXwNd55Kzx2PWiU5WwgxlmJ8cM5RhfnLMUI4Zypg5P/P2jAw2mw25ubmw2WxHG72WnK3AoXoXDtQ6g9A7a2g1Q2oXZijD/OSYoQzzk2OGcsxQxuz5sbCwAF3XUVNzzBmJRO8zFgAncB9PqxlSuzBDGeYnxwxlmJ8cM5RjhjJmz4+FhQVomobS0lLvFQCSvO++DQBF+2sC3TXLaDVDahdmKMP85JihDPOTY4ZyzFDG7PmxsLCquHRAtQNoVljwjAURERERBQkLC6uy2YH4HgCaVoUCgKL9LCyIiIiIKDhYWFiAoigIDw+HoijeTxy5HCpeOYx41HCOxXG0mSH5jBnKMD85ZijD/OSYoRwzlDF7fvZgd4BOTFVV9OnTp+UTid7zLPIPxMGt6bCp5jzYgqnNDMlnzFCG+ckxQxnmJ8cM5ZihjNnz4xkLC9B1HVVVVS1XAGg2gbu3sheNbh2lBw8HuHfW0GaG5DNmKMP85JihDPOTY4ZyzFDG7PmxsLAATdNQXl7ecgWAbv2Mh7lqKQBO4G5LmxmSz5ihDPOTY4YyzE+OGcoxQxmz58fCwsq6DzAe5iolAIBiTuAmIiIioiBgYWFlSb0BeyQAIFfxnLHgvSyIiIiIKPBYWFiAoiiIiYlpuQKAagO65QEAeivliICTK0O1oc0MyWfMUIb5yTFDGeYnxwzlmKGM2fNjYWEBqqoiMzMTqtrK7jpyOZRN0dFX2cN7WbThuBmST5ihDPOTY4YyzE+OGcoxQxmz52fOXpEXTdNQUVHR+kSd7v2Nh7lKCcoc9TjsdAWwd9Zw3AzJJ8xQhvnJMUMZ5ifHDOWYoYzZ82NhYQG6rqOioqL1pcWaTeDOO7Iy1I4KLjl7rONmSD5hhjLMT44ZyjA/OWYoxwxlzJ4fCwurO+aMBcAJ3EREREQUeCwsrC6+BxARD6DZvSw4z4KIiIiIAoyFhQUoioKEhITWVwBQFOOsRYZSgVgcxo97qgPcQ/M7bobkE2Yow/zkmKEM85NjhnLMUMbs+bGwsABVVZGWltb2CgBel0OVYtNuR4B6Zh0nzJBOiBnKMD85ZijD/OSYoRwzlDF7fubsFXnRNA1lZWVtrwDQ/A7cail2V9XhQE1DgHpnDSfMkE6IGcowPzlmKMP85JihHDOUMXt+LCwsQNd1OByOtlcAaHbGIu/IBG6etfB2wgzphJihDPOTY4YyzE+OGcoxQxmz58fCIhQ0P2OhNE3g3szCgoiIiIgCiIVFKIhJBmK6AQDy1KYzFj+UsrAgIiIiosBhYWEBiqIgOTn5+CsAHLkcKlmpRlc4eMbiGD5lSMfFDGWYnxwzlGF+csxQjhnKmD0/FhYWoKoqkpOTj78CQPeBxsNctRR7HPXYf4gTuD18ypCOixnKMD85ZijD/OSYoRwzlDF7fubsFXnRNA0lJSXHXwGg2QTufsouAJxn0ZxPGdJxMUMZ5ifHDGWYnxwzlGOGMmbPj4WFBei6jtra2uOvAJBy9IzFKWoxAK4M1ZxPGdJxMUMZ5ifHDGWYnxwzlGOGMmbPj4VFqEgdBNgiAADDlXwAnMBNRERERIHDwiJU2COAHqcBAHqp+9ANVbwUioiIiIgChoWFBaiqitTU1BNP1MkcYTw8Tc1HeXU99h2q7+TeWYPPGVKbmKEM85NjhjLMT44ZyjFDGbPnZ85ekRdFUZCYmHjipcUyzzAeDlMLAHACt4fPGVKbmKEM85NjhjLMT44ZyjFDGbPnx8LCAjRNQ1FR0YlXAMgcaTwcrm4DwHkWHj5nSG1ihjLMT44ZyjA/OWYoxwxlzJ4fCwsL0HUdTqfzxCsAxHQFuvYFAJyiFCMCThYWR/icIbWJGcowPzlmKMP85JihHDOUMXt+LCxCzZHLocIVNwYpRVhTdABOlzmrWiIiIiIKHSwsQk3P5pdD5aPW6cZ3uw4GsUNEREREdDIIamGxcuVKXHTRRUhPT4eiKHjnnXe8ntd1HXPnzkVaWhqioqIwfvx4FBQUeG1TWVmJqVOnIj4+HomJiZgxYwZqamq8tvnhhx9w9tlnIzIyEpmZmXjiiSc6e2h+paoqMjIyfFsBoNk8i2Fq0/0sVhbs76yuWUa7MqRWMUMZ5ifHDGWYnxwzlGOGMmbPL6i9qq2txamnnornnnuu1eefeOIJPPPMM3jxxRexZs0axMTEYMKECaivP7qE6tSpU7FlyxYsX74c77//PlauXImZM2caz1dXV+O8885Dr169sH79evzhD3/AvHnz8NJLL3X6+PxFURTExsb6tgJA1xwgKgmAp7DQsTK/onM7aAHtypBaxQxlmJ8cM5RhfnLMUI4Zypg9v6AWFpMmTcKjjz6Kyy67rMVzuq7jqaeewkMPPYRLLrkEgwcPxmuvvYY9e/YYZza2bt2KZcuW4eWXX8bIkSMxevRoLFy4EP/617+wZ88eAMCSJUvgdDrxyiuvYODAgZgyZQpuv/12/OlPfwrkUEXcbjfy8/PhdrtPvLGqAhlN97PootSgj1KGzXscOFDT0Mm9NLd2ZUitYoYyzE+OGcowPzlmKMcMZcyenznPowAoLi5GeXk5xo8fb7QlJCRg5MiRWL16NQBg9erVSExMxPDhw41txo8fD1VVsWbNGmObMWPGIDw83NhmwoQJ2LZtGw4etM7cg3YtK9bT+3IoXQe+2s6zFmZdms1KmKEM85NjhjLMT44ZyjFDGTPnZw92B9pSXl4OAEhJSfFqT0lJMZ4rLy9H9+7dvZ632+3o0qWL1zZZWVktXsPzXFJSUov3bmhoQEPD0b/wV1dXA2iqEj0VoqIoUFUVmqZ5LfnVVruqqlAUpc32YytPz7VzmqbB7XYb/2/e3pzNZoOu603tPUbAdqR9mJKPtzAWn2/bhwsHpXao750xJl/avcZ0TF/aaj9e3z0ZhtKYArmfdF2HrusttrfymAK5nzyfY03TYLPZQmJMJ2r395ia/ywMlTEFcj95vre1vlh1TIHeT55jEEDIjMkjUPvp2N9pQmFMgdxPAFr8W9zZY2rP0ramLSyCacGCBZg/f36L9sLCQsTGxgJoOnuSlpaGvXv3wuE4eq+I5ORkJCcnY/fu3aitrTXaU1NTkZiYiB07dsDpdBrtGRkZiI2NRWFhodfBkJWVBbvdjoKCAmiahsrKSmzfvh15eXlwuVwoLi42tlVVFbm5uaitrUVpaSkUVxxy1DCoWiPG2DYBLh2f/7QX+fn5UBQFMTExyMzMRGVlJSoqjp7JCOSYmsvJyTnhmDzCw8PRp08fOBwOo3gEcMIx7du3z8hQVdWQGFOg91OfPn3gdruNDENhTIHcT57PcWVlJVJSUkJiTIHeT4WFhcbn2G63h8SYArmfPH9I27NnD+rq6kJiTIHeT5qmGVc7hMqYgMDup0OHDhmf4/T09JAYUyD3U3Z2NhobG73+Le7sMUVHR8NXim6SO2woioK3334bl156KQCgqKgI2dnZ+O677zBkyBBju5/97GcYMmQInn76abzyyiu46667vC5pcrlciIyMxFtvvYXLLrsM06dPR3V1tdeKUytWrMC5556LyspKn89YeHZMfHy80d9AVbC6rqOxsRFhYWGw2WxGe3PHVuXq61dCKfwUAHBRw6PYpPfBB7edhX6pcZapyv35lwa32w2n04mwsDAoihISYwr0flIUBU6nE3a73WvSmJXHFMj95Pkch4eH84yF4IyF52ehoighMaZA7ieg6d9Iu937b4pWHlOg95PncxwZGdlie6uOySNQ+0nTNK/faUJhTIHcT6qqoqGhwevf4s4eU01NDRITE+FwOIzfg9ti2jMWWVlZSE1NxaeffmoUFtXV1VizZg1uueUWAMCoUaNQVVWF9evXY9iwYQCAzz77DJqmYeTIkcY2Dz74oHEQA8Dy5cuRl5fXalEBABEREYiIiGjRbrPZjF/sPTw7/ljtbT/2dZu367pu7GzPQdTa9p5/aAEA/S8CjhQW59nWYZOrD77afgADeyT6ve8dGZOv7V5j8qH9eH0MDw/3yvBE20v73la7P8fkj3Zf+67rOsLCwlpkCFh3TMdr9/eYmn+Ofdle0ve22kNhPx37szAUxnSszhqTruuw2+2tfoaP9zpmHlNH2zs6Js/nGAidMTUXiDE1/+OeJ0urj6k97dIxdeTfYmnfW/t50ZagTt6uqanBxo0bsXHjRgBNE7Y3btyIXbt2QVEUzJkzB48++ijee+89bNq0CdOnT0d6erpxVqN///6YOHEibrrpJnz77bf4+uuvMXv2bEyZMgXp6ekAgGuvvRbh4eGYMWMGtmzZgjfeeANPP/007rzzziCNuv00TTMuifJZ3vkAmg6ECepaACf3/Sw6lCF5YYYyzE+OGcowPzlmKMcMZcyeX1DPWKxbtw7nnHOO8bXnl/3rrrsOixcvxr333ova2lrMnDkTVVVVGD16NJYtW2acggSalpOdPXs2xo0bB1VVMXnyZDzzzDPG8wkJCfj4448xa9YsDBs2DMnJyZg7d67XvS5CUlwKkDkCKFmDXHU3spQyfFus4EBNA7rGtjwbQ0REREQkEdTCYuzYscedaa4oCh555BE88sgjbW7TpUsXvP7668d9n8GDB+PLL7/scD8tq9+FQEnTsrvnqevwF/dF+O+G3bhpTJ8gd4yIiIiIQo1p72NBftD/QuPhBFvT5VD/XLurXcuGERERERH5wjSrQplZdXU1EhISfJoN3xk8s/zbmnB3XM+fCezbAgA4vf457EcS3vzVKIzI6tIJPTUvUYYEgBlKMT85ZijD/OSYoRwzlAlGfu35PZhnLCzC5XJ17Bv7XWA8PM+2HgDwr7W7/NEly+lwhmRghjLMT44ZyjA/OWYoxwxlzJwfCwsL0DQNxcXFHVsBoNnlUOeHNRUWSzeVwVHX6K/uWYIoQwLADKWYnxwzlGF+csxQjhnKmD0/FhahLnUwkNATADAKm9ED+1HfqOHdjbuD3DEiIiIiCiUsLEKdogBDpwIAVLhxg30ZAOCf35ZwEjcRERER+Q0LC4to6+6IPjn9RsDedO+PqWGfIx612FpWjZUFFX7qnTWIMiQAzFCK+ckxQxnmJ8cM5ZihjJnz46pQPgj2qlB+8f4dwLpXAAALGq/BX9wXIbtbDJbNGYMwm3kPUCIiIiIKHq4KFWJ0XUdNTY3s0qUzZgFoWpZsZsTHCIMLhftr8eqqHX7po9n5JcOTHDOUYX5yzFCG+ckxQzlmKGP2/FhYWICmaSgtLZWtAJDc11h6tqt2ABfbVgEAnv6kAPsPNfijm6bmlwxPcsxQhvnJMUMZ5ifHDOWYoYzZ82NhcTI58zbj4d2xHwPQcajBhT989FPw+kREREREIYGFxcmk5xlAxggAQFpDEW6I+AIA8Nb6UqzafnJN5CYiIiIi/2JhYQGKoiA8PNw/t24/5wHj4f/Z/44MZR90HZj59/XYvNshf32T8muGJylmKMP85JihDPOTY4ZyzFDG7PlxVSgfhMSqUM29dzuw4VUAwE8RgzHJcS90qEiODce/bz4TvZNjgtxBIiIiIjIDrgoVYnRdR1VVlf9WAJjwGJDYdDfufg0/4OFuXwIAKmqcmPbKGpRUHvbP+5iI3zM8CTFDGeYnxwxlmJ8cM5RjhjJmz4+FhQVomoby8nL/rQAQEQdc8pzx5Q11r+LCruUAgJLKOkx8aiVeXbUDmmbOg7Yj/J7hSYgZyjA/OWYow/zkmKEcM5Qxe34sLE5WWWOAkTcDABRXPZ5pnIsLE4oBALVON37z3hZc8eIqfL29Au4QKjCIiIiIqHOwsDiZjZ8H9DoLAKA6a7DQ/Sh+03+P8fSGXVWY+vIanLHgU8x7bws+3lKOov01cLnNWSUTERERUfDYg90BOjFFURATE+P/FQDCooCp/wbenAZs/wSKqw437HwA40bdjpn5I/HTARcAYP+hBixetQOLj9ylO9ymIiUhAglRYYiPDENMhB1hNgV2VYXdpiDM83+bf+vW/mlxuGp4Zody6LQMTyLMUIb5yTFDGeYnxwzlmKGM2fPjqlA+CLlVoY7lcgL/mQFsfc9o0uPS8EPfW/AXxwh8kl8Fp8scZynuPi8Xs8/NCXY3iIiIiE4KXBUqxGiahoqKis6bqGMPB65YBJxxK6A0HRLKoTKc+t1cPF96JbYM+AfeO7MI80cquHRgAnJTYtElJhw2NfDV8pMf52PZ5rJ2f1+nZ3gSYIYyzE+OGcowPzlmKMcMZcyeHy+FsgBd11FRUYGkpKTOexObHZi4ABh2PfDJfGDbB03tzkMIy38fg/E+BgO4DgBiU4CUHtBjkuGK7IrGsDi4FTs0xQ437HArNrgVO9yKHbpqh66GQVdsgGJrfXytns7zbvtxjwN/3BKD7XoG7njje2QkReOUHgk+Dy8gGYY4ZijD/OSYoQzzk2OGcsxQxuz5sbAgb93ygGteB3Z9A6x7BShYDtRVem9Tsxeo2QsFQNiR/zpbLwDnRdhwXsPvUdjYAze9tg7/u200kmMjAvDuRERERHQiLCyodT3PaPpPcwO7NwBFnwMHtgMHdwAHi4GafQACOz3HBjemdf0J8yp6oMxRj9dW7cCd5+UFtA9ERERE1DoWFhagKAoSEhKCswKAagMyT2/6rznNDdQdBGr3Aw2HAHcjoDUCbteR/7fytX686wGPU6TUVgCfLwAAXJm2H/Mqmpo37Xb4PIygZhgimKEM85NjhjLMT44ZyjFDGbPnx1WhfBDyq0KZndsFLOgBuOqhJ2VhcOXjONTgQo/EKHx9/7nB7h0RERFRyOKqUCFG0zSUlZWZdgWATmezA6mDAADKwWIM6dZUC++uqkNNg8unlzjpM/QDZijD/OSYoQzzk2OGcsxQxuz5sbCwAF3X4XA4cFKfXEofajwcE3f07uAFew/59O3MUI4ZyjA/OWYow/zkmKEcM5Qxe34sLMgamhUWQ2w7jMcFe2uC0BkiIiIiOhYLC7KGZoVFljPfeLzNxzMWRERERNS5WFhYgKIoSE5ONu0KAAGRnAuERQMAkqo2G835PhYWzFCOGcowPzlmKMP85JihHDOUMXt+LCwsQFVVJCcnQ1VP4t2l2oDUwQAAW3UJekbWA/D9UihmKMcMZZifHDOUYX5yzFCOGcqYPT9z9oq8aJqGkpIS064AEDDNLoc6L6kMAFBeXQ9HXeMJv5UZyjFDGeYnxwxlmJ8cM5RjhjJmz4+FhQXouo7a2lrTrgAQMM0KixERO43HvqwMxQzlmKEM85NjhjLMT44ZyjFDGbPnx8KCrKNZYZGnbTce53NlKCIiIqKgY2FB1tG1LxAeCwBIrf3JaPZ1AjcRERERdR4WFhagqipSU1NNO1EnYFQVSDsVABBRuwdd4QAAFOw7cWHBDOWYoQzzk2OGMsxPjhnKMUMZs+dnzl6RF0VRkJiYaNqlxQKq2eVQZ0aXAAC2lZ/4UihmKMcMZZifHDOUYX5yzFCOGcqYPT8WFhagaRqKiopMuwJAQDUrLM6OaSosKmoacLDWedxvY4ZyzFCG+ckxQxnmJ8cM5ZihjNnzY2FhAbquw+l0mnYFgIA6cikUAAxUdxmPTzTPghnKMUMZ5ifHDGWYnxwzlGOGMmbPj4UFWUuXPkBYDACgp7PZylD7uDIUERERUTCxsCBrUW1AykAAQFzdbsSjFgCQX86VoYiIiIiCiYWFBaiqioyMDNOuABBwqYOMh/2UpsuhTnQpFDOUY4YyzE+OGcowPzlmKMcMZcyenzl7RV4URUFsbKxpVwAIuLTBxsPTo0oBAEUVtcf9FmYoxwxlmJ8cM5RhfnLMUI4Zypg9PxYWFuB2u5Gfnw+32x3srphDszMWwyOaCov9hxpwqL6xzW9hhnLMUIb5yTFDGeYnxwzlmKGM2fNjYWERZl1WLCi6DwCUpkM3T99hNBftP/5ZC2YoxwxlmJ8cM5RhfnLMUI4Zypg5PxYWZD1hUUByLgAgpaEYYXABAIoquDIUERERUbCwsCBrSm2aZ2HTXeir7AYAFO47/hkLIiIiIuo8LCwsQFVVZGVlmXYFgKBoNs9ioLoDwPHPWDBDOWYow/zkmKEM85NjhnLMUMbs+ZmzV9SC3W4PdhfMxauw2AngxHMsmKEcM5RhfnLMUIb5yTFDOWYoY+b8WFhYgKZpKCgoMPVknYBLPbrk7NDwppWhiitqoWmt3+KeGcoxQxnmJ8cMZZifHDOUY4YyZs+PhQVZU0xXIL4HACBXLwago8GlYXdVXXD7RURERHSSYmFB1nXkcqhorRYZyn4AJ75RHhERERF1DhYWZF3N51konnkWXHKWiIiIKBhYWFiAqqrIyckx7QoAQdOssBhwggnczFCOGcowPzlmKMP85JihHDOUMXt+5uzVEW63Gw8//DCysrIQFRWF7Oxs/Pa3v4WuH52gq+s65s6di7S0NERFRWH8+PEoKCjwep3KykpMnToV8fHxSExMxIwZM1BTY62/bLtcrmB3wXyaTeAeoOwAcPwlZ5mhHDOUYX5yzFCG+ckxQzlmKGPm/ExdWDz++ON44YUX8Oyzz2Lr1q14/PHH8cQTT2DhwoXGNk888QSeeeYZvPjii1izZg1iYmIwYcIE1NfXG9tMnToVW7ZswfLly/H+++9j5cqVmDlzZjCG1CGapqG4uNi0KwAETVJvICwGAJBn2wOg7TMWzFCOGcowPzlmKMP85JihHDOUMXt+5l0IF8CqVatwySWX4IILLgAA9O7dG//85z/x7bffAmg6W/HUU0/hoYcewiWXXAIAeO2115CSkoJ33nkHU6ZMwdatW7Fs2TKsXbsWw4cPBwAsXLgQ559/Pp588kmkp6cHZ3AkpyhA12yg/Af0wD6EwYUyRz0OO12IDjf1oU1EREQUckx9xuLMM8/Ep59+ivz8fADA999/j6+++gqTJk0CABQXF6O8vBzjx483vichIQEjR47E6tWrAQCrV69GYmKiUVQAwPjx46GqKtasWRPA0VCnSM4BANigoaeyF8CJb5RHRERERP5n6j/r3n///aiurka/fv1gs9ngdrvx2GOPYerUqQCA8vJyAEBKSorX96WkpBjPlZeXo3v37l7P2+12dOnSxdjmWA0NDWhoaDC+rq6uBtA058PtdgMAFEWBqqrQNM1rzkdb7aqqQlGUNts9r9u8HWg65eV5zu12e7U3Z7PZoOu6V7unL221+9r3zhiTL+2+jElJ6mNUx1lKOQr1Hti+7xD6p8a26LsnQ7OP6dh2s+wnXddb3d7KYwrkfvL0SdM02Gy2kBjTidr9PabmPwtDZUyB3E+apnn9PAyFMQV6PzX/vlAZk0eg9tOxv9OEwpgCuZ887938dTp7TM0fn4ipC4s333wTS5Ysweuvv46BAwdi48aNmDNnDtLT03Hdddd12vsuWLAA8+fPb9FeWFiI2NimX1gTEhKQlpaGvXv3wuFwGNskJycjOTkZu3fvRm3t0b+cp6amIjExETt27IDT6TTaMzIyEBsbi8LCQq+DISsrC3a73WsielFREXJycuByuVBcXGy0q6qK3Nxc1NbWorS01GgPDw9Hnz594HA4vIqomJgYZGZmorKyEhUVFUZ7MMYEQDSm+MZYeC5my1LKAADrtu1Cv6garzF5xllUVGT6MXmYcT/17t3byDBUxhTo/eRwOEJuTIHeT0VFRSE3JiAw+yk3NxclJSUhNaZg7CebzYaampqQGlOg91NRUVHIjQkIzH7q0aOH17/FnT2m6Oho+ErR21OGBFhmZibuv/9+zJo1y2h79NFH8Y9//AM//fQTioqKkJ2dje+++w5DhgwxtvnZz36GIUOG4Omnn8Yrr7yCu+66CwcPHjSed7lciIyMxFtvvYXLLrusxfu2dsbCs2Pi4+MBBLaC1XUdhw8fRnR0NGw2m9HeXChW5T6Nac8G2P7WdCnc665z8H+um3DR4DQ8dfWpXtu73W7U1tYiOjoaiqKYe0zHtJtlPymKgtraWkRFRUFRlJAYUyD3k+dzHBMTwzMWgjMWnp+FiqKExJgCuZ8AoK6uDlFRUS36YtUxBXo/eT7HcXFxLba36pg8ArWfNE3z+p0mFMYUyP2kqipqamq8/i3u7DHV1NQgMTERDofD+D24LaY+Y3H48GEjWA/PP8hAU5WXmpqKTz/91CgsqqursWbNGtxyyy0AgFGjRqGqqgrr16/HsGHDAACfffYZNE3DyJEjW33fiIgIREREtGi32WzGL/Yex/avo+3Hvm7zdrfbjT179iAnJ8c4iFrb3vMPra/t/up7R8bka/sJx9Qt12jLVpvOWBQfqG31ezwZNn/OlGPysS+B3k9utxu7d+9ukSFg3TEdr93fY2r+OfZle0nf22q3+n5SFKXF59jqYwrkfnK73SgtLW31M3y81zHzmDra3tExNf8ct/Y7AWC9MTUXiP2k63qL32msPqb2tEvH1JF/i6V9b/7HxBMxdWFx0UUX4bHHHkPPnj0xcOBAfPfdd/jTn/6EX/7ylwCaBjpnzhw8+uijyMnJQVZWFh5++GGkp6fj0ksvBQD0798fEydOxE033YQXX3wRjY2NmD17NqZMmcIVoUJBZAIQ0x2o3Ye+tqOTt3Vdb9cHgYiIiIhkTF1YLFy4EA8//DBuvfVW7Nu3D+np6fjVr36FuXPnGtvce++9qK2txcyZM1FVVYXRo0dj2bJliIyMNLZZsmQJZs+ejXHjxkFVVUyePBnPPPNMMIZEnaFrX6B2H7rqBxGLw6hxRqO8uh5pCVEn/l4iIiIi8gtTFxZxcXF46qmn8NRTT7W5jaIoeOSRR/DII4+0uU2XLl3w+uuvd0IPA0NRFISHh/Mv8G1J7gvsWgWgaWWoTXofFO2v9SosmKEcM5RhfnLMUIb5yTFDOWYoY/b8TH0fC2qiqir69OnT5rVwJ72ufY2HnpWhivbXeG3CDOWYoQzzk2OGMsxPjhnKMUMZs+dnzl6RF13XUVVV1a51hE8qzQqLPkcmcBcec5M8ZijHDGWYnxwzlGF+csxQjhnKmD0/FhYWoGkaysvLW71JCsG7sPCcsajwLiyYoRwzlGF+csxQhvnJMUM5Zihj9vxYWJD1JWUBStOh3FdtujHMsZdCEREREVHnYmFB1mcPBxJ7AQCy1HIAOnZX1aG+0X387yMiIiIiv2FhYQGKoiAmJsa0KwCYwpHLoaL0OnRDFXQd2HHg6OVQzFCOGcowPzlmKMP85JihHDOUMXt+LCwsQFVVZGZmmnYFAFNIzjEeeu7AXdRsAjczlGOGMsxPjhnKMD85ZijHDGXMnp85e0VeNE1DRUWFaSfqmELXbONha0vOMkM5ZijD/OSYoQzzk2OGcsxQxuz5sbCwAF3XUVFRYdqlxUzB614WngncR89YMEM5ZijD/OSYoQzzk2OGcsxQxuz5sbCg0NCssMhW9gAACo9ZcpaIiIiIOg8LCwoNcelAWDQAIMe+F0DTpVBmreiJiIiIQg0LCwtQFAUJCQmmXQHAFFQV6NI0zyJd34swuHCo3oWKGicAZugPzFCG+ckxQxnmJ8cM5ZihjNnzY2FhAaqqIi0tzbQrAJhGtzwAgB3uFhO4maEcM5RhfnLMUIb5yTFDOWYoY/b8zNkr8qJpGsrKyky7AoBpdO9vPMxTSgAARUfmWTBDOWYow/zkmKEM85NjhnLMUMbs+bGwsABd1+FwODhf4ES6DzAe5qqlAIDCfU1nLJihHDOUYX5yzFCG+ckxQzlmKGP2/FhYUOg4zhkLIiIiIupcLCwodCT2MlaGyjtyxqL5TfKIiIiIqPOwsLAARVGQnJxs2hUATENVgW79AACZyj5EoR4lB+vgdGnM0A+YoQzzk2OGMsxPjhnKMUMZs+fXocKipKQEpaWlxtfffvst5syZg5deeslvHaOjVFVFcnKyaVcAMJWUpnkWKnT0VfbArenYVVnLDP2AGcowPzlmKMP85JihHDOUMXt+HerVtddeixUrVgAAysvL8fOf/xzffvstHnzwQTzyyCN+7SA1rQBQUlJi2hUATKXZBO48tWmeRcHeGmboB8xQhvnJMUMZ5ifHDOWYoYzZ8+tQYbF582aMGDECAPDmm2/ilFNOwapVq7BkyRIsXrzYn/0jNK0AUFtba9oVAEyl2QTuXKXprFr+3hpm6AfMUIb5yTFDGeYnxwzlmKGM2fPrUGHR2NiIiIgIAMAnn3yCiy++GADQr18/lJWV+a93RO3V/IzFkZWh8vceClZviIiIiE4aHSosBg4ciBdffBFffvklli9fjokTJwIA9uzZg65du/q1g0TtEpsCRCUBOLoyFAsLIiIios7XocLi8ccfx1/+8heMHTsW11xzDU499VQAwHvvvWdcIkX+o6oqUlNTTTtRx1QUxThrkapUIh41KK6ohUsDMxTicSjD/OSYoQzzk2OGcsxQxuz52TvyTWPHjkVFRQWqq6uRlJRktM+cORPR0dF+6xw1URQFiYmJwe6GdXTvD+z8GkDTPIt1Wj/sOHAYeamJwe2XxfE4lGF+csxQhvnJMUM5Zihj9vw6VO7U1dWhoaHBKCp27tyJp556Ctu2bUP37t392kFqWgGgqKjItCsAmE7zO3AfuRzqpzIHMxTicSjD/OSYoQzzk2OGcsxQxuz5daiwuOSSS/Daa68BAKqqqjBy5Ej88Y9/xKWXXooXXnjBrx2kphUAnE6naVcAMJ1WJ3DXMEMhHocyzE+OGcowPzlmKMcMZcyeX4cKiw0bNuDss88GAPz73/9GSkoKdu7ciddeew3PPPOMXztI1G5H7r4NHL2XBSdwExEREXWuDhUWhw8fRlxcHADg448/xuWXXw5VVXHGGWdg586dfu0gUbtFdwHi0gAAeUopAB0F+2qC2yciIiKiENehwqJv37545513UFJSgo8++gjnnXceAGDfvn2Ij4/3awepaQWAjIwM064AYEpH5lkkKjXohirsrDyM5JQ0ZijA41CG+ckxQxnmJ8cM5ZihjNnz61Cv5s6di7vvvhu9e/fGiBEjMGrUKABNZy+GDh3q1w5S0woAsbGxUBQl2F2xjmbzLPqpJdB1oPwwmKEAj0MZ5ifHDGWYnxwzlGOGMmbPr0OFxRVXXIFdu3Zh3bp1+Oijj4z2cePG4c9//rPfOkdN3G438vPz4Xa7g90V60gZaDzsrzRdnrfy+wJmKMDjUIb5yTFDGeYnxwzlmKGM2fPr0H0sgKabjaWmpqK0tGk5z4yMDN4crxOZdVkx00o5xXjYX90FuIEdlQ1B7FBo4HEow/zkmKEM85NjhnLMUMbM+XXojIWmaXjkkUeQkJCAXr16oVevXkhMTMRvf/tbUw+WTiLd8gC1qW7up+wCAOyocgazR0REREQhrUNnLB588EH87W9/w+9//3ucddZZAICvvvoK8+bNQ319PR577DG/dpKo3ewRQHIesG8L+ip7EI5G7GRhQURERNRpFL0Dd9hIT0/Hiy++iIsvvtir/d1338Wtt96K3bt3+62DZlBdXY2EhAQ4HI6grHrluRlKeHi4aSfrmNJ/bgI2vQkAOL/hd/hR741N885DXGRYkDtmTTwOZZifHDOUYX5yzFCOGcoEI7/2/B7coUuhKisr0a9fvxbt/fr1Q2VlZUdekk7Abu/wdJiTV+rReRaey6EK9vJ+FhI8DmWYnxwzlGF+csxQjhnKmDm/DhUWp556Kp599tkW7c8++ywGDx4s7hR50zQNBQUFnL/SXsdO4Aawrbw6WL2xPB6HMsxPjhnKMD85ZijHDGXMnl+HSp4nnngCF1xwAT755BPjHharV69GSUkJli5d6tcOEnVY88LiyJKzP5axsCAiIiLqDB06Y/Gzn/0M+fn5uOyyy1BVVYWqqipcfvnl2LJlC/7+97/7u49EHROXAsR0A+A5Y6Fjyx4WFkRERESdocMXaaWnp7dY/en777/H3/72N7z00kvijhH5RcopQNEKdFUOoRuqsLXMDremw6ZywhgRERGRP3XojAUFlqqqyMnJgapyd7VbswncA9RdqGt0o2g/J3B3BI9DGeYnxwxlmJ8cM5RjhjJmz8+cvaIWXC5XsLtgTa3Ms9i02xGs3lgej0MZ5ifHDGWYnxwzlGOGMmbOj4WFBWiahuLiYtOuAGBqzQqLfkdWhtq8m/MsOoLHoQzzk2OGMsxPjhnKMUMZs+fXrjkWl19++XGfr6qqkvSFyP+ScwE1DNAa0V/xFBY8Y0FERETkb+0qLBISEk74/PTp00UdIvIrezjQrR+wdxOy1T2IgBNb9jigaTpUTuAmIiIi8pt2FRaLFi3qrH7QCZh1ko4lpAwE9m6CHRr6KruxxZmF4gO1yO4WG+yeWQ6PQxnmJ8cMZZifHDOUY4YyZs7PvD0jg81mQ25uLmw2W7C7Yk1eK0M1TeDm5VDtx+NQhvnJMUMZ5ifHDOWYoYzZ82NhYQG6rqOmpga6rge7K9aUOsh4OFDZAYCFRUfwOJRhfnLMUIb5yTFDOWYoY/b8WFhYgKZpKC0tNe0KAKaXdqrxcJBaDIArQ3UEj0MZ5ifHDGWYnxwzlGOGMmbPj4UFhb6oJOhJWQCAgepO2ODG5j0O01b7RERERFbEwoJOCnraEABAJJzIUXbjUL0LOw8cDm6niIiIiEIICwsLUBQF4eHhUBQuj9ph6UONh4PUIgDA5j2cZ9EePA5lmJ8cM5RhfnLMUI4Zypg9PxYWFqCqKvr06WPq5cXMTu3RrLBQmuZZbOIE7nbhcSjD/OSYoQzzk2OGcsxQxuz5mbNX5EXXdVRVVXFOgICeOth4fHQCNwuL9uBxKMP85JihDPOTY4ZyzFDG7PmxsLAATdNQXl5u2hUArEALj4MzLhNA070s7HDhh9KmO3CTb3gcyjA/OWYow/zkmKEcM5Qxe36mLyx2796NX/ziF+jatSuioqIwaNAgrFu3znhe13XMnTsXaWlpiIqKwvjx41FQUOD1GpWVlZg6dSri4+ORmJiIGTNmoKamJtBDoSCrS+oPAIhAI3KVUhyqd2HHgdog94qIiIgoNJi6sDh48CDOOusshIWF4cMPP8SPP/6IP/7xj0hKSjK2eeKJJ/DMM8/gxRdfxJo1axATE4MJEyagvr7e2Gbq1KnYsmULli9fjvfffx8rV67EzJkzgzEkCqKGLv2Mx6ccuRzqh1JeDkVERETkD/Zgd+B4Hn/8cWRmZmLRokVGW1ZWlvFY13U89dRTeOihh3DJJZcAAF577TWkpKTgnXfewZQpU7B161YsW7YMa9euxfDhwwEACxcuxPnnn48nn3wS6enpgR1UByiKgpiYGNOuAGAFiqI0rQy1senrwUoR3sQ5+L60CpcO7RHUvlkFj0MZ5ifHDGWYnxwzlGOGMmbPz9SFxXvvvYcJEybgyiuvxBdffIEePXrg1ltvxU033QQAKC4uRnl5OcaPH298T0JCAkaOHInVq1djypQpWL16NRITE42iAgDGjx8PVVWxZs0aXHbZZS3et6GhAQ0NDcbX1dVNd2l2u91wu90AmnasqqrQNM1rAk1b7aqqQlGUNts9r9u8HYBxDV16ejp0XTe+99hr62w2G3Rd92r39KWtdl/73lljOlG7P8cEAF1POQf6UgUKdGMC9w8lVV79tNKYgrGfMjIyoGma1/dYfUyttXfWmJr/ISNUxnS8dn+PSdd142eh2+0OiTEFej9lZma2+AxbfUyB3k/p6enH7bsVxwQEdj81/50mVMZ0bN87c0zH/lvc2WNqz0RxUxcWRUVFeOGFF3DnnXfi//7v/7B27VrcfvvtCA8Px3XXXYfy8nIAQEpKitf3paSkGM+Vl5eje/fuXs/b7XZ06dLF2OZYCxYswPz581u0FxYWIjY2FkBTAZOWloa9e/fC4Th6OU1ycjKSk5Oxe/du1NYevX4/NTUViYmJ2LFjB5xOp9GekZGB2NhYFBYWeh0MWVlZsNvtKCgogK7rqKurQ1RUFHJzc+FyuVBcXGxsq6oqcnNzUVtbi9LSUqM9PDwcffr0gcPh8BprTEwMMjMzUVlZiYqKCqM9kGNqLicnp9PHVF5ejvLycgyM64mIQzvRX92FMLiweY8DP23Lh01VLDemQO+n7Oxs7N+/Hw6Hw/hLidXHFMj95PkcZ2RkoHv37iExpkDvp6KiIuNnoc1mC4kxBXI/denSBaqqora2FnV1dSExpkDvJ13X0dDQgMGDB+Pw4cMhMSYgsPuppqbG+BynpaWFxJgCuZ/69u1r5Oj5t7izxxQdHQ1fKbpZ16tCU1DDhw/HqlWrjLbbb78da9euxerVq7Fq1SqcddZZ2LNnD9LS0oxtrrrqKiiKgjfeeAO/+93v8Oqrr2Lbtm1er929e3fMnz8ft9xyS4v3be2MhWfHxMfHAwhsBet2u7F9+3b07dsXYWFhRntzoVqV+2tMjY2NKCgoQN6WP8K25d8AgAsaHsMWPQsf3HYW+qXGWW5Mgd5Puq6joKAA2dnZsNlsITGmQO4nz+c4JycHYWFhITGmE7X7e0yNjY3Gz0KbzRYSYwrkftI0DYWFhcjOzjbe3+pjCvR+8nyO8/LyjPe1+pg8ArWfXC6X1+80oTCmQO4nAMjPz/f6t7izx1RTU4PExEQ4HA7j9+C2mPqMRVpaGgYMGODV1r9/f/znP/8B0FQVAsDevXu9Cou9e/diyJAhxjb79u3zeg2Xy4XKykrj+48VERGBiIiIFu2ef8iaa/7DWdJ+7Ose266qKmw2m1Gdtra9oijtavdX3zs6Jl/a/TkmVVWhpA8BjhQWg9UibHFnYfOeagzskejT65htTP5o97Xvbrfb6OOxz1l1TMdr74wxeY5DX7c/UR/b2x4K+8nzs7D5P6hWH9OxAjGm9ryOVcbUnnbJmDyvGUpj8gjUsXfs7zRWH1N72qVj6si/xdK+e/aTL0y9KtRZZ53V4kxDfn4+evXqBaDp9FFqaio+/fRT4/nq6mqsWbMGo0aNAgCMGjUKVVVVWL9+vbHNZ599Bk3TMHLkyACMgsxET29+B+4iAMD3XBmKiIiISMzUZyzuuOMOnHnmmfjd736Hq666Ct9++y1eeuklvPTSSwCaKqg5c+bg0UcfRU5ODrKysvDwww8jPT0dl156KYCmMxwTJ07ETTfdhBdffBGNjY2YPXs2pkyZYokVoYCmcSYkJLSrYiRvRoZJ6YBiA3Q3hqlN1zP+UFoV3M5ZBI9DGeYnxwxlmJ8cM5RjhjJmz8/UcywA4P3338cDDzyAgoICZGVl4c477zRWhQKarvv+zW9+g5deeglVVVUYPXo0nn/+eeTm5hrbVFZWYvbs2fjf//4HVVUxefJkPPPMM8ZE7BOprq5GQkKCT9eWkQW8NBbY8x0AYEj9X1CjxmPz/AmIDGv9NCQRERHRyao9vwebvrAwg2AXFpqmYe/evUhJSWnzejg6Pq8MP34I+OY5AMCNzrvwiTYMb996Job2TDrBq5zceBzKMD85ZijD/OSYoRwzlAlGfu35PZh71AJ0XTeWq6SO8cqw1yij/XT1JwC8A7cveBzKMD85ZijD/OSYoRwzlDF7fiws6OTT82hhMUJtWhyAhQURERGRDAsLOvnEJAPJTXNwTlGKEYV6fM8J3EREREQiLCwsQFEUJCcnm3YFACtokWHPMwAAYYobQ9RCbN9XA0ddYxB7aH48DmWYnxwzlGF+csxQjhnKmD0/FhYWoKoqkpOTOclJoEWGPc80njtdaboc6vuSqiD0zDp4HMowPzlmKMP85JihHDOUMXt+5uwVedE0DSUlJa3e1p180yLDViZwb9h1MBhdswwehzLMT44ZyjA/OWYoxwxlzJ4fCwsL0HUdtbW1pl0BwApaZJjYC4hrukHiaWoBbHDju11VweugBfA4lGF+csxQhvnJMUM5Zihj9vxYWNDJSVGMsxYxSgMGKjvw3a6D0DRzflCJiIiIzI6FBZ28enpfDlVd70JRRU0QO0RERERkXSwsLEBVVaSmppp2oo4VtJphr6MTuD33s9iwsyrAPbMOHocyzE+OGcowPzlmKMcMZcyenzl7RV4URUFiYqJplxazglYz7NYfiEwEAJym5gPQ8V0JJ3C3hcehDPOTY4YyzE+OGcoxQxmz58fCwgI0TUNRUZFpVwCwglYzVFWgx2kAgG5KNdJQyTMWx8HjUIb5yTFDGeYnxwzlmKGM2fNjYWEBuq7D6XSadgUAK2gzw7QhxsNBahHy9x1CdT1vlNcaHocyzE+OGcowPzlmKMcMZcyeHwsLOrmlDzEeDlKLoevADyWO4PWHiIiIyKJYWNDJLX2o8XCQUgyAN8ojIiIi6ggWFhagqioyMjJMuwKAFbSZYUImENUFAHCKWgxAZ2HRBh6HMsxPjhnKMD85ZijHDGXMnp85e0VeFEVBbGysaVcAsII2M1QU46xFslKNdBzAd7uqTHvtYjDxOJRhfnLMUIb5yTFDOWYoY/b8WFhYgNvtRn5+Ptxud7C7YlnHzfCYeRaOukYUVdQGrnMWweNQhvnJMUMZ5ifHDOWYoYzZ82NhYRFmXVbMStrM8JiVoQBgw05eDtUaHocyzE+OGcowPzlmKMcMZcycHwsLolYncFcFqTNERERE1sTCgighA4juCsBzxkLHd5zATURERNQuLCwsQFVVZGVlmXYFACs4bobNJnB3UWrQAxXI33sINQ2uAPfS3HgcyjA/OWYow/zkmKEcM5Qxe37m7BW1YLfbg90Fyztuhs3mWZyiFkPTge9Lqjq9T1bD41CG+ckxQxnmJ8cM5ZihjJnzY2FhAZqmoaCgwNSTdczuhBk2Wxlq8JEJ3LwcyhuPQxnmJ8cMZZifHDOUY4YyZs+PhQURwAncREREREIsLIgAIL4HEJ0MABhsa7oD93e7DvJGeUREREQ+YmFBBHhN4E5EDTKVfTh4uBE7DhwOcseIiIiIrIGFhQWoqoqcnBzTrgBgBT5lmDHceDhMKQDAG+U1x+NQhvnJMUMZ5ifHDOWYoYzZ8zNnr6gFl4tLn0qdMMPMkcbD4eo2AMAGTuD2wuNQhvnJMUMZ5ifHDOWYoYyZ82NhYQGapqG4uNi0KwBYgU8ZZgwHlKaPxDD1yBkLTuA28DiUYX5yzFCG+ckxQzlmKGP2/FhYEHlExAEpAwEAeWoJ4nAY28qrUcsb5RERERGdEAsLouYyzwAAqNAxVC1oulFeaVVw+0RERERkASwsLMKsk3SsxKcMe55hPPRcDvUdL4cy8DiUYX5yzFCG+ckxQzlmKGPm/BSdC/WfUHV1NRISEuBwOBAfHx/s7lBnqtoFPDUIAPCVeyB+0fggxvfvjpevOz3IHSMiIiIKvPb8HmzekocMuq6jpqaGN2sT8DnDhEwgLh0AMNRWCBvc2LCritmDx6EU85NjhjLMT44ZyjFDGbPnx8LCAjRNQ2lpqWlXALACnzNUFCBzBAAgBvXop5SgstaJnbxRHo9DIeYnxwxlmJ8cM5RjhjJmz4+FBdGxvOZZNN3P4rsS3s+CiIiI6HhYWBAdq9mN8oz7WeysClJniIiIiKyBhYUFKIqC8PBwKIoS7K5YVrsyTB0EhEUDAIar+QB4B26Ax6EU85NjhjLMT44ZyjFDGbPnx1WhfMBVoU5Ciy8EdnwJADijfiH2q8nYNO88RIfbg9wxIiIiosDhqlAhRtd1VFVxZSKJdmd4zOVQbk3HD6WOTuqdNfA4lGF+csxQhvnJMUM5Zihj9vxYWFiApmkoLy837QoAVtDuDJsVFsOPTOA+2S+H4nEow/zkmKEM85NjhnLMUMbs+bGwIGpN5tEb4p3GCdxEREREJ8TCgqg1UUlAt34AgIHqDkShHhtLDpr21CMRERFRsLGwsABFURATE2PaFQCsoEMZHrkcyg4NQ9RCVNQ4UVJZ10k9ND8ehzLMT44ZyjA/OWYoxwxlzJ4fCwsLUFUVmZmZUFXuro7qUIbNbpR3mnLkcqiTeJ4Fj0MZ5ifHDGWYnxwzlGOGMmbPz5y9Ii+apqGiosK0E3WsoEMZtjKBe/3Ok7ew4HEow/zkmKEM85NjhnLMUMbs+bGwsABd11FRUcHr+wU6lGGXPkB0MoCmCdwKNKzdUdlJPTQ/HocyzE+OGcowPzlmKMcMZcyeHwsLorYoinE5VIJyGH2VPdi29xAcdY1B7hgRERGR+bCwIDqezBHGw+HqNuj6yT3PgoiIiKgtLCwsQFEUJCQkmHYFACvocIaZRydwDztyP4u1xSfn5VA8DmWYnxwzlGF+csxQjhnKmD0/e7A7QCemqirS0tKC3Q1L63CGaacCtnDA7cQwpWkC97odJ+cZCx6HMsxPjhnKMD85ZijHDGXMnh/PWFiApmkoKysz7QoAVtDhDMMigfShAIAsdS+6woGNpVVocLk7oZfmxuNQhvnJMUMZ5ifHDOWYoYzZ82NhYQG6rsPhcJh2BQArEGXYbNnZYWo+nC4Nm0odfuydNfA4lGF+csxQhvnJMUM5Zihj9vxYWBCdiNf9LPIBAN+exMvOEhEREbXGUoXF73//eyiKgjlz5hht9fX1mDVrFrp27YrY2FhMnjwZe/fu9fq+Xbt24YILLkB0dDS6d++Oe+65By6XK8C9J8vqOcp4OFLdCuDknWdBRERE1BbLFBZr167FX/7yFwwePNir/Y477sD//vc/vPXWW/jiiy+wZ88eXH755cbzbrcbF1xwAZxOJ1atWoVXX30Vixcvxty5cwM9hA5TFAXJycmmXQHACkQZxnQFug8AAJyi7kAsDmPdjkpomjlPQ3YWHocyzE+OGcowPzlmKMcMZcyenyUKi5qaGkydOhV//etfkZSUZLQ7HA787W9/w5/+9Cece+65GDZsGBYtWoRVq1bhm2++AQB8/PHH+PHHH/GPf/wDQ4YMwaRJk/Db3/4Wzz33HJxOZ7CG1C6qqiI5ORmqaondZUriDHuPBgDYoGG4ug3V9S7k7zvkxx6aH49DGeYnxwxlmJ8cM5RjhjJmz88Sy83OmjULF1xwAcaPH49HH33UaF+/fj0aGxsxfvx4o61fv37o2bMnVq9ejTPOOAOrV6/GoEGDkJKSYmwzYcIE3HLLLdiyZQuGDh3a4v0aGhrQ0NBgfF1dXQ2g6eyH2920GpCiKFBVFZqmeU2gaatdVVUoitJmu+d1m7cDTbP/NU3Dnj17kJ6eDrvdbrQ3Z7PZoOu6V7unL221+9r3zhiTL+3+HJPL5cLu3buRnp5u9K9dY+p1FpRvXwIAnKFuxefaUKwpPIC8lLigjSnQ+wkAdu/ejbS0NK8faFYeUyD3k+dz3KNHD9jt9pAY04na/T0ml8tl/CxUVTUkxhTI/aTrOsrKypCWlub1104rjynQ+8nzOc7MzDRe3+pj8gjUfnK73V6/04TCmAK5nxRFQWlpqde/xZ09pvZMFDd9YfGvf/0LGzZswNq1a1s8V15ejvDwcCQmJnq1p6SkoLy83NimeVHhed7zXGsWLFiA+fPnt2gvLCxEbGwsACAhIQFpaWnYu3cvHI6jKwQlJycjOTkZu3fvRm1trdGempqKxMRE7Nixw+tMSUZGBmJjY1FYWOh1MGRlZcFut6OgoACapqGyshK1tbXIy8uDy+VCcXGxsa2qqsjNzUVtbS1KS0uN9vDwcPTp0wcOh8NrrDExMcjMzERlZSUqKiqM9kCOqbmcnJyAjKmkpAS1tbVGtd+uMXUbitgjj89QfwQAfLZ5F6YMTw/qmAK5n/r06YPq6mrU1NQYP8ysPqZA7ifP5zgyMhIpKSkhMaZA76fCwkLjZ6Hdbg+JMQVyPyUlJaG2tha7d+9GXV1dSIwp0PtJ0zQcPHgQGRkZOHz4cEiMCQjsfjp06JDxOU5PTw+JMQVyP2VnZ6Oqqsrr3+LOHlN0dDR8pehmXa8KQElJCYYPH47ly5cbcyvGjh2LIUOG4KmnnsLrr7+OG264wevsAgCMGDEC55xzDh5//HHMnDkTO3fuxEcffWQ8f/jwYcTExGDp0qWYNGlSi/dt7YyFZ8fEx8cDCGwF63a7sX37dvTt2xdhYWFGe3OhWJX7c0yNjY0oKChA3759YbPZOjam50cB+7fCpasY0vASYuOTsOr+c1v81SpQYwr0ftJ1HQUFBcjOzobNZguJMQVyP3k+xzk5OQgLCwuJMZ2o3d9jamxsNH4W2my2kBhTIPeTpmkoLCxEdna28f5WH1Og95Pnc5yXl2e8r9XH5BGo/eRyubx+pwmFMQVyPwFAfn6+17/FnT2mmpoaJCYmwuFwGL8Ht8XUZyzWr1+Pffv24bTTTjPa3G43Vq5ciWeffRYfffQRnE4nqqqqvM5a7N27F6mpqQCaKsdvv/3W63U9q0Z5tjlWREQEIiIiWrR7/iFrrvkPZ0n7sa97bLuqqsYvxG1tryhKu9r91feOjsmXdn+OyZNh8+9r15h6jwb2b4Vd0TBczcfn1UOwq7IOvZNjgjYmf7T72ne322308djnrDqm47V3xpg8x6Gv25+oj+1tD4X9dOznOBTGdKxAjKk9r2OVMbWnXTImz2uG0pg8AnXsHfs7jdXH1J526Zg68m+xtO+e/eQLc878OGLcuHHYtGkTNm7caPw3fPhwTJ061XgcFhaGTz/91Piebdu2YdeuXRg1qmmJ0FGjRmHTpk3Yt2+fsc3y5csRHx+PAQMGBHxMHaGqKlJTU9s8AOjE/JLhkQncQNM8CwD4puiAtGuWweNQhvnJMUMZ5ifHDOWYoYzZ8zP1GYu4uDiccsopXm0xMTHo2rWr0T5jxgzceeed6NKlC+Lj43Hbbbdh1KhROOOMMwAA5513HgYMGIBp06bhiSeeQHl5OR566CHMmjWr1bMSZqQoSot5JNQ+fsmw11nGQ888izXFlZgyoqfsdS2Cx6EM85NjhjLMT44ZyjFDGbPnZ85ypx3+/Oc/48ILL8TkyZMxZswYpKam4r///a/xvM1mw/vvvw+bzYZRo0bhF7/4BaZPn45HHnkkiL1uH03TUFRU1Op1duQbv2QY2w3o1g8AcIpSjBjU4ZuiA+1aLcHKeBzKMD85ZijD/OSYoRwzlDF7fqY+Y9Gazz//3OvryMhIPPfcc3juuefa/J5evXph6dKlndyzzqPrOpxO50nzC2xn8FuGvUcD+38y5ll84TgVuyoPo1fXGP901MR4HMowPzlmKMP85JihHDOUMXt+lj9jQRRQXvMsmi6HOpnmWRARERG1hYUFUXv0ajmBe01RZbB6Q0RERGQaLCwsQFVVZGRkmHYFACvwW4ax3YDkPADAIKXopJpnweNQhvnJMUMZ5ifHDOWYoYzZ8zNnr8iLoiiIjY1t1zrC5M2vGR65HMquaBim5mOPox4llXUn+Cbr43Eow/zkmKEM85NjhnLMUMbs+bGwsAC32438/PwWd2sk3/k1w9buZ1Ec+vMseBzKMD85ZijD/OSYoRwzlDF7fiwsLMKsy4pZid8yPIkncPM4lGF+csxQhvnJMUM5Zihj5vxYWBC1V2x3Y57FYKUI0ajHmqLKk2KeBREREVFbWFgQdUTvprtwe+ZZ7K6qw84Dh4PcKSIiIqLgYWFhAaqqIisry7QrAFiB3zNs5XKoL7dX+Oe1TYrHoQzzk2OGMsxPjhnKMUMZs+dnzl5RC3a75W6Sbjp+zbCV+1l8VbDff69vUjwOZZifHDOUYX5yzFCOGcqYOT8WFhagaRoKCgpMPVnH7PyeYVwKkJwLABisNs2zWFV4AC536O4jHocyzE+OGcowPzlmKMcMZcyeHwsLoo7q1TTPIgxuDFPzcajehR92O4LcKSIiIqLgYGFB1FHN5lmMNC6HCu15FkRERERtYWFB1FGt3Cjvy5NgngURERFRaxSdi++fUHV1NRISEuBwOBAfHx/w99d1HZqmQVVV097C3ew6LcOFw4EDBXDBhiH1f0G9GoONvzkPsRHmnVjVUTwOZZifHDOUYX5yzFCOGcoEI7/2/B7MMxYW4XK5gt0Fy+uUDLPPBQDY4cbZ6ia4NB3fFIbuXbh5HMowPzlmKMP85JihHDOUMXN+LCwsQNM0FBcXm3YFACvotAxzJxgPx9m+AwB8FaL3s+BxKMP85JihDPOTY4ZyzFDG7PmxsCCS6D0aCIsBAJyjboQKjfMsiIiI6KTEwoJIwh4BZJ8DAOiqVONUpRCF+2uxp6ouyB0jIiIiCiwWFhZh1lu3W0mnZZg70Xh47pHLoT7durdz3ivIeBzKMD85ZijD/OSYoRwzlDFzflwVygfBXhWKTO7QXuCPTXfh3qr1xCTn73FW365YcuMZQe4YERERkQxXhQoxuq6jpqYGrAE7rlMzjEsBegwDAPRXdyEdFfimqBIHa53+f68g4nEow/zkmKEM85NjhnLMUMbs+bGwsABN01BaWmraFQCsoNMzPOZyKLem49Of9nXOewUJj0MZ5ifHDGWYnxwzlGOGMmbPj4UFkT80W3b2XLVpnsWyzeXB6g0RERFRwLGwIPKH1MFAXBoA4CzbFkShHisL9qO2wbw3sSEiIiLyJxYWFqAoCsLDwwN26/ZQ1OkZKopx1iICjfi5ugFOl4Yv8kPnnhY8DmWYnxwzlGF+csxQjhnKmD0/rgrlA64KRT4p/hJ49UIAwEr3IExvfAAXn5qOZ64ZGuSOEREREXUMV4UKMbquo6qqyrQrAFhBQDLsdRaQ1BsAMNq2GemowGc/7UODy9157xlAPA5lmJ8cM5RhfnLMUI4Zypg9PxYWFqBpGsrLy027AoAVBCRDVQWG/KLpIXRMtq1ETYMLq7Yf6Lz3DCAehzLMT44ZyjA/OWYoxwxlzJ4fCwsifxpyDYCm6x6vtH0BBRo+2FQW3D4RERERBQALCyJ/SsgAss8BAPRU92Ok+hM+2lIeMpdDEREREbWFhYUFKIqCmJgY064AYAUBzXDoL4yHV9o+x6F6F77Mr+j89+1kPA5lmJ8cM5RhfnLMUI4Zypg9P64K5QOuCkXt0lgP/DEPqK9CnR6OEQ3PY/zQHPz56iHB7hkRERFRu3BVqBCjaRoqKipMO1HHCgKaYVgkMPgqAECU4sQk2xos/3Ev6hutfTkUj0MZ5ifHDGWYnxwzlGOGMmbPj4WFBei6joqKCtMuLWYFAc/w1CnGwwnqOtQ0uPD5NmvfLI/HoQzzk2OGMsxPjhnKMUMZs+fHwoKoM6SfBsSlAQBGq5sRhXq8/8OeIHeKiIiIqPOwsCDqDIoC5E0CAEQojRijbsKnW/fhsNMV5I4RERERdQ4WFhagKAoSEhJMuwKAFQQlw7wLjIc/t61HXaMbn/20L3Dv72c8DmWYnxwzlGF+csxQjhnKmD0/rgrlA64KRR3iagCe6AM4a1Cpx2J4w4uYcEo6XvjFsGD3jIiIiMgnXBUqxGiahrKyMtOuAGAFQcnQHgH0HQcA6KLUYJiSj89+2oeaBmteDsXjUIb5yTFDGeYnxwzlmKGM2fNjYWEBuq7D4XCYdgUAKwhahnnnGw9/bluPBpeGT7fuDWwf/ITHoQzzk2OGMsxPjhnKMUMZs+fHwoKoM+WcByg2AMDP1XUAdPzv+7Lg9omIiIioE7CwIOpM0V2AXmcCALLUvchW9mBl/n446hqD3DEiIiIi/2JhYQGKoiA5Odm0KwBYQVAzPLLsLABMUNfC6daw/EfrXQ7F41CG+ckxQxnmJ8cM5ZihjNnzY2FhAaqqIjk5GarK3dVRQc2w2TyLX9nfRwoq8YEFb5bH41CG+ckxQxnmJ8cM5ZihjNnzM2evyIumaSgpKTHtCgBWENQMu2QBg64CACQoh/F42F/xZcF+VB12Br4vAjwOZZifHDOUYX5yzFCOGcqYPT8WFhag6zpqa2tNuwKAFQQ9w0mPA7GpAICxtu9xhfIZPtpSHpy+dFDQM7Q45ifHDGWYnxwzlGOGMmbPj4UFUSBEdwEuXmh8+ZD9H/hm/YYgdoiIiIjIv1hYEAVK7nnQT7sOABCr1OPi3X9CSeXhIHeKiIiIyD9YWFiAqqpITU017UQdKzBLhsqEx3AoPAUAMEb9Ae+u2hTU/rSHWTK0KuYnxwxlmJ8cM5RjhjJmz8+cvSIviqIgMTHRtEuLWYFpMoyIg23Q5QAAm6Jjz3cfotFtzglYxzJNhhbF/OSYoQzzk2OGcsxQxuz5sbCwAE3TUFRUZNoVAKzATBlGD5xoPD7NuR6fWOSeFmbK0IqYnxwzlGF+csxQjhnKmD0/FhYWoOs6nE6naVcAsAJTZdhzFNz2aADAz9Tv8c81O4LbHx+ZKkMLYn5yzFCG+ckxQzlmKGP2/FhYEAWaPQJqnzEAgG6KAwcKN2BHRW2QO0VEREQkw8KCKAiUvuONxz9TN+Kfa3cFsTdEREREcqYuLBYsWIDTTz8dcXFx6N69Oy699FJs27bNa5v6+nrMmjULXbt2RWxsLCZPnoy9e72vWd+1axcuuOACREdHo3v37rjnnnvgcrkCORQRVVWRkZFh2hUArMB0GTYvLGw/4N/rStHgcgexQydmugwthvnJMUMZ5ifHDOWYoYzZ8zNnr4744osvMGvWLHzzzTdYvnw5Ghsbcd5556G29uhlI3fccQf+97//4a233sIXX3yBPXv24PLLLzeed7vduOCCC+B0OrFq1Sq8+uqrWLx4MebOnRuMIXWIoiiIjY017QoAVmC6DLtkAV37AgCGKflorD2ID34oC3Knjs90GVoM85NjhjLMT44ZyjFDGbPnZ+rCYtmyZbj++usxcOBAnHrqqVi8eDF27dqF9evXAwAcDgf+9re/4U9/+hPOPfdcDBs2DIsWLcKqVavwzTffAAA+/vhj/Pjjj/jHP/6BIUOGYNKkSfjtb3+L5557Dk6nM5jD85nb7UZ+fj7cbnP/RdvMTJlh358DAOyKhjPVLVi8aodpJ2MBJs3QQpifHDOUYX5yzFCOGcqYPT9TFxbHcjgcAIAuXboAANavX4/GxkaMH3/0spJ+/fqhZ8+eWL16NQBg9erVGDRoEFJSUoxtJkyYgOrqamzZsiWAvZcx67JiVmK6DJtdDjVW3YgfSh34rqQqeP3xgekytBjmJ8cMZZifHDOUY4YyZs7PHuwO+ErTNMyZMwdnnXUWTjnlFABAeXk5wsPDkZiY6LVtSkoKysvLjW2aFxWe5z3PtaahoQENDQ3G19XV1QCaqkRPhagoClRVhaZpXn9lbqtdVVUoitJm+7GVp+faOU3T4Ha7jf83b2/OZrNB13Wvdk9f2mr3te+dMSZf2v09Jk+GphlT5hlQ7ZFQXPX4me0HwKVj8dfFOLVHvM9jCuR+0nUduq632J7Hnm9993yONU2DzWYLiTGdqN3fY2r+szBUxhTI/eT53tb6YtUxBXo/eY5BACEzJo9A7adjf6cJhTEFcj8BaPFvcWePqT1XU1imsJg1axY2b96Mr776qtPfa8GCBZg/f36L9sLCQsTGxgIAEhISkJaWhr179xpnUgAgOTkZycnJ2L17t9dckNTUVCQmJmLHjh1el2BlZGQgNjYWhYWFXgdDVlYW7HY7CgoKoGkaKisrsX37duTl5cHlcqG4uNjYVlVV5Obmora2FqWlpUZ7eHg4+vTpA4fD4VVExcTEIDMzE5WVlaioqDDaAzmm5nJycjp9TPv27TMyVFXVNGPK6DYUsWWrkaZUYqTyEz74QcHVeeHoGm033X7q06cP3G63kWFn7KdQPPY8Y/J8jisrK5GSkhISYwr0fiosLDQ+x3a7PSTGFMj9lJSUBADYs2cP6urqQmJMgd5Pmqbh4MGDABAyYwICu58OHTpkfI7T09NDYkyB3E/Z2dlobGz0+re4s8cUHR0NXym6mS/qPmL27Nl49913sXLlSmRlZRntn332GcaNG4eDBw96nbXo1asX5syZgzvuuANz587Fe++9h40bNxrPFxcXo0+fPtiwYQOGDh3a4v1aO2Ph2THx8R37a7KkgtV1HY2NjQgLC4PNZjPamwvFqtyfY3K73XA6nQgLC4OiKKYZk7LpLajv/AoA8KH7dNzSeAduPzcbvx6XY7r9pCgKnE4n7Ha716QxHnu+9d3zOQ4PD+cZC8EZC8/PQkVRQmJMgdxPAOByuWC3e/9N0cpjCvR+8nyOIyMjW2xv1TF5BGo/aZrm9TtNKIwpkPtJVVU0NDR4/Vvc2WOqqalBYmIiHA6H8XtwW0xdWOi6jttuuw1vv/02Pv/8c+Tk5Hg973A40K1bN/zzn//E5MmTAQDbtm1Dv379sHr1apxxxhn48MMPceGFF6KsrAzdu3cHALz00ku45557sG/fPkRERJywH9XV1UhISPAp0M7gOVA8ByW1n2kzdDmBpwYBNeVw6wrGNDyFhtgMrLr/XITbzTUFyrQZWgTzk2OGMsxPjhnKMUOZYOTXnt+DzfWbyzFmzZqFf/zjH3j99dcRFxeH8vJylJeXG6dwExISMGPGDNx5551YsWIF1q9fjxtuuAGjRo3CGWecAQA477zzMGDAAEybNg3ff/89PvroIzz00EOYNWuWT0WFGWiaZlwSRR1j2gzt4cDpMwAANkXHNPtyVNQ0YOkm8y09a9oMLYL5yTFDGeYnxwzlmKGM2fMzdWHxwgsvwOFwYOzYsUhLSzP+e+ONN4xt/vznP+PCCy/E5MmTMWbMGKSmpuK///2v8bzNZsP7778Pm82GUaNG4Re/+AWmT5+ORx55JBhDImpp2PWALRwAcI3tM0ShHotW7Qhql4iIiIjay9STt325SisyMhLPPfccnnvuuTa36dWrF5YuXerPrhH5T2x34JQrgO9fR4JyGJfZvsbrJePw3a6DGNozKdi9IyIiIvKJqc9YEJ00Rv7KeHi9bRkAHa/yrAURERFZiKknb5sFJ29bnyUyfGUisKvpxo7TnfdhtTIEX99/LrrHRQa5Y00skaGJMT85ZijD/OSYoRwzlOHkbfILl8sV7C5YnukzPOMW4+Es+7todOt4fc2uIHaoJdNnaHLMT44ZyjA/OWYoxwxlzJwfCwsL0DQNxcXFpl0BwAoskWG/C4GuTUsqj1R/wnDlJyxZswtOlzn6bIkMTYz5yTFDGeYnxwzlmKGM2fNjYUFkFqoNOPtO48vZ9nex/1ADPti0J4idIiIiIvINCwsiMxl0JZDYEwAw1vY9BirFWPjZdrjc5vzLBBEREZEHCwuL8NzanTrOEhnawoCzfm18Ocv+Lor21+Lt73YHsVNHWSJDE2N+csxQhvnJMUM5Zihj5vy4KpQPgr0qFJ1kGuuBpwcDNXuh6Qp+7nwC9Ql98dndP0OE3Rbs3hEREdFJhKtChRhd11FTU+PTDQOpdZbKMCwSGDUbAKAqOu62v4ndVXV4Y21JULtlqQxNiPnJMUMZ5ifHDOWYoYzZ82NhYQGapqG0tNS0KwBYgeUyHP5LIKY7AGCSbS1GKlux8LPtqHO6g9Yly2VoMsxPjhnKMD85ZijHDGXMnh8LCyIziogFxj1sfDk37DUcOFSHxbwbNxEREZkUCwsisxoyFUgdDAAYqO7EFbYv8PyK7dh/qCHIHSMiIiJqiYWFBSiKgvDw8IDduj0UWTJD1QZMXGB8eY/9TegN1Xjyo21B6Y4lMzQR5ifHDGWYnxwzlGOGMmbPj6tC+YCrQlFQvTEN2PoeAOBF10V43H0N3ps1GoMyEoLcMSIiIgp1XBUqxOi6jqqqKtOuAGAFls7w548AtnAAwC9tS5GFPZj3vy0BH4ulMzQB5ifHDGWYnxwzlGOGMmbPj4WFBWiahvLyctOuAGAFls6wSxZw5m0AgHDFjXn2V7F+ZyXe3bgnoN2wdIYmwPzkmKEM85NjhnLMUMbs+bGwILKCs+8CEjIBAGNsmzBRXYsFH25FbYMryB0jIiIiasLCgsgKwmO8JnI/HPZ3VFc78Pzn24PYKSIiIqKjWFhYgKIoiImJMe0KAFYQEhn2uxDIHgcA6KEcwG32d/DXL4ux68DhgLx9SGQYRMxPjhnKMD85ZijHDGXMnh9XhfIBV4Ui0zhQCDx/BuB2wqWruMI5D937n4WXpg8Pds+IiIgoBHFVqBCjaRoqKipMO1HHCkImw67ZwOg7AQB2RcMzYQux+sdifFVQ0elvHTIZBgnzk2OGMsxPjhnKMUMZs+fHwsICdF1HRUWFaZcWs4KQynDM3UDGCABAT3U/Hg17BfPf2wynq3N/yIRUhkHA/OSYoQzzk2OGcsxQxuz5sbAgshpbGDD5ZegRTacjL7GtwuADH+LJj4NzR24iIiIigIUFkTUl9YJy0VPGl4+ELcLyL7/Gim37gtcnIiIiOqmxsLAARVGQkJBg2hUArCAkMzxlMjB0GgAgRmnAM2ELcf8b67C3ur5T3i4kMwwg5ifHDGWYnxwzlGOGMmbPj6tC+YCrQpFpOWuh/+VnUA4UAABecl2AFT1vxz9uHAmbas4fOkRERGQdXBUqxGiahrKyMtOuAGAFIZtheAyUK/4G3RYOAJhp/wDhOz7Ds5/5/8Z5IZthgDA/OWYow/zkmKEcM5Qxe34sLCxA13U4HA7TrgBgBSGdYdqpUH7+iPHlk2EvYMmn3+KbogN+fZuQzjAAmJ8cM5RhfnLMUI4Zypg9PxYWRKFg5M1AzgQAQDelGn+yP4c7/rkOlbXOIHeMiIiIThYsLIhCgaIAlz4PPS4NADDatgVXHn4Td7/1vWn/qkFEREShhYWFBSiKguTkZNOuAGAFJ0WGMclQJv8NutL0sZ5j/w/q8lfg+c8L/fLyJ0WGnYj5yTFDGeYnxwzlmKGM2fPjqlA+4KpQZCkrnwQ++y0AYL+egPMbFuCuy8/GlBE9g9wxIiIishquChViNE1DSUmJaVcAsIKTKsPRdwLZ5wIAuikOPB/+FOa9vQEfbioTvexJlWEnYH5yzFCG+ckxQzlmKGP2/FhYWICu66itreW18gInVYaqClz+V+hx6QCA09V8PGb/K379r+/wVUFFh1/2pMqwEzA/OWYow/zkmKEcM5Qxe34sLIhCUUwylGteh26PAgBMtn2Fm/A2bv7Hemwtqw5y54iIiCgUsbAgClXpQ6Fc/pLx5T1hb+J611u4f9GH2FtdH8SOERERUShiYWEBqqoiNTUVqsrd1VEnbYYDLgbG/cb48u6wt/CucyYqnx6D+k3vtuulTtoM/YT5yTFDGeYnxwzlmKGM2fPjqlA+4KpQZGm6Dnx4L/DtSy2ecl7zH4TnjQ9Cp4iIiMgKuCpUiNE0DUVFRaZdAcAKTuoMFQU4/w/ArG9x4PS7sB2ZxlM1b9yE6opyn17mpM7QD5ifHDOUYX5yzFCOGcqYPT8WFhag6zqcTqdpVwCwAmYIoFseul4wFxXTVuBrfTAAoItWic0vTseeg4dP+O3MUIb5yTFDGeYnxwzlmKGM2fNjYUF0kjkjuxu6TP0bDqLpdOaZrjV49dlHsK38UJB7RkRERFbGwoLoJNQ/NxeNFzxtfP1r1yv47YuLsaboQBB7RURERFbGwsICVFVFRkaGaVcAsAJm2FL30y9H3anXAQCilQa8pP8Wf1n01zbv0M0MZZifHDOUYX5yzFCOGcqYPT+uCuUDrgpFIauxDu4lV8O24wsAgFO34U7XLGSPnYbbzu0Lu82cP7iIiIgoMLgqVIhxu93Iz8+H2+0Odlcsixm2ISwKtl+8Ba3fxQCAcMWNZ+wLkfjFQ7jhhY+x68DRSd3MUIb5yTFDGeYnxwzlmKGM2fOzB7sD5BuzLitmJcywDfYIqFcthv6/X0P57u9QFR032D/CJfu/xgtPX4mBp52J8wZ0R0RYGHRndLB7a2k8BuWYoQzzk2OGcsxQxsz5sbAgIkC1Qbl4IdA1G9qK30N116OLUoMHlUXAxkXAxqbNesb2Anp/BsQmB7W7REREZD68FIqImigKMPoOqLevR+OAK1rdJKpmJyoXXwPd1RDgzhEREZHZcfK2D4I9edtzM5Tw8HAoihLw9w8FzLADStbi4MZ38f2OCmzbV4PL1S/QTakGAHwZOwk5Ny5CamJUkDtpHTwG5ZihDPOTY4ZyzFAmGPm15/dgFhY+MENhoWkaVFXlh7CDmKHM7qo6LPn3v/HrkjsQoTQCAJ7WroLzjF/jxp/lIikmPMg9ND8eg3LMUIb5yTFDOWYoE4z8uCpUiNE0DQUFBaaerGN2zFCmR2IU7rrhWnyde6/R9mv1Tdy4ZgKWPzEF/3zzdZQcqA1iD82Px6AcM5RhfnLMUI4Zypg9P07eJiKfpQ+7GIeTGxC9+kkAQJJSg6vwKfDjp9i6eQGeS7oS3Uddi4lDeiEuMizIvSUiIqJAYmFBRO0SMf7/gOxROPzt32HfvgzhWj0AoL/6/+3de5AV5Z3/8ffT3ec6V2CYG8hNEY0iUZHZqWyySeQnsP4STdhoXGpFk2hMwLhrkmK11luytVqxVq1YBrf25yVVpkyWlLc1XgpUTBRERVG8TQARVGa4Ordz5ly6+/v745w5cJxhGGyZcwa+r6oDM919znmeTz+nu58+T/ds5+Su/2T3k/+P5584hVTjbKad/jVmzv4yoZB2MpRSSqmjnXYslFKH74S5xE+YC5kEna8/QuqF39DY8xYA400X/9esgV1r4Olfs/upWl6v+iqJE89j2he/yikTavUveiullFJHIb14exj04u3RTzMMbsgMRZAP19H17B1UbHuGkGQGfY0uifMOx7Ov9lSsCWcQmzKb5kkn0FAdI+V6JDO5vyQ6eWwcyzq61pO2weA0w2A0v+A0w+A0w2DK/eLtY6pjcdddd3HrrbfS0dHBrFmzuPPOO5kzZ84hn1cOHQu9NVswmmFww87Qy5L5+A02v7Yad/OznNS7jjDuQRffLTVskwaqSVBrEoRwec2ZRfdJFzD7aws5rq7qCNRm5GkbDE4zDEbzC04zDE4zDEZvN1sm/vCHP3DxxRdz991309LSwh133MGKFStoa2ujvr5+yOeWumPheR6bNm1i+vTp2LY94u9/NNAMg/usGUpfJ7tffYj0xseo2buBau+TYT+3XcbykX0coUiMSCSKHx9Psnoq2dpp2GMmM66unob68URjlWzb08OWjk46upJMaRzHWVPHEQ+Xz2hPbYPBaYbBaH7BaYbBaYbBlCK/wzkOLp+97hF22223cdlll3HppZcCcPfdd/OnP/2Je++9l3/9138tcemUOnqZWC31X/4efPl7uSFT3R/T/s4aEltfJbxrA3Xd71Dh95A2UZJ2NSEvSaX0AtBk9tHk74M+co9OYMfg73NC/tEvKRG67CiuHcd14nhOnHSolq7QeDqd8STCdUQraqisqqGiqgY3VEHWjpGxYoRsm6gDEQt836Uv7ZLKZBDfJx6CipBFLOwQHdNARW09sXAI0/cJdGxEdr9HV8awLVvNe8lqUpF6ZkybwqnNwzwp4fu5v4L+OZyJ8nwh6/lEQ0PsfHwfuraDHYaqps/lfQ9bugfcNMTHfe7vn/V8HMsc3pk93wdLrwMaEdkUbH0e2p4ELwsz5sMJ/wdC0VKXrKylXY+/dvTycWcfJzZUMrWuQs/+j5DetItjmaG3q8ewY6JjkclkWL9+Pddcc01hmmVZzJ07l7Vr15awZEodY4zB1EykufUCaL0gN00EvCwRJ0wEwM2w743H6VpzP8ftfQEH7zO9VdykwU+D3wXZz60GA2TFposotSb3dzwMUJt/zMovk34hRAdj8K1qNlsWxoCNAD5GfBAhJkkqJUEFSTKE2GvGsNeMJWEqcXBxjEdIXIy42H4WIx5ZK4JrRfGcGFHpo9rrosrvAoR9Us0uv4oeiRG3PCpsj4jtkzVh0iZKBofxbgeTvO1EyV0T00MF250pdIfHE5YsETJEJIXtpQn5KWzJ4poQGStK1ophEGzJYvtZEI+sGFzf4InBtu3CQzD4GAQLD4OPhS9Q5XdR73ZQJT0A9BGh3dSz24wjYvnETJaocfFMiIwVIWPCZFyfDcZC8rlZRrAMWAgWkuuXiJD2IOUKGQ+MZREOOURCoUJ5xFiFMomxcCTLmOxOxmR3UuPtI2lV8olTT2eonpS1/y/Me56Qcn3Sro/rC2HHJhqyCDsWxgzeGXH9XAcn6wmeCI5l4VgG2zaAoX/cgGCA3C9hP0VMEsT9BEZ8+qwKElYlGRPBEg9bXGxc7PzPFh4uDmkTJW2iYCwilkfY+Dh4GMliPBffTbPZeDiSwfGzpK0YfU416VANvgkhSO4zmc+X/O+C5Noqgm0oPAyCiGDIP6ewfH59GxswRCRFxO8jIilc45A1YXxjMzX1HjFJ7g9rwwMkTAVvxWbjWjEcS7CNYMQD30PEx+TXu+lf57kUsfDz9cpgi0vGipAyMdImip9fN/2H3gceghtk4M/5g3TTv3JM/jkiuK7LxpCDBfgiZL1cB97zBewwlhPBCoWxjWCJhyVu/v/cA3HByyKeh+CTteJ4oTi+E8+v29xnypZM/n8XW7I4ksWWLJ4v7MrG2CdVdEucdVi8EbKoq4wQcfa3QSn8AwcOTTH4OH6GkGSwJbdx9LDxjYWLjSe5z6lgYywLy7KwERw/tx1w8tfQiYB/YKKmPz2DmOKkhf0nSwRDNuuyMRSCos+MwQc8Ac83CIJlDCZ/LYEvBl9yQ4H2v54hImlifoKY34sjmdw2jghZE8ptI00Yz4rktrsm12ZsL4XjJQm5fYCQsipI2zFcK5bbnhiDYzyibjdxt5uY10NCQux0K9jlVZImTDxsUx0NEQ/buc8Bn/KpAUHyqXZXqLV4GC+D7aexxMU1YVwrgmeFwRhscu3dRqj66k+YNrN1kFcpH8dEx2LPnj14nkdDQ0PR9IaGBt57770By6fTadLpdOH37u5uIPf1k+flDnKMMViWhe/7HDia7GDT+y+yOdj0/tc9cDrk/hBK/zzP84qmH8i27cIFPZ8uy8GmD7fsR6JOw5n+edepP8OjqU4juZ5EZNDlP5c6WfmdtO8jxqbmi+dR88XzML6LeCl27u1i++5PyO7bTqjzfaLdW7ETO5FUNybdjeOlcEIhIuEIoVCIVF8Ct6+HkJckbtJUkCJmBr+gPKiQ8ahl6D8OGDFZJrMLZBfD6SdFyTBBdjJBdh58of5jUI9BX7OWbqYduM/2848hVJHgFPdthrgkJudQdTAHvN9hdOpipJkmHzJNPjxkWYet/6Siy6HrdYC430s808uEzPtDL3iYr1t2fHLlT5W6IPtVSIKW5POlLsbQDpaXC6QPMm8oh/mcKZ/uwwrQ8xnet5TKqM0N1wmwf5viAYn8Y4Ss7zgXZrYO2Bcf6eOIw7lq4pjoWByum2++mZtuumnA9C1btlBZWQlATU0NTU1N7Ny5k66ursIydXV11NXV8fHHH5NI7G9tjY2N1NbW8sEHH5DJ7D/AmThxIpWVlWzZsqWoMUydOhXHcdi0aVNh2vvvv8/06dNxXZetW7cWpluWxYknnkgikeCjjz4qTA+Hw0ybNo2uri46OjoK0ysqKjjuuOPYt28fe/bsKUwvRZ2AEalT/7T333//qKlTKdbTlClTChmOZJ2yfjfV0Qg0T4fm6YU6vf/++2QyGYTc8WtDvk5//etf8X2fnUmXrrRHTV0jfekse7a9S427h+rsbsLpT4iELPp6u0h27yXq9xGV3BlVJxQi60Eq6yMmd8bOsh3C0RipbO7uVeK5VHid1HidVEoPHaaezUxii5lEVdjixIpejo8lsLo/xk60U5XdS1V+D+RJ/qx9/lyrjyFJjB5TQS8xoqQZzyfUHGSPlcXGwyZMFuuA82RJibCPKsAw1nQTP8TRiieGj00j2+1J2H6aqf52Gs2+QZdLESZDiDBZYmSwzP73zYqNj5WrUf6MoDXw/N0Avhh2MI6PZDxZQky09jCBXUUX+2fFJmQ+27dWn8VOqaVDxlJLL01mL+ERfO/B+JI7C2yb4e/YhyMtDi42FeazHAV/dp9en51SwSr/TFb6Z+EahwVmLfOsV6gyfYHexxdT1EaPBlmxyeBg4xM1R/Br2FHME/O5f1YAEhIhSuaIvPbh6O78BNu2mTBhQtG++EgfR8Tj8WGX8Zi4eDuTyRCPx/njH//I+eefX5i+ePFiOjs7efTRR4uWH+wbi/4V03/RykieNRYRkskk8Xi8cKHO0XQmfCTO7nueRyKRIB6PY4w5Kuo00uvJGEMikSAWixWN5R3NdRrJ9SQiJBMJKiorsSyLjOvlhpLksxy0Ttk+LLcP44TxsHLXQRgb8q+NCH4mQaavF3FiONFKQs4B24hsEjIJsCPY4RhZMXjpBCabxHhpnOpGrEhFUZ28ZCfp7t14TpwMYVwrTDwWIxZ2CDk2xhj60lmSyR6MsbBDUcIhh2jIgfywGABEyHg+nYkU4nlYxi98nR+ywLHAhOI4oUjh1sK5Ovn4yU7EckgTpjudG5JjvBReOomXSTKmupJoKEwo5OD5kPEET8CV3DAKYyxqojaId8CwHp9PEmlS6Qzi759u4WMZ8AXceD3iRPevPwMkdueuA8iLhW1q42EsA5LPLJlx6Um7iBh834OioTUQCdnEQxYRx8p9mSNC2hPS2dzQnv7lTL7tIT6+HUGitZhwBcaysL0+TKo7vz5tsMIYJ4wdiuS6cZaD8TIYN4mVTeL7QsKFpGtwsQmHI4TCYXxfqKkdW1iXbjZLKtFJsmsPvpvBkN8+2haO7RTWjW1ZYFlYxuBhkXGFVNbLDW80FpZt49j54WAmP4RIfPBdbAO+E8d1Yrk2LAJeGstLE4rXEgk5hA8YwuNmU6R3biHt+qR9Q9YH28kNZbMsC4zB98HHyg25MRaeL3hiME4U40SwHAdbMphML1YmkR+eRb4+BteXfDPNDVAx+Xr6niCm0ISxLJMbyudLrsngk0pliMVzw+MsoDIaojJsY9sGz3Xp7u2lpzeBh4WxHYzlYJwwYiwwDo7jUBGLUhGPEXZsEr2dJLq76Ev2gOVghyLYoSjYIYwdxgpFME64sD7Cdm74D32fQKorV0aBj/YlyPpSaE/9664wmis/nMtYFmJH8K0wYkcw+aFmjhEs8bHFxTE+4ntkPcH1PFwfxI7krn1xothWbrij+B5G+j/3+SxN/tjFzw2Nk8JwNYPne+D79KVSRCMRrHyhcts9Hwsh4lhEQja2MaSz2dz7e7kheCEnN4xQfA/xBV8EL/9Z8cOV+Fg4+IQkg+WnMW4GL9OHn0nmtw+54VShWCWReBXhWCU24KW68dIJJJMkkx+26IrBxGox8bGEIzHGxCzC2V5I7gEvd5KuM+XSmchy4EAnK39Nl/h+YUiYASyTa7v9n/l+tu3gRGI4oQi2E0LcDH6mD/HS+evkBI/cENO6xknU1I6lt7e3aF98pPe5vb291NbW6l2hDtTS0sKcOXO48847gVwjnjRpEkuXLj3kxdt6V6jRTzMMTjMMRvMLTjMMRvMLTjMMTjMMRu8KVSauvvpqFi9ezOzZs5kzZw533HEHiUSicJcopZRSSiml1Gd3zHQsLrzwQnbv3s31119PR0cHX/ziF3nqqacGXNCtlFJKKaWUOnzHTMcCYOnSpSxdurTUxThsxhj9C5UBaYbBaYbBaH7BaYbBaH7BaYbBaYbBlHt+x8w1FkGU+hoLpZRSSimlSuFwjoP1T4uOAiJCZ2fnYd1HWBXTDIPTDIPR/ILTDIPR/ILTDIPTDIMp9/y0YzEK+L5PR0fHgFtiquHTDIPTDIPR/ILTDIPR/ILTDIPTDIMp9/y0Y6GUUkoppZQKTDsWSimllFJKqcC0YzEKGGOoqKgo2zsAjAaaYXCaYTCaX3CaYTCaX3CaYXCaYTDlnp/eFWoY9K5QSimllFLqWKR3hTrK+L7Pnj17yvZCndFAMwxOMwxG8wtOMwxG8wtOMwxOMwym3PPTjsUoICLs2bOnbG8tNhpohsFphsFofsFphsFofsFphsFphsGUe37asVBKKaWUUkoFph0LpZRSSimlVGDasRgFjDHU1NSU7R0ARgPNMDjNMBjNLzjNMBjNLzjNMDjNMJhyz0/vCjUMelcopZRSSil1LNK7Qh1lfN+nvb29bO8AMBpohsFphsFofsFphsFofsFphsFphsGUe37asRgFRISurq6yvQPAaKAZBqcZBqP5BacZBqP5BacZBqcZBlPu+WnHQimllFJKKRWYU+oCjAb9vcLu7u6SvL/nefT29tLd3Y1t2yUpw2inGQanGQaj+QWnGQaj+QWnGQanGQZTivz6j3+H8y2JdiyGoaenB4DjjjuuxCVRSimllFJq5PX09FBTUzPkMnpXqGHwfZ8dO3ZQVVVVktt7dXd3c9xxx/Hhhx/qXak+I80wOM0wGM0vOM0wGM0vOM0wOM0wmFLkJyL09PTQ3NyMZQ19FYV+YzEMlmUxceLEUheD6upq/RAGpBkGpxkGo/kFpxkGo/kFpxkGpxkGM9L5Heqbin568bZSSimllFIqMO1YKKWUUkoppQLTjsUoEIlEuOGGG4hEIqUuyqilGQanGQaj+QWnGQaj+QWnGQanGQZT7vnpxdtKKaWUUkqpwPQbC6WUUkoppVRg2rFQSimllFJKBaYdC6WUUkoppVRg2rEYBe666y6mTJlCNBqlpaWFl19+udRFKks333wzZ511FlVVVdTX13P++efT1tZWtMxXv/pVjDFFjyuuuKJEJS4/N95444B8TjrppML8VCrFkiVLGDduHJWVlSxcuJCdO3eWsMTlZ8qUKQMyNMawZMkSQNvgp/35z3/mG9/4Bs3NzRhjeOSRR4rmiwjXX389TU1NxGIx5s6dy6ZNm4qW2bdvH4sWLaK6upra2lq+//3v09vbO4K1KK2hMsxmsyxbtoyZM2dSUVFBc3MzF198MTt27Ch6jcHa7S233DLCNSmNQ7XBSy65ZEA28+fPL1pG2+DQGQ62TTTGcOuttxaWOZbb4HCOX4az/92+fTvnnnsu8Xic+vp6fv7zn+O67khWRTsW5e4Pf/gDV199NTfccAOvvfYas2bNYt68eezatavURSs7zz//PEuWLOGll15i5cqVZLNZzjnnHBKJRNFyl112Ge3t7YXHr371qxKVuDydcsopRfm88MILhXn/8i//wv/+7/+yYsUKnn/+eXbs2MG3v/3tEpa2/LzyyitF+a1cuRKA73znO4VltA3ul0gkmDVrFnfdddeg83/1q1/x61//mrvvvpt169ZRUVHBvHnzSKVShWUWLVrE22+/zcqVK3n88cf585//zOWXXz5SVSi5oTJMJpO89tprXHfddbz22ms89NBDtLW18c1vfnPAsr/4xS+K2uWVV145EsUvuUO1QYD58+cXZfPggw8Wzdc2OHSGB2bX3t7OvffeizGGhQsXFi13rLbB4Ry/HGr/63ke5557LplMhjVr1vDb3/6W+++/n+uvv35kKyOqrM2ZM0eWLFlS+N3zPGlubpabb765hKUaHXbt2iWAPP/884Vpf/d3fydXXXVV6QpV5m644QaZNWvWoPM6OzslFArJihUrCtPeffddAWTt2rUjVMLR56qrrpLjjz9efN8XEW2DQwHk4YcfLvzu+740NjbKrbfeWpjW2dkpkUhEHnzwQREReeeddwSQV155pbDMk08+KcYY+fjjj0es7OXi0xkO5uWXXxZAtm3bVpg2efJkuf32249s4UaBwfJbvHixnHfeeQd9jrbBYsNpg+edd558/etfL5qmbXC/Tx+/DGf/+8QTT4hlWdLR0VFYZvny5VJdXS3pdHrEyq7fWJSxTCbD+vXrmTt3bmGaZVnMnTuXtWvXlrBko0NXVxcAY8eOLZr+u9/9jrq6Ok499VSuueYakslkKYpXtjZt2kRzczPTpk1j0aJFbN++HYD169eTzWaL2uNJJ53EpEmTtD0eRCaT4YEHHuB73/sexpjCdG2Dw7N161Y6OjqK2lxNTQ0tLS2FNrd27Vpqa2uZPXt2YZm5c+diWRbr1q0b8TKPBl1dXRhjqK2tLZp+yy23MG7cOE4//XRuvfXWER9CUc5Wr15NfX09M2bM4Ec/+hF79+4tzNM2eHh27tzJn/70J77//e8PmKdtMOfTxy/D2f+uXbuWmTNn0tDQUFhm3rx5dHd38/bbb49Y2Z0Reyd12Pbs2YPneUWNBKChoYH33nuvRKUaHXzf55//+Z/50pe+xKmnnlqY/o//+I9MnjyZ5uZm3nzzTZYtW0ZbWxsPPfRQCUtbPlpaWrj//vuZMWMG7e3t3HTTTXz5y1/mrbfeoqOjg3A4POBgpKGhgY6OjtIUuMw98sgjdHZ2cskllxSmaRscvv52Ndg2sH9eR0cH9fX1RfMdx2Hs2LHaLgeRSqVYtmwZF110EdXV1YXpP/nJTzjjjDMYO3Ysa9as4ZprrqG9vZ3bbruthKUtD/Pnz+fb3/42U6dOZcuWLVx77bUsWLCAtWvXYtu2tsHD9Nvf/paqqqoBw2i1DeYMdvwynP1vR0fHoNvK/nkjRTsW6qi0ZMkS3nrrraLrA4CiMa8zZ86kqamJs88+my1btnD88cePdDHLzoIFCwo/n3baabS0tDB58mT+53/+h1gsVsKSjU733HMPCxYsoLm5uTBN26AqlWw2ywUXXICIsHz58qJ5V199deHn0047jXA4zA9/+ENuvvnmsv0LvyPlu9/9buHnmTNnctppp3H88cezevVqzj777BKWbHS69957WbRoEdFotGi6tsGcgx2/jBY6FKqM1dXVYdv2gKv+d+7cSWNjY4lKVf6WLl3K448/znPPPcfEiROHXLalpQWAzZs3j0TRRp3a2lpOPPFENm/eTGNjI5lMhs7OzqJltD0Obtu2baxatYof/OAHQy6nbfDg+tvVUNvAxsbGATezcF2Xffv2abs8QH+nYtu2baxcubLo24rBtLS04LouH3zwwcgUcBSZNm0adXV1hc+stsHh+8tf/kJbW9sht4twbLbBgx2/DGf/29jYOOi2sn/eSNGORRkLh8OceeaZPPPMM4Vpvu/zzDPP0NraWsKSlScRYenSpTz88MM8++yzTJ069ZDP2bBhAwBNTU1HuHSjU29vL1u2bKGpqYkzzzyTUChU1B7b2trYvn27tsdB3HfffdTX13PuuecOuZy2wYObOnUqjY2NRW2uu7ubdevWFdpca2srnZ2drF+/vrDMs88+i+/7hU7bsa6/U7Fp0yZWrVrFuHHjDvmcDRs2YFnWgCE+Cj766CP27t1b+MxqGxy+e+65hzPPPJNZs2YdctljqQ0e6vhlOPvf1tZWNm7cWNTJ7T+J8IUvfGFkKgJ6V6hy9/vf/14ikYjcf//98s4778jll18utbW1RVf9q5wf/ehHUlNTI6tXr5b29vbCI5lMiojI5s2b5Re/+IW8+uqrsnXrVnn00Udl2rRp8pWvfKXEJS8fP/3pT2X16tWydetWefHFF2Xu3LlSV1cnu3btEhGRK664QiZNmiTPPvusvPrqq9La2iqtra0lLnX58TxPJk2aJMuWLSuarm1woJ6eHnn99dfl9ddfF0Buu+02ef311wt3LLrllluktrZWHn30UXnzzTflvPPOk6lTp0pfX1/hNebPny+nn366rFu3Tl544QWZPn26XHTRRaWq0ogbKsNMJiPf/OY3ZeLEibJhw4aibWP/nWLWrFkjt99+u2zYsEG2bNkiDzzwgIwfP14uvvjiEtdsZAyVX09Pj/zsZz+TtWvXytatW2XVqlVyxhlnyPTp0yWVShVeQ9vg0J9jEZGuri6Jx+OyfPnyAc8/1tvgoY5fRA69/3VdV0499VQ555xzZMOGDfLUU0/J+PHj5ZprrhnRumjHYhS48847ZdKkSRIOh2XOnDny0ksvlbpIZQkY9HHfffeJiMj27dvlK1/5iowdO1YikYiccMIJ8vOf/1y6urpKW/AycuGFF0pTU5OEw2GZMGGCXHjhhbJ58+bC/L6+Pvnxj38sY8aMkXg8Lt/61rekvb29hCUuT08//bQA0tbWVjRd2+BAzz333KCf28WLF4tI7paz1113nTQ0NEgkEpGzzz57QK579+6Viy66SCorK6W6ulouvfRS6enpKUFtSmOoDLdu3XrQbeNzzz0nIiLr16+XlpYWqampkWg0KieffLL8x3/8R9GB89FsqPySyaScc845Mn78eAmFQjJ58mS57LLLBpzc0zY49OdYROS//uu/JBaLSWdn54DnH+tt8FDHLyLD2/9+8MEHsmDBAonFYlJXVyc//elPJZvNjmhdTL5CSimllFJKKfWZ6TUWSimllFJKqcC0Y6GUUkoppZQKTDsWSimllFJKqcC0Y6GUUkoppZQKTDsWSimllFJKqcC0Y6GUUkoppZQKTDsWSimllFJKqcC0Y6GUUkoppZQKTDsWSimljkrGGB555JFSF0MppY4Z2rFQSin1ubvkkkswxgx4zJ8/v9RFU0opdYQ4pS6AUkqpo9P8+fO57777iqZFIpESlUYppdSRpt9YKKWUOiIikQiNjY1FjzFjxgC5YUrLly9nwYIFxGIxpk2bxh//+Mei52/cuJGvf/3rxGIxxo0bx+WXX05vb2/RMvfeey+nnHIKkUiEpqYmli5dWjR/z549fOtb3yIejzN9+nQee+yxI1tppZQ6hmnHQimlVElcd911LFy4kDfeeINFixbx3e9+l3fffReARCLBvHnzGDNmDK+88gorVqxg1apVRR2H5cuXs2TJEi6//HI2btzIY489xgknnFD0HjfddBMXXHABb775Jn//93/PokWL2Ldv34jWUymljhVGRKTUhVBKKXV0ueSSS3jggQeIRqNF06+99lquvfZajDFcccUVLF++vDDvb/7mbzjjjDP4zW9+w3//93+zbNkyPvzwQyoqKgB44okn+MY3vsGOHTtoaGhgwoQJXHrppfz7v//7oGUwxvBv//Zv/PKXvwRynZXKykqefPJJvdZDKaWOAL3GQiml1BHxta99rajjADB27NjCz62trUXzWltb2bBhAwDvvvsus2bNKnQqAL70pS/h+z5tbW0YY9ixYwdnn332kGU47bTTCj9XVFRQXV3Nrl27PmuVlFJKDUE7FkoppY6IioqKAUOTPi+xWGxYy4VCoaLfjTH4vn8kiqSUUsc8vcZCKaVUSbz00ksDfj/55JMBOPnkk3njjTdIJBKF+S+++CKWZTFjxgyqqqqYMmUKzzzzzIiWWSml1MHpNxZKKaWOiHQ6TUdHR9E0x3Goq6sDYMWKFcyePZu//du/5Xe/+x0vv/wy99xzDwCLFi3ihhtuYPHixdx4443s3r2bK6+8kn/6p3+ioaEBgBtvvJErrriC+vp6FixYQE9PDy+++CJXXnnlyFZUKaUUoB0LpZRSR8hTTz1FU1NT0bQZM2bw3nvvAbk7Nv3+97/nxz/+MU1NTTz44IN84QtfACAej/P0009z1VVXcdZZZxGPx1m4cCG33XZb4bUWL15MKpXi9ttv52c/+xl1dXX8wz/8w8hVUCmlVBG9K5RSSqkRZ4zh4Ycf5vzzzy91UZRSSn1O9BoLpZRSSimlVGDasVBKKaWUUkoFptdYKKWUGnE6ClcppY4++o2FUkoppZRSKjDtWCillFJKKaUC046FUkoppZRSKjDtWCillFJKKaUC046FUkoppZRSKjDtWCillFJKKaUC046FUkoppZRSKjDtWCillFJKKaUC046FUkoppZRSKrD/D+aGBIruU60IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
