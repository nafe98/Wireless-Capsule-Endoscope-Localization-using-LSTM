{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f_over.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.151730</td>\n",
       "      <td>199.542510</td>\n",
       "      <td>93.204352</td>\n",
       "      <td>141.610133</td>\n",
       "      <td>208.230033</td>\n",
       "      <td>220.674816</td>\n",
       "      <td>158.863186</td>\n",
       "      <td>189.306563</td>\n",
       "      <td>144.184843</td>\n",
       "      <td>160.406591</td>\n",
       "      <td>...</td>\n",
       "      <td>125.388714</td>\n",
       "      <td>134.794757</td>\n",
       "      <td>137.019077</td>\n",
       "      <td>133.083826</td>\n",
       "      <td>188.991982</td>\n",
       "      <td>178.155730</td>\n",
       "      <td>161.186367</td>\n",
       "      <td>192.737363</td>\n",
       "      <td>73.724932</td>\n",
       "      <td>138.491142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.398547</td>\n",
       "      <td>184.865378</td>\n",
       "      <td>100.886692</td>\n",
       "      <td>142.019092</td>\n",
       "      <td>200.934261</td>\n",
       "      <td>231.777862</td>\n",
       "      <td>156.058008</td>\n",
       "      <td>203.353192</td>\n",
       "      <td>140.903362</td>\n",
       "      <td>165.565921</td>\n",
       "      <td>...</td>\n",
       "      <td>131.875666</td>\n",
       "      <td>144.307763</td>\n",
       "      <td>144.125796</td>\n",
       "      <td>127.552055</td>\n",
       "      <td>183.965512</td>\n",
       "      <td>179.206473</td>\n",
       "      <td>153.405338</td>\n",
       "      <td>192.374917</td>\n",
       "      <td>83.476169</td>\n",
       "      <td>127.547566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168.699960</td>\n",
       "      <td>195.483494</td>\n",
       "      <td>102.242983</td>\n",
       "      <td>151.909099</td>\n",
       "      <td>206.502507</td>\n",
       "      <td>224.945880</td>\n",
       "      <td>161.279681</td>\n",
       "      <td>183.895077</td>\n",
       "      <td>145.834056</td>\n",
       "      <td>158.360219</td>\n",
       "      <td>...</td>\n",
       "      <td>126.723309</td>\n",
       "      <td>138.780776</td>\n",
       "      <td>128.601055</td>\n",
       "      <td>130.194494</td>\n",
       "      <td>181.584965</td>\n",
       "      <td>176.214339</td>\n",
       "      <td>163.883558</td>\n",
       "      <td>187.026603</td>\n",
       "      <td>74.422808</td>\n",
       "      <td>145.994348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.108359</td>\n",
       "      <td>195.489980</td>\n",
       "      <td>95.088657</td>\n",
       "      <td>146.039074</td>\n",
       "      <td>194.145214</td>\n",
       "      <td>233.067175</td>\n",
       "      <td>159.540317</td>\n",
       "      <td>187.301984</td>\n",
       "      <td>135.145034</td>\n",
       "      <td>168.508921</td>\n",
       "      <td>...</td>\n",
       "      <td>120.952913</td>\n",
       "      <td>134.810703</td>\n",
       "      <td>133.864237</td>\n",
       "      <td>144.482189</td>\n",
       "      <td>181.923756</td>\n",
       "      <td>181.988992</td>\n",
       "      <td>158.399493</td>\n",
       "      <td>187.715639</td>\n",
       "      <td>83.992892</td>\n",
       "      <td>138.966512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168.168172</td>\n",
       "      <td>185.840023</td>\n",
       "      <td>99.153895</td>\n",
       "      <td>139.952871</td>\n",
       "      <td>194.029188</td>\n",
       "      <td>225.089917</td>\n",
       "      <td>155.754544</td>\n",
       "      <td>188.479389</td>\n",
       "      <td>141.177966</td>\n",
       "      <td>170.835688</td>\n",
       "      <td>...</td>\n",
       "      <td>117.921545</td>\n",
       "      <td>139.772629</td>\n",
       "      <td>138.342093</td>\n",
       "      <td>130.464550</td>\n",
       "      <td>192.744036</td>\n",
       "      <td>183.586504</td>\n",
       "      <td>153.567238</td>\n",
       "      <td>188.398368</td>\n",
       "      <td>79.339979</td>\n",
       "      <td>133.595655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>191.446233</td>\n",
       "      <td>163.863018</td>\n",
       "      <td>120.196717</td>\n",
       "      <td>81.691845</td>\n",
       "      <td>232.944152</td>\n",
       "      <td>213.767370</td>\n",
       "      <td>188.466104</td>\n",
       "      <td>177.347899</td>\n",
       "      <td>156.237599</td>\n",
       "      <td>139.888740</td>\n",
       "      <td>...</td>\n",
       "      <td>121.416400</td>\n",
       "      <td>158.272110</td>\n",
       "      <td>135.787415</td>\n",
       "      <td>110.038074</td>\n",
       "      <td>204.484919</td>\n",
       "      <td>181.772215</td>\n",
       "      <td>183.221858</td>\n",
       "      <td>165.974294</td>\n",
       "      <td>129.016781</td>\n",
       "      <td>91.163284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>192.210071</td>\n",
       "      <td>169.472938</td>\n",
       "      <td>126.408295</td>\n",
       "      <td>80.555287</td>\n",
       "      <td>226.421779</td>\n",
       "      <td>212.131048</td>\n",
       "      <td>189.024391</td>\n",
       "      <td>161.463544</td>\n",
       "      <td>146.955323</td>\n",
       "      <td>136.197913</td>\n",
       "      <td>...</td>\n",
       "      <td>124.351245</td>\n",
       "      <td>155.065263</td>\n",
       "      <td>138.527748</td>\n",
       "      <td>112.555788</td>\n",
       "      <td>199.509706</td>\n",
       "      <td>186.175599</td>\n",
       "      <td>183.299073</td>\n",
       "      <td>175.290848</td>\n",
       "      <td>130.406301</td>\n",
       "      <td>90.511453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>182.613814</td>\n",
       "      <td>174.438416</td>\n",
       "      <td>116.411536</td>\n",
       "      <td>91.348961</td>\n",
       "      <td>233.275747</td>\n",
       "      <td>210.510062</td>\n",
       "      <td>189.619989</td>\n",
       "      <td>163.533342</td>\n",
       "      <td>152.167692</td>\n",
       "      <td>144.491181</td>\n",
       "      <td>...</td>\n",
       "      <td>124.840993</td>\n",
       "      <td>154.881355</td>\n",
       "      <td>129.234448</td>\n",
       "      <td>110.663062</td>\n",
       "      <td>193.894392</td>\n",
       "      <td>184.388362</td>\n",
       "      <td>181.968458</td>\n",
       "      <td>177.496974</td>\n",
       "      <td>121.539625</td>\n",
       "      <td>87.932897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>179.865005</td>\n",
       "      <td>159.415279</td>\n",
       "      <td>123.315088</td>\n",
       "      <td>80.356934</td>\n",
       "      <td>226.317654</td>\n",
       "      <td>215.812200</td>\n",
       "      <td>185.093114</td>\n",
       "      <td>175.015502</td>\n",
       "      <td>161.770560</td>\n",
       "      <td>143.224881</td>\n",
       "      <td>...</td>\n",
       "      <td>119.455255</td>\n",
       "      <td>156.300451</td>\n",
       "      <td>131.414059</td>\n",
       "      <td>116.108234</td>\n",
       "      <td>193.147585</td>\n",
       "      <td>181.736531</td>\n",
       "      <td>183.386660</td>\n",
       "      <td>165.651099</td>\n",
       "      <td>129.228967</td>\n",
       "      <td>81.105373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>176.406070</td>\n",
       "      <td>160.883552</td>\n",
       "      <td>123.303897</td>\n",
       "      <td>90.376887</td>\n",
       "      <td>229.966006</td>\n",
       "      <td>218.518262</td>\n",
       "      <td>186.131661</td>\n",
       "      <td>172.264626</td>\n",
       "      <td>158.317146</td>\n",
       "      <td>144.987837</td>\n",
       "      <td>...</td>\n",
       "      <td>125.298911</td>\n",
       "      <td>162.548268</td>\n",
       "      <td>134.958068</td>\n",
       "      <td>112.415088</td>\n",
       "      <td>196.695747</td>\n",
       "      <td>186.109119</td>\n",
       "      <td>191.853005</td>\n",
       "      <td>173.257644</td>\n",
       "      <td>123.993322</td>\n",
       "      <td>90.839897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     159.151730  199.542510   93.204352  141.610133  208.230033  220.674816   \n",
       "1     166.398547  184.865378  100.886692  142.019092  200.934261  231.777862   \n",
       "2     168.699960  195.483494  102.242983  151.909099  206.502507  224.945880   \n",
       "3     165.108359  195.489980   95.088657  146.039074  194.145214  233.067175   \n",
       "4     168.168172  185.840023   99.153895  139.952871  194.029188  225.089917   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  191.446233  163.863018  120.196717   81.691845  232.944152  213.767370   \n",
       "2439  192.210071  169.472938  126.408295   80.555287  226.421779  212.131048   \n",
       "2440  182.613814  174.438416  116.411536   91.348961  233.275747  210.510062   \n",
       "2441  179.865005  159.415279  123.315088   80.356934  226.317654  215.812200   \n",
       "2442  176.406070  160.883552  123.303897   90.376887  229.966006  218.518262   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     158.863186  189.306563  144.184843  160.406591  ...  125.388714   \n",
       "1     156.058008  203.353192  140.903362  165.565921  ...  131.875666   \n",
       "2     161.279681  183.895077  145.834056  158.360219  ...  126.723309   \n",
       "3     159.540317  187.301984  135.145034  168.508921  ...  120.952913   \n",
       "4     155.754544  188.479389  141.177966  170.835688  ...  117.921545   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  188.466104  177.347899  156.237599  139.888740  ...  121.416400   \n",
       "2439  189.024391  161.463544  146.955323  136.197913  ...  124.351245   \n",
       "2440  189.619989  163.533342  152.167692  144.491181  ...  124.840993   \n",
       "2441  185.093114  175.015502  161.770560  143.224881  ...  119.455255   \n",
       "2442  186.131661  172.264626  158.317146  144.987837  ...  125.298911   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     134.794757  137.019077  133.083826  188.991982  178.155730  161.186367   \n",
       "1     144.307763  144.125796  127.552055  183.965512  179.206473  153.405338   \n",
       "2     138.780776  128.601055  130.194494  181.584965  176.214339  163.883558   \n",
       "3     134.810703  133.864237  144.482189  181.923756  181.988992  158.399493   \n",
       "4     139.772629  138.342093  130.464550  192.744036  183.586504  153.567238   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  158.272110  135.787415  110.038074  204.484919  181.772215  183.221858   \n",
       "2439  155.065263  138.527748  112.555788  199.509706  186.175599  183.299073   \n",
       "2440  154.881355  129.234448  110.663062  193.894392  184.388362  181.968458   \n",
       "2441  156.300451  131.414059  116.108234  193.147585  181.736531  183.386660   \n",
       "2442  162.548268  134.958068  112.415088  196.695747  186.109119  191.853005   \n",
       "\n",
       "              45          46          47  \n",
       "0     192.737363   73.724932  138.491142  \n",
       "1     192.374917   83.476169  127.547566  \n",
       "2     187.026603   74.422808  145.994348  \n",
       "3     187.715639   83.992892  138.966512  \n",
       "4     188.398368   79.339979  133.595655  \n",
       "...          ...         ...         ...  \n",
       "2438  165.974294  129.016781   91.163284  \n",
       "2439  175.290848  130.406301   90.511453  \n",
       "2440  177.496974  121.539625   87.932897  \n",
       "2441  165.651099  129.228967   81.105373  \n",
       "2442  173.257644  123.993322   90.839897  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159.151730</td>\n",
       "      <td>199.542510</td>\n",
       "      <td>93.204352</td>\n",
       "      <td>141.610133</td>\n",
       "      <td>208.230033</td>\n",
       "      <td>220.674816</td>\n",
       "      <td>158.863186</td>\n",
       "      <td>189.306563</td>\n",
       "      <td>144.184843</td>\n",
       "      <td>160.406591</td>\n",
       "      <td>...</td>\n",
       "      <td>125.388714</td>\n",
       "      <td>134.794757</td>\n",
       "      <td>137.019077</td>\n",
       "      <td>133.083826</td>\n",
       "      <td>188.991982</td>\n",
       "      <td>178.155730</td>\n",
       "      <td>161.186367</td>\n",
       "      <td>192.737363</td>\n",
       "      <td>73.724932</td>\n",
       "      <td>138.491142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.398547</td>\n",
       "      <td>184.865378</td>\n",
       "      <td>100.886692</td>\n",
       "      <td>142.019092</td>\n",
       "      <td>200.934261</td>\n",
       "      <td>231.777862</td>\n",
       "      <td>156.058008</td>\n",
       "      <td>203.353192</td>\n",
       "      <td>140.903362</td>\n",
       "      <td>165.565921</td>\n",
       "      <td>...</td>\n",
       "      <td>131.875666</td>\n",
       "      <td>144.307763</td>\n",
       "      <td>144.125796</td>\n",
       "      <td>127.552055</td>\n",
       "      <td>183.965512</td>\n",
       "      <td>179.206473</td>\n",
       "      <td>153.405338</td>\n",
       "      <td>192.374917</td>\n",
       "      <td>83.476169</td>\n",
       "      <td>127.547566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168.699960</td>\n",
       "      <td>195.483494</td>\n",
       "      <td>102.242983</td>\n",
       "      <td>151.909099</td>\n",
       "      <td>206.502507</td>\n",
       "      <td>224.945880</td>\n",
       "      <td>161.279681</td>\n",
       "      <td>183.895077</td>\n",
       "      <td>145.834056</td>\n",
       "      <td>158.360219</td>\n",
       "      <td>...</td>\n",
       "      <td>126.723309</td>\n",
       "      <td>138.780776</td>\n",
       "      <td>128.601055</td>\n",
       "      <td>130.194494</td>\n",
       "      <td>181.584965</td>\n",
       "      <td>176.214339</td>\n",
       "      <td>163.883558</td>\n",
       "      <td>187.026603</td>\n",
       "      <td>74.422808</td>\n",
       "      <td>145.994348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.108359</td>\n",
       "      <td>195.489980</td>\n",
       "      <td>95.088657</td>\n",
       "      <td>146.039074</td>\n",
       "      <td>194.145214</td>\n",
       "      <td>233.067175</td>\n",
       "      <td>159.540317</td>\n",
       "      <td>187.301984</td>\n",
       "      <td>135.145034</td>\n",
       "      <td>168.508921</td>\n",
       "      <td>...</td>\n",
       "      <td>120.952913</td>\n",
       "      <td>134.810703</td>\n",
       "      <td>133.864237</td>\n",
       "      <td>144.482189</td>\n",
       "      <td>181.923756</td>\n",
       "      <td>181.988992</td>\n",
       "      <td>158.399493</td>\n",
       "      <td>187.715639</td>\n",
       "      <td>83.992892</td>\n",
       "      <td>138.966512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168.168172</td>\n",
       "      <td>185.840023</td>\n",
       "      <td>99.153895</td>\n",
       "      <td>139.952871</td>\n",
       "      <td>194.029188</td>\n",
       "      <td>225.089917</td>\n",
       "      <td>155.754544</td>\n",
       "      <td>188.479389</td>\n",
       "      <td>141.177966</td>\n",
       "      <td>170.835688</td>\n",
       "      <td>...</td>\n",
       "      <td>117.921545</td>\n",
       "      <td>139.772629</td>\n",
       "      <td>138.342093</td>\n",
       "      <td>130.464550</td>\n",
       "      <td>192.744036</td>\n",
       "      <td>183.586504</td>\n",
       "      <td>153.567238</td>\n",
       "      <td>188.398368</td>\n",
       "      <td>79.339979</td>\n",
       "      <td>133.595655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>191.446233</td>\n",
       "      <td>163.863018</td>\n",
       "      <td>120.196717</td>\n",
       "      <td>81.691845</td>\n",
       "      <td>232.944152</td>\n",
       "      <td>213.767370</td>\n",
       "      <td>188.466104</td>\n",
       "      <td>177.347899</td>\n",
       "      <td>156.237599</td>\n",
       "      <td>139.888740</td>\n",
       "      <td>...</td>\n",
       "      <td>121.416400</td>\n",
       "      <td>158.272110</td>\n",
       "      <td>135.787415</td>\n",
       "      <td>110.038074</td>\n",
       "      <td>204.484919</td>\n",
       "      <td>181.772215</td>\n",
       "      <td>183.221858</td>\n",
       "      <td>165.974294</td>\n",
       "      <td>129.016781</td>\n",
       "      <td>91.163284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>192.210071</td>\n",
       "      <td>169.472938</td>\n",
       "      <td>126.408295</td>\n",
       "      <td>80.555287</td>\n",
       "      <td>226.421779</td>\n",
       "      <td>212.131048</td>\n",
       "      <td>189.024391</td>\n",
       "      <td>161.463544</td>\n",
       "      <td>146.955323</td>\n",
       "      <td>136.197913</td>\n",
       "      <td>...</td>\n",
       "      <td>124.351245</td>\n",
       "      <td>155.065263</td>\n",
       "      <td>138.527748</td>\n",
       "      <td>112.555788</td>\n",
       "      <td>199.509706</td>\n",
       "      <td>186.175599</td>\n",
       "      <td>183.299073</td>\n",
       "      <td>175.290848</td>\n",
       "      <td>130.406301</td>\n",
       "      <td>90.511453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>182.613814</td>\n",
       "      <td>174.438416</td>\n",
       "      <td>116.411536</td>\n",
       "      <td>91.348961</td>\n",
       "      <td>233.275747</td>\n",
       "      <td>210.510062</td>\n",
       "      <td>189.619989</td>\n",
       "      <td>163.533342</td>\n",
       "      <td>152.167692</td>\n",
       "      <td>144.491181</td>\n",
       "      <td>...</td>\n",
       "      <td>124.840993</td>\n",
       "      <td>154.881355</td>\n",
       "      <td>129.234448</td>\n",
       "      <td>110.663062</td>\n",
       "      <td>193.894392</td>\n",
       "      <td>184.388362</td>\n",
       "      <td>181.968458</td>\n",
       "      <td>177.496974</td>\n",
       "      <td>121.539625</td>\n",
       "      <td>87.932897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>179.865005</td>\n",
       "      <td>159.415279</td>\n",
       "      <td>123.315088</td>\n",
       "      <td>80.356934</td>\n",
       "      <td>226.317654</td>\n",
       "      <td>215.812200</td>\n",
       "      <td>185.093114</td>\n",
       "      <td>175.015502</td>\n",
       "      <td>161.770560</td>\n",
       "      <td>143.224881</td>\n",
       "      <td>...</td>\n",
       "      <td>119.455255</td>\n",
       "      <td>156.300451</td>\n",
       "      <td>131.414059</td>\n",
       "      <td>116.108234</td>\n",
       "      <td>193.147585</td>\n",
       "      <td>181.736531</td>\n",
       "      <td>183.386660</td>\n",
       "      <td>165.651099</td>\n",
       "      <td>129.228967</td>\n",
       "      <td>81.105373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>176.406070</td>\n",
       "      <td>160.883552</td>\n",
       "      <td>123.303897</td>\n",
       "      <td>90.376887</td>\n",
       "      <td>229.966006</td>\n",
       "      <td>218.518262</td>\n",
       "      <td>186.131661</td>\n",
       "      <td>172.264626</td>\n",
       "      <td>158.317146</td>\n",
       "      <td>144.987837</td>\n",
       "      <td>...</td>\n",
       "      <td>125.298911</td>\n",
       "      <td>162.548268</td>\n",
       "      <td>134.958068</td>\n",
       "      <td>112.415088</td>\n",
       "      <td>196.695747</td>\n",
       "      <td>186.109119</td>\n",
       "      <td>191.853005</td>\n",
       "      <td>173.257644</td>\n",
       "      <td>123.993322</td>\n",
       "      <td>90.839897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     159.151730  199.542510   93.204352  141.610133  208.230033  220.674816   \n",
       "1     166.398547  184.865378  100.886692  142.019092  200.934261  231.777862   \n",
       "2     168.699960  195.483494  102.242983  151.909099  206.502507  224.945880   \n",
       "3     165.108359  195.489980   95.088657  146.039074  194.145214  233.067175   \n",
       "4     168.168172  185.840023   99.153895  139.952871  194.029188  225.089917   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  191.446233  163.863018  120.196717   81.691845  232.944152  213.767370   \n",
       "2439  192.210071  169.472938  126.408295   80.555287  226.421779  212.131048   \n",
       "2440  182.613814  174.438416  116.411536   91.348961  233.275747  210.510062   \n",
       "2441  179.865005  159.415279  123.315088   80.356934  226.317654  215.812200   \n",
       "2442  176.406070  160.883552  123.303897   90.376887  229.966006  218.518262   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     158.863186  189.306563  144.184843  160.406591  ...  125.388714   \n",
       "1     156.058008  203.353192  140.903362  165.565921  ...  131.875666   \n",
       "2     161.279681  183.895077  145.834056  158.360219  ...  126.723309   \n",
       "3     159.540317  187.301984  135.145034  168.508921  ...  120.952913   \n",
       "4     155.754544  188.479389  141.177966  170.835688  ...  117.921545   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  188.466104  177.347899  156.237599  139.888740  ...  121.416400   \n",
       "2439  189.024391  161.463544  146.955323  136.197913  ...  124.351245   \n",
       "2440  189.619989  163.533342  152.167692  144.491181  ...  124.840993   \n",
       "2441  185.093114  175.015502  161.770560  143.224881  ...  119.455255   \n",
       "2442  186.131661  172.264626  158.317146  144.987837  ...  125.298911   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     134.794757  137.019077  133.083826  188.991982  178.155730  161.186367   \n",
       "1     144.307763  144.125796  127.552055  183.965512  179.206473  153.405338   \n",
       "2     138.780776  128.601055  130.194494  181.584965  176.214339  163.883558   \n",
       "3     134.810703  133.864237  144.482189  181.923756  181.988992  158.399493   \n",
       "4     139.772629  138.342093  130.464550  192.744036  183.586504  153.567238   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  158.272110  135.787415  110.038074  204.484919  181.772215  183.221858   \n",
       "2439  155.065263  138.527748  112.555788  199.509706  186.175599  183.299073   \n",
       "2440  154.881355  129.234448  110.663062  193.894392  184.388362  181.968458   \n",
       "2441  156.300451  131.414059  116.108234  193.147585  181.736531  183.386660   \n",
       "2442  162.548268  134.958068  112.415088  196.695747  186.109119  191.853005   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     192.737363   73.724932  138.491142  \n",
       "1     192.374917   83.476169  127.547566  \n",
       "2     187.026603   74.422808  145.994348  \n",
       "3     187.715639   83.992892  138.966512  \n",
       "4     188.398368   79.339979  133.595655  \n",
       "...          ...         ...         ...  \n",
       "2438  165.974294  129.016781   91.163284  \n",
       "2439  175.290848  130.406301   90.511453  \n",
       "2440  177.496974  121.539625   87.932897  \n",
       "2441  165.651099  129.228967   81.105373  \n",
       "2442  173.257644  123.993322   90.839897  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 18s 20ms/step - loss: 1416.1000 - val_loss: 1286.3234\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1242.2191 - val_loss: 1180.9828\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1150.8405 - val_loss: 1101.4777\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1081.0861 - val_loss: 1042.8564\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1030.1652 - val_loss: 999.3762\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 992.6408 - val_loss: 967.5541\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 965.8292 - val_loss: 945.3695\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 947.3976 - val_loss: 930.3527\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 935.4987 - val_loss: 921.1873\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.3818 - val_loss: 915.7631\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.5209 - val_loss: 913.0859\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.6157 - val_loss: 911.7424\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.8032 - val_loss: 911.2075\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.4743 - val_loss: 911.0804\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.3799 - val_loss: 911.0297\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.3963 - val_loss: 910.9808\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7420 - val_loss: 910.9264\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.4768 - val_loss: 910.8909\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.3882 - val_loss: 910.9021\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.4154 - val_loss: 910.9141\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 921.3655 - val_loss: 910.9626\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 921.3804 - val_loss: 910.9533\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.3666 - val_loss: 910.9604\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.8127 - val_loss: 895.7283\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 885.0600 - val_loss: 853.8430\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 851.4207 - val_loss: 829.7310\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 831.0253 - val_loss: 802.5516\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 785.8973 - val_loss: 747.0483\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 726.3821 - val_loss: 687.8933\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 667.1766 - val_loss: 637.7375\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 613.3712 - val_loss: 582.8157\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 560.8740 - val_loss: 531.8181\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 514.0862 - val_loss: 483.2878\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 469.1884 - val_loss: 441.0062\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 428.5396 - val_loss: 408.1653\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 390.5178 - val_loss: 368.5699\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 354.6786 - val_loss: 335.5909\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 315.6454 - val_loss: 298.0314\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 283.0641 - val_loss: 265.2713\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 253.0509 - val_loss: 233.4691\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 226.0014 - val_loss: 208.8773\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 200.6779 - val_loss: 185.0509\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 178.7985 - val_loss: 164.7323\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 157.2357 - val_loss: 146.2269\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 137.7928 - val_loss: 126.3811\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 119.6803 - val_loss: 110.3717\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 105.0945 - val_loss: 95.8850\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 91.4655 - val_loss: 82.4548\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 78.8233 - val_loss: 74.7583\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 69.2729 - val_loss: 62.8406\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 59.4623 - val_loss: 54.1693\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 51.7999 - val_loss: 48.4015\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 44.7219 - val_loss: 41.6234\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 40.9410 - val_loss: 39.0758\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.0687 - val_loss: 31.1523\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.1973 - val_loss: 29.0928\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.7486 - val_loss: 23.2262\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9403 - val_loss: 23.2269\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.1805 - val_loss: 19.3057\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.5906 - val_loss: 18.0378\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7288 - val_loss: 14.8681\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.4258 - val_loss: 13.6126\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6070 - val_loss: 13.3944\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.2739 - val_loss: 12.0060\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0393 - val_loss: 10.0931\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.8053 - val_loss: 9.9182\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6848 - val_loss: 9.3718\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6101 - val_loss: 8.1137\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5966 - val_loss: 7.6822\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1724 - val_loss: 8.9541\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.8281 - val_loss: 6.9122\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2636 - val_loss: 9.0135\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7897 - val_loss: 6.7051\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7935 - val_loss: 6.1660\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7119 - val_loss: 5.8057\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3335 - val_loss: 5.6913\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3046 - val_loss: 5.7053\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5141 - val_loss: 7.1478\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8472 - val_loss: 5.1105\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1864 - val_loss: 5.8807\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9874 - val_loss: 6.4059\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7227 - val_loss: 5.0458\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7867 - val_loss: 4.7904\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6711 - val_loss: 6.1165\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2869 - val_loss: 4.5129\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4823 - val_loss: 4.2028\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1081 - val_loss: 8.0111\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8060 - val_loss: 4.9928\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5888 - val_loss: 5.0920\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5947 - val_loss: 4.7501\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9084 - val_loss: 7.2311\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1827 - val_loss: 3.7566\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9928 - val_loss: 4.7122\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3081 - val_loss: 4.3005\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1349 - val_loss: 4.3838\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3857 - val_loss: 4.0734\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7759 - val_loss: 6.7419\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1445 - val_loss: 3.7664\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2248 - val_loss: 5.1280\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5520 - val_loss: 3.6182\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5533 - val_loss: 3.6572\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7793 - val_loss: 3.5894\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5112 - val_loss: 3.5698\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8720 - val_loss: 7.4413\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2889 - val_loss: 4.1827\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6954 - val_loss: 3.5030\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7010 - val_loss: 3.4230\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6911 - val_loss: 3.7171\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7332 - val_loss: 4.1360\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.9156 - val_loss: 3.1286\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7299 - val_loss: 3.3748\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7318 - val_loss: 3.8617\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.6619 - val_loss: 3.6397\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7588 - val_loss: 4.5969\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2351 - val_loss: 3.7318\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9960 - val_loss: 2.9812\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4490 - val_loss: 3.3582\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2908 - val_loss: 3.1677\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0487 - val_loss: 3.5169\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1979 - val_loss: 3.1018\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0897 - val_loss: 3.0419\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.1782 - val_loss: 2.8932\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6699 - val_loss: 4.8741\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1974 - val_loss: 3.5036\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3378 - val_loss: 3.4159\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0966 - val_loss: 3.0966\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0111 - val_loss: 2.7497\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9632 - val_loss: 2.8241\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1597 - val_loss: 3.0488\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.6920 - val_loss: 7.0615\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.9379 - val_loss: 2.8665\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3212 - val_loss: 3.3114\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0309 - val_loss: 3.2694\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1609 - val_loss: 3.5459\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5898 - val_loss: 3.5862\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2000 - val_loss: 4.1797\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0215 - val_loss: 2.9402\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0299 - val_loss: 2.8733\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8381 - val_loss: 2.9372\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8292 - val_loss: 2.7841\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6710 - val_loss: 3.0525\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8169 - val_loss: 3.0902\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0073 - val_loss: 3.2264\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7964 - val_loss: 2.8509\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8815 - val_loss: 28.3560\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8788 - val_loss: 2.9338\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8080 - val_loss: 3.2422\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6940 - val_loss: 2.7555\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6301 - val_loss: 2.5890\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.7578 - val_loss: 2.8618\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8069 - val_loss: 2.8347\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3686 - val_loss: 2.7778\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7114 - val_loss: 2.7191\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6541 - val_loss: 2.7105\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8616 - val_loss: 2.9843\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7711 - val_loss: 3.3419\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8380 - val_loss: 2.9460\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7759 - val_loss: 3.4780\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0343 - val_loss: 3.6388\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3074 - val_loss: 5.8530\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7072 - val_loss: 2.7219\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5589 - val_loss: 2.6712\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5159 - val_loss: 3.1978\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0572 - val_loss: 2.6812\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6534 - val_loss: 3.5761\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2840 - val_loss: 14.0526\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.4004 - val_loss: 2.7298\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4754 - val_loss: 2.7522\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5155 - val_loss: 2.9608\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5267 - val_loss: 2.6677\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4184 - val_loss: 2.8054\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.5247 - val_loss: 3.2243\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.5197 - val_loss: 3.2477\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2096 - val_loss: 3.4824\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7311 - val_loss: 2.4252\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3351 - val_loss: 2.9093\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3601 - val_loss: 2.8814\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2814 - val_loss: 3.4902\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3670 - val_loss: 2.9468\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4592 - val_loss: 2.6985\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6222 - val_loss: 3.0935\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5167 - val_loss: 2.4958\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5834 - val_loss: 2.7901\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5220 - val_loss: 3.2060\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4877 - val_loss: 2.9942\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6538 - val_loss: 43.7942\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3418 - val_loss: 2.8083\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4809 - val_loss: 2.5846\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2815 - val_loss: 2.5451\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.1824 - val_loss: 3.4431\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2653 - val_loss: 2.3445\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4065 - val_loss: 2.6881\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2711 - val_loss: 2.6654\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8311 - val_loss: 15.5668\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6830 - val_loss: 3.2068\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7424 - val_loss: 2.6000\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2229 - val_loss: 2.4262\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1411 - val_loss: 2.6951\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1338 - val_loss: 2.4703\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0886 - val_loss: 2.6443\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 2.644318897086657\n",
      "Mean Absolute Error (MAE): 1.1001585466312718\n",
      "Root Mean Squared Error (RMSE): 1.6261361865128816\n",
      "Time taken: 1253.3001096248627\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1376.7810 - val_loss: 1222.0608\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1216.1849 - val_loss: 1121.9813\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1131.0380 - val_loss: 1049.7899\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1068.0938 - val_loss: 996.1417\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1021.6771 - val_loss: 956.6918\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 988.1208 - val_loss: 928.5696\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 964.1385 - val_loss: 908.7343\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 949.2485 - val_loss: 897.7254\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 940.0058 - val_loss: 889.7433\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 931.0226 - val_loss: 881.3868\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 909.1570 - val_loss: 846.6995\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 875.6857 - val_loss: 820.9195\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 852.8920 - val_loss: 800.7588\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 829.4301 - val_loss: 779.2805\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 808.8251 - val_loss: 761.4604\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 788.3520 - val_loss: 744.3476\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 771.2476 - val_loss: 725.3016\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 745.0190 - val_loss: 692.2592\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 701.2050 - val_loss: 638.7374\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 647.7026 - val_loss: 591.2658\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 594.2632 - val_loss: 533.8342\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 542.6411 - val_loss: 494.7881\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 501.8775 - val_loss: 448.3370\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 459.6395 - val_loss: 412.0416\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 417.6383 - val_loss: 377.1613\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 381.3611 - val_loss: 341.4946\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 346.3294 - val_loss: 311.7987\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 313.0789 - val_loss: 277.0775\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 279.2288 - val_loss: 245.9792\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 248.3484 - val_loss: 227.0351\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 214.6946 - val_loss: 180.7412\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 185.8178 - val_loss: 158.8963\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 165.2723 - val_loss: 141.3505\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 146.2857 - val_loss: 116.6742\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 121.5654 - val_loss: 102.6145\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 104.0716 - val_loss: 87.8259\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 90.3315 - val_loss: 80.3843\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 80.1614 - val_loss: 78.0924\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 67.9770 - val_loss: 54.7539\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.4168 - val_loss: 47.7001\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.3029 - val_loss: 40.2050\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.4016 - val_loss: 37.3508\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.4317 - val_loss: 31.4449\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.2574 - val_loss: 27.2460\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.7671 - val_loss: 24.9906\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.9531 - val_loss: 19.4740\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.0298 - val_loss: 17.1072\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.0756 - val_loss: 24.6513\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.2519 - val_loss: 15.2450\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.4475 - val_loss: 17.4690\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.3119 - val_loss: 13.1152\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.2545 - val_loss: 13.9564\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.6870 - val_loss: 11.6309\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0896 - val_loss: 9.7222\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.1016 - val_loss: 11.1098\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0660 - val_loss: 7.4936\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3520 - val_loss: 9.7584\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2713 - val_loss: 11.2543\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3947 - val_loss: 9.5142\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.4929 - val_loss: 9.0230\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5967 - val_loss: 8.5056\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2619 - val_loss: 8.1313\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7456 - val_loss: 9.0429\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.0539 - val_loss: 7.0027\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4535 - val_loss: 6.1060\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2445 - val_loss: 5.5692\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.2548 - val_loss: 14.2835\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5465 - val_loss: 6.2872\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2115 - val_loss: 7.9658\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9171 - val_loss: 6.3369\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5121 - val_loss: 5.5395\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.1746 - val_loss: 5.6029\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2603 - val_loss: 7.3599\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.1283 - val_loss: 5.7808\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8232 - val_loss: 5.4424\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5807 - val_loss: 5.5793\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0244 - val_loss: 8.1523\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3243 - val_loss: 5.0143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6625 - val_loss: 5.6487\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8243 - val_loss: 5.2398\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9882 - val_loss: 5.7174\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5843 - val_loss: 4.8745\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.0300 - val_loss: 6.0712\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.6920 - val_loss: 4.7996\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.5665 - val_loss: 4.6780\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.2591 - val_loss: 5.9986\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9705 - val_loss: 7.4125\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2545 - val_loss: 4.7560\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4043 - val_loss: 21.5103\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3574 - val_loss: 4.8114\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8278 - val_loss: 9.0390\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0107 - val_loss: 5.4801\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8516 - val_loss: 5.4050\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5102 - val_loss: 3.9513\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9158 - val_loss: 5.6385\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2074 - val_loss: 7.0259\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6986 - val_loss: 3.9071\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5459 - val_loss: 4.3748\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3190 - val_loss: 7.2929\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5251 - val_loss: 4.0820\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3853 - val_loss: 4.2501\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2664 - val_loss: 6.3832\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3329 - val_loss: 6.0883\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5514 - val_loss: 6.5731\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2714 - val_loss: 3.4405\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.0032 - val_loss: 3.5358\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0405 - val_loss: 6.6573\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4603 - val_loss: 4.0821\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5518 - val_loss: 5.4969\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4337 - val_loss: 3.6550\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6400 - val_loss: 3.0597\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8895 - val_loss: 3.7028\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6649 - val_loss: 3.8049\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1801 - val_loss: 3.4716\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3474 - val_loss: 3.9890\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6635 - val_loss: 5.8637\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7629 - val_loss: 3.2939\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1760 - val_loss: 3.5033\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3625 - val_loss: 4.3158\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9033 - val_loss: 6.5099\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0511 - val_loss: 3.3250\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6714 - val_loss: 3.2852\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.3752 - val_loss: 5.2133\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9774 - val_loss: 6.2090\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7506 - val_loss: 3.4415\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5817 - val_loss: 3.1059\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5481 - val_loss: 3.7500\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7773 - val_loss: 8.1230\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0566 - val_loss: 6.8748\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3738 - val_loss: 3.7842\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9420 - val_loss: 3.1577\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4210 - val_loss: 3.7230\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4127 - val_loss: 3.0979\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2948 - val_loss: 3.9323\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4576 - val_loss: 2.9502\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7221 - val_loss: 3.8659\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3543 - val_loss: 3.9035\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.7288 - val_loss: 4.4954\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4448 - val_loss: 4.8401\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5965 - val_loss: 3.8432\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3367 - val_loss: 12.6977\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5036 - val_loss: 3.5527\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4153 - val_loss: 3.0440\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1813 - val_loss: 2.8610\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0814 - val_loss: 3.8991\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5937 - val_loss: 3.0324\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2616 - val_loss: 3.0002\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4674 - val_loss: 3.2798\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2436 - val_loss: 9.1885\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3937 - val_loss: 4.2296\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2534 - val_loss: 3.5717\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1272 - val_loss: 3.1713\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2280 - val_loss: 3.1742\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0193 - val_loss: 2.9302\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4739 - val_loss: 3.4928\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5141 - val_loss: 4.8422\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1119 - val_loss: 3.0171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2019 - val_loss: 4.8027\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5125 - val_loss: 3.6422\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0123 - val_loss: 4.2031\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2401 - val_loss: 3.8617\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8328 - val_loss: 3.4443\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2587 - val_loss: 3.3445\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9416 - val_loss: 2.9882\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0521 - val_loss: 3.2174\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9960 - val_loss: 2.9085\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0776 - val_loss: 4.2200\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0677 - val_loss: 3.1734\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1407 - val_loss: 3.5624\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2644 - val_loss: 3.2245\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8792 - val_loss: 5.2774\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4458 - val_loss: 2.9062\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7285 - val_loss: 2.8127\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0129 - val_loss: 2.9450\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9298 - val_loss: 2.8561\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8562 - val_loss: 2.8397\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7647 - val_loss: 3.5838\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7885 - val_loss: 3.7225\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.2698 - val_loss: 2.9064\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.7106 - val_loss: 3.1655\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.8772 - val_loss: 4.3401\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1.7775 - val_loss: 2.9678\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7625 - val_loss: 3.6565\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.7737 - val_loss: 3.4819\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 3.3728 - val_loss: 3.6439\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9335 - val_loss: 3.2009\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6738 - val_loss: 2.6593\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8882 - val_loss: 3.1088\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7361 - val_loss: 3.9884\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5905 - val_loss: 2.6070\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7530 - val_loss: 4.5332\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1215 - val_loss: 4.3675\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6269 - val_loss: 3.3388\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1767 - val_loss: 13.7355\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4646 - val_loss: 7.3173\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0352 - val_loss: 3.8448\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9572 - val_loss: 3.1377\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5508 - val_loss: 2.5578\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3720 - val_loss: 2.6962\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4340 - val_loss: 2.6310\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 2.6310495022840477\n",
      "Mean Absolute Error (MAE): 1.0753704129605175\n",
      "Root Mean Squared Error (RMSE): 1.6220510171643947\n",
      "Time taken: 1243.3597388267517\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 20ms/step - loss: 1368.7241 - val_loss: 1257.5917\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1206.1012 - val_loss: 1157.9000\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1120.1373 - val_loss: 1086.4771\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1056.9503 - val_loss: 1034.4298\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1010.2851 - val_loss: 996.9982\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 976.3174 - val_loss: 970.9876\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 952.1288 - val_loss: 953.5683\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 936.3281 - val_loss: 942.9320\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.5002 - val_loss: 937.2440\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.8716 - val_loss: 934.5755\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.9087 - val_loss: 933.6558\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5731 - val_loss: 933.5182\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.0723 - val_loss: 933.5090\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.8666 - val_loss: 933.6642\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7918 - val_loss: 933.9077\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7651 - val_loss: 933.8792\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7410 - val_loss: 934.0803\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7357 - val_loss: 934.1313\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7535 - val_loss: 934.1954\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7715 - val_loss: 934.1736\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7556 - val_loss: 934.0717\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7382 - val_loss: 934.2968\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.2409 - val_loss: 932.3688\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 911.2516 - val_loss: 933.9373\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 880.2590 - val_loss: 868.0349\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 829.8556 - val_loss: 832.9741\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 788.3190 - val_loss: 786.7341\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 749.1522 - val_loss: 742.6883\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 692.8392 - val_loss: 679.8314\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 628.7369 - val_loss: 609.3728\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 563.7239 - val_loss: 544.6540\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 506.9058 - val_loss: 494.9675\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 461.4503 - val_loss: 453.5409\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 418.9942 - val_loss: 410.2079\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 381.6048 - val_loss: 373.6007\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 343.9900 - val_loss: 342.1190\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 310.2568 - val_loss: 305.9745\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 277.8938 - val_loss: 273.4407\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 249.2027 - val_loss: 245.5465\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 221.2207 - val_loss: 217.8468\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 196.3867 - val_loss: 192.6106\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 174.3711 - val_loss: 170.5810\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 152.7805 - val_loss: 151.4558\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 133.9294 - val_loss: 133.2844\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 117.1244 - val_loss: 118.3281\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 102.0950 - val_loss: 101.8924\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.2648 - val_loss: 89.9821\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.8174 - val_loss: 76.2437\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.0000 - val_loss: 66.4347\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.3275 - val_loss: 62.2921\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.9618 - val_loss: 49.7470\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.5861 - val_loss: 42.6884\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 37.6336 - val_loss: 38.6309\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.1173 - val_loss: 36.3174\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.6470 - val_loss: 29.6597\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.3804 - val_loss: 30.7403\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.9722 - val_loss: 22.3595\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.1457 - val_loss: 19.8357\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.1802 - val_loss: 17.0374\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.3241 - val_loss: 16.2285\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.1010 - val_loss: 13.3116\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.1731 - val_loss: 11.7078\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.9234 - val_loss: 10.8213\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7724 - val_loss: 9.2741\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9692 - val_loss: 11.8212\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4291 - val_loss: 8.3084\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4065 - val_loss: 14.5021\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3548 - val_loss: 8.0126\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3635 - val_loss: 6.5984\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6790 - val_loss: 7.1088\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7811 - val_loss: 7.0489\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.3227 - val_loss: 6.7128\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 5.6401 - val_loss: 6.5713\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6051 - val_loss: 7.1739\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0978 - val_loss: 5.0321\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4301 - val_loss: 4.4021\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.8509 - val_loss: 5.3287\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 4.2752 - val_loss: 8.1029\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.2141 - val_loss: 3.9006\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1579 - val_loss: 5.4854\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0499 - val_loss: 5.1703\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6608 - val_loss: 11.0451\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6426 - val_loss: 5.5297\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4217 - val_loss: 3.8219\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5725 - val_loss: 4.5179\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7648 - val_loss: 6.1421\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6210 - val_loss: 3.6032\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6705 - val_loss: 5.8507\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0972 - val_loss: 3.8470\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2093 - val_loss: 4.2251\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0821 - val_loss: 3.5087\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1349 - val_loss: 3.8791\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0399 - val_loss: 3.7810\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0893 - val_loss: 3.6808\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9618 - val_loss: 5.4833\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3371 - val_loss: 5.1329\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0220 - val_loss: 3.6687\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7307 - val_loss: 3.2595\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5467 - val_loss: 4.3317\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6271 - val_loss: 3.5411\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9052 - val_loss: 3.0464\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5183 - val_loss: 3.5650\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6888 - val_loss: 26.2320\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8258 - val_loss: 3.5466\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.3223 - val_loss: 3.0832\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2640 - val_loss: 3.5566\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3669 - val_loss: 3.5164\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3822 - val_loss: 3.2175\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0368 - val_loss: 8.5785\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8562 - val_loss: 3.4003\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3919 - val_loss: 3.3997\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1996 - val_loss: 2.9121\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1428 - val_loss: 3.2257\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3892 - val_loss: 2.9472\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0818 - val_loss: 3.1106\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3239 - val_loss: 4.4779\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4368 - val_loss: 3.3131\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7473 - val_loss: 4.5250\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9685 - val_loss: 3.7839\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0386 - val_loss: 2.7788\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8354 - val_loss: 2.7592\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9914 - val_loss: 6.1016\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2264 - val_loss: 2.5367\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0462 - val_loss: 3.3514\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9352 - val_loss: 3.3464\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.1398 - val_loss: 3.6945\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5682 - val_loss: 3.2086\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9158 - val_loss: 3.0033\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8751 - val_loss: 2.8350\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8975 - val_loss: 2.8492\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9159 - val_loss: 2.8188\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9151 - val_loss: 3.0317\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8567 - val_loss: 2.7002\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2540 - val_loss: 2.6046\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7668 - val_loss: 3.1916\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7908 - val_loss: 3.9835\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.4596 - val_loss: 2.7945\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6510 - val_loss: 3.0368\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5548 - val_loss: 2.3558\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7170 - val_loss: 4.5226\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6183 - val_loss: 2.5177\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6825 - val_loss: 3.8375\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4035 - val_loss: 2.5758\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6582 - val_loss: 2.3393\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5798 - val_loss: 2.6371\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0123 - val_loss: 4.8644\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5338 - val_loss: 3.1095\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5016 - val_loss: 3.1507\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4647 - val_loss: 3.1047\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4753 - val_loss: 3.3544\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7446 - val_loss: 4.2151\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7928 - val_loss: 2.6706\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8067 - val_loss: 2.6129\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3122 - val_loss: 2.6443\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.4659 - val_loss: 2.5418\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5036 - val_loss: 2.7192\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5741 - val_loss: 2.6333\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.3501 - val_loss: 2.8742\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7231 - val_loss: 3.2170\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5420 - val_loss: 3.2257\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0394 - val_loss: 2.6543\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3424 - val_loss: 2.3215\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2429 - val_loss: 2.7624\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1012 - val_loss: 2.5865\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7742 - val_loss: 7.9884\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4155 - val_loss: 2.3280\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1853 - val_loss: 2.4187\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1712 - val_loss: 2.8292\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2147 - val_loss: 2.8102\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6698 - val_loss: 2.5703\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4606 - val_loss: 2.4212\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3301 - val_loss: 2.9583\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5898 - val_loss: 2.6999\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.6696 - val_loss: 9.7552\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1473 - val_loss: 2.7522\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2170 - val_loss: 2.5914\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.1695 - val_loss: 2.5616\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0634 - val_loss: 2.8439\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4447 - val_loss: 3.3342\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3476 - val_loss: 2.8960\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5302 - val_loss: 3.6383\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3345 - val_loss: 2.8535\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1216 - val_loss: 2.7452\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1927 - val_loss: 2.2606\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1100 - val_loss: 2.4888\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2621 - val_loss: 2.4304\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3312 - val_loss: 2.7857\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2100 - val_loss: 2.6386\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4243 - val_loss: 2.8362\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2055 - val_loss: 2.5773\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9853 - val_loss: 2.1761\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1263 - val_loss: 2.5260\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6437 - val_loss: 3.1707\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1641 - val_loss: 2.5735\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0341 - val_loss: 2.7808\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0104 - val_loss: 2.2849\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1752 - val_loss: 2.4616\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9709 - val_loss: 2.4628\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8444 - val_loss: 7.5172\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7202 - val_loss: 7.8191\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 7.8190778510638\n",
      "Mean Absolute Error (MAE): 1.5539546068133252\n",
      "Root Mean Squared Error (RMSE): 2.796261406067716\n",
      "Time taken: 1240.929451227188\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1354.1658 - val_loss: 1266.3896\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1193.5616 - val_loss: 1170.2600\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1108.8660 - val_loss: 1100.4717\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1046.8582 - val_loss: 1048.9116\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1001.5244 - val_loss: 1011.3965\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 968.2256 - val_loss: 983.4521\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 944.7327 - val_loss: 967.5369\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.7561 - val_loss: 942.7944\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 903.6014 - val_loss: 919.9570\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 867.6470 - val_loss: 873.7658\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 843.5574 - val_loss: 851.2697\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 812.1431 - val_loss: 811.2773\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 766.6168 - val_loss: 758.3361\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 710.1997 - val_loss: 702.2124\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 657.8514 - val_loss: 654.1943\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 612.5535 - val_loss: 608.1949\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 571.4490 - val_loss: 568.7073\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 533.9001 - val_loss: 527.2050\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 491.5455 - val_loss: 483.1864\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 453.3482 - val_loss: 445.2964\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 418.3453 - val_loss: 424.7700\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 384.8225 - val_loss: 378.9836\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 352.2872 - val_loss: 347.5142\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 323.9580 - val_loss: 320.7116\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 295.3409 - val_loss: 296.7024\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 269.8758 - val_loss: 262.3365\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 242.4178 - val_loss: 238.9233\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 218.8303 - val_loss: 212.8532\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 191.7750 - val_loss: 198.0289\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 169.4016 - val_loss: 164.8082\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 147.2498 - val_loss: 147.6319\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 130.3781 - val_loss: 136.9704\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 114.7599 - val_loss: 113.0805\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 99.4228 - val_loss: 100.9764\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 87.6076 - val_loss: 86.9581\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 77.6159 - val_loss: 94.4731\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.0017 - val_loss: 65.0283\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.8162 - val_loss: 60.2264\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 51.1469 - val_loss: 53.2068\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.3577 - val_loss: 43.4890\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.0908 - val_loss: 40.1929\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 34.9271 - val_loss: 36.7560\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.1889 - val_loss: 35.1860\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.6268 - val_loss: 27.4063\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.0235 - val_loss: 29.8808\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.0364 - val_loss: 23.1419\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.6780 - val_loss: 20.1086\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.4927 - val_loss: 19.6530\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.7863 - val_loss: 23.1618\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 15.7158 - val_loss: 18.6250\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.4791 - val_loss: 15.0006\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.4136 - val_loss: 13.0897\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1900 - val_loss: 11.7693\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2485 - val_loss: 10.5351\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6579 - val_loss: 12.5347\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.8621 - val_loss: 12.3529\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7891 - val_loss: 11.7138\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.0019 - val_loss: 11.1515\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2560 - val_loss: 8.4176\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.7503 - val_loss: 7.9855\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3547 - val_loss: 12.9074\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2784 - val_loss: 9.0798\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2912 - val_loss: 7.8587\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7415 - val_loss: 7.2926\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9765 - val_loss: 9.5798\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8195 - val_loss: 9.2033\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8794 - val_loss: 7.4161\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7102 - val_loss: 6.3380\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7427 - val_loss: 8.4647\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.9112 - val_loss: 8.1969\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4160 - val_loss: 8.6737\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5266 - val_loss: 5.9776\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4744 - val_loss: 6.3680\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0703 - val_loss: 7.2008\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5852 - val_loss: 6.3827\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8996 - val_loss: 6.5882\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5339 - val_loss: 6.6363\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.9941 - val_loss: 5.6978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9958 - val_loss: 7.1925\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7071 - val_loss: 6.6568\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8178 - val_loss: 7.3624\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.4222 - val_loss: 5.3549\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1199 - val_loss: 5.5330\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7404 - val_loss: 5.5196\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3296 - val_loss: 4.7182\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.5561 - val_loss: 4.6232\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6300 - val_loss: 4.6569\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.8230 - val_loss: 4.9326\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7988 - val_loss: 6.0325\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.6620 - val_loss: 4.3134\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5533 - val_loss: 4.7104\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5800 - val_loss: 4.2512\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0281 - val_loss: 5.8396\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0609 - val_loss: 4.9194\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4926 - val_loss: 5.7867\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6009 - val_loss: 5.8672\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0515 - val_loss: 8.6537\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0302 - val_loss: 4.9599\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0229 - val_loss: 3.9259\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5869 - val_loss: 3.9091\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3621 - val_loss: 4.2941\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6874 - val_loss: 4.5030\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4566 - val_loss: 3.7194\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.2816 - val_loss: 4.0967\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8388 - val_loss: 6.6184\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9797 - val_loss: 4.9106\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0777 - val_loss: 3.8103\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.8715 - val_loss: 3.8613\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1097 - val_loss: 5.2806\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0374 - val_loss: 4.7362\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9043 - val_loss: 3.6246\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3038 - val_loss: 3.9220\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6715 - val_loss: 4.4224\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4001 - val_loss: 4.1593\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0857 - val_loss: 6.0829\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8659 - val_loss: 3.6307\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9579 - val_loss: 5.0412\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3124 - val_loss: 3.4889\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.0848 - val_loss: 3.9913\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5322 - val_loss: 3.3887\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5704 - val_loss: 3.9524\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5198 - val_loss: 3.3459\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8621 - val_loss: 4.0612\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2934 - val_loss: 7.6187\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7067 - val_loss: 4.5011\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2116 - val_loss: 3.8907\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3895 - val_loss: 3.3268\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.7477 - val_loss: 3.7436\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5693 - val_loss: 4.2574\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4817 - val_loss: 3.5836\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4466 - val_loss: 4.4276\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6082 - val_loss: 3.7713\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.6115 - val_loss: 3.9930\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2703 - val_loss: 3.6116\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1542 - val_loss: 3.2085\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3407 - val_loss: 3.8265\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3893 - val_loss: 3.4663\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1849 - val_loss: 4.3662\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3126 - val_loss: 3.9025\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4756 - val_loss: 2.9185\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.3317 - val_loss: 3.3019\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.6202 - val_loss: 3.2259\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1615 - val_loss: 3.7512\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1712 - val_loss: 3.5018\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3839 - val_loss: 2.8884\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.2946 - val_loss: 3.7847\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9502 - val_loss: 3.9035\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.3841 - val_loss: 3.9822\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.2014 - val_loss: 3.7072\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.0894 - val_loss: 2.9889\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0733 - val_loss: 4.1350\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9694 - val_loss: 3.1541\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.1080 - val_loss: 3.1928\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8795 - val_loss: 3.6335\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8424 - val_loss: 3.2554\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7518 - val_loss: 3.8272\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4742 - val_loss: 13.8711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3750 - val_loss: 2.8207\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7962 - val_loss: 4.7153\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7911 - val_loss: 2.8835\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7410 - val_loss: 3.6304\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0063 - val_loss: 3.8241\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4683 - val_loss: 5.0828\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0499 - val_loss: 2.9120\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.7871 - val_loss: 3.0575\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7299 - val_loss: 3.3086\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8084 - val_loss: 3.3795\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6514 - val_loss: 3.2526\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8616 - val_loss: 3.6181\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.3023 - val_loss: 3.8624\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6979 - val_loss: 3.0181\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4878 - val_loss: 3.6577\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6459 - val_loss: 2.9769\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5724 - val_loss: 3.0349\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7029 - val_loss: 2.8571\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5748 - val_loss: 2.9608\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7180 - val_loss: 3.5666\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1042 - val_loss: 2.9780\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8945 - val_loss: 2.8270\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6517 - val_loss: 3.5919\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8453 - val_loss: 2.8472\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7078 - val_loss: 3.5349\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8475 - val_loss: 4.1928\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0065 - val_loss: 3.1072\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5178 - val_loss: 2.8935\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7924 - val_loss: 3.0777\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9338 - val_loss: 3.4328\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4690 - val_loss: 3.1309\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4578 - val_loss: 3.0705\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4655 - val_loss: 3.1254\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3761 - val_loss: 3.1458\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.3999 - val_loss: 3.0479\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6488 - val_loss: 2.6588\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7305 - val_loss: 3.6596\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 1.4302 - val_loss: 2.6894\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3657 - val_loss: 3.0344\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5724 - val_loss: 2.8562\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3957 - val_loss: 2.7706\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6269 - val_loss: 3.0152\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5238 - val_loss: 3.1045\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 3.104484147956575\n",
      "Mean Absolute Error (MAE): 1.1483821306562987\n",
      "Root Mean Squared Error (RMSE): 1.7619546384503135\n",
      "Time taken: 1168.62930560112\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1371.2440 - val_loss: 1307.2041\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1214.4736 - val_loss: 1203.6188\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1129.7114 - val_loss: 1125.8185\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1065.5923 - val_loss: 1067.2178\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1017.4234 - val_loss: 1022.9393\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 982.0551 - val_loss: 990.6348\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 957.0289 - val_loss: 968.1252\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 940.0705 - val_loss: 952.5153\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 929.4130 - val_loss: 942.7311\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.9744 - val_loss: 936.6760\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.4994 - val_loss: 933.4347\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 917.8260 - val_loss: 931.7456\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.1700 - val_loss: 930.8611\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8950 - val_loss: 930.3825\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.9145 - val_loss: 931.6772\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.1085 - val_loss: 933.4460\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.2598 - val_loss: 932.2330\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.9590 - val_loss: 931.3773\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8376 - val_loss: 930.9911\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.7744 - val_loss: 930.6378\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.7901 - val_loss: 930.5192\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8335 - val_loss: 930.3648\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8072 - val_loss: 930.3696\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 905.0753 - val_loss: 888.1180\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 848.7229 - val_loss: 831.5057\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 788.4691 - val_loss: 770.0330\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 732.1464 - val_loss: 725.3317\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 682.0173 - val_loss: 666.6343\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 630.6323 - val_loss: 617.2852\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 586.4182 - val_loss: 574.0599\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 546.9125 - val_loss: 536.9840\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 517.5933 - val_loss: 509.8445\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 484.4323 - val_loss: 475.5699\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 449.1413 - val_loss: 431.7168\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 404.5204 - val_loss: 389.2292\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 365.1143 - val_loss: 354.4968\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 331.3214 - val_loss: 323.0244\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 297.2633 - val_loss: 286.4397\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 268.9137 - val_loss: 259.8025\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 242.7803 - val_loss: 238.2867\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 218.1390 - val_loss: 209.7484\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 196.1906 - val_loss: 196.9453\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 174.7095 - val_loss: 166.6535\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 156.4568 - val_loss: 155.4370\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 140.1974 - val_loss: 135.9068\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 122.7799 - val_loss: 117.7920\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 108.4012 - val_loss: 106.8379\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 93.5714 - val_loss: 96.0322\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 81.5791 - val_loss: 78.1210\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 71.2990 - val_loss: 68.0281\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.6081 - val_loss: 60.3829\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.4735 - val_loss: 53.2536\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 48.3643 - val_loss: 47.4652\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 43.2685 - val_loss: 41.7736\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 38.5361 - val_loss: 36.3519\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 34.2005 - val_loss: 32.7440\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 30.3939 - val_loss: 29.4836\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 27.0391 - val_loss: 26.7985\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.5300 - val_loss: 25.2822\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4930 - val_loss: 20.1582\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.1729 - val_loss: 21.9100\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.1662 - val_loss: 17.8013\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.9256 - val_loss: 18.8646\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.4601 - val_loss: 14.0760\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.1101 - val_loss: 16.1258\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.0375 - val_loss: 14.2076\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.8692 - val_loss: 11.6803\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.0688 - val_loss: 10.0957\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 11.0278 - val_loss: 9.6828\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9825 - val_loss: 11.0585\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8136 - val_loss: 9.7605\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0433 - val_loss: 8.6703\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 7.9991 - val_loss: 10.7726\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.9348 - val_loss: 8.4450\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9010 - val_loss: 14.6857\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1883 - val_loss: 7.4290\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4278 - val_loss: 6.1584\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1948 - val_loss: 6.8432\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3005 - val_loss: 8.1059\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4806 - val_loss: 8.1653\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.7868 - val_loss: 5.8044\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6634 - val_loss: 8.5113\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1494 - val_loss: 5.5585\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0201 - val_loss: 6.0929\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.1912 - val_loss: 6.5643\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8614 - val_loss: 6.8471\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7353 - val_loss: 5.3827\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9252 - val_loss: 4.9201\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6105 - val_loss: 5.5464\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1520 - val_loss: 4.9973\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2206 - val_loss: 5.1643\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3519 - val_loss: 5.3535\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1334 - val_loss: 5.7541\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6895 - val_loss: 4.6822\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4444 - val_loss: 7.2144\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9363 - val_loss: 5.0995\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6070 - val_loss: 5.2968\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9166 - val_loss: 5.1744\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6776 - val_loss: 4.8145\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3779 - val_loss: 4.9848\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9578 - val_loss: 6.2066\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2984 - val_loss: 5.5236\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0255 - val_loss: 4.7265\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8911 - val_loss: 4.3187\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9768 - val_loss: 4.1301\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4470 - val_loss: 4.2695\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3149 - val_loss: 3.7753\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.9879 - val_loss: 4.2876\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1236 - val_loss: 4.9599\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.5861 - val_loss: 4.0651\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9989 - val_loss: 5.8849\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.3128 - val_loss: 5.6504\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7633 - val_loss: 4.8203\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.8022 - val_loss: 3.6736\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1555 - val_loss: 4.1683\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8068 - val_loss: 5.4814\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 2.7694 - val_loss: 3.6891\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.9783 - val_loss: 3.7272\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.4703 - val_loss: 3.6193\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5161 - val_loss: 5.3789\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7037 - val_loss: 4.2695\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7351 - val_loss: 3.8988\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1751 - val_loss: 4.5941\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7989 - val_loss: 4.5004\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2425 - val_loss: 3.8548\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6998 - val_loss: 4.6000\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3349 - val_loss: 3.5148\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9664 - val_loss: 3.6515\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6940 - val_loss: 3.8146\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7376 - val_loss: 4.4906\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4982 - val_loss: 3.7627\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4024 - val_loss: 3.4127\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5955 - val_loss: 4.0724\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5687 - val_loss: 3.5794\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5875 - val_loss: 3.7775\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.6370 - val_loss: 3.8552\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2256 - val_loss: 3.1524\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1408 - val_loss: 3.4214\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1435 - val_loss: 3.0538\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3756 - val_loss: 3.7672\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3241 - val_loss: 2.9867\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2710 - val_loss: 3.4881\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1403 - val_loss: 2.9588\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4767 - val_loss: 4.2421\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2353 - val_loss: 3.0880\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3078 - val_loss: 3.2449\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4839 - val_loss: 9.2888\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3466 - val_loss: 3.0063\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0601 - val_loss: 3.0225\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6931 - val_loss: 3.3016\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9985 - val_loss: 2.9970\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8130 - val_loss: 3.2419\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0138 - val_loss: 3.1275\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7834 - val_loss: 3.4589\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1008 - val_loss: 3.1146\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9739 - val_loss: 3.2783\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2794 - val_loss: 4.1381\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1687 - val_loss: 3.0111\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9007 - val_loss: 3.0523\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9315 - val_loss: 3.5128\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0744 - val_loss: 3.2691\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7835 - val_loss: 3.1044\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8850 - val_loss: 3.2255\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7512 - val_loss: 7.4576\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2644 - val_loss: 2.9015\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6719 - val_loss: 2.8931\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6539 - val_loss: 3.0816\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7191 - val_loss: 3.3511\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6287 - val_loss: 3.2864\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7413 - val_loss: 3.5180\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8236 - val_loss: 4.2175\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7779 - val_loss: 3.4476\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7769 - val_loss: 3.1575\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8263 - val_loss: 3.0910\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9967 - val_loss: 3.8978\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.0492 - val_loss: 4.1573\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.8191 - val_loss: 3.3224\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.2327 - val_loss: 3.3251\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8872 - val_loss: 3.0957\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5268 - val_loss: 2.6524\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4446 - val_loss: 3.5144\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8751 - val_loss: 3.4674\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8326 - val_loss: 4.0519\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6122 - val_loss: 2.9552\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5835 - val_loss: 3.2912\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4933 - val_loss: 6.2783\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5129 - val_loss: 2.6197\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4148 - val_loss: 2.7635\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3814 - val_loss: 2.8668\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4248 - val_loss: 2.9295\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4040 - val_loss: 3.5526\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5802 - val_loss: 3.3638\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.7552 - val_loss: 3.5673\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5612 - val_loss: 3.0004\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.4877 - val_loss: 3.4011\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4419 - val_loss: 4.4764\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8422 - val_loss: 3.0276\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3456 - val_loss: 2.8318\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5261 - val_loss: 2.6645\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4093 - val_loss: 2.6672\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 2.6671586668396636\n",
      "Mean Absolute Error (MAE): 1.1233628700385883\n",
      "Root Mean Squared Error (RMSE): 1.6331437985798016\n",
      "Time taken: 1166.4515302181244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_17500\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  2.644319  1.100159  1.626136  1253.300110\n",
      "1        2  2.631050  1.075370  1.622051  1243.359739\n",
      "2        3  7.819078  1.553955  2.796261  1240.929451\n",
      "3        4  3.104484  1.148382  1.761955  1168.629306\n",
      "4        5  2.667159  1.123363  1.633144  1166.451530\n",
      "5  Average  3.773218  1.200246  1.887909  1214.534027\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f_over.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f_over.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f_over.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
