{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 20ms/step - loss: 1366.0074 - val_loss: 1266.4077\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1209.1660 - val_loss: 1164.5310\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1124.3821 - val_loss: 1091.5669\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1062.2095 - val_loss: 1036.7523\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1015.8676 - val_loss: 995.3607\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 981.2304 - val_loss: 966.0168\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 957.2668 - val_loss: 945.7115\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 941.3582 - val_loss: 932.8230\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 931.4333 - val_loss: 924.8983\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.6877 - val_loss: 920.4073\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 922.5916 - val_loss: 918.1946\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.1027 - val_loss: 917.1965\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.5513 - val_loss: 916.9214\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.3084 - val_loss: 916.4506\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.2209 - val_loss: 916.3128\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 920.1869 - val_loss: 916.3422\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.1971 - val_loss: 916.2087\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1995 - val_loss: 916.3876\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1967 - val_loss: 916.3181\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1757 - val_loss: 916.4454\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.2171 - val_loss: 916.3985\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1826 - val_loss: 916.5363\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1938 - val_loss: 916.4848\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1856 - val_loss: 916.4934\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1779 - val_loss: 916.3727\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.2403 - val_loss: 916.5201\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.2046 - val_loss: 916.4460\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1947 - val_loss: 916.4369\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1806 - val_loss: 916.5559\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1766 - val_loss: 916.5193\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.1993 - val_loss: 916.5646\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.8341 - val_loss: 913.4919\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 883.3138 - val_loss: 871.4122\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 859.9161 - val_loss: 852.6890\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 844.5474 - val_loss: 837.5828\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 829.2217 - val_loss: 822.0507\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 808.5630 - val_loss: 787.6887\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 767.8535 - val_loss: 751.9987\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 726.1783 - val_loss: 702.6035\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 680.6673 - val_loss: 656.9580\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 633.3719 - val_loss: 606.3981\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 577.2024 - val_loss: 553.2187\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 528.8203 - val_loss: 506.6779\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 495.0510 - val_loss: 469.0116\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 447.5334 - val_loss: 428.8258\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 413.8511 - val_loss: 391.9985\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 374.2064 - val_loss: 360.6088\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 342.3412 - val_loss: 332.9189\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 314.3261 - val_loss: 299.0620\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 287.7445 - val_loss: 312.8922\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 260.6683 - val_loss: 244.4017\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 232.7484 - val_loss: 224.5734\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 218.2712 - val_loss: 201.1525\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 193.3195 - val_loss: 184.0412\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 172.6494 - val_loss: 170.6175\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 156.5285 - val_loss: 148.5582\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 140.0318 - val_loss: 131.5108\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 125.7568 - val_loss: 116.9552\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 113.7199 - val_loss: 106.3712\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 102.5075 - val_loss: 94.5834\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 89.9910 - val_loss: 84.1904\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 81.7574 - val_loss: 75.6571\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 71.1371 - val_loss: 67.5479\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.7177 - val_loss: 56.6861\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.3178 - val_loss: 50.3746\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 62.9930 - val_loss: 47.2490\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.4434 - val_loss: 37.5247\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.1809 - val_loss: 33.3220\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.9291 - val_loss: 27.9526\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.8144 - val_loss: 23.5372\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.2299 - val_loss: 22.1336\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.6439 - val_loss: 20.9745\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.3915 - val_loss: 14.7307\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 16.1462 - val_loss: 14.7787\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6125 - val_loss: 11.8571\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.2045 - val_loss: 12.8978\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6794 - val_loss: 7.7474\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 8.9780 - val_loss: 19.0970\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3923 - val_loss: 6.8528\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9352 - val_loss: 6.3039\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.1477 - val_loss: 7.9037\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2155 - val_loss: 4.7852\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5061 - val_loss: 6.0074\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0402 - val_loss: 4.0558\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.7907 - val_loss: 2.9092\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5903 - val_loss: 3.3366\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2313 - val_loss: 5.0883\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3916 - val_loss: 2.4159\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6145 - val_loss: 2.2409\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.5209 - val_loss: 2.5349\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4344 - val_loss: 3.3493\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1450 - val_loss: 1.7018\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0860 - val_loss: 1.3998\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3547 - val_loss: 1.8861\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8171 - val_loss: 1.6611\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1421 - val_loss: 1.4514\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8503 - val_loss: 4.3367\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2273 - val_loss: 1.1737\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6168 - val_loss: 1.8587\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1818 - val_loss: 1.2520\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8159 - val_loss: 2.1995\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6837 - val_loss: 1.9515\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3339 - val_loss: 1.2132\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9343 - val_loss: 1.2321\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1469 - val_loss: 0.6313\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1759 - val_loss: 1.4172\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.5134 - val_loss: 4.1669\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0470 - val_loss: 1.4426\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1679 - val_loss: 0.9968\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9644 - val_loss: 1.1670\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8659 - val_loss: 0.8765\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0138 - val_loss: 1.4706\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8916 - val_loss: 0.7296\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0054 - val_loss: 0.7192\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.1842 - val_loss: 1.9706\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.0770 - val_loss: 3.2744\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1337 - val_loss: 0.8922\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9140 - val_loss: 0.6841\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9492 - val_loss: 1.3115\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9457 - val_loss: 0.6571\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5694 - val_loss: 1.0365\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6781 - val_loss: 0.9675\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1209 - val_loss: 1.5754\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2162 - val_loss: 0.6263\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6869 - val_loss: 1.5794\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3196 - val_loss: 0.6161\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5625 - val_loss: 0.8930\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8429 - val_loss: 1.6049\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9423 - val_loss: 1.0076\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7381 - val_loss: 0.5414\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7331 - val_loss: 0.9071\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8241 - val_loss: 0.6586\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6366 - val_loss: 0.8752\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0475 - val_loss: 1.1174\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4913 - val_loss: 0.3457\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3160 - val_loss: 0.3673\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0111 - val_loss: 2.8031\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0796 - val_loss: 0.3572\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3545 - val_loss: 0.3339\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7896 - val_loss: 0.6329\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3893 - val_loss: 0.5625\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5397 - val_loss: 0.7141\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2326 - val_loss: 0.4020\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3290 - val_loss: 0.2234\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4842 - val_loss: 0.4632\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5769 - val_loss: 0.5785\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9148 - val_loss: 1.1608\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4413 - val_loss: 0.3462\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4210 - val_loss: 0.4107\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4539 - val_loss: 0.7550\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2860 - val_loss: 0.3453\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2794 - val_loss: 0.2519\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3699 - val_loss: 0.6309\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5280 - val_loss: 0.3029\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5100 - val_loss: 0.2879\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3410 - val_loss: 0.3232\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9436 - val_loss: 0.5302\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4402 - val_loss: 0.2022\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4373 - val_loss: 0.6057\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3884 - val_loss: 0.1876\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1633 - val_loss: 0.2544\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2363 - val_loss: 0.2059\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2469 - val_loss: 0.3352\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3199 - val_loss: 0.3678\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5779 - val_loss: 0.9926\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7017 - val_loss: 0.5158\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2837 - val_loss: 0.4359\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4981 - val_loss: 0.3635\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5795 - val_loss: 0.9155\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3608 - val_loss: 0.2580\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5143 - val_loss: 0.6428\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4348 - val_loss: 1.1777\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5749 - val_loss: 0.2201\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2410 - val_loss: 0.2269\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7404 - val_loss: 0.3912\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3106 - val_loss: 0.3249\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2547 - val_loss: 0.3166\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3808 - val_loss: 0.4346\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4553 - val_loss: 0.5130\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4856 - val_loss: 0.3038\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3270 - val_loss: 0.3654\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3127 - val_loss: 0.3867\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3492 - val_loss: 0.4737\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5884 - val_loss: 0.3195\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4644 - val_loss: 0.4332\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2439 - val_loss: 0.1722\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8184 - val_loss: 0.3823\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2416 - val_loss: 0.1560\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1949 - val_loss: 0.3544\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3256 - val_loss: 0.9135\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6812 - val_loss: 0.4258\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2448 - val_loss: 0.1827\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3628 - val_loss: 1.3078\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3432 - val_loss: 0.2718\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3932 - val_loss: 0.5522\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3450 - val_loss: 0.2987\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2326 - val_loss: 0.4974\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3001 - val_loss: 0.2845\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3004 - val_loss: 0.2411\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3726 - val_loss: 0.2365\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.2365474255383699\n",
      "Mean Absolute Error (MAE): 0.3555347407292362\n",
      "Root Mean Squared Error (RMSE): 0.48636141452460013\n",
      "Time taken: 1212.9279692173004\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1382.4926 - val_loss: 1335.5641\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1222.5825 - val_loss: 1224.9855\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1133.8865 - val_loss: 1143.8514\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1068.0046 - val_loss: 1081.4108\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1018.1276 - val_loss: 1034.8588\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 982.0054 - val_loss: 1000.8679\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 956.5825 - val_loss: 976.7086\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 939.4485 - val_loss: 960.4239\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.4771 - val_loss: 949.6608\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.0175 - val_loss: 943.2861\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5232 - val_loss: 939.4702\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.8366 - val_loss: 937.3323\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.1900 - val_loss: 936.2242\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 915.8958 - val_loss: 935.9025\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.8306 - val_loss: 935.6229\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7502 - val_loss: 935.6837\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7556 - val_loss: 935.4780\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7619 - val_loss: 935.5421\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7861 - val_loss: 935.5140\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7691 - val_loss: 935.6287\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7827 - val_loss: 935.4527\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.8058 - val_loss: 935.4180\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7438 - val_loss: 935.4332\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7550 - val_loss: 935.3382\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7678 - val_loss: 935.5771\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7571 - val_loss: 935.5278\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7563 - val_loss: 935.4113\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7643 - val_loss: 935.2585\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7680 - val_loss: 935.4901\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.7645 - val_loss: 935.1508\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.8093 - val_loss: 935.2988\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7511 - val_loss: 935.2492\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7571 - val_loss: 935.3442\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7911 - val_loss: 935.4420\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 915.7924 - val_loss: 935.3408\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 892.6068 - val_loss: 896.2874\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 861.2141 - val_loss: 871.7164\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 841.4183 - val_loss: 848.3130\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 822.6712 - val_loss: 830.8514\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 801.8508 - val_loss: 806.4724\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 773.2713 - val_loss: 776.0305\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 732.1022 - val_loss: 722.5499\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 670.8982 - val_loss: 660.6245\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 613.4978 - val_loss: 607.8840\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 562.3004 - val_loss: 561.9727\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 515.3970 - val_loss: 511.4089\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 471.2132 - val_loss: 469.2512\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 431.7130 - val_loss: 434.9575\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 395.6583 - val_loss: 397.9423\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 360.1602 - val_loss: 363.4573\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 330.5622 - val_loss: 331.8978\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 300.7398 - val_loss: 303.1216\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 274.0367 - val_loss: 276.5780\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 247.9639 - val_loss: 249.1699\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 223.9881 - val_loss: 226.3179\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 203.8151 - val_loss: 206.1284\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 183.7775 - val_loss: 185.7460\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 167.0970 - val_loss: 168.1976\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 148.4627 - val_loss: 150.6279\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 134.7858 - val_loss: 134.2124\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 118.9799 - val_loss: 120.9575\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 105.9649 - val_loss: 106.6267\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 96.3043 - val_loss: 96.3679\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 84.1039 - val_loss: 85.1482\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.8061 - val_loss: 75.8494\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 66.2991 - val_loss: 66.6774\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.3975 - val_loss: 59.2983\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 51.0196 - val_loss: 52.0767\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.3144 - val_loss: 46.1305\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.6643 - val_loss: 39.1845\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.5855 - val_loss: 33.3105\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.7145 - val_loss: 31.4894\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.6464 - val_loss: 27.2708\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.5728 - val_loss: 21.1712\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.0544 - val_loss: 19.0345\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3588 - val_loss: 16.4886\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.0248 - val_loss: 15.7461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.0745 - val_loss: 12.5979\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.6834 - val_loss: 10.1793\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.0007 - val_loss: 8.1723\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 8.4867 - val_loss: 8.0721\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3459 - val_loss: 9.7455\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4931 - val_loss: 9.5228\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8833 - val_loss: 5.0769\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8369 - val_loss: 4.5952\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6122 - val_loss: 4.8116\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1422 - val_loss: 3.7622\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7164 - val_loss: 6.7920\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4769 - val_loss: 2.5902\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2500 - val_loss: 11.9335\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3373 - val_loss: 2.5472\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4750 - val_loss: 4.2982\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6589 - val_loss: 1.8880\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2648 - val_loss: 1.7333\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9959 - val_loss: 2.2237\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0210 - val_loss: 1.5891\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4929 - val_loss: 1.3982\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7273 - val_loss: 1.0898\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4659 - val_loss: 1.6825\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5523 - val_loss: 1.3723\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4403 - val_loss: 2.1274\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3259 - val_loss: 1.2773\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6759 - val_loss: 3.0486\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3560 - val_loss: 0.8733\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2261 - val_loss: 1.9637\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1262 - val_loss: 0.7646\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8261 - val_loss: 0.9524\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9462 - val_loss: 0.8133\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3132 - val_loss: 0.7594\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9767 - val_loss: 1.0714\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2021 - val_loss: 0.8687\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0183 - val_loss: 1.3807\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0387 - val_loss: 1.2404\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6291 - val_loss: 1.1456\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1194 - val_loss: 0.6573\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9257 - val_loss: 1.1830\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2903 - val_loss: 6.8453\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4988 - val_loss: 0.5380\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5045 - val_loss: 0.8990\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6311 - val_loss: 0.7567\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7085 - val_loss: 0.8471\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0769 - val_loss: 2.4642\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1719 - val_loss: 0.7631\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6440 - val_loss: 0.5887\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5801 - val_loss: 0.6531\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9763 - val_loss: 1.2606\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6241 - val_loss: 0.5333\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7135 - val_loss: 0.6498\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7345 - val_loss: 0.6210\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6506 - val_loss: 1.4354\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0485 - val_loss: 1.0201\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5880 - val_loss: 0.3733\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5241 - val_loss: 0.5231\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5632 - val_loss: 0.5752\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4874 - val_loss: 0.7010\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3963 - val_loss: 0.8367\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4462 - val_loss: 0.2716\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3971 - val_loss: 1.1984\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7080 - val_loss: 0.5638\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6058 - val_loss: 0.4905\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3886 - val_loss: 0.4712\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4961 - val_loss: 0.9793\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0556 - val_loss: 0.3602\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2327 - val_loss: 0.2010\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1921 - val_loss: 0.2824\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2368 - val_loss: 0.3913\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4113 - val_loss: 0.6414\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5652 - val_loss: 0.7741\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5898 - val_loss: 0.3458\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3145 - val_loss: 0.3892\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4687 - val_loss: 0.4044\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6334 - val_loss: 0.7873\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2293 - val_loss: 0.3324\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1889 - val_loss: 0.1405\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1635 - val_loss: 0.2221\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3246 - val_loss: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9193 - val_loss: 0.4995\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2859 - val_loss: 0.2723\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4197 - val_loss: 0.7694\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3445 - val_loss: 0.3785\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4548 - val_loss: 1.0136\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4074 - val_loss: 0.2677\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3848 - val_loss: 0.4643\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9243 - val_loss: 0.2314\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1932 - val_loss: 0.2639\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4306 - val_loss: 0.5176\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2850 - val_loss: 0.3757\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2835 - val_loss: 0.9577\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4972 - val_loss: 1.0967\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6145 - val_loss: 0.2928\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1797 - val_loss: 0.1687\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2364 - val_loss: 0.4981\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4543 - val_loss: 0.5155\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3780 - val_loss: 0.3334\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1824 - val_loss: 0.1451\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1055 - val_loss: 0.1108\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1149 - val_loss: 0.2485\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1595 - val_loss: 0.1435\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3246 - val_loss: 0.1722\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1540 - val_loss: 0.2202\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5684 - val_loss: 0.3224\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2839 - val_loss: 0.4025\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2509 - val_loss: 0.3997\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4435 - val_loss: 0.4584\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2270 - val_loss: 0.1518\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2151 - val_loss: 0.2556\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5928 - val_loss: 0.3752\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2004 - val_loss: 0.2350\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2590 - val_loss: 0.1426\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3634 - val_loss: 0.9745\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2938 - val_loss: 0.1566\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7719 - val_loss: 2.6806\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4981 - val_loss: 0.1608\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0878 - val_loss: 0.0872\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0901 - val_loss: 0.1824\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2120 - val_loss: 0.1762\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1490 - val_loss: 0.1543\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4073 - val_loss: 0.7975\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5712 - val_loss: 0.1624\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.0983 - val_loss: 0.0824\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.08237996765625831\n",
      "Mean Absolute Error (MAE): 0.20863034956953505\n",
      "Root Mean Squared Error (RMSE): 0.28701910677907544\n",
      "Time taken: 1206.2291958332062\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1372.2744 - val_loss: 1195.7439\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1212.4708 - val_loss: 1106.7177\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1125.8507 - val_loss: 1042.5300\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1062.3923 - val_loss: 995.4714\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1015.6289 - val_loss: 961.8333\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 981.9677 - val_loss: 938.5741\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 958.6450 - val_loss: 923.3430\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 943.0561 - val_loss: 914.0969\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 933.2544 - val_loss: 909.3481\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 927.6813 - val_loss: 907.3779\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 924.7236 - val_loss: 906.7323\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.3167 - val_loss: 906.9000\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 922.7678 - val_loss: 907.3300\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 922.5731 - val_loss: 907.5162\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4709 - val_loss: 907.6535\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.4576 - val_loss: 907.8799\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4684 - val_loss: 907.9650\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.4490 - val_loss: 908.1677\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4428 - val_loss: 908.1114\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4544 - val_loss: 908.0974\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4519 - val_loss: 908.1919\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.5021 - val_loss: 908.2599\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4556 - val_loss: 908.1310\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4507 - val_loss: 907.9641\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4244 - val_loss: 908.2339\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.4461 - val_loss: 908.1900\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.5078 - val_loss: 908.3000\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4691 - val_loss: 908.3494\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4852 - val_loss: 908.1573\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.4688 - val_loss: 908.3339\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.4298 - val_loss: 906.6624\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 891.4929 - val_loss: 852.0482\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 859.5501 - val_loss: 830.6673\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 840.1617 - val_loss: 815.7628\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 822.1181 - val_loss: 792.5680\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 785.5191 - val_loss: 749.3419\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 751.2786 - val_loss: 717.7808\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 718.2632 - val_loss: 677.6518\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 665.0461 - val_loss: 617.3765\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 619.3266 - val_loss: 577.7745\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 569.7245 - val_loss: 532.1396\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 528.7831 - val_loss: 488.3381\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 485.6484 - val_loss: 455.5003\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 448.7748 - val_loss: 417.8325\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 410.8167 - val_loss: 380.7763\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 378.3931 - val_loss: 350.3859\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 346.4208 - val_loss: 329.0190\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 318.9957 - val_loss: 292.7015\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 290.5822 - val_loss: 268.0777\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 266.1069 - val_loss: 246.1431\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 242.1312 - val_loss: 225.3727\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 222.0930 - val_loss: 208.8816\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 200.7567 - val_loss: 185.5879\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 183.6180 - val_loss: 173.3587\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 164.4914 - val_loss: 153.6386\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 149.5123 - val_loss: 141.0373\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 133.4279 - val_loss: 125.4207\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 120.4577 - val_loss: 115.2118\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 106.8776 - val_loss: 101.4288\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 99.1941 - val_loss: 91.9501\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 87.6263 - val_loss: 83.4962\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 77.2653 - val_loss: 75.4012\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.0961 - val_loss: 67.4077\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.8826 - val_loss: 61.1363\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 56.0147 - val_loss: 54.6014\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.7153 - val_loss: 55.6391\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 45.0901 - val_loss: 42.9011\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.7938 - val_loss: 38.6793\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.9453 - val_loss: 34.9302\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.8095 - val_loss: 32.7069\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.4521 - val_loss: 26.5544\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.6420 - val_loss: 25.8568\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.5665 - val_loss: 20.0413\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6916 - val_loss: 17.8720\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5894 - val_loss: 23.1163\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.4412 - val_loss: 12.8913\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0100 - val_loss: 14.5990\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 10.4705 - val_loss: 39.3240\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.3844 - val_loss: 11.4740\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8035 - val_loss: 7.8792\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9722 - val_loss: 7.5378\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3704 - val_loss: 15.5768\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5581 - val_loss: 5.4861\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3272 - val_loss: 5.1261\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.0271 - val_loss: 4.6919\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7274 - val_loss: 4.6091\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6441 - val_loss: 7.3996\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1956 - val_loss: 3.7016\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8265 - val_loss: 4.2432\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2538 - val_loss: 5.7750\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7253 - val_loss: 2.6633\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1500 - val_loss: 2.5326\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.9605 - val_loss: 2.6673\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1475 - val_loss: 2.2140\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9295 - val_loss: 2.4133\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2463 - val_loss: 1.8847\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6670 - val_loss: 3.5904\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0454 - val_loss: 2.5406\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8555 - val_loss: 2.6884\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7219 - val_loss: 5.0867\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3159 - val_loss: 1.4482\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1921 - val_loss: 1.7346\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2703 - val_loss: 1.3407\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8307 - val_loss: 1.1459\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2176 - val_loss: 2.2320\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.5740 - val_loss: 16.5655\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.4260 - val_loss: 1.6567\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0706 - val_loss: 1.0534\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7962 - val_loss: 1.0949\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7618 - val_loss: 0.9667\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8587 - val_loss: 0.9622\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1913 - val_loss: 1.2847\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3901 - val_loss: 1.1508\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7503 - val_loss: 0.8959\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8397 - val_loss: 0.8668\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9074 - val_loss: 5.4534\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3410 - val_loss: 0.8917\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5871 - val_loss: 0.7798\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6075 - val_loss: 1.0094\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6149 - val_loss: 1.5349\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6642 - val_loss: 0.8517\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8964 - val_loss: 0.7556\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6269 - val_loss: 0.7252\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0894 - val_loss: 12.8123\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0241 - val_loss: 1.0612\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4651 - val_loss: 0.5818\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3852 - val_loss: 0.5811\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4075 - val_loss: 0.6348\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.4537 - val_loss: 0.6035\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7329 - val_loss: 0.8271\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6091 - val_loss: 0.7496\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7103 - val_loss: 0.7122\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7817 - val_loss: 0.7909\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5667 - val_loss: 1.8517\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5218 - val_loss: 0.6037\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5688 - val_loss: 0.6755\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1407 - val_loss: 0.6765\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5644 - val_loss: 0.5053\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4548 - val_loss: 0.7783\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8814 - val_loss: 0.5426\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8392 - val_loss: 1.7510\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6265 - val_loss: 0.6316\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9101 - val_loss: 4.3599\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1889 - val_loss: 0.3531\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4090 - val_loss: 0.5067\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4319 - val_loss: 0.5389\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8876 - val_loss: 0.6998\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6565 - val_loss: 0.7066\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5253 - val_loss: 0.4115\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7010 - val_loss: 0.9235\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3055 - val_loss: 0.4560\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2876 - val_loss: 0.6448\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3637 - val_loss: 0.5125\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5868 - val_loss: 0.5668\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3709 - val_loss: 0.3339\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5872 - val_loss: 0.9105\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8794 - val_loss: 1.0984\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3942 - val_loss: 0.8707\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2523 - val_loss: 0.8627\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3815 - val_loss: 0.3233\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2835 - val_loss: 0.2592\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3693 - val_loss: 0.4604\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4289 - val_loss: 0.7078\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5455 - val_loss: 0.6747\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0650 - val_loss: 0.2920\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1465 - val_loss: 0.2040\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1461 - val_loss: 0.2117\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1939 - val_loss: 0.2880\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2974 - val_loss: 0.2852\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5278 - val_loss: 0.5198\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.8820 - val_loss: 1.5397\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5390 - val_loss: 0.4628\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2568 - val_loss: 0.3009\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1949 - val_loss: 0.2556\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1977 - val_loss: 0.2327\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1803 - val_loss: 0.2303\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1985 - val_loss: 0.2669\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5109 - val_loss: 1.1377\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4144 - val_loss: 0.2818\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2774 - val_loss: 0.3234\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2515 - val_loss: 7.7451\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2932 - val_loss: 0.2598\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1676 - val_loss: 0.2000\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1541 - val_loss: 0.2532\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1823 - val_loss: 0.2638\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2940 - val_loss: 0.5907\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2704 - val_loss: 0.3053\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5113 - val_loss: 0.4775\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3398 - val_loss: 0.3257\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5005 - val_loss: 0.8469\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5003 - val_loss: 0.5478\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2375 - val_loss: 0.3089\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4877 - val_loss: 0.4200\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1577 - val_loss: 3.8735\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4247 - val_loss: 0.1626\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1232 - val_loss: 0.1650\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1410 - val_loss: 0.2636\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2398 - val_loss: 0.4397\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6923 - val_loss: 2.1469\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7878 - val_loss: 0.1555\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.1555239639532752\n",
      "Mean Absolute Error (MAE): 0.28043009346646625\n",
      "Root Mean Squared Error (RMSE): 0.39436526717406944\n",
      "Time taken: 1215.000935792923\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1355.2620 - val_loss: 1273.8550\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1200.1729 - val_loss: 1175.1216\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1117.5345 - val_loss: 1102.5579\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1055.9360 - val_loss: 1048.1639\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1009.9730 - val_loss: 1007.8409\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 976.5553 - val_loss: 979.0142\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 953.0638 - val_loss: 959.0086\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 937.3680 - val_loss: 946.2833\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 927.5076 - val_loss: 938.5420\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.8721 - val_loss: 934.0770\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 918.8777 - val_loss: 932.0611\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.4836 - val_loss: 931.0893\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8945 - val_loss: 930.6108\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6870 - val_loss: 930.4898\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6537 - val_loss: 930.2653\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.5974 - val_loss: 930.3094\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6467 - val_loss: 930.3402\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6186 - val_loss: 930.3314\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6322 - val_loss: 930.2640\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.5864 - val_loss: 930.2567\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6332 - val_loss: 930.3210\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6132 - val_loss: 930.3650\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.6060 - val_loss: 930.2247\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.9164 - val_loss: 930.2566\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 916.6151 - val_loss: 930.2540\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6205 - val_loss: 930.1460\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6713 - val_loss: 930.2204\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.6440 - val_loss: 930.1980\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5968 - val_loss: 930.2816\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.5906 - val_loss: 930.2568\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.8359 - val_loss: 930.5428\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 903.7115 - val_loss: 891.8449\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 871.0743 - val_loss: 876.0933\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 855.1607 - val_loss: 861.5335\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 843.9878 - val_loss: 850.8997\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 834.3287 - val_loss: 843.7628\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 825.1244 - val_loss: 832.1600\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 816.0790 - val_loss: 822.9814\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 805.9507 - val_loss: 813.7922\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 788.6537 - val_loss: 785.1157\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 758.2863 - val_loss: 747.3810\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 711.3979 - val_loss: 699.0240\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 663.2336 - val_loss: 648.0623\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 618.3163 - val_loss: 604.3916\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 578.2361 - val_loss: 568.1729\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 540.4648 - val_loss: 529.5050\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 504.8464 - val_loss: 503.7677\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 471.4080 - val_loss: 461.5196\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 439.9833 - val_loss: 429.8264\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 408.3962 - val_loss: 398.7043\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 378.6646 - val_loss: 371.1699\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 351.1240 - val_loss: 341.6089\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 325.6060 - val_loss: 318.7994\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 300.2626 - val_loss: 291.1482\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 277.3607 - val_loss: 270.8085\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 255.9738 - val_loss: 248.6617\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 235.3991 - val_loss: 229.7202\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 216.7831 - val_loss: 209.8342\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 199.6208 - val_loss: 192.7415\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 181.9310 - val_loss: 174.7652\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 166.9990 - val_loss: 159.9652\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 152.0412 - val_loss: 148.1098\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 138.4109 - val_loss: 139.3691\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 125.6487 - val_loss: 119.9982\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 113.2121 - val_loss: 110.1120\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 104.6862 - val_loss: 97.5949\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 92.2842 - val_loss: 87.7247\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 83.5319 - val_loss: 78.7178\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.7266 - val_loss: 71.1849\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 68.0144 - val_loss: 65.0232\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 59.8637 - val_loss: 57.6945\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 54.9034 - val_loss: 51.5866\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.1033 - val_loss: 45.0931\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 41.9989 - val_loss: 40.3234\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 37.4959 - val_loss: 35.5252\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.3667 - val_loss: 32.2240\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.3438 - val_loss: 27.3429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.0772 - val_loss: 25.5790\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.0314 - val_loss: 22.1165\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.1720 - val_loss: 18.9144\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.5344 - val_loss: 17.0818\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.1254 - val_loss: 14.7509\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5242 - val_loss: 14.9528\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.1676 - val_loss: 13.2481\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.7800 - val_loss: 10.2209\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.4754 - val_loss: 10.9377\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 9.0175 - val_loss: 8.1343\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.1015 - val_loss: 7.5411\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4476 - val_loss: 8.6459\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6026 - val_loss: 13.6805\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4721 - val_loss: 5.1416\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2778 - val_loss: 4.8707\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2633 - val_loss: 4.9521\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1542 - val_loss: 3.9183\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8271 - val_loss: 4.1548\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8898 - val_loss: 3.7730\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6137 - val_loss: 2.6037\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9276 - val_loss: 2.9030\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2792 - val_loss: 2.4422\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5241 - val_loss: 2.0316\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4158 - val_loss: 1.8126\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4102 - val_loss: 1.8984\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1217 - val_loss: 1.6580\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7958 - val_loss: 2.2253\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.4065 - val_loss: 1.9879\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9790 - val_loss: 2.0136\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6434 - val_loss: 1.5731\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5084 - val_loss: 1.4701\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7877 - val_loss: 2.4602\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5345 - val_loss: 2.2229\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4433 - val_loss: 0.8751\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2960 - val_loss: 2.0346\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1868 - val_loss: 0.8444\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1391 - val_loss: 1.2066\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7542 - val_loss: 1.1253\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9722 - val_loss: 0.8399\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2645 - val_loss: 1.9403\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1163 - val_loss: 0.9004\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3227 - val_loss: 2.4312\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.1078 - val_loss: 0.9835\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2796 - val_loss: 1.0392\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7486 - val_loss: 1.2953\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7567 - val_loss: 0.8564\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0660 - val_loss: 0.6469\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7426 - val_loss: 0.7416\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4478 - val_loss: 1.6973\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.9208 - val_loss: 0.7248\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9827 - val_loss: 0.8197\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7466 - val_loss: 0.4469\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8976 - val_loss: 0.8228\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0329 - val_loss: 0.6868\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5575 - val_loss: 0.5665\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6571 - val_loss: 0.6259\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8606 - val_loss: 0.7549\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5789 - val_loss: 1.8944\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4356 - val_loss: 2.3352\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6273 - val_loss: 0.4768\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5253 - val_loss: 0.7322\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9351 - val_loss: 1.3296\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0662 - val_loss: 0.4077\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6452 - val_loss: 0.8004\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4761 - val_loss: 0.3615\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8026 - val_loss: 0.8697\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6820 - val_loss: 0.5511\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.6849 - val_loss: 0.6291\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7726 - val_loss: 0.4889\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6955 - val_loss: 1.5962\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9479 - val_loss: 0.5077\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4016 - val_loss: 0.4395\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5763 - val_loss: 0.6378\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7211 - val_loss: 0.6903\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8049 - val_loss: 0.9680\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5295 - val_loss: 0.5293\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3960 - val_loss: 0.3736\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5243 - val_loss: 0.8729\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5383 - val_loss: 0.4436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5322 - val_loss: 0.8865\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7105 - val_loss: 0.3298\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5967 - val_loss: 0.6684\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5972 - val_loss: 0.5388\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4904 - val_loss: 0.7683\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4354 - val_loss: 1.3394\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4867 - val_loss: 0.3274\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3826 - val_loss: 0.3276\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9613 - val_loss: 0.4088\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2350 - val_loss: 0.2163\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1786 - val_loss: 0.1451\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2477 - val_loss: 0.2704\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5528 - val_loss: 0.7291\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3782 - val_loss: 0.2514\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5830 - val_loss: 0.4653\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4282 - val_loss: 0.8434\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7197 - val_loss: 0.5238\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5201 - val_loss: 0.2177\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2188 - val_loss: 0.2843\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8136 - val_loss: 0.8265\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4074 - val_loss: 0.1783\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3014 - val_loss: 0.4112\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3784 - val_loss: 0.4074\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3807 - val_loss: 0.4681\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3904 - val_loss: 1.3615\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9191 - val_loss: 0.1936\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1966 - val_loss: 0.1747\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2193 - val_loss: 0.2575\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4133 - val_loss: 2.4048\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6929 - val_loss: 0.3870\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2529 - val_loss: 0.2531\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2837 - val_loss: 0.3186\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5603 - val_loss: 1.2094\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3398 - val_loss: 0.2976\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3115 - val_loss: 0.4913\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3589 - val_loss: 0.2300\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5063 - val_loss: 0.5841\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1953 - val_loss: 0.1348\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1072 - val_loss: 0.1921\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1507 - val_loss: 0.1541\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2589 - val_loss: 0.1684\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.3706 - val_loss: 0.2191\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3839 - val_loss: 0.3132\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4680 - val_loss: 0.3549\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.35491205652319696\n",
      "Mean Absolute Error (MAE): 0.43496620249680457\n",
      "Root Mean Squared Error (RMSE): 0.59574495929315\n",
      "Time taken: 1173.1984131336212\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1376.6655 - val_loss: 1253.4165\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1211.4991 - val_loss: 1151.9065\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1124.5585 - val_loss: 1079.2213\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1061.0182 - val_loss: 1025.6334\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1014.1553 - val_loss: 986.6685\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 980.1705 - val_loss: 958.6956\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 956.5557 - val_loss: 940.0912\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 940.7987 - val_loss: 928.1431\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 931.0811 - val_loss: 921.1733\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.5533 - val_loss: 917.4788\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.6933 - val_loss: 915.7181\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2889 - val_loss: 914.9810\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.7415 - val_loss: 914.7510\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.5469 - val_loss: 914.6873\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4987 - val_loss: 914.6960\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4604 - val_loss: 914.6781\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4254 - val_loss: 914.6545\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4667 - val_loss: 914.6219\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4672 - val_loss: 914.7147\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4445 - val_loss: 914.6947\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4655 - val_loss: 914.6754\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4385 - val_loss: 914.6497\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4402 - val_loss: 914.6870\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4497 - val_loss: 914.6648\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4728 - val_loss: 914.6429\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4398 - val_loss: 914.6528\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4099 - val_loss: 914.6722\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4631 - val_loss: 914.6412\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4421 - val_loss: 914.7020\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4679 - val_loss: 914.7032\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.4434 - val_loss: 914.7463\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4453 - val_loss: 914.7125\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4288 - val_loss: 914.7184\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.4644 - val_loss: 914.7173\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.4517 - val_loss: 914.7217\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 920.4409 - val_loss: 914.6891\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.5046 - val_loss: 914.7239\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4531 - val_loss: 914.7252\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4379 - val_loss: 914.7392\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4136 - val_loss: 914.6816\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4708 - val_loss: 914.6264\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4205 - val_loss: 914.6814\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4347 - val_loss: 914.6696\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.8947 - val_loss: 914.6701\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.4314 - val_loss: 914.4146\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 903.0393 - val_loss: 879.1508\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 877.4703 - val_loss: 858.5428\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 849.3818 - val_loss: 822.6273\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 814.0456 - val_loss: 789.2865\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 783.4075 - val_loss: 764.6260\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 759.1885 - val_loss: 740.0060\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 738.0701 - val_loss: 720.9647\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 714.8189 - val_loss: 695.0893\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 685.7725 - val_loss: 664.1804\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 657.5129 - val_loss: 631.8775\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 615.0567 - val_loss: 584.0890\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 570.6986 - val_loss: 545.7075\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 531.7141 - val_loss: 506.7700\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 494.5226 - val_loss: 469.7932\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 459.5657 - val_loss: 438.7758\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 425.8796 - val_loss: 405.2823\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 394.3831 - val_loss: 393.9356\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 364.4347 - val_loss: 346.2480\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 336.9662 - val_loss: 320.4305\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 311.5185 - val_loss: 294.7039\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 287.7646 - val_loss: 272.3570\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 266.4407 - val_loss: 250.3502\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 245.6821 - val_loss: 233.3322\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 225.3706 - val_loss: 212.3575\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 206.9332 - val_loss: 199.3008\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 189.6707 - val_loss: 214.1528\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 174.6720 - val_loss: 162.6673\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 159.1479 - val_loss: 148.2865\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 143.8565 - val_loss: 135.2817\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 130.8002 - val_loss: 124.7298\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 118.5330 - val_loss: 111.0038\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 108.8735 - val_loss: 99.6854\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 97.7722 - val_loss: 95.5386\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 88.2132 - val_loss: 85.4730\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 79.7939 - val_loss: 76.5763\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 71.7853 - val_loss: 69.2530\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 63.9984 - val_loss: 58.6397\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 58.1047 - val_loss: 63.4347\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.1115 - val_loss: 47.0185\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 45.7126 - val_loss: 41.3885\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 40.8890 - val_loss: 37.3547\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.6394 - val_loss: 34.1292\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.1033 - val_loss: 33.3347\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.9058 - val_loss: 26.9601\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.5187 - val_loss: 22.8218\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.9982 - val_loss: 20.8836\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.4342 - val_loss: 20.8094\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.1477 - val_loss: 16.0524\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0622 - val_loss: 15.1420\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.0271 - val_loss: 14.8017\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.7606 - val_loss: 12.9413\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.0545 - val_loss: 10.3337\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2074 - val_loss: 9.2140\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.3458 - val_loss: 8.3691\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5674 - val_loss: 9.5459\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.6838 - val_loss: 7.2671\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6278 - val_loss: 6.2151\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5696 - val_loss: 6.0496\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6049 - val_loss: 5.4761\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4061 - val_loss: 6.6057\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0855 - val_loss: 4.4035\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5852 - val_loss: 5.2869\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2655 - val_loss: 4.7644\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7549 - val_loss: 3.4194\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7095 - val_loss: 7.7603\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4669 - val_loss: 3.1670\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3605 - val_loss: 3.9253\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1306 - val_loss: 2.3498\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0120 - val_loss: 3.1330\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3441 - val_loss: 2.0359\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3800 - val_loss: 2.5784\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0918 - val_loss: 2.3205\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1899 - val_loss: 2.6122\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0185 - val_loss: 1.5970\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9009 - val_loss: 1.6844\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2428 - val_loss: 1.3309\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6616 - val_loss: 1.7763\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8434 - val_loss: 2.9214\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7558 - val_loss: 1.6162\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6804 - val_loss: 8.2180\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1901 - val_loss: 1.1431\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1344 - val_loss: 0.9847\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.2200 - val_loss: 1.0696\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4780 - val_loss: 1.6507\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2575 - val_loss: 1.1800\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0251 - val_loss: 0.8276\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1917 - val_loss: 1.0706\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.5072 - val_loss: 1.8560\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1668 - val_loss: 0.7726\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9272 - val_loss: 8.6212\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8571 - val_loss: 0.7531\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6892 - val_loss: 0.6355\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9701 - val_loss: 3.4672\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9724 - val_loss: 0.8045\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8358 - val_loss: 1.2533\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0974 - val_loss: 0.6500\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0335 - val_loss: 1.4782\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0044 - val_loss: 0.8977\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9483 - val_loss: 1.1848\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9043 - val_loss: 0.6450\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.0156 - val_loss: 0.6339\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7004 - val_loss: 0.6763\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5086 - val_loss: 0.5170\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7479 - val_loss: 0.6134\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7704 - val_loss: 0.5734\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8781 - val_loss: 0.6959\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7485 - val_loss: 1.0002\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0073 - val_loss: 0.9410\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6952 - val_loss: 0.4849\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9400 - val_loss: 1.4766\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1741 - val_loss: 0.7101\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6044 - val_loss: 0.8483\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5859 - val_loss: 0.5660\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5579 - val_loss: 0.7512\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9700 - val_loss: 0.6898\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9605 - val_loss: 0.4841\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5154 - val_loss: 0.3525\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8690 - val_loss: 0.5714\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6016 - val_loss: 0.9273\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3670 - val_loss: 1.8740\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5845 - val_loss: 0.4209\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3518 - val_loss: 0.3759\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5246 - val_loss: 0.5100\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9694 - val_loss: 0.3619\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3739 - val_loss: 0.3520\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5137 - val_loss: 0.9808\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8763 - val_loss: 0.4737\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.7281 - val_loss: 0.2882\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5858 - val_loss: 0.8092\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6675 - val_loss: 1.4801\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4691 - val_loss: 0.5144\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5280 - val_loss: 0.9305\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6806 - val_loss: 1.4408\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7322 - val_loss: 0.2617\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4002 - val_loss: 0.3897\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3455 - val_loss: 0.3552\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1033 - val_loss: 1.2314\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6151 - val_loss: 0.6905\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4449 - val_loss: 0.2988\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3291 - val_loss: 0.9477\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.7692 - val_loss: 0.3627\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2707 - val_loss: 0.2896\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6129 - val_loss: 1.3681\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.8281 - val_loss: 0.5464\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4328 - val_loss: 0.4266\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5910 - val_loss: 0.5368\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2982 - val_loss: 0.2727\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6174 - val_loss: 0.5391\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.4509 - val_loss: 0.2494\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.4486 - val_loss: 0.4191\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5222 - val_loss: 0.5420\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3268 - val_loss: 0.2641\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3019 - val_loss: 0.1737\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3872 - val_loss: 1.0617\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9881 - val_loss: 0.7488\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.7485571823805253\n",
      "Mean Absolute Error (MAE): 0.6288200533724622\n",
      "Root Mean Squared Error (RMSE): 0.8651919916299071\n",
      "Time taken: 1177.994732618332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_17948\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.236547  0.355535  0.486361  1212.927969\n",
      "1        2  0.082380  0.208630  0.287019  1206.229196\n",
      "2        3  0.155524  0.280430  0.394365  1215.000936\n",
      "3        4  0.354912  0.434966  0.595745  1173.198413\n",
      "4        5  0.748557  0.628820  0.865192  1177.994733\n",
      "5  Average  0.315584  0.381676  0.525737  1197.070249\n",
      "Results saved to 'PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
