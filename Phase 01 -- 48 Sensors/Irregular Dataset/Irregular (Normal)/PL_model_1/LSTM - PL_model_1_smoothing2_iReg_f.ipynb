{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1392.7065 - val_loss: 1308.5054\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1233.1212 - val_loss: 1203.5781\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1143.7026 - val_loss: 1122.5215\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1075.4130 - val_loss: 1063.0524\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1025.3124 - val_loss: 1018.5815\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 988.3085 - val_loss: 985.9052\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 961.8719 - val_loss: 962.7808\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 943.7031 - val_loss: 947.2399\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 931.9639 - val_loss: 937.3105\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 924.9631 - val_loss: 931.4697\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.0421 - val_loss: 928.2791\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 919.0942 - val_loss: 926.6830\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.2371 - val_loss: 925.9229\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.9083 - val_loss: 925.5576\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 917.8228 - val_loss: 925.4523\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7794 - val_loss: 925.3787\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7806 - val_loss: 925.3212\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7742 - val_loss: 925.3013\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7929 - val_loss: 925.2739\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 917.7869 - val_loss: 925.2742\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 917.7823 - val_loss: 925.2908\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7682 - val_loss: 925.2996\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7445 - val_loss: 925.2766\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7709 - val_loss: 925.3113\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.7805 - val_loss: 925.3275\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.9650 - val_loss: 927.0092\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 886.7993 - val_loss: 867.2519\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 849.9388 - val_loss: 849.1601\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 834.6530 - val_loss: 832.1294\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 817.5671 - val_loss: 816.8389\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 806.1625 - val_loss: 803.3666\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 791.3231 - val_loss: 789.1806\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 769.7433 - val_loss: 756.6172\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 734.1559 - val_loss: 715.3519\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 697.0413 - val_loss: 683.6023\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 647.3618 - val_loss: 621.6451\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 594.8463 - val_loss: 575.5671\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 547.6566 - val_loss: 527.7977\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 504.5686 - val_loss: 484.4287\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 457.0772 - val_loss: 439.6953\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 413.8399 - val_loss: 397.7689\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 375.4449 - val_loss: 359.1494\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 339.0016 - val_loss: 331.6204\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 307.1524 - val_loss: 295.5671\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 279.0229 - val_loss: 268.9314\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 249.9999 - val_loss: 239.8526\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 229.1441 - val_loss: 216.4752\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 202.3215 - val_loss: 193.3408\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 182.0031 - val_loss: 173.3625\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 162.8215 - val_loss: 156.6594\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 144.2598 - val_loss: 136.5906\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 127.7563 - val_loss: 119.6302\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 111.8943 - val_loss: 106.0117\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 97.7075 - val_loss: 93.5495\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 87.9263 - val_loss: 79.8545\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.6875 - val_loss: 70.8861\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 66.3530 - val_loss: 61.1583\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.8195 - val_loss: 54.2388\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.9632 - val_loss: 44.2552\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.9596 - val_loss: 43.3181\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 37.8698 - val_loss: 33.0092\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.3470 - val_loss: 27.5165\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.7475 - val_loss: 24.1447\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.1602 - val_loss: 20.0438\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.7429 - val_loss: 16.7837\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.4134 - val_loss: 16.5813\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.9659 - val_loss: 16.2079\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 12.0236 - val_loss: 9.7491\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.4020 - val_loss: 8.3681\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.3669 - val_loss: 8.1447\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6409 - val_loss: 6.5494\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 6.5452 - val_loss: 6.7522\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3193 - val_loss: 4.6014\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0399 - val_loss: 4.0607\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9152 - val_loss: 5.0090\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4706 - val_loss: 3.3502\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 2.9387 - val_loss: 2.8463\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 19ms/step - loss: 2.2398 - val_loss: 1.5511\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1.7925 - val_loss: 1.4093\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 2.1060 - val_loss: 2.2508\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 2.9392 - val_loss: 1.3818\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 1.1584 - val_loss: 1.5256\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 1.1648 - val_loss: 1.1001\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.6128 - val_loss: 1.2705\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1.2081 - val_loss: 1.3808\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.1743 - val_loss: 0.6721\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7098 - val_loss: 0.6800\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.8748 - val_loss: 0.9421\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.0317 - val_loss: 1.2630\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8791 - val_loss: 0.4442\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 1.0191 - val_loss: 1.4727\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.7392 - val_loss: 0.3887\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.4988 - val_loss: 0.4279\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4825 - val_loss: 1.0563\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0807 - val_loss: 1.5767\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4975 - val_loss: 0.2853\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6481 - val_loss: 0.8280\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.7598 - val_loss: 0.2977\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.6350 - val_loss: 0.7286\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.8623 - val_loss: 0.3888\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.6721 - val_loss: 0.3613\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7517 - val_loss: 1.6411\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5624 - val_loss: 0.1648\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3283 - val_loss: 0.3594\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.4971 - val_loss: 0.9815\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.4861 - val_loss: 0.2836\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3446 - val_loss: 0.2780\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.5213 - val_loss: 0.1579\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.4759 - val_loss: 0.2119\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3978 - val_loss: 0.3585\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3982 - val_loss: 0.7959\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4452 - val_loss: 0.4254\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6927 - val_loss: 0.1744\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.4204 - val_loss: 0.7131\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5641 - val_loss: 0.5216\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3244 - val_loss: 0.5744\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4922 - val_loss: 0.2505\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3674 - val_loss: 0.6338\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6266 - val_loss: 1.2271\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4461 - val_loss: 0.2763\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2976 - val_loss: 0.2464\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4553 - val_loss: 0.2501\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3943 - val_loss: 0.1933\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2435 - val_loss: 0.2684\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5933 - val_loss: 0.9434\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.5878 - val_loss: 0.2774\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2373 - val_loss: 0.1994\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3605 - val_loss: 0.8100\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6345 - val_loss: 0.9630\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5201 - val_loss: 0.1398\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1620 - val_loss: 0.1008\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3964 - val_loss: 0.8310\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3815 - val_loss: 0.6064\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3181 - val_loss: 0.2455\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4957 - val_loss: 0.6489\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2684 - val_loss: 0.7927\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1556 - val_loss: 0.0837\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0668 - val_loss: 0.0612\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1331 - val_loss: 0.1896\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5099 - val_loss: 0.2263\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1527 - val_loss: 0.1010\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1725 - val_loss: 0.4416\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.6178 - val_loss: 0.3187\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2377 - val_loss: 0.1026\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1315 - val_loss: 0.1918\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3729 - val_loss: 0.2166\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5126 - val_loss: 0.2361\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1791 - val_loss: 0.2469\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2379 - val_loss: 0.3339\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2365 - val_loss: 0.1447\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1769 - val_loss: 0.3322\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3666 - val_loss: 0.3716\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7646 - val_loss: 0.0697\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0710 - val_loss: 0.0921\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1558 - val_loss: 0.3472\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2726 - val_loss: 0.2223\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2044 - val_loss: 0.2074\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2988 - val_loss: 0.3942\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2350 - val_loss: 0.1608\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2578 - val_loss: 0.3625\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4690 - val_loss: 0.6761\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1960 - val_loss: 0.2999\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1136 - val_loss: 0.1117\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3035 - val_loss: 0.2583\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2695 - val_loss: 0.1049\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2973 - val_loss: 0.5578\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4673 - val_loss: 0.0679\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0980 - val_loss: 0.1366\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3174 - val_loss: 0.5295\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1643 - val_loss: 0.1447\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1492 - val_loss: 0.2433\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3264 - val_loss: 0.1946\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3544 - val_loss: 0.1075\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2364 - val_loss: 0.1846\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3233 - val_loss: 0.1139\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1200 - val_loss: 0.1184\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2541 - val_loss: 0.1035\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1762 - val_loss: 0.2278\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2218 - val_loss: 0.1133\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2645 - val_loss: 0.2587\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3586 - val_loss: 0.1324\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0908 - val_loss: 0.0996\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.2722 - val_loss: 0.2830\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2594 - val_loss: 0.1249\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1102 - val_loss: 0.1219\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2947 - val_loss: 0.2076\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2243 - val_loss: 0.2453\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1149 - val_loss: 0.0900\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 0.4152 - val_loss: 1.0232\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1684 - val_loss: 0.0342\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0651 - val_loss: 0.0920\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2183 - val_loss: 0.1386\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3534 - val_loss: 0.2606\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1131 - val_loss: 0.0646\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1109 - val_loss: 0.1473\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3981 - val_loss: 0.8541\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.3278 - val_loss: 0.0808\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.0639 - val_loss: 0.0567\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.1097 - val_loss: 0.2840\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.2672 - val_loss: 0.2263\n",
      "16/16 [==============================] - 2s 31ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.22625539582172025\n",
      "Mean Absolute Error (MAE): 0.3626563820563937\n",
      "Root Mean Squared Error (RMSE): 0.4756631116890611\n",
      "Time taken: 2856.0945389270782\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 17s 35ms/step - loss: 1368.9645 - val_loss: 1268.8783\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 1203.7909 - val_loss: 1172.7854\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1118.1003 - val_loss: 1102.4180\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1054.6824 - val_loss: 1050.3947\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1007.8425 - val_loss: 1012.2327\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 973.8112 - val_loss: 985.1709\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 950.0674 - val_loss: 966.8686\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 934.1545 - val_loss: 955.1168\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 924.3370 - val_loss: 948.2947\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 918.6860 - val_loss: 944.7363\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 915.7535 - val_loss: 943.1250\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 914.4198 - val_loss: 942.5474\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.8549 - val_loss: 942.3361\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.6187 - val_loss: 942.3232\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5785 - val_loss: 942.3393\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5883 - val_loss: 942.3219\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5654 - val_loss: 942.4065\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5410 - val_loss: 942.4052\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5635 - val_loss: 942.5386\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5351 - val_loss: 942.4037\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5817 - val_loss: 942.4554\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5239 - val_loss: 942.4336\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5794 - val_loss: 942.3692\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.6475 - val_loss: 942.1504\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.6830 - val_loss: 942.2228\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5762 - val_loss: 942.2450\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5696 - val_loss: 942.3126\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5613 - val_loss: 942.3553\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5982 - val_loss: 942.3565\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 913.5991 - val_loss: 942.4199\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 913.5965 - val_loss: 942.3456\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 914.3539 - val_loss: 941.7740\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 910.0093 - val_loss: 910.8842\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 852.8961 - val_loss: 850.2910\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 798.9478 - val_loss: 796.6291\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 738.4562 - val_loss: 726.4413\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 683.9098 - val_loss: 676.6412\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 633.8214 - val_loss: 628.1772\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 578.6434 - val_loss: 569.7985\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 533.8304 - val_loss: 525.1028\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 491.2285 - val_loss: 490.2416\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 452.4854 - val_loss: 446.7699\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 416.3106 - val_loss: 409.4061\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 382.0485 - val_loss: 376.4557\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 349.2521 - val_loss: 344.5941\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 318.0364 - val_loss: 310.1159\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 287.5135 - val_loss: 281.1144\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 259.7364 - val_loss: 253.9830\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 234.9328 - val_loss: 228.8981\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 213.2543 - val_loss: 206.2657\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 192.4396 - val_loss: 184.7711\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 166.8638 - val_loss: 161.6370\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 147.3323 - val_loss: 143.1864\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 129.9308 - val_loss: 125.7101\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 116.2575 - val_loss: 112.1563\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 101.3589 - val_loss: 96.4406\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 88.1468 - val_loss: 86.3757\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 77.1609 - val_loss: 72.3985\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 67.0156 - val_loss: 64.0270\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 58.1447 - val_loss: 56.3370\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 51.3248 - val_loss: 48.7608\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 42.9165 - val_loss: 41.0885\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 36.6472 - val_loss: 37.7780\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 33.0702 - val_loss: 35.0689\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 28.0500 - val_loss: 25.6958\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 23.5089 - val_loss: 23.6698\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 21.3851 - val_loss: 21.2529\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 18.0546 - val_loss: 19.1406\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 15.4356 - val_loss: 14.1748\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 12.6697 - val_loss: 13.4060\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 11.0830 - val_loss: 10.1606\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 9.2990 - val_loss: 11.0926\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 8.1081 - val_loss: 8.2146\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 6.9816 - val_loss: 7.0139\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 6.5232 - val_loss: 7.1420\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 7.0951 - val_loss: 5.4905\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 32ms/step - loss: 4.7708 - val_loss: 3.6416\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 3.3147 - val_loss: 3.1943\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.9896 - val_loss: 3.5358\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 3.8589 - val_loss: 3.0638\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.7876 - val_loss: 3.5050\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.1647 - val_loss: 2.1438\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.7884 - val_loss: 1.9398\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.5757 - val_loss: 1.5047\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.6278 - val_loss: 1.9909\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4839 - val_loss: 2.5071\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.1077 - val_loss: 2.4154\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.2596 - val_loss: 1.1571\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9392 - val_loss: 1.1074\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.3521 - val_loss: 2.0372\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4478 - val_loss: 1.6867\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8319 - val_loss: 0.5078\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.7297 - val_loss: 0.6554\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.0612 - val_loss: 0.5836\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.1342 - val_loss: 1.6712\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4997 - val_loss: 0.8516\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 1.2924 - val_loss: 1.1721\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 1.0855 - val_loss: 0.7376\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5272 - val_loss: 0.4648\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6791 - val_loss: 1.1055\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.1657 - val_loss: 0.5345\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5815 - val_loss: 1.2722\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9569 - val_loss: 0.6111\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.7516 - val_loss: 0.6718\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4067 - val_loss: 0.3751\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.6128 - val_loss: 0.6799\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.8468 - val_loss: 0.5996\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4143 - val_loss: 0.6266\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4884 - val_loss: 0.7603\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4395 - val_loss: 0.5361\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3973 - val_loss: 0.2872\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3004 - val_loss: 1.1874\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8394 - val_loss: 1.0098\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8866 - val_loss: 0.6550\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5521 - val_loss: 0.4054\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.6371 - val_loss: 1.7668\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 1.1968 - val_loss: 0.3608\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2475 - val_loss: 0.2162\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.8312 - val_loss: 2.1286\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.5344 - val_loss: 0.3942\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5693 - val_loss: 0.2760\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2350 - val_loss: 0.1769\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.2342 - val_loss: 0.5005\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2173 - val_loss: 0.1011\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4743 - val_loss: 0.7124\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.5454 - val_loss: 0.4419\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4356 - val_loss: 0.1987\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3825 - val_loss: 0.2844\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5121 - val_loss: 0.3113\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.2131 - val_loss: 0.4867\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9213 - val_loss: 0.7009\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.6192 - val_loss: 0.3089\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.2863 - val_loss: 0.2604\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3313 - val_loss: 0.6605\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6752 - val_loss: 0.2925\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.3834 - val_loss: 0.3259\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.3738 - val_loss: 0.9355\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.5501 - val_loss: 0.3816\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2993 - val_loss: 0.3446\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.7518 - val_loss: 0.4386\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2280 - val_loss: 0.1191\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3879 - val_loss: 0.6484\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4428 - val_loss: 0.3243\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3924 - val_loss: 0.5004\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4754 - val_loss: 0.2381\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2443 - val_loss: 0.3436\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2070 - val_loss: 0.2110\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3185 - val_loss: 0.3375\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6223 - val_loss: 0.2521\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3311 - val_loss: 0.3747\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1778 - val_loss: 0.2754\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6987 - val_loss: 0.7169\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2864 - val_loss: 0.1424\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1140 - val_loss: 0.2004\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4976 - val_loss: 0.2570\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3207 - val_loss: 0.1713\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8318 - val_loss: 5.5633\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.2480 - val_loss: 0.0947\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0775 - val_loss: 0.0634\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0553 - val_loss: 0.0545\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3454 - val_loss: 0.1552\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1361 - val_loss: 0.1600\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1505 - val_loss: 0.2852\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4263 - val_loss: 0.5845\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3702 - val_loss: 0.0877\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1297 - val_loss: 0.1895\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3947 - val_loss: 0.2199\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2360 - val_loss: 0.5517\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3619 - val_loss: 0.4453\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3413 - val_loss: 0.3679\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6520 - val_loss: 0.4179\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1436 - val_loss: 0.0997\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1846 - val_loss: 0.1459\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2465 - val_loss: 0.6340\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5289 - val_loss: 0.2252\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0825 - val_loss: 0.1080\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3704 - val_loss: 1.3487\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5646 - val_loss: 0.0728\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1377 - val_loss: 0.1291\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.1107 - val_loss: 0.1254\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.1653 - val_loss: 0.2713\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3469 - val_loss: 0.6811\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.3024 - val_loss: 0.0909\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1120 - val_loss: 0.0839\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3213 - val_loss: 0.2506\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4966 - val_loss: 0.5251\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1055 - val_loss: 0.0493\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.1470 - val_loss: 0.2813\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4194 - val_loss: 0.1961\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1391 - val_loss: 0.3271\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2493 - val_loss: 0.4975\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2681 - val_loss: 0.8087\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3637 - val_loss: 0.1341\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0907 - val_loss: 0.1160\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2417 - val_loss: 0.4073\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.2373 - val_loss: 0.8476\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3642 - val_loss: 0.4904\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.4007 - val_loss: 0.3016\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1151 - val_loss: 0.1034\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1219 - val_loss: 0.2106\n",
      "16/16 [==============================] - 2s 36ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.21060522460707956\n",
      "Mean Absolute Error (MAE): 0.32450014074631306\n",
      "Root Mean Squared Error (RMSE): 0.4589174485755358\n",
      "Time taken: 2463.671211719513\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 21s 44ms/step - loss: 1391.8936 - val_loss: 1283.7544\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 1225.8325 - val_loss: 1181.6813\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 1137.2736 - val_loss: 1106.3706\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1071.6257 - val_loss: 1049.8588\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1022.5450 - val_loss: 1007.4567\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 986.4811 - val_loss: 976.7138\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 960.7241 - val_loss: 955.0149\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 943.0870 - val_loss: 940.6214\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 931.9258 - val_loss: 931.5354\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 925.2981 - val_loss: 926.3494\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 921.7181 - val_loss: 923.6093\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 920.0282 - val_loss: 922.3715\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 919.2927 - val_loss: 921.8684\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 919.0251 - val_loss: 921.5786\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 918.9111 - val_loss: 921.5621\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 918.8677 - val_loss: 921.6688\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 918.9297 - val_loss: 921.6605\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 918.8785 - val_loss: 921.7757\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 918.8583 - val_loss: 921.7495\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 919.1546 - val_loss: 921.3611\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 918.3909 - val_loss: 917.5844\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 876.3217 - val_loss: 848.0965\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 807.6353 - val_loss: 781.8693\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 742.6885 - val_loss: 724.1797\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 700.6599 - val_loss: 668.7866\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 633.2403 - val_loss: 614.1102\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 577.7822 - val_loss: 554.4557\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 520.8839 - val_loss: 503.2219\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 475.6628 - val_loss: 460.5700\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 434.2119 - val_loss: 422.0621\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 399.5066 - val_loss: 385.1412\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 359.9887 - val_loss: 348.3735\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 327.1153 - val_loss: 318.0140\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 296.3861 - val_loss: 288.4405\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 269.7960 - val_loss: 261.3144\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 242.1834 - val_loss: 236.1582\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 222.4733 - val_loss: 211.9427\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 195.2970 - val_loss: 190.1927\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 176.2587 - val_loss: 170.7891\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 156.0700 - val_loss: 151.3044\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 139.0035 - val_loss: 136.7758\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 123.2389 - val_loss: 118.7164\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 109.2157 - val_loss: 104.6199\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 96.3285 - val_loss: 92.4434\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 84.4439 - val_loss: 81.3292\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 74.0503 - val_loss: 73.1131\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 66.5860 - val_loss: 61.7986\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 55.9676 - val_loss: 54.0548\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 53.4680 - val_loss: 47.5308\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 41.8551 - val_loss: 40.2025\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 36.6851 - val_loss: 34.6724\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 32.4104 - val_loss: 30.5153\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 27.2723 - val_loss: 31.8056\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 25.9950 - val_loss: 22.8675\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 20.0372 - val_loss: 19.1039\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 18.1828 - val_loss: 17.6836\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 15.4062 - val_loss: 14.4484\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 14.2326 - val_loss: 11.6581\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 10.1054 - val_loss: 10.7266\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 9.4166 - val_loss: 8.8206\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 9.1969 - val_loss: 8.2327\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 8.4522 - val_loss: 6.5288\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 5.7005 - val_loss: 6.8686\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 6.4026 - val_loss: 6.1169\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 4.2454 - val_loss: 4.1000\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 3.8993 - val_loss: 3.6339\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 3.6867 - val_loss: 3.8899\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 3.4767 - val_loss: 4.3530\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 3.0894 - val_loss: 2.5260\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 3.2895 - val_loss: 2.9229\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.1684 - val_loss: 2.3254\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.7168 - val_loss: 2.9167\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.8328 - val_loss: 2.5330\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.8507 - val_loss: 2.2549\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.9850 - val_loss: 1.6574\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.1834 - val_loss: 6.3144\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.8823 - val_loss: 1.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9112 - val_loss: 0.9642\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.0016 - val_loss: 1.1783\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.6805 - val_loss: 1.1724\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.4020 - val_loss: 1.3952\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9818 - val_loss: 0.9819\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.0591 - val_loss: 1.0728\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9273 - val_loss: 0.9468\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8362 - val_loss: 1.1755\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.7153 - val_loss: 1.0692\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.7702 - val_loss: 0.6572\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4515 - val_loss: 0.6740\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9659 - val_loss: 1.1778\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.2007 - val_loss: 1.2308\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.7003 - val_loss: 0.6955\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8079 - val_loss: 1.4065\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9941 - val_loss: 1.0527\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.7340 - val_loss: 0.8528\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5775 - val_loss: 0.4483\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8947 - val_loss: 0.9789\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5334 - val_loss: 0.6919\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6617 - val_loss: 1.4104\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.8218 - val_loss: 0.4603\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2866 - val_loss: 0.3492\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2789 - val_loss: 0.8012\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5016 - val_loss: 0.6136\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2623 - val_loss: 0.5121\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.3325 - val_loss: 0.5044\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3974 - val_loss: 0.9095\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4293 - val_loss: 0.3620\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2867 - val_loss: 0.2878\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 1.1996 - val_loss: 0.7172\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3120 - val_loss: 0.4542\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3607 - val_loss: 0.4198\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3947 - val_loss: 0.5706\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.7708 - val_loss: 0.4859\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4190 - val_loss: 0.4282\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3993 - val_loss: 1.2654\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8705 - val_loss: 0.4184\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2782 - val_loss: 0.5643\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6363 - val_loss: 1.9973\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8561 - val_loss: 0.2611\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1519 - val_loss: 0.2339\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3399 - val_loss: 0.7916\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4422 - val_loss: 0.2692\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3058 - val_loss: 0.3423\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.9954 - val_loss: 0.3940\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1875 - val_loss: 0.1797\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3966 - val_loss: 0.7077\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4706 - val_loss: 0.6812\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4379 - val_loss: 0.6305\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5669 - val_loss: 0.2189\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4792 - val_loss: 0.4873\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6015 - val_loss: 0.5045\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2317 - val_loss: 0.5880\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3770 - val_loss: 0.6646\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4672 - val_loss: 0.4120\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3869 - val_loss: 0.6462\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6241 - val_loss: 0.5520\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2849 - val_loss: 0.1436\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4984 - val_loss: 0.7837\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6122 - val_loss: 0.2111\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1770 - val_loss: 0.1615\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2926 - val_loss: 0.5803\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6887 - val_loss: 0.2547\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1900 - val_loss: 0.2385\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4698 - val_loss: 0.6666\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2710 - val_loss: 0.2049\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2203 - val_loss: 0.2879\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.6317 - val_loss: 0.8017\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4475 - val_loss: 0.4101\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2756 - val_loss: 0.3226\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1774 - val_loss: 0.4303\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3620 - val_loss: 0.6326\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.3512 - val_loss: 0.2058\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1060 - val_loss: 0.1309\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0698 - val_loss: 0.1092\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0734 - val_loss: 0.0932\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1169 - val_loss: 0.1325\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2372 - val_loss: 0.4508\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2414 - val_loss: 0.1499\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3160 - val_loss: 0.6138\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5194 - val_loss: 0.1984\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1528 - val_loss: 0.2663\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3069 - val_loss: 0.3149\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.3894 - val_loss: 0.2563\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 0.2744 - val_loss: 0.3796\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2076 - val_loss: 0.6040\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5299 - val_loss: 1.0143\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2693 - val_loss: 0.1285\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1610 - val_loss: 0.2146\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3442 - val_loss: 0.2964\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4917 - val_loss: 0.6184\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2934 - val_loss: 0.1005\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.0915 - val_loss: 0.1820\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2412 - val_loss: 0.6547\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.8493 - val_loss: 0.2618\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1067 - val_loss: 0.1273\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1067 - val_loss: 0.3471\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 0.3436 - val_loss: 0.1563\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2170 - val_loss: 0.3554\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4110 - val_loss: 0.6547\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2491 - val_loss: 0.4402\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1937 - val_loss: 0.1999\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2370 - val_loss: 0.4128\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3366 - val_loss: 0.1808\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2666 - val_loss: 0.2658\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2005 - val_loss: 0.2314\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2858 - val_loss: 0.2326\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3055 - val_loss: 0.5352\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3971 - val_loss: 0.1721\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1267 - val_loss: 0.0774\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1823 - val_loss: 0.2903\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.4531 - val_loss: 0.1693\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1484 - val_loss: 0.4541\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2639 - val_loss: 0.1853\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1255 - val_loss: 0.1085\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1918 - val_loss: 0.2363\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.5316 - val_loss: 0.5128\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1588 - val_loss: 0.0934\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2025 - val_loss: 0.1768\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.1179 - val_loss: 0.4197\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.2863 - val_loss: 0.2809\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 0.3273 - val_loss: 0.1073\n",
      "16/16 [==============================] - 1s 19ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.10732962468619212\n",
      "Mean Absolute Error (MAE): 0.2483438042791226\n",
      "Root Mean Squared Error (RMSE): 0.3276120032694042\n",
      "Time taken: 3068.9820947647095\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 33ms/step - loss: 1386.8221 - val_loss: 1281.4124\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1222.4229 - val_loss: 1172.5175\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1133.1481 - val_loss: 1092.7871\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1067.5137 - val_loss: 1033.8662\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1019.6075 - val_loss: 990.8119\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 984.7990 - val_loss: 959.8360\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 960.4330 - val_loss: 938.3845\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 944.1884 - val_loss: 924.1125\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 933.9822 - val_loss: 915.6479\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 928.1537 - val_loss: 910.8407\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 925.0880 - val_loss: 908.1110\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 923.6738 - val_loss: 906.8069\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 922.5748 - val_loss: 904.2570\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 912.1907 - val_loss: 871.3715\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 868.2580 - val_loss: 837.9042\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 831.1149 - val_loss: 788.4111\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 763.7150 - val_loss: 718.9700\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 706.2904 - val_loss: 681.3192\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 651.1540 - val_loss: 611.1762\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 592.2458 - val_loss: 561.1456\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 541.2584 - val_loss: 508.8556\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 489.2962 - val_loss: 459.4374\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 441.5910 - val_loss: 417.9833\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 400.7460 - val_loss: 374.7386\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 359.2992 - val_loss: 339.0316\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 323.6312 - val_loss: 303.5946\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 289.8476 - val_loss: 271.5506\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 258.4673 - val_loss: 241.7618\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 228.2878 - val_loss: 213.2224\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 203.3026 - val_loss: 191.6268\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 180.7984 - val_loss: 172.6209\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 154.9047 - val_loss: 146.0477\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 136.4612 - val_loss: 128.0172\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 117.7673 - val_loss: 113.1961\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 104.2964 - val_loss: 99.7052\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 88.3859 - val_loss: 84.0677\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 77.5436 - val_loss: 76.4673\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 64.5094 - val_loss: 61.7728\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 57.3499 - val_loss: 54.6155\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 49.1987 - val_loss: 44.9121\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 40.5891 - val_loss: 38.5669\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 34.9240 - val_loss: 35.8670\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 31.1640 - val_loss: 30.9967\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 26.5072 - val_loss: 27.1998\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 23.4906 - val_loss: 24.1979\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 19.3574 - val_loss: 18.6589\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 15.7363 - val_loss: 14.6957\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 13.0779 - val_loss: 12.6426\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 11.8138 - val_loss: 12.9440\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 10.2285 - val_loss: 9.5624\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 9.0481 - val_loss: 8.5494\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 8.3679 - val_loss: 6.7787\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 6.7773 - val_loss: 6.3725\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 5.5706 - val_loss: 5.1553\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 5.7710 - val_loss: 7.3124\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 4.4238 - val_loss: 4.3183\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.2813 - val_loss: 3.6508\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.5279 - val_loss: 4.7004\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 4.3669 - val_loss: 3.5444\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.3195 - val_loss: 2.2933\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.3193 - val_loss: 1.7848\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 6.2584 - val_loss: 12.9620\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 4.0594 - val_loss: 3.7700\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.9378 - val_loss: 1.4388\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.8841 - val_loss: 1.6320\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.7130 - val_loss: 1.7384\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.5738 - val_loss: 1.9298\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.0251 - val_loss: 4.7637\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.9122 - val_loss: 1.7512\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.0870 - val_loss: 2.7944\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.6921 - val_loss: 2.6240\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.8345 - val_loss: 1.0632\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.1881 - val_loss: 2.0888\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.3108 - val_loss: 1.3589\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.1322 - val_loss: 2.1924\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.2429 - val_loss: 0.5558\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.4160 - val_loss: 1.9680\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9463 - val_loss: 1.0619\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.0355 - val_loss: 1.0881\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.8022 - val_loss: 2.3187\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8861 - val_loss: 0.7333\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9700 - val_loss: 1.8680\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.5243 - val_loss: 0.4570\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5758 - val_loss: 0.5835\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8944 - val_loss: 0.8186\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9150 - val_loss: 0.5549\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5251 - val_loss: 0.8430\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9682 - val_loss: 0.6739\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9836 - val_loss: 0.5706\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.2930 - val_loss: 0.8787\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3845 - val_loss: 0.3186\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5910 - val_loss: 0.8652\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6976 - val_loss: 1.4676\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.8539 - val_loss: 1.0084\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5795 - val_loss: 0.4236\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.5375 - val_loss: 0.4271\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9526 - val_loss: 1.2355\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4311 - val_loss: 0.5807\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7686 - val_loss: 0.9680\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8531 - val_loss: 0.4045\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5910 - val_loss: 0.5346\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 5.3707 - val_loss: 0.3141\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2146 - val_loss: 0.1837\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1742 - val_loss: 0.1766\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3580 - val_loss: 0.3624\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3855 - val_loss: 0.7166\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5400 - val_loss: 0.9511\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7730 - val_loss: 0.4239\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3437 - val_loss: 0.4448\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6692 - val_loss: 1.1543\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.0584 - val_loss: 0.4433\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4117 - val_loss: 0.8893\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.9711 - val_loss: 0.5508\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3935 - val_loss: 0.2131\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3530 - val_loss: 0.3585\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5759 - val_loss: 0.2862\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3760 - val_loss: 0.5719\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8162 - val_loss: 0.5913\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6672 - val_loss: 0.3498\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.6663 - val_loss: 1.1987\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4862 - val_loss: 0.2080\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1933 - val_loss: 0.5846\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4293 - val_loss: 0.2812\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5458 - val_loss: 0.3385\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3195 - val_loss: 0.1545\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6547 - val_loss: 0.4603\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3472 - val_loss: 0.4471\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7556 - val_loss: 0.3641\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7044 - val_loss: 0.4003\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2852 - val_loss: 0.2468\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7719 - val_loss: 2.0880\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7376 - val_loss: 0.1849\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2745 - val_loss: 0.2428\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.8520 - val_loss: 1.7085\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5628 - val_loss: 0.2778\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1606 - val_loss: 0.2161\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.2013 - val_loss: 0.4390\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4290 - val_loss: 0.5781\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.6023 - val_loss: 0.4586\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.9981 - val_loss: 0.2543\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.1398 - val_loss: 0.1103\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.1938 - val_loss: 0.1842\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.5283 - val_loss: 0.4509\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.5672 - val_loss: 0.1598\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4526 - val_loss: 1.0737\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.6454 - val_loss: 0.5457\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.8485 - val_loss: 0.8611\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3265 - val_loss: 0.1151\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2774 - val_loss: 0.5114\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6364 - val_loss: 4.5538\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6636 - val_loss: 0.1404\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1884 - val_loss: 0.4017\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 4.1712 - val_loss: 1.9141\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2782 - val_loss: 0.1179\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1151 - val_loss: 0.0802\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0708 - val_loss: 0.1096\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.0890 - val_loss: 0.0802\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1776 - val_loss: 0.5340\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2374 - val_loss: 0.2555\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3998 - val_loss: 0.4312\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6599 - val_loss: 0.4566\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2318 - val_loss: 0.2598\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2751 - val_loss: 0.4987\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4479 - val_loss: 0.2514\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6573 - val_loss: 1.6730\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4470 - val_loss: 0.8134\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4765 - val_loss: 0.2513\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3303 - val_loss: 0.2667\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2098 - val_loss: 0.1001\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4945 - val_loss: 0.8490\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7762 - val_loss: 0.3106\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2005 - val_loss: 0.1913\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3399 - val_loss: 0.2347\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2463 - val_loss: 0.2137\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8282 - val_loss: 1.3712\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.4534 - val_loss: 0.0774\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2938 - val_loss: 0.2264\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2139 - val_loss: 0.2004\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4556 - val_loss: 0.2619\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.9250 - val_loss: 0.5576\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2058 - val_loss: 0.2185\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.1133 - val_loss: 0.1018\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.1418 - val_loss: 0.1169\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2500 - val_loss: 0.6968\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9973 - val_loss: 4.0546\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7846 - val_loss: 0.1215\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.0977 - val_loss: 0.0993\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.1077 - val_loss: 0.1848\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.3024 - val_loss: 0.4997\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.3516 - val_loss: 0.1277\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.2840 - val_loss: 0.5206\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.3011 - val_loss: 0.1463\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4724 - val_loss: 0.6219\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3767 - val_loss: 0.0970\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.1574 - val_loss: 0.1056\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.3652 - val_loss: 0.5753\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.2835 - val_loss: 0.4260\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.3613 - val_loss: 0.3791\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 0.2894 - val_loss: 0.1516\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1645 - val_loss: 0.1875\n",
      "16/16 [==============================] - 1s 17ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.18745183850904393\n",
      "Mean Absolute Error (MAE): 0.32255338193381217\n",
      "Root Mean Squared Error (RMSE): 0.43295708622107565\n",
      "Time taken: 2415.4501538276672\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 52ms/step - loss: 1365.5890 - val_loss: 1221.1715\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 1210.2861 - val_loss: 1126.9412\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1126.5747 - val_loss: 1058.9430\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1064.8400 - val_loss: 1007.8234\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1018.3131 - val_loss: 970.4886\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 983.9702 - val_loss: 943.8421\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 960.1205 - val_loss: 926.1484\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 944.1494 - val_loss: 915.1917\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 934.1519 - val_loss: 908.8402\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 928.3702 - val_loss: 905.8526\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 925.3928 - val_loss: 904.5164\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 924.0201 - val_loss: 904.2213\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 923.4695 - val_loss: 904.1960\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 923.1770 - val_loss: 904.1884\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 923.0931 - val_loss: 904.4852\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 923.0995 - val_loss: 904.4490\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 923.0779 - val_loss: 904.3915\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 923.0894 - val_loss: 904.4409\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 917.3613 - val_loss: 875.2731\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 873.2107 - val_loss: 833.5897\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 849.1125 - val_loss: 816.3816\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 829.1248 - val_loss: 797.3340\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 803.0190 - val_loss: 764.3834\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 774.4060 - val_loss: 748.6080\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 753.1640 - val_loss: 728.6503\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 735.4911 - val_loss: 713.3380\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 719.7899 - val_loss: 700.8121\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 706.0576 - val_loss: 690.4467\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 696.8464 - val_loss: 678.0052\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 681.2836 - val_loss: 667.0856\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 669.1531 - val_loss: 671.7916\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 656.7449 - val_loss: 640.8558\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 631.6865 - val_loss: 605.3419\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 587.4153 - val_loss: 549.0728\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 531.6132 - val_loss: 503.9069\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 487.7694 - val_loss: 473.9396\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 429.7876 - val_loss: 377.2121\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 366.4726 - val_loss: 333.8207\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 323.5313 - val_loss: 292.3848\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 286.0396 - val_loss: 255.5905\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 253.6156 - val_loss: 237.9048\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 222.2984 - val_loss: 197.2854\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 193.9182 - val_loss: 199.0419\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 168.9570 - val_loss: 150.2596\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 146.5276 - val_loss: 128.3558\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 125.8628 - val_loss: 115.6034\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 110.3979 - val_loss: 98.8221\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 92.4974 - val_loss: 79.1547\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 78.7354 - val_loss: 67.1396\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 70.9256 - val_loss: 89.3434\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 58.4726 - val_loss: 47.7318\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 47.1510 - val_loss: 39.4404\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 39.4839 - val_loss: 41.6346\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 35.2929 - val_loss: 31.5927\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 28.4381 - val_loss: 23.0210\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 23.0221 - val_loss: 19.0259\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 19.7566 - val_loss: 17.2402\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 16.7019 - val_loss: 18.9131\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 13.9748 - val_loss: 10.6599\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 10.8733 - val_loss: 8.5387\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 9.4099 - val_loss: 8.1021\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 8.0383 - val_loss: 6.8223\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 11.6138 - val_loss: 10.7162\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 6.8880 - val_loss: 4.4344\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.2944 - val_loss: 3.3459\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.5016 - val_loss: 3.3590\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.8406 - val_loss: 3.4876\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.9421 - val_loss: 2.0119\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.9620 - val_loss: 1.5311\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.6327 - val_loss: 4.1125\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.2011 - val_loss: 1.1829\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.4468 - val_loss: 2.3371\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.5763 - val_loss: 1.8079\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.7968 - val_loss: 1.5120\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2469 - val_loss: 1.1801\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2525 - val_loss: 0.6805\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.3407 - val_loss: 1.4830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.5923 - val_loss: 2.0209\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1055 - val_loss: 0.6227\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0528 - val_loss: 1.3363\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9933 - val_loss: 1.2417\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6886 - val_loss: 1.0786\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.7412 - val_loss: 2.3253\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8243 - val_loss: 0.7266\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5541 - val_loss: 1.0186\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9932 - val_loss: 0.7393\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8887 - val_loss: 0.5345\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7872 - val_loss: 0.7733\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7372 - val_loss: 0.5379\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1002 - val_loss: 0.2793\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5934 - val_loss: 0.4637\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9364 - val_loss: 0.7577\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6146 - val_loss: 0.2466\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9270 - val_loss: 0.9584\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6208 - val_loss: 1.5733\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.1911 - val_loss: 0.8880\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4943 - val_loss: 1.9812\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4802 - val_loss: 1.1855\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8298 - val_loss: 0.2723\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3246 - val_loss: 0.8320\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9769 - val_loss: 0.5752\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5839 - val_loss: 0.6742\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4593 - val_loss: 0.2051\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2999 - val_loss: 1.2025\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.0337 - val_loss: 0.3437\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3802 - val_loss: 0.2589\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3615 - val_loss: 0.7248\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.8101 - val_loss: 0.2227\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9146 - val_loss: 2.6047\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9738 - val_loss: 0.4034\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1763 - val_loss: 0.1914\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3250 - val_loss: 0.8656\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6080 - val_loss: 0.5904\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4546 - val_loss: 0.3125\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9499 - val_loss: 0.3948\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6910 - val_loss: 0.4779\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1662 - val_loss: 0.1079\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5352 - val_loss: 0.7696\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7207 - val_loss: 0.2361\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2556 - val_loss: 0.3579\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7544 - val_loss: 1.2017\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5144 - val_loss: 0.3845\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6907 - val_loss: 0.1978\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1883 - val_loss: 0.2077\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3768 - val_loss: 0.5254\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6199 - val_loss: 0.6008\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5550 - val_loss: 0.7190\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4261 - val_loss: 0.3901\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5669 - val_loss: 1.2601\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4560 - val_loss: 0.0984\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2648 - val_loss: 0.3609\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3072 - val_loss: 0.5317\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6283 - val_loss: 0.2305\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2734 - val_loss: 0.1946\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4471 - val_loss: 0.2445\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7959 - val_loss: 4.3109\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5602 - val_loss: 0.1291\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1933 - val_loss: 0.0634\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1551 - val_loss: 0.3015\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 1.2680 - val_loss: 0.5466\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2912 - val_loss: 0.2706\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1016 - val_loss: 0.1056\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4400 - val_loss: 0.7216\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7100 - val_loss: 0.4917\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2738 - val_loss: 0.3818\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4221 - val_loss: 0.3154\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2259 - val_loss: 0.3401\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1923 - val_loss: 0.6759\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7353 - val_loss: 0.1657\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1366 - val_loss: 0.1358\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3223 - val_loss: 0.4388\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4452 - val_loss: 0.2610\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2273 - val_loss: 0.2502\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4282 - val_loss: 0.5573\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3382 - val_loss: 0.1100\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1536 - val_loss: 0.4347\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5047 - val_loss: 0.4653\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2770 - val_loss: 0.1608\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4014 - val_loss: 1.3251\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3646 - val_loss: 0.0975\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1030 - val_loss: 0.0861\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7411 - val_loss: 0.4288\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1539 - val_loss: 0.2059\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2314 - val_loss: 0.4570\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2694 - val_loss: 0.1087\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3072 - val_loss: 0.2526\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2527 - val_loss: 0.2793\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2418 - val_loss: 0.4800\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4191 - val_loss: 0.2526\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2279 - val_loss: 0.0878\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1608 - val_loss: 0.4366\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.7305 - val_loss: 0.2977\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1203 - val_loss: 0.0953\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0782 - val_loss: 0.1126\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4805 - val_loss: 1.9330\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4127 - val_loss: 0.1236\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1340 - val_loss: 0.2547\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.4285 - val_loss: 0.1145\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1274 - val_loss: 0.2371\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5593 - val_loss: 0.1449\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0897 - val_loss: 0.0962\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3656 - val_loss: 0.7208\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1560 - val_loss: 0.1053\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2558 - val_loss: 0.4243\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6019 - val_loss: 0.0942\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1077 - val_loss: 0.1142\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1641 - val_loss: 0.1544\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3860 - val_loss: 1.5575\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.6106 - val_loss: 0.0447\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1326 - val_loss: 0.5804\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.1875 - val_loss: 0.1085\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0955 - val_loss: 0.2003\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2333 - val_loss: 0.1903\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.3465 - val_loss: 2.5176\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5758 - val_loss: 0.0424\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0424 - val_loss: 0.0459\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0962 - val_loss: 0.1616\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.5391 - val_loss: 0.6568\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.2130 - val_loss: 0.1150\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.0575 - val_loss: 0.3081\n",
      "16/16 [==============================] - 1s 30ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.3080811201185676\n",
      "Mean Absolute Error (MAE): 0.3575275607078614\n",
      "Root Mean Squared Error (RMSE): 0.5550505563627223\n",
      "Time taken: 3750.3185012340546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_8772\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.226255  0.362656  0.475663  2856.094539\n",
      "1        2  0.210605  0.324500  0.458917  2463.671212\n",
      "2        3  0.107330  0.248344  0.327612  3068.982095\n",
      "3        4  0.187452  0.322553  0.432957  2415.450154\n",
      "4        5  0.308081  0.357528  0.555051  3750.318501\n",
      "5  Average  0.207945  0.323116  0.450040  2910.903300\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6tUlEQVR4nOzdeXgUVdo28LuqO/tKCNlIwCSEVTZBEEEGhQFBHRdcUEZcUEcFHMbdD+UVN8Z1XMdlHEXnhXGZd1DcWFQUFURWRUAIIUACSSAk6ZCQrbvq+6PpIk0IJHk63VXN/bsuLyunK93n3NUN/VB1Tim6rusgIiIiIiISUAPdASIiIiIisj4WFkREREREJMbCgoiIiIiIxFhYEBERERGRGAsLIiIiIiISY2FBRERERERiLCyIiIiIiEiMhQUREREREYmxsCAiIiIiIjEWFkREREREJMbCgojoFDRv3jwoioK1a9cGuistsnHjRvzxj39ERkYGwsLCkJCQgDFjxuDtt9+Gy+UKdPeIiAiAPdAdICIiOpE333wTt956K5KTk3HttdciJycHhw4dwldffYWpU6eiqKgI/+///b9Ad5OI6JTHwoKIiEzrxx9/xK233ophw4bh888/R0xMjPHYzJkzsXbtWvz6668+ea3q6mpERUX55LmIiE5FvBSKiIiatWHDBowfPx6xsbGIjo7G6NGj8eOPP3rt09DQgDlz5iAnJwfh4eHo2LEjRowYgWXLlhn7FBcX44YbbkB6ejrCwsKQmpqKiy++GLt27Trh68+ZMweKomD+/PleRYXH4MGDcf311wMAvvnmGyiKgm+++cZrn127dkFRFMybN89ou/766xEdHY28vDxMmDABMTExmDx5MqZPn47o6GgcPny4yWtdffXVSElJ8br06osvvsA555yDqKgoxMTE4IILLsDmzZtPOCYiomDFwoKIiI5r8+bNOOecc/Dzzz/j3nvvxUMPPYT8/HyMGjUKq1evNvZ7+OGHMWfOHJx77rl4+eWXMWvWLHTp0gXr16839pk4cSIWLlyIG264AX//+99xxx134NChQ9izZ0+zr3/48GF89dVXGDlyJLp06eLz8TmdTowbNw5JSUl45plnMHHiRFx11VWorq7GZ5991qQvn3zyCS6//HLYbDYAwL/+9S9ccMEFiI6OxpNPPomHHnoIW7ZswYgRI05aMBERBSNeCkVERMf14IMPoqGhAd9//z2ysrIAAFOmTEGPHj1w77334ttvvwUAfPbZZ5gwYQLeeOON4z5PRUUFVq5ciaeffhp333230f7AAw+c8PV37NiBhoYG9O3b10cj8lZXV4crrrgCc+fONdp0XUfnzp3x/vvv44orrjDaP/vsM1RXV+Oqq64CAFRVVeGOO+7ATTfd5DXu6667Dj169MATTzzRbB5ERMGKZyyIiKgJl8uFpUuX4pJLLjGKCgBITU3FNddcg++//x6VlZUAgPj4eGzevBm5ubnHfa6IiAiEhobim2++QXl5eYv74Hn+410C5Su33Xab18+KouCKK67A559/jqqqKqP9/fffR+fOnTFixAgAwLJly1BRUYGrr74apaWlxn82mw1Dhw7F8uXL263PRERmxcKCiIiaOHDgAA4fPowePXo0eaxXr17QNA0FBQUAgEceeQQVFRXo3r07+vbti3vuuQe//PKLsX9YWBiefPJJfPHFF0hOTsbIkSPx1FNPobi4+IR9iI2NBQAcOnTIhyM7ym63Iz09vUn7VVddhZqaGixatAiA++zE559/jiuuuAKKogCAUUSdd9556NSpk9d/S5cuxf79+9ulz0REZsbCgoiIREaOHIm8vDy89dZbOP300/Hmm2/ijDPOwJtvvmnsM3PmTGzfvh1z585FeHg4HnroIfTq1QsbNmxo9nm7desGu92OTZs2tagfni/9x2ruPhdhYWFQ1aZ/DZ511lk47bTT8MEHHwAAPvnkE9TU1BiXQQGApmkA3PMsli1b1uS/jz/+uEV9JiIKJiwsiIioiU6dOiEyMhLbtm1r8thvv/0GVVWRkZFhtCUkJOCGG27Av//9bxQUFKBfv354+OGHvX4vOzsbd911F5YuXYpff/0V9fX1ePbZZ5vtQ2RkJM477zysWLHCODtyIh06dADgntPR2O7du0/6u8e68sorsXjxYlRWVuL999/HaaedhrPOOstrLACQlJSEMWPGNPlv1KhRrX5NIiKrY2FBRERN2Gw2jB07Fh9//LHXCkclJSVYsGABRowYYVyqdPDgQa/fjY6ORrdu3VBXVwfAvaJSbW2t1z7Z2dmIiYkx9mnO//zP/0DXdVx77bVecx481q1bh3feeQcA0LVrV9hsNqxYscJrn7///e8tG3QjV111Ferq6vDOO+9g8eLFuPLKK70eHzduHGJjY/HEE0+goaGhye8fOHCg1a9JRGR1XBWKiOgU9tZbb2Hx4sVN2v/85z/jsccew7JlyzBixAjcfvvtsNvteP3111FXV4ennnrK2Ld3794YNWoUBg0ahISEBKxduxb/+c9/MH36dADA9u3bMXr0aFx55ZXo3bs37HY7Fi5ciJKSEkyaNOmE/Tv77LPxyiuv4Pbbb0fPnj297rz9zTffYNGiRXjssccAAHFxcbjiiivw0ksvQVEUZGdn49NPP23TfIczzjgD3bp1w6xZs1BXV+d1GRTgnv/x6quv4tprr8UZZ5yBSZMmoVOnTtizZw8+++wzDB8+HC+//HKrX5eIyNJ0IiI65bz99ts6gGb/Kygo0HVd19evX6+PGzdOj46O1iMjI/Vzzz1XX7lypddzPfbYY/qQIUP0+Ph4PSIiQu/Zs6f++OOP6/X19bqu63ppaak+bdo0vWfPnnpUVJQeFxenDx06VP/ggw9a3N9169bp11xzjZ6WlqaHhIToHTp00EePHq2/8847usvlMvY7cOCAPnHiRD0yMlLv0KGD/qc//Un/9ddfdQD622+/bex33XXX6VFRUSd8zVmzZukA9G7dujW7z/Lly/Vx48bpcXFxenh4uJ6dna1ff/31+tq1a1s8NiKiYKHouq4HrKohIiIiIqKgwDkWREREREQkxsKCiIiIiIjEWFgQEREREZEYCwsiIiIiIhJjYUFERERERGIsLIiIiIiISIw3yGsBTdOwb98+xMTEQFGUQHeHiIiIiMgvdF3HoUOHkJaWBlU98TkJFhYtsG/fPmRkZAS6G0REREREAVFQUID09PQT7sPCogViYmIAuAONjY31++u7XC7k5eUhOzsbNpvN768fDJihHDOUYX5yzFCG+ckxQzlmKBOI/CorK5GRkWF8Hz4RFhYt4Ln8KTY2NmCFRXR0NGJjY/khbCNmKMcMZZifHDOUYX5yzFCOGcoEMr+WTAfg5G0iIiIiIhJjYWERJ5ssQyfHDOWYoQzzk2OGMsxPjhnKMUMZM+en6LquB7oTZldZWYm4uDg4HI6AXApFRERERBQIrfkezDkWFqDrOqqrqxEVFcXlbtuIGcoxQxnmJ8cMZZifXKAz1DQN9fX1fn9dX9J1HYcPH0ZkZCTfh23QHvmFhIT4bL5GQAuLFStW4Omnn8a6detQVFSEhQsX4pJLLjnuvrfeeitef/11/O1vf8PMmTON9rKyMsyYMQOffPIJVFXFxIkT8cILLyA6OtrY55dffsG0adOwZs0adOrUCTNmzMC9997bzqPzHU3TUFhYiJycHE50aiNmKMcMZZifHDOUYX5ygcywvr4e+fn50DTNr6/ra7quw+l0wm63s7Bog/bKLz4+HikpKeLnDGhhUV1djf79++PGG2/EZZdd1ux+CxcuxI8//oi0tLQmj02ePBlFRUVYtmwZGhoacMMNN+CWW27BggULALhP34wdOxZjxozBa6+9hk2bNuHGG29EfHw8brnllnYbGxEREZEv6LqOoqIi2Gw2ZGRkmPoa+5PRdR11dXUICwtjYdEGvs7PcwZk//79AIDU1FTR8wW0sBg/fjzGjx9/wn327t2LGTNmYMmSJbjgggu8Htu6dSsWL16MNWvWYPDgwQCAl156CRMmTMAzzzyDtLQ0zJ8/H/X19XjrrbcQGhqKPn36YOPGjXjuuedYWBAREZHpOZ1OHD58GGlpaYiMjAx0d0Q8U3vDw8NZWLRBe+QXEREBANi/fz+SkpJEZ+NMXfJqmoZrr70W99xzD/r06dPk8VWrViE+Pt4oKgBgzJgxUFUVq1evNvYZOXIkQkNDjX3GjRuHbdu2oby8vP0H4QOKoiA0NJQfQAFmKMcMZZifHDOUYX5ygcrQ5XIBgNd3GSuz8hkXM2iP/DwFa0NDg+h5TD15+8knn4Tdbscdd9xx3MeLi4uRlJTk1Wa325GQkIDi4mJjn8zMTK99kpOTjcc6dOjQ5Hnr6upQV1dn/FxZWQnA/cH2fLgVRYGqqtA0DY0X1mquXVVVKIrSbLvneRu3AzCupezatSt0XTd+99hrLG02G3Rd92r39KW59pb2vb3GdLJ2X46pcYYulysoxhSI45SZmQlN07x+x+pjOl57e42pa9euxuPBMqYTtft6TLque32Og2FM/j5OWVlZTT7DVh+Tv49T165dT9j39hhT4+8ex1vMs7n21mjtc0vaPQWSrutBMyZJe2t4ilsATd430r40/o7Z+LHW9Nm0hcW6devwwgsvYP369X7/l4G5c+dizpw5Tdrz8vKMSeFxcXFITU1FSUkJHA6HsU9iYiISExOxd+9eVFdXG+0pKSmIj4/Hrl27vFZ0SE9PR3R0NPLy8rz+IMrMzITdbkdubq7X9XTdu3eH0+lEfn6+sa+qqujevTuqq6tRWFhotIeGhiIrKwsOh8MotAAgKioKGRkZKCsrQ2lpqdHuzzE1lpOT45cxlZSUGNckBsuY/HmcunXrhrKyMhw8eND4TFp9TP48Tp7PcefOndGpU6egGJO/j9POnTuNPwttNltQjMmfx6ljx44ICQlBZWUlDh8+HBRj8vdx0nUdDQ0N6Nu3r1/H1PiLXn19vVffQ0NDYbPZUFdX5/UF0PP3XW1trdeYwsPDjT+PPBRFQXh4eJNVp1RVRVhYGFwul9e/ZNtsNoSGhsLpdMLpdDZpb2ho8Cre7HY7QkJCjHZPQRESEgK73R4UY/Jo7zGFhYU1eU1fjKmurs7o77Gfp9Zcfmea+1goiuK1KtTzzz+PO++80+t0j8vlgqqqyMjIwK5du/DWW2/hrrvu8rqkyel0Ijw8HB9++CEuvfRSTJkyBZWVlfjoo4+MfZYvX47zzjsPZWVlLT5j4flDwbN+rz//9cTlcmHHjh3o1q0bQkJCjPbGgu1fhHw9poaGBuTm5qJbt26w2WxBMSZ/Hydd15Gbm4vs7Gyv6y+tPCZ/HifP5zgnJwchISFBMaaTtft6TA0NDcafhTabLSjG5M/jpGka8vLykJ2d7fV3q5XH5O/j5Pkc9+jRw3hdf4yptrYWe/bsQWZmJsLCwnAsK/3rfuN/LFUUpdXPk5mZiT//+c9eK4SeaEyNv/PFx8f7dawtbW+t2traJpO3pX2pra1Ffn4+srKyEBoa6vVYVVUV4uPjrX0fi2uvvRZjxozxahs3bhyuvfZa3HDDDQCAYcOGoaKiAuvWrcOgQYMAAF9//TU0TcPQoUONfWbNmoWGhgbjS/myZcvQo0eP4xYVgLt6PN4H1/MXWWON/3CWtDc3UcbTrqqq8YW4uf0VRWlVu6/63tYxtaTdl2PyZNj496w+Jl+0t7TvnkvIjvc5sOqYTtTeHmPyvA9buv/J+tja9mA4Tsd+joNhTMfyx5ha8zxWGVNr2iVj8jynP8fU+Pmau4rDF1d3tPa5T9R+sv7Mnj3buDqkNc+/Zs2aVt1HZPjw4SgqKkJ8fHyTL+Itfc2WtH/zzTc499xzUV5e3uS1WvI8LeX5wn+8jCVjavx8x74nW9PngBYWVVVV2LFjh/Fzfn4+Nm7ciISEBHTp0gUdO3b02j8kJAQpKSno0aMHAKBXr144//zzcfPNN+O1115DQ0MDpk+fjkmTJhlL015zzTWYM2cOpk6divvuuw+//vorXnjhBfztb3/z30CJiIiITiFFRUXG9vvvv4/Zs2dj27Zt0HUdtbW1SExMNB73nNW120/+tbRTp06t6kdoaChSUlJa9TvUdgGdlr927VoMHDgQAwcOBADceeedGDhwIGbPnt3i55g/fz569uyJ0aNHY8KECRgxYgTeeOMN4/G4uDgsXboU+fn5GDRoEO666y7Mnj3bUkvNKorCO6UKMUM5ZijD/OSYoQzzk2OGLZeSkmL8FxcXB0VRjJ937NiB2NhYfPHFFxg0aBDCwsLw/fffIy8vDxdffDGSk5MRHR2NM888E19++aXX85522ml4/vnnjZ8VRcGbb76JSy+9FJGRkcjJycGiRYuMx7/55hsoioKKigoAwLx58xAfH48lS5agV69eiI6Oxvnnn+9VCDmdTtxxxx2Ij49Hx44dcd999+G6665r9ibOLVFeXo4pU6agQ4cOiIyMxPjx473m9OzevRsXXXQROnTogKioKPTp0weff/658buTJ09Gp06dEBkZib59++Ltt99uc1/aU0DPWIwaNapV15rt2rWrSVtCQoJxM7zm9OvXD999911ru2caquqeV0JtxwzlmKEM85NjhjLMT44ZyimKYlyafv/99+OZZ55BVlYWOnTogIKCAkyYMAGPP/44wsLC8O677+Kiiy7Ctm3b0KVLl2afc86cOXjqqafw9NNP46WXXsLkyZOxe/duJCQkHHf/w4cP45lnnsG//vUvqKqKP/7xj7j77rsxf/58AO5VSefPn4+3334bvXr1wgsvvICPPvoI5557bpvHff311yM3NxeLFi1CbGws7rvvPkyYMAFbtmxBSEgIpk2bhvr6eqxYsQJRUVHYsmWLsWDQQw89hC1btuCLL75AYmIiduzYgZqamjb3pT2Zdo4FHaVpGsrKypCQkNDstZh0YsxQjhnKMD85ZijD/OTMlOFFL32PA4fqTr6jj3WKCcMnM0a0+fd1XTdWKHrkkUfw+9//3ngsISEB/fv3N35+9NFHsXDhQixatAjTp09v9jmvv/56XH311QCAJ554Ai+++CJ++uknnH/++cfdv6GhAa+99hqys7MBANOnT8cjjzxiPP7SSy/hgQcewKWXXgoAePnll42zB23hKSh++OEHnH322QDcV9xkZGTgo48+whVXXIE9e/Zg4sSJ6Nu3LwAgKyvL+P09e/Zg4MCBGDx4MHRdR+fOnVt02VggmLNX5EXXdZSWljY72ZxOjhnKMUMZ5ifHDGWYn5yZMjxwqA7FlbUn39GEPKtyNb7BMeCee/vwww/js88+Q1FREZxOJ2pqarBnz54TPl+/fv2M7aioKMTGxmL//v3N7h8ZGWkUFQCQmppq7O9wOFBSUoIhQ4YYj9tsNgwaNKjJqmEttXXrVtjtdmNhIcC9/HOPHj2wdetWAMAdd9yB2267DUuXLsWYMWMwceJEY1y33XYbJk6ciPXr1+P3v/89JkyYgFGjRrWpL+2NhQURERGRxXSKabp6pdVeNyoqyuvnu+++G8uWLcMzzzyDbt26ISIiApdffrnXfSiOx3NplYdnqeHW7B/ouy/cdNNNGDduHD777DMsXboUc+fOxbPPPosZM2Zg/Pjx2L17Nz7//HMsW7YMEyZMwO23345nn302oH0+HhYWFlBxuB5FhxpgL61Gt+QTrx9MREREwU9yOZJZ/fDDD7j++uuNS5CqqqqOO7+2PcXFxSE5ORlr1qzByJEjAbjPsKxfvx4DBgxo03P26tULTqcTq1evNi6FOnjwILZt24bevXsb+2VkZODWW2/FrbfeigceeAD/+Mc/MGPGDADu1bCuu+46TJkyBUOHDsWsWbNYWFDbnP3kN6hzauiRUo4lM0cGujuWpCiKsSoFtQ0zlGF+csxQhvnJMUPfaG5+Sk5ODv773//ioosugqIoeOihh9p8+ZHEjBkzMHfuXHTr1g09e/bESy+9hPLy8hYd902bNiEmJsb4WVEU9O/fHxdffDFuvvlmvP7664iJicH999+Pzp074+KLLwYAzJw5E+PHj0f37t1RXl6O5cuXo1evXgDc9/wYNGgQ+vTpg9raWixevNh4zGxYWFhAbEQIDhyqw6GahpPvTMelqipSU1MD3Q1LY4YyzE+OGcowPzlmKNd4VahjPffcc7jxxhtx9tlnIzExEffddx8qKyv93EPgvvvuQ3FxMaZMmQKbzYZbbrkF48aNa/YGio15znJ42Gw2OJ1OvP322/jzn/+MCy+8EPX19Rg5ciQ+//xzIwuXy4Vp06ahsLAQsbGxOP/88417roWGhuKBBx7Arl27EBERgXPOOQfvvfee7wfuA4oe6IvKLKCyshJxcXEtupV5exj97DfIO1CN6DA7fp0zzu+vHww0TUNJSQmSk5MDvpKHVTFDGeYnxwxlmJ9coDKsra1Ffn4+MjMzER4e7rfXbQ+6rqOhoQEhISGWOfOjaRp69eqFK6+8Eo8++mhA+9Je+Z3oPdaa78H8k8UCYsLdJ5aq6pxwaawD20LXdTgcjoBPzrIyZijD/OSYoQzzk2OGvuFZFcqsdu/ejX/84x/Yvn07Nm3ahNtuuw35+fm45pprAt01AObOj4WFBcSEHz1lWFXrDGBPiIiIiIKbqqqYN28ezjzzTAwfPhybNm3Cl19+adp5DWbCORYWEBt+9DBV1jYgLvL41yYSERERkUxGRgZ++OGHQHfDknjGwgJiI44WEg5O4G4TRVGQmJhomes5zYgZyjA/OWYow/zkmKFvmPWu0VZh5vzM2zMyxEWEGtuVtSws2kJVVSQmJga6G5bGDGWYnxwzlGF+csxQ7kSrQtHJmT0/nrGwgJjwo8ubHeIcizbRNA0FBQUBWQ87WDBDGeYnxwxlmJ8cM5TTdR319fWcAN9GZs+PhYUFxIQ1mmPBS6HaRNd1VFdXm/aDaAXMUIb5yTFDGeYnxwx9w8yrGlmBmfNjYWEBMV6Tt3nGgoiIiIjMh4WFBcQ2Wm6WZyyIiIiIyIxYWFhAfCQnb0upqoqUlBTebVaAGcowPzlmKMP85Jihb7Rm8vGoUaMwc+ZM4+fTTjsNzz///Al/R1EUfPTRR23rXDs8j69x8jaJNL5vBSdvt42iKIiPj+cSgQLMUIb5yTFDGeYnxwxb7qKLLsL555/fpF1RFKxatQqqquKXX35p9fOuWbMGt9xyiy+6aHj44YcxYMCAJu1FRUUYP368T1/rWPPmzUN8fHyL91cUBXa73bTvQRYWFhAdenRVKF4K1TaapmHnzp1cyUOAGcowPzlmKMP85Jhhy02dOhXLli1DYWGhV7uu63jzzTcxePBg9OvXr9XP26lTJ0RGRvqqmyeUkpKCsLAwv7xWS+m6jrq6OtMuIMDCwgKiw7zvvE2tZ/bl2ayAGcowPzlmKMP85Jhhy1144YXo1KkT5s2b59VeVVWF//73v7jxxhtx8OBBXH311ejcuTMiIyPRt29f/Pvf/z7h8x57KVRubi5GjhyJ8PBw9O7dG8uWLWvyO/fddx+6d++OyMhIZGVl4aGHHkJDg/v71Lx58zBnzhz8/PPPUBQFiqIYfT72UqhNmzbhvPPOQ0REBDp27IhbbrkFVVVVxuPXX389LrnkEjzzzDNITU1Fx44dMW3aNOO12mLPnj24+OKLER0djdjYWFx11VUoKioyHv/5559x7rnnIiYmBrGxsRg0aBDWrl0LANi9ezcuuugidOjQAVFRUejTpw8+//zzNvelJXiDPAsID1FhVwGnBlTW8FIoIiIiMje73Y4pU6Zg3rx5mDVrlnHpzocffgiXy4Wrr74a1dXVGDRoEO677z7Exsbis88+w7XXXovs7GwMGTLkpK+haRouu+wyJCcnY/Xq1XA4HF7zMTxiYmIwb948pKWlYdOmTbj55psRExODe++9F1dddRV+/fVXLF68GF9++SUAIC4urslzVFdXY9y4cRg2bBjWrFmD/fv346abbsL06dO9iqfly5cjNTUVy5cvx44dO3DVVVdhwIABuPnmm1udoaZpRlHx7bffwul0Ytq0aZgyZQq+/fZbAMDkyZMxcOBAvPrqq7DZbNi4caMxB2PatGmor6/HihUrEBUVhS1btiA6OrrV/WgNFhYWoCgKokJVOGo1nrEgIiIi4PXfAVX7/f+60UnAn75t0a433ngjnn76aXz77bcYNWoUAPcZgksuuQRxcXGIj4/H3Xffbew/Y8YMLFmyBB988EGLCosvv/wSv/32G5YsWYK0tDQAwBNPPNFkXsSDDz5obJ922mm4++678d577+Hee+9FREQEoqOjYbfbkZKS0uxrLViwALW1tXj33XcRFRUFAHj55Zdx0UUX4cknn0RycjIAoEOHDnj55Zdhs9nQs2dPXHDBBfjqq6/aVFh89dVX2LRpE/Lz85GRkQEAeOedd3D66adjzZo1GDJkCPbs2YN77rkHPXv2BADk5OQYv79nzx5MnDgRffv2BQBkZWW1ug+txcLCAlRVRXxkGBy1NZy83UaqqiI9PZ0reQgwQxnmJ8cMZZifnKkyrNoPHNoX6F6cUM+ePXH22WfjrbfewqhRo7Bjxw589913xpkBl8uFJ554Ah988AH27t2L+vp61NXVtXgOxdatW5GRkWEUFQAwbNiwJvu9//77ePHFF5GXl4eqqio4nU7Exsa2aixbt25F//79jaICAIYPHw5N07Bt2zajsOjTpw9stqNzY1NTU7Fp06ZWvVbj18zIyDCKCgDo3bs34uPjsXXrVgwZMgR33nknbrrpJvzrX//CmDFjcMUVVyA7OxsAcMcdd+C2227D0qVLMWbMGEycOLFN81pawwSfDDoZRVGMJWcP1TZA03htZ2spioLo6GjTrqJgBcxQhvnJMUMZ5idnqgyjk4CYNP//F53Uqm5OnToV//d//4dDhw7h7bffRnZ2Ns477zwoioKnn34aL7zwAu677z4sX74cGzduxLhx41BfX++zmFatWoXJkydjwoQJ+PTTT7FhwwbMmjXLp6/R2LFLwSqK4tPJ/p73nuf/Dz/8MDZv3owLLrgAX3/9NXr37o2FCxcCAG666Sbs3LkT1157LTZt2oTBgwfjpZde8llfjodnLCzA5XLBprkvgdJ0oLreiZhw865hbEYulwt5eXnIzs72+pcEajlmKMP85JihDPOTM1WGLbwcKdCuvPJK/PnPf8aCBQvw7rvv4tZbb0VdXR3CwsLwww8/4OKLL8Yf//hHAO45Bdu3b0fv3r1b9Ny9evVCQUEBioqKkJqaCgD48ccfvfZZuXIlunbtilmzZhltu3fv9tonNDQULpfrpK81b948VFdXG2ctfvjhB6iqih49erSov63lGV9BQYFx1mLz5s2oqKhAr169jP26d++O7t274y9/+QuuvvpqvP3227j00ksBABkZGbj11ltx66234oEHHsA//vEPzJgxo136C/CMhWVEhhz915FKXg7VJlweUI4ZyjA/OWYow/zkmGHrREdH46qrrsIDDzyAoqIiXH/99caqWjk5OVi2bBlWrlyJrVu34k9/+hNKSkpa/NxjxoxB9+7dcd111+Hnn3/Gd99951VAeF5jz549eO+995CXl4cXX3zR+Bd9j9NOOw35+fnYuHEjSktLUVdX1+S1Jk+ejPDwcFx33XX49ddfsXz5csyYMQPXXnutcRlUW7lcLmzcuNHrv61bt2LMmDHo27cvJk+ejPXr1+Onn37Cddddh3POOQeDBw9GTU0Npk+fjm+++Qa7d+/GDz/8gDVr1hhFx8yZM7FkyRLk5+dj/fr1WL58uVdB0h5YWFhEdOjRQ8V7WRAREZFVTJ06FeXl5Rg3bpzXfIgHH3wQZ5xxBsaNG4dRo0YhJSUFl1xySYufV1VVLFy4EDU1NRgyZAhuuukmPP744177/OEPf8Bf/vIXTJ8+HQMGDMDKlSvx0EMPee0zceJEnH/++Tj33HPRqVOn4y55GxkZiSVLlqCsrAxnnnkmLr/8cowePRovv/xy68I4jqqqKgwcONDrv4suugiKouDjjz9Ghw4dMHLkSIwZMwZZWVl49913AQA2mw0HDx7ElClT0L17d1x55ZUYP3485syZA8BdsEybNg29evXC+eefj+7du+Pvf/+7uL8nouhcjPmkKisrERcXB4fD0erJPr7gcrlw74JV+L/NDgDA+7echaFZHf3eDytzuVzIzc1FTk5O4E9fWxQzlGF+csxQhvnJBSrD2tpa5OfnIzMzE+Hh4X573fag6zpqa2sRHh5ujrkqFtNe+Z3oPdaa78E8Y2EBqqoiIznR+JkrQ7WeqqrIzMw0x0oeFsUMZZifHDOUYX5yzNA3zHY3a6sxc378ZFhEfFSosc17WbSN3c61CqSYoQzzk2OGMsxPjhnK8UyFjJnzY2FhAZqm4XDFQeNnzrFoPU3TkJuby0l3AsxQhvnJMUMZ5ifHDH2jtrY20F2wNDPnx8LCIqIaT97mpVBEREREZDIsLCwiiqtCEREREZGJsbCwiMbLzXLyNhER0amHC3lSe/HV5X2cgWQBqqri9B7dABQC4OTttlBVFTk5OVzJQ4AZyjA/OWYow/zkApVhSEgIFEXBgQMH0KlTJ1NP3j0ZT3FUW1tr6XEEiq/z03Ud9fX1OHDgAFRVRWho6Ml/6QRYWFhEZKMjxcKibZxOp/gDc6pjhjLMT44ZyjA/uUBkaLPZkJ6ejsLCQuzatcuvr90edF1nUSHQHvlFRkaiS5cu4qKZhYUFaJqGkr17YFMVuDQdlTW8FKq1NE1Dfn4+bwwlwAxlmJ8cM5RhfnKBzDA6Oho5OTloaLD2Py66XC7s3r0bXbp04fuwDdojP5vNBrvd7pNihYWFRSiKgpgwOypqGnjGgoiI6BRks9ks/2Xc5XJBVVWEh4dbfiyBYPb8eKGlhcRGuOtArgpFRERERGbDwsIiVFVFTHgIAPeqUFwZovU4YVGOGcowPzlmKMP85JihHDOUMXN+is5vqCdVWVmJuLg4OBwOxMbGBqwf1/zjR6zMc9+Be8sj4xAZyivZiIiIiKj9tOZ7sHlLHjLouo6qqirEhh8tJDiBu3U8GbKObjtmKMP85JihDPOTY4ZyzFDG7PmxsLAATdNQWFiI6LBGhQUncLeKJ0Nf3QDmVMQMZZifHDOUYX5yzFCOGcqYPT8WFhbimbwNcAI3EREREZkLCwsLiT0yeRtwT+AmIiIiIjILzv61AEVREBoaitiIo228FKp1PBnyTp9txwxlmJ8cM5RhfnLMUI4Zypg9P56xsABVVZGVlYW4iFCjjZdCtY4nQzMv0WZ2zFCG+ckxQxnmJ8cM5ZihjNnzM2evyIuu66ioqEBM41WheClUq3gyNOsqClbADGWYnxwzlGF+csxQjhnKmD0/FhYWoGkaiouLER129NbtPGPROp4MzbqKghUwQxnmJ8cMZZifHDOUY4YyZs+PhYWFxPKMBRERERGZFAsLC2m8KhQnbxMRERGRmXBVKAtQFAVRUVGIiWxUWPBSqFbxZGjWVRSsgBnKMD85ZijD/OSYoRwzlDF7fopu1tkfJlJZWYm4uDg4HA7Exsb698U1F7BsNnCoGHpUJ2StOAe6DvTPiMfH04b7ty9EREREdEppzfdgXgpldqoN+rp5wK//AXZ8iZgw90mmQzxj0SqapqG0tNS0k52sgBnKMD85ZijD/OSYoRwzlDF7fiwsrCA6xf3/Q0WIOTLPgnMsWkfXdZSWlpp2eTYrYIYyzE+OGcowPzlmKMcMZcyeHwsLK4hxFxZKfRWSw90FBVeFIiIiIiIzCWhhsWLFClx00UVIS0uDoij46KOPjMcaGhpw3333oW/fvoiKikJaWhqmTJmCffv2eT1HWVkZJk+ejNjYWMTHx2Pq1Kmoqqry2ueXX37BOeecg/DwcGRkZOCpp57yx/B8Rj9SWABAl5BKAEC9U0NtgytQXSIiIiIi8hLQwqK6uhr9+/fHK6+80uSxw4cPY/369XjooYewfv16/Pe//8W2bdvwhz/8wWu/yZMnY/PmzVi2bBk+/fRTrFixArfccovxeGVlJcaOHYuuXbti3bp1ePrpp/Hwww/jjTfeaPfx+YoSk2psp9sdxnb54fpAdMeSFEVBXFycaVdRsAJmKMP85JihDPOTY4ZyzFDG7PmZZlUoRVGwcOFCXHLJJc3us2bNGgwZMgS7d+9Gly5dsHXrVvTu3Rtr1qzB4MGDAQCLFy/GhAkTUFhYiLS0NLz66quYNWsWiouLERoaCgC4//778dFHH+G3335rUd8CuioUAKz6O7DkAQDAf7r+D+7e1gMA8N/bz8YZXTr4vz9EREREdEoI2lWhHA4HFEVBfHw8AGDVqlWIj483igoAGDNmDFRVxerVq419Ro4caRQVADBu3Dhs27YN5eXlfu1/W2nRycZ250ZnLPZV1ASiO5akaRqKiopMu4qCFTBDGeYnxwxlmJ8cM5RjhjJmz88yN8irra3Ffffdh6uvvtqoloqLi5GUlOS1n91uR0JCAoqLi419MjMzvfZJTk42HuvQoem/+NfV1aGurs74ubLSPa/B5XLB5XLPa1AUBaqqQtM0r5n5zbWrqgpFUZpt9zxv43bA/QbSIjsZFWAnHC2G9pYfNn7PZrNB13WvN5qnL821t7Tv7TGmlrT7ckwulwvl5eXo2LEjbDZbUIzJ38dJ13VUVFQYGQbDmPx5nDzvwcTExKAZ08nafT0mp9Pp9TkOhjH58zhpmgaHw4HExMSgGZO/j5Pnc5yUlBQ0Y/Lw13Fq/DkOCQkJijH58zgBaPJ3cXuPqTUXN1misGhoaMCVV14JXdfx6quvtvvrzZ07F3PmzGnSnpeXh+joaABAXFwcUlNTUVJSAofj6FmExMREJCYmYu/evaiurjbaU1JSEB8fj127dqG+/ujciPT0dERHRyMvL8/rzZCZmQm73Y7c3FzYKuuQc6S9g6vU2GfLrmLkJjuhqiq6d++O6upqFBYWGo+HhoYiKysLDofDKLQAICoqChkZGSgrK0Np6dHn8+eYGsvJyYHT6UR+fr7R5usx7d+/H2VlZdixYwdUVQ2KMfn7OGVlZcHlchkZBsOY/HmcNE1DWVkZysrKkJycHBRj8vdxysvLMz7Hdrs9KMbkz+Pk+Ye0ffv2oabm6BlvK4/J38dJ0zTjaodgGRPg3+N06NAh43OclpYWFGPy53HKzs5GQ0OD19/F7T2myMhItJTp51h4ioqdO3fi66+/RseOHY3H3nrrLdx1111elzQ5nU6Eh4fjww8/xKWXXoopU6agsrLSa8Wp5cuX47zzzkNZWVmLz1h4DoznbIk/K1hXbRVCn+7i7lvaEPTYORMAMLZ3Ml6dPBBAcFblvhxTQ0MDcnNz0a1bN56xaOOYdF1Hbm4usrOzecaijWcsduzYgZycHISEhATFmE7W7usxef4y9XyOg2FM/j5jkZeXh+zsbOP1rT6mQJyx2LFjB3r06GG8rtXH5OHPMxaezzHPWLTtjMX27du9/i5u7zFVVVUhPj6+RXMsTH3GwlNU5ObmYvny5V5FBQAMGzYMFRUVWLduHQYNGgQA+Prrr6FpGoYOHWrsM2vWLDQ0NCAkxH1zuWXLlqFHjx7HLSoAICwsDGFhYU3aPX+RNdb4D2dJ+7HP27hdiYiBFhYHtc6B0Jr9UBVA04Hiylqv31MU5bjP01y7r/reljG1tN1XY7LZbEhKSoLdbm/yF+rxWGFM/j5OmqahU6dOTTIErDumE7X7ekyKoiApKcn43WAYk7S9tWOy2+1NPsdWH5M/j5OiKEhMTITNZjvu71hxTG1tb+uYPJ9jRVGCZkyN+WNMjT/HiqKccH+rjKk17dIxteXvYmnfPcepJQI6ebuqqgobN27Exo0bAQD5+fnYuHEj9uzZg4aGBlx++eVYu3Yt5s+fD5fLheLiYhQXFxunlnr16oXzzz8fN998M3766Sf88MMPmD59OiZNmoS0tDQAwDXXXIPQ0FBMnToVmzdvxvvvv48XXngBd955Z6CG3WqqqkKNdY9HOVSM5Bh30cPJ2y2nqqpxbTu1DTOUYX5yzFCG+ckxQzlmKGP2/ALaq7Vr12LgwIEYONB9Oc+dd96JgQMHYvbs2di7dy8WLVqEwsJCDBgwAKmpqcZ/K1euNJ5j/vz56NmzJ0aPHo0JEyZgxIgRXveoiIuLw9KlS5Gfn49BgwbhrrvuwuzZs73udWF2mqahNiTe/YOzFt1i3XfdLq2qR52TN8lrCU3TUFBQcNxTitQyzFCG+ckxQxnmJ8cM5ZihjNnzC+ilUKNGjTrhTPOWTP9ISEjAggULTrhPv3798N1337W6f2ah6zrqQhMQfuTnHlHV+O7IT8WOWnTtGBW4zlmEruuorq5u1coG5I0ZyjA/OWYow/zkmKEcM5Qxe37mPI9CTTREJBrbWeGHjO29vByKiIiIiEyAhYVFOCM6GdsZjW6SV1RRG4juEBERERF5YWFhAaqqIiYtx/g5WakwtoscPGPREqqqIiUlxbSTnayAGcowPzlmKMP85JihHDOUMXt+pl5ultwURUFUSjfj5wStzNjeyzMWLaIoCuLj4wPdDUtjhjLMT44ZyjA/OWYoxwxlzJ6fOcsd8qJpGnaXH717Y0zDAWObZyxaRtM07Ny507SrKFgBM5RhfnLMUIb5yTFDOWYoY/b8WFhYgK7rqLEdvdNhaM1+hNrdh45zLFpG13XU19ebdhUFK2CGMsxPjhnKMD85ZijHDGXMnh8LC6tQ7dCj3BO4lcoipMW5l5vlTfKIiIiIyAxYWFhJdIr7/1XFSIt13337UJ0Th2obAtgpIiIiIiIWFpagqirS09OB2FR3g+ZETszRORdFDl4OdTKeDM26ioIVMEMZ5ifHDGWYnxwzlGOGMmbPz5y9Ii+KoiA6OhpKTIrRlh3Bm+S1hpGhogS6K5bFDGWYnxwzlGF+csxQjhnKmD0/FhYW4HK5sH37dmhRyUZbF94kr1U8GbpcrkB3xbKYoQzzk2OGMsxPjhnKMUMZs+fHwsIiNE0DYlKNn1NtFcY2l5xtGbMuzWYlzFCG+ckxQxnmJ8cM5ZihjJnzY2FhIXqjS6E66o1vksfCgoiIiIgCi4WFlTQqLGIbSo1tXgpFRERERIHGwsICVFVFZmYm1LjORlvo4f2ICbcD4KVQLWFkaNJVFKyAGcowPzlmKMP85JihHDOUMXt+5uwVNWG324HIRECxuRsOFSEtLgIAsM9Ra9o7MJqJ3W4PdBcsjxnKMD85ZijD/OSYoRwzlDFzfiwsLEDTNOTm5kKDAkQfWRnqUDHS4t133653ajhYXX+CZyAjQxNPeDI7ZijD/OSYoQzzk2OGcsxQxuz5sbCwGs88i+r96BwXYjRzngURERERBRILC6uJTXP/X9eQE1FlNBeUHw5Qh4iIiIiIWFhYT4fTjM0eIUdXhsovrQ5AZ4iIiIiI3FhYWICqqsjJyXGvAJCQabR3VYuN7bz9Vcf7VTrCK0NqE2Yow/zkmKEM85NjhnLMUMbs+ZmzV9SE0+l0byRkG22d6vdCUdzbeQdYWJyMkSG1GTOUYX5yzFCG+ckxQzlmKGPm/FhYWICmacjPz3evAJCQZbTbK/KR3sG95GzegWouOXsCXhlSmzBDGeYnxwxlmJ8cM5RjhjJmz4+FhdXEpQPqkdWgyvKR3SkaAFBV58SBQ3UB7BgRERERncpYWFiNajs6gbtsJ7ITo4yHdvByKCIiIiIKEBYWFuE1ScdzOZSzBqfH1hjNeQe4MtSJmHWik5UwQxnmJ8cMZZifHDOUY4YyZs7PvPcEJ4PNZkP37t2PNjSaZ9E9ZL+xzZWhmtckQ2o1ZijD/OSYoQzzk2OGcsxQxuz5mbfkIYOu66iqqjo6ObtRYdEFjZac5aVQzWqSIbUaM5RhfnLMUIb5yTFDOWYoY/b8WFhYgKZpKCwsPLoCQKPCIrp6D2LD3SeedvJSqGY1yZBajRnKMD85ZijD/OSYoRwzlDF7fiwsrKjj0cJCKd+J7CT3ylB7K2pQU+8KVK+IiIiI6BTGwsKK4roA6pHpMWU7jSVnAWBnKS+HIiIiIiL/Y2FhAYqiIDQ0FIrnNts2OxDfxb1dlo+sxEhjX64MdXxNMqRWY4YyzE+OGcowPzlmKMcMZcyeH1eFsgBVVZGVleXdmJAFlO0E6qvQK/bojfG4MtTxHTdDahVmKMP85JihDPOTY4ZyzFDG7PnxjIUF6LqOiooK7xUAGk3gzrEfMLa5MtTxHTdDahVmKMP85JihDPOTY4ZyzFDG7PmxsLAATdNQXFzsvQJAo8IixbUXdtV9SowrQx3fcTOkVmGGMsxPjhnKMD85ZijHDGXMnh8LC6tqVFjYy/PRJcE9z2JnaRU0zZxVLBEREREFLxYWVtWosEDZTmQdWRmqtkHDPkdNgDpFRERERKcqFhYWoCgKoqKivFcAiO8KKEcOX9lOZCdFGQ9xZaimjpshtQozlGF+csxQhvnJMUM5Zihj9vxYWFiAqqrIyMiAqjY6XPZQIC7DvV2Wj+zERoUFV4Zq4rgZUqswQxnmJ8cMZZifHDOUY4YyZs/PnL0iL5qmobS0tOlEHc/lUHUO9IhrMJp/K670Y++sodkMqcWYoQzzk2OGMsxPjhnKMUMZs+fHwsICdF1HaWlp06XFGs2z6BFyALYjK0P9UujwZ/csodkMqcWYoQzzk2OGMsxPjhnKMUMZs+fHwsLKOnYzNsPLt6N7cgwAYHvJIRyudwaqV0RERER0CmJhYWUppx/dLt6E/ulxAABNBzbv4+VQREREROQ/LCwsQFEUxMXFNV0BINm7sOiXHm/8+HNBhV/6ZhXNZkgtxgxlmJ8cM5RhfnLMUI4Zypg9PxYWFqCqKlJTU5uuABCZcHRlqOJf0a9zjPHQpr2cZ9FYsxlSizFDGeYnxwxlmJ8cM5RjhjJmz8+cvSIvmqahqKjo+CsApPR1/7/+EHqElSHU7j6knMDt7YQZUoswQxnmJ8cMZZifHDOUY4YyZs+PhYUF6LoOh8Nx/BUAPIUFgJADv6J3aiwAIL+0Go6ahqb7n6JOmCG1CDOUYX5yzFCG+ckxQzlmKGP2/FhYWF2jwqLxBG4A2MSzFkRERETkJywsrO6YwsJrAndhhd+7Q0RERESnJhYWFqAoChITE4+/AkB8VyDMffkTijehf8bRMxa/sLAwnDBDahFmKMP85JihDPOTY4ZyzFDG7PmxsLAAVVWRmJh4/BUAFOXoWYvKvciKrEN0mB0AL4Vq7IQZUoswQxnmJ8cMZZifHDOUY4YyZs/PnL0iL5qmoaCgoPkVAFL6GZvq/l9xemf3GYx9jlocOFTnjy6a3kkzpJNihjLMT44ZyjA/OWYoxwxlzJ4fCwsL0HUd1dXVza8AcIJ5Frwcyu2kGdJJMUMZ5ifHDGWYnxwzlGOGMmbPj4VFMGhcWBT9gn6NVob6mZdDEREREZEfsLAIBp16AmqIe7t4E/o3OmOxsaAiIF0iIiIiolMLCwsLUFUVKSkpzU/UsYe6iwsAKN2O9GggOTYMALAmvwy1DS4/9dS8TpohnRQzlGF+csxQhvnJMUM5Zihj9vzM2SvyoigK4uPjT7y0mOdyKN0F5cBvOCenEwCgpsGFtbvK/dBLc2tRhnRCzFCG+ckxQxnmJ8cM5ZihjNnzC2hhsWLFClx00UVIS0uDoij46KOPvB7XdR2zZ89GamoqIiIiMGbMGOTm5nrtU1ZWhsmTJyM2Nhbx8fGYOnUqqqqqvPb55ZdfcM455yA8PBwZGRl46qmn2ntoPqVpGnbu3HniFQCOmcD9u+6djB9X5B5ox95ZQ4sypBNihjLMT44ZyjA/OWYoxwxlzJ5fQAuL6upq9O/fH6+88spxH3/qqafw4osv4rXXXsPq1asRFRWFcePGoba21thn8uTJ2Lx5M5YtW4ZPP/0UK1aswC233GI8XllZibFjx6Jr165Yt24dnn76aTz88MN444032n18vqLrOurr60+8AkDq0SVnUbgGI7olwlPMfruNhUWLMqQTYoYyzE+OGcowPzlmKMcMZcyenz2QLz5+/HiMHz/+uI/puo7nn38eDz74IC6++GIAwLvvvovk5GR89NFHmDRpErZu3YrFixdjzZo1GDx4MADgpZdewoQJE/DMM88gLS0N8+fPR319Pd566y2EhoaiT58+2LhxI5577jmvAsTyOg8CbKGAqx7Y9R06RIWif3o8NhZUYFvJIRQ5apAaFxHoXhIRERFRkApoYXEi+fn5KC4uxpgxY4y2uLg4DB06FKtWrcKkSZOwatUqxMfHG0UFAIwZMwaqqmL16tW49NJLsWrVKowcORKhoaHGPuPGjcOTTz6J8vJydOjQoclr19XVoa7u6I3lKisrAQAulwsul3sitKIoUFUVmqZ5VY3NtauqCkVRmm33PG/jdsB9ysvlchn/b9zemC0kAnr6mVB2/wCU74LrYD7OyUk0VoX69rf9uGJwepv63h5jakm7zWaDrute7Z6+NNd+or57MgymMfnzOOm6Dl3Xm+xv5TH58zh5PseapsFmswXFmE7W7usxNf6zMFjG5M/j5Pnd4/XFqmPy93HyvAcBBM2YPPx1nI79ThMMY/LncQLQ5O/i9h5Ta86OmLawKC4uBgAkJyd7tScnJxuPFRcXIykpyetxu92OhIQEr30yMzObPIfnseMVFnPnzsWcOXOatOfl5SE6OhqAu8hJTU1FSUkJHI6j94pITExEYmIi9u7di+rqaqM9JSUF8fHx2LVrF+rr64329PR0REdHIy8vz+vNkJmZCbvdjtzcXOi6DqfTiby8PHTv3h1OpxP5+fnGvqqqonv37qjvPAxhu38AAOxf/R9073D0bNBnG/IxIK4GABAVFYWMjAyUlZWhtLTU2MefY2osJyen2TFVV1ejsLDQaA8NDUVWVhYcDodxjFsyptLSUiNDRVGCYkz+Pk7dunVDcnKykWEwjMmfx8nzOa6oqECnTp2CYkz+Pk47d+40Psc2my0oxuTP49SxY0ekp6ejqKgIhw8fDoox+fs4eb64qaoaNGMC/HucqqqqjM9xampqUIzJn8cpJycHHTt29Pq7uL3HFBkZiZZSdJNcpKUoChYuXIhLLrkEALBy5UoMHz4c+/btQ2pqqrHflVdeCUVR8P777+OJJ57AO++8g23btnk9V1JSEubMmYPbbrsNY8eORWZmJl5//XXj8S1btqBPnz7YsmULevXq1aQvxztj4TkwsbGxRn/NVMHabDbou1dCedtdTGh9r4TzD69i8ONfobLWibiIEPz0wLmw21TLVOXB+C8NHBPHxDFxTBwTx8QxcUxWGlNVVRXi4+PhcDiM78HNMe0Zi5SUFABASUmJV2FRUlKCAQMGGPvs37/f6/ecTifKysqM309JSUFJSYnXPp6fPfscKywsDGFhYU3abTYbbDabV5vnwB+rte3HPm/jdpfLhby8PGRnZxvV6fH2VzoPBkIigYbDUHd9j1C7DefkdMJnm4rgqGnAr0VVGNT16BkaX/W9LWNqabuiKK1qb64vuq4bGTb+PSuPyd/HqfH78NjHrDqmE7X7ekyN82vJ/pK+N9du9eMEoMl70Opj8udxcrlc2L59+3E/wyd6HjOPqa3tbR3TsX8OBsOYGvPHcTredxqrj6k17dIxteXvYmnfPcepJUx7H4vMzEykpKTgq6++MtoqKyuxevVqDBs2DAAwbNgwVFRUYN26dcY+X3/9NTRNw9ChQ419VqxYgYaGBmOfZcuWoUePHse9DMqsjneNXRP2UKDLWe7tQ/uAg3kY2T3ReHjF9lN7dagWZUgnxAxlmJ8cM5RhfnLMUI4Zypg5v4AWFlVVVdi4cSM2btwIwD1he+PGjdizZw8URcHMmTPx2GOPYdGiRdi0aROmTJmCtLQ043KpXr164fzzz8fNN9+Mn376CT/88AOmT5+OSZMmIS0tDQBwzTXXIDQ0FFOnTsXmzZvx/vvv44UXXsCdd94ZoFG3s8yRR7fzv8XIRvez+OYULyyIiIiIqP0E9FKotWvX4txzzzV+9nzZv+666zBv3jzce++9qK6uxi233IKKigqMGDECixcvRnh4uPE78+fPx/Tp0zF69GioqoqJEyfixRdfNB6Pi4vD0qVLMW3aNAwaNAiJiYmYPXt2cC0125hXYbECqWdORY/kGGwrOYSfCyqw+2A1unaMClz/iIiIiCgomWbytplVVlYiLi6uRZNW2oOuu2+GEhoaevLr3FxO4KksoM4BRHYE7t6BV1fk48nFvwEAbv1dNu4f39MPvTaXVmVIx8UMZZifHDOUYX5yzFCOGcoEIr/WfA827RwL8ma3t/Dkks0OnDbcvX34ILB/C64YnI4Qm/vN9591Bah3mvfavPbU4gypWcxQhvnJMUMZ5ifHDOWYoYyZ82NhYQGapiE3N7flk3VOO+fo9q7vkBgdhrF93CtglVbVY+mW4mZ+MXi1OkNqghnKMD85ZijD/OSYoRwzlDF7fiwsglHjeRZ5XwMAJg/pYjQtWL3H3z0iIiIioiDHwiIYJfUGYo7c+yPva6D6IIZld0RmonvS9sq8g8gvrT7BExARERERtQ4Li2CkqkDfy93bmhPYshCKouDqIRnGLv/+iWctiIiIiMh3uCpUC5hhVShN04zbwbdI8SbgtRHu7YyhwNSlKKuux1lPfIV6l4YOkSFY9cBohIc0f4fbYNKmDMkLM5RhfnLMUIb5yTFDOWYoE4j8uCpUEHI6na37heTT3ZdEAUDBaqAsHwlRoTj/dPck7vLDDXhjxU4f99LcWp0hNcEMZZifHDOUYX5yzFCOGcqYOT8WFhagaRry8/NbtwKAogD9rjz68y8fAACmndsNNtVd4b6yfAcKyg77squm1aYMyQszlGF+csxQhvnJMUM5Zihj9vxYWASzvlcAOHKa7Jf3AV1Hj5QYXH/2aQCAOqeGOZ9sCVj3iIiIiCh4sLAIZnHpwGlH5lmU5QF71wMAZo7JQaeYMADAl1tL8PVvJYHqIREREREFCRYWFqGqbTxU/a46uv3L+wCAmPAQPHhBL6P54UVbUNvgknTPEtqcIRmYoQzzk2OGMsxPjhnKMUMZM+fHVaFaINCrQonUOoCncwBXHRDRAfjzL0B4LHRdx6Q3fsTq/DIAwNjeyXhl8hkIsZn3zUpERERE/sVVoYKMruuoqqpCm2rA8Digz6Xu7ZpyYOVLAABFUfDYJacj4shys0u3lOCuD36GSwvOOlOUIQFghlLMT44ZyjA/OWYoxwxlzJ4fCwsL0DQNhYWFbV8BYNT9gBri3l71MnCoGACQkxyDf0wZjFC7+22w6Od9uP//foEWhMWFOENihkLMT44ZyjA/OWYoxwxlzJ4fC4tTQUImMPhG93bDYeCbvxoPjchJxGt/PAMhNvfqUR+uK8TkN1cjt+RQIHpKRERERBbFwuJU8bt7gdAY9/b6d4HSXOOh83om44VJA3Hk9hZYtfMgxr/wHeZ+vhUHq+oC0FkiIiIishoWFhagKApCQ0Nlt26PSgSG/9m9rbuALx/2enhC31TMu2EIMhIiAABOTcfrK3bizMe/xJWvrcI/VuzE+j3lKKuuN+11fSfikwxPccxQhvnJMUMZ5ifHDOWYoYzZ8+OqUC1g6VWhGquvBl48A6hyz7HAH14CzpjitUttgwuvfZuHv3+Th3rn8a/fiw23Iyk2HJGhtiP/2Y3t8BAb1GPe7IoCKEdu1Od5SGn0WHxkKK4YlI6k2HCfDZWIiIiI5FrzPZiFRQsEurDQdR0OhwNxcXHyCnXjAuCj29zbig2Y/AHQbUyT3fYcPIwFP+3Bsi3FyDtQLXvNFogOs2PmmBxcd/Zp7bLkrU8zPEUxQxnmJ8cMZZifHDOUY4YygciPy80GGU3TUFxc7JsVAAZcAww9UljoLuCD64CiX5rs1qVjJO4f3xNf3TUKX931O8y+sDcmD+2C4d06onN8BKJCbfDl+7mqzonHPtuKC178DpsKHb574iN8muEpihnKMD85ZijD/OSYoRwzlDF7fvZAd4ACYNzjQGUhsPUToL4KmH8FMGk+kD74uLtnd4pGdqfoJu26rqO2QcPheicO17tQ0+BCTb37Dt76kcc92+79jd80ftZ0YOGGQry3pgC6DmwvqcKdH2zEsjt/58MBExEREVF7Y2FxKlJtwGX/AN75A1D4k3vOxT9/DwybDpz7/4CQiBY9jaIoiAi1ISLUho6C7gzJTMCkM7vg9vnrsbeiBrn7q1BWXY+EqFDBsxIRERGRP/FSKAtQFAVRUVG+vZYuJAK4+j2g8yD3z7oGrHwReG0EsOoVoCzfd6/VAv0z4jH+9BTj558LKnz6/O2S4SmGGcowPzlmKMP85JihHDOUMXt+nLzdAoGevN2uXE53QfHNXMBV7/1YUm8gtT/QIdN9k72wWMBmB9TG/4W4l3bSdQC6+/+6dnQbR35udhtAVEcgdQA+3VSE6Qs2AADuOK8b7hzbw69REBEREZG31nwP5qVQFqBpGsrKypCQkABV9fFJJpsdOOdOoMcE4JM7gILVRx/bv8X9nz+MfRwDe081ftzg4zMW7ZrhKYIZyjA/OWYow/zkmKEcM5Qxe37m6xE1oes6SktL2/fGdEk9galLgWlrgNH/A3Q+/kTudrPmH0iLDUNSTBgAYGNBBTTNd+P1S4ZBjhnKMD85ZijD/OSYoRwzlDF7fjxjQd46dQc63ek+i1FfDZTvAsp2AuW7gYYaQHMe+a8B0FxHtl2Aoh65+53SaBtH7o6nHmk/9nHFvTLVga1A+S4oJZswICMeS7eU4FCtEztLq9AtKSZgURARERFRy7GwoOaFRgHJfdz/tZeYFOCzO93bmz/CwC5/xNItJQCA9XsqWFgQERERWQQvhbIARVGC9w6VvS46ckYDwJaPMDAjznhoow/nWQR1hn7CDGWYnxwzlGF+csxQjhnKmD0/FhYWoKoqUlNTTTlJRyw6Ceg63L1dthP9QvdCPfJZ2bCnwmcvE9QZ+gkzlGF+csxQhvnJMUM5Zihj9vzM2SvyomkaioqKTHv7drHeFxubkbmfokeKeymzbcWVqK5z+uQlgj5DP2CGMsxPjhnKMD85ZijHDGXMnh8LCwvQdR0Oh8O0KwCI9bwQwJHTFI0uh9J0YNNeh09eIugz9ANmKMP85JihDPOTY4ZyzFDG7PmxsKDAi00Fupzl3i7djt91OGg85MvLoYiIiIio/bCwIHPofYmxOfjwCmN7w57yAHSGiIiIiFqLhYUFKIqCxMRE064A4BO9LjI2E/YsQUy4eyXkDQUVPjndd0pk2M6YoQzzk2OGMsxPjhnKMUMZs+fHwsICVFVFYmKiaVcA8Im4zkBqfwCAUrIZw9PcYz1wqA6/FR8SP/0pkWE7Y4YyzE+OGcowPzlmKMcMZcyenzl7RV40TUNBQYFpVwDwmS7DjM0rUoqN7ffXFIif+pTJsB0xQxnmJ8cMZZifHDOUY4YyZs+PhYUF6LqO6upq064A4DMZQ43N4WF5CA9xvz3/u74QNfUu0VOfMhm2I2Yow/zkmKEM85NjhnLMUMbs+bGwIPPwrAwFIHzfGlzYLw0AUFnrxOebigLVKyIiIiJqARYWZB6xaUBcF/f23nW4enCq8dC/f9oToE4RERERUUuwsLAAVVWRkpJi2ok6PtXlyOVQzhqcEbIHPZJjAABrd5dje0nbJ3GfUhm2E2Yow/zkmKEM85NjhnLMUMbs+ZmzV+RFURTEx8ebdmkxn2o0z0IpWI2rh2QYP0vOWpxSGbYTZijD/OSYoQzzk2OGcsxQxuz5sbCwAE3TsHPnTtOuAOBTjeZZYM+PuHRgOsLsnknce1Hb0LZJ3KdUhu2EGcowPzlmKMP85JihHDOUMXt+LCwsQNd11NfXm3YFAJ9K6g2Exbq3C1YjLsKOC/q551o4ahqwcMPeNj3tKZVhO2GGMsxPjhnKMD85ZijHDGXMnh8LCzIX1QakD3ZvV5UA5btw3bDTjIdf/SYPTpc5q3QiIiKiUxkLCzKfjEaXQxX8hP4Z8RjRLREAsKfsMD79hUvPEhEREZkNCwsLUFUV6enppl0BwOe6HJ3AjYIfAQDTzu1mNL2yfAc0rXWnAE+5DNsBM5RhfnLMUIb5yTFDOWYoY/b8zNkr8qIoCqKjo027AoDPdR4MKDb39p7VAICzshIwqGsHAEDu/ios3VLSqqc85TJsB8xQhvnJMUMZ5ifHDOWYoYzZ82NhYQEulwvbt2+Hy9W2FZEsJywaSDndvb1/C1CxB4qiYPoxZy1aM3HplMuwHTBDGeYnxwxlmJ8cM5RjhjJmz4+FhUWYdVmxdtP9/CMbOvDVIwCAUT06oU+ae8WoTXsdWJFb2qqnPOUybAfMUIb5yTFDGeYnxwzlmKGMmfNjYUHmdNbtQESCe3vTh0DhOiiK4jXXYsHq3QHqHBEREREdi4UFmVNEPDDqgaM/L50F6DrG9k5GUkwYAODr3/ajvLo+MP0jIiIiIi8sLCxAVVVkZmaadgWAdjP4BqDjkTMUe1YBWz+B3abikoGdAQANLh2f/LKvRU91ymboQ8xQhvnJMUMZ5ifHDOWYoYzZ8zNnr6gJu90e6C74ny0E+P0jR3/+8n8AZz0uO6Oz0fR/61t+J+5TMkMfY4YyzE+OGcowPzlmKMcMZcycHwsLC9A0Dbm5uaaerNNuekwAuo5wb5ftBDa8i54pseid6p7E/XNBBXbsrzrp05zSGfoIM5RhfnLMUIb5yTFDOWYoY/b8WFiQuSkKMLbRWYvvngOcdV5nLRZuKAxAx4iIiIioMVMXFi6XCw899BAyMzMRERGB7OxsPProo173L9B1HbNnz0ZqaioiIiIwZswY5Obmej1PWVkZJk+ejNjYWMTHx2Pq1Kmoqjr5v3KTSXQedHT52cq9wPp3cfGAzrCp7pvDLFy/t9V34iYiIiIi3zJ1YfHkk0/i1Vdfxcsvv4ytW7fiySefxFNPPYWXXnrJ2Oepp57Ciy++iNdeew2rV69GVFQUxo0bh9raWmOfyZMnY/PmzVi2bBk+/fRTrFixArfccksghkRtNer+o9vfPYtO4Tp+170TAGCfoxY/5h8MUMeIiIiICAAUvTW3L/azCy+8EMnJyfjnP/9ptE2cOBERERH43//9X+i6jrS0NNx11124++67AQAOhwPJycmYN28eJk2ahK1bt6J3795Ys2YNBg8eDABYvHgxJkyYgMLCQqSlpZ20H5WVlYiLi4PD4UBsbGz7DPYEdF2HpmlQVdW0t3D3i39fDWz73L09/il8GnERpi/YAACYeEY6nr2yf7O/ygzlmKEM85NjhjLMT44ZyjFDmUDk15rvweadVg7g7LPPxhtvvIHt27eje/fu+Pnnn/H999/jueeeAwDk5+ejuLgYY8aMMX4nLi4OQ4cOxapVqzBp0iSsWrUK8fHxRlEBAGPGjIGqqli9ejUuvfTSJq9bV1eHuro64+fKykoA7kuzPLdQVxQFqqpC0zSvS7Oaa/e8AZprP/bW7J5lxDz7NzQ0ICQkBDabzWhvzGazGW+2Y/vSXHtL+94eY2pJe5MxnXMvbEcKC/2753DenyYhOsyOqjonlmwuxhPO0xGiKs32vb6+HiEhIVAUxTxjgnWOk6IoaGhogN1u9/rDzMpj8udx8nyOQ0NDYbPZgmJMJ2v39ZhcLpfxZ6GiKEExJn8eJwBwOp1NVpSx8pj8fZw8n+Pw8PCgGZOHv46Tpmle32mCYUz+PE6qqjb5u7i9x9SacxCmLizuv/9+VFZWomfPnrDZbHC5XHj88ccxefJkAEBxcTEAIDk52ev3kpOTjceKi4uRlJTk9bjdbkdCQoKxz7Hmzp2LOXPmNGnPy8tDdHQ0AHcBk5qaipKSEjgcDmOfxMREJCYmYu/evaiurjbaU1JSEB8fj127dqG+/uhN3dLT0xEdHY28vDyvN0NmZibsdrsx87+srAwJCQno0aMHnE4n8vPzjX1VVUX37t1RXV2NwsKjE5lDQ0ORlZUFh8PhNdaoqChkZGSgrKwMpaWlRrs/x9RYTk5OC8cUgYyMcxFVsBxKVTEqv3kJZ6Wfgy/zqlBV58R320sxIMl23DEVFxcjPz8fCQkJUFXVRGOyznHKysrCjh07oKqq8Qee1cfkz+Pk+Rzn5OQgOTk5KMbk7+OUl5dn/Flot9uDYkz+PE4dOnRAeXk5IiIiUFNTExRj8vdx0jQN5eXlOOuss1BTUxMUYwL8e5wOHTpkfI7T0tKCYkz+PE7Z2dnYvn077Ha78Xdxe48pMjISLWXqS6Hee+893HPPPXj66afRp08fbNy4ETNnzsRzzz2H6667DitXrsTw4cOxb98+pKamGr935ZVXQlEUvP/++3jiiSfwzjvvYNu2bV7PnZSUhDlz5uC2225r8rrHO2PhOTCeU0D+rGBdLhd27NiBbt26ISQkxGhvLBir8uOOqehnqG+eCwDQTxuJr858Azf9az0A4NKBnfHsFf2O2/eGhgbk5uaiW7duxr+QmGZMFjlOuq4jNzcX2dnZxpkzq4/Jn8fJ8znOyclBSEhIUIzpZO2+HlNDQ4PxZ6HNZguKMfnzOGmahry8PGRnZxuvb/Ux+fs4eT7HPXr0MF7X6mPy8NdxcjqdXt9pgmFM/jxOALB9+3avv4vbe0xVVVWIj4+3/qVQ99xzD+6//35MmjQJANC3b1/s3r0bc+fOxXXXXYeUlBQAQElJiVdhUVJSggEDBgBwV4779+/3el6n04mysjLj948VFhaGsLCwJu2ev8gaa/yHs6T92Oc9tl1VVeMLcXP7ey4NaGm7r/re1jG1pL1J3zsPBOK7ABV7oOxZhZFXRCAm3I5DtU4s21KCepeO8JDjj9WTYePnM8WYTtJuluPkcrmMPh77mFXHdKL29hiT533Y0v1P1sfWtgfDcTr2cxwMYzqWP8bUmuexypha0y4Zk+c5g2lMHv567x37ncbqY2pNu3RMbfm7WNp3z3FqCVOvCnX48OEmg/Ncmwy4Tx+lpKTgq6++Mh6vrKzE6tWrMWzYMADAsGHDUFFRgXXr1hn7fP3119A0DUOHDvXDKHyjuYN/ylEUIGece1trQOjuFRjb210gVtU58V1uabO/ygzlmKEM85NjhjLMT44ZyjFDGTPnZ96eAbjooovw+OOP47PPPsOuXbuwcOFCPPfcc8aEa0VRMHPmTDz22GNYtGgRNm3ahClTpiAtLQ2XXHIJAKBXr144//zzcfPNN+Onn37CDz/8gOnTp2PSpEktWhHKDGw2G7p3795sNXvK6T7u6HbuUlzY7+jZqs9+2XfcX2GGcsxQhvnJMUMZ5ifHDOWYoYzZ8zN1YfHSSy/h8ssvx+23345evXrh7rvvxp/+9Cc8+uijxj733nsvZsyYgVtuuQVnnnkmqqqqsHjxYoSHhxv7zJ8/Hz179sTo0aMxYcIEjBgxAm+88UYghtQmuq6jqqqqVbPyg9ppIwB7hHs7dxmGZ3dEbLj7qr5lW0pQ2+Bq8ivMUI4ZyjA/OWYow/zkmKEcM5Qxe36mnrxtFoG+j4XL5UJubi5ycnJMW6H63fwrgdwl7u0/rcA93wMfrnOv+vD6tYMwro/3/BlmKMcMZZifHDOUYX5yzFCOGcoEIr/WfA829RkLombl/P7o9valuMDrcqiiAHSIiIiI6NTGwoKsKWfs0e3cJRjeLRFxEe6leJdtKYHjcEOAOkZERER0amJhYQGKoiA0NLRVy30FvQ5dgU493duFaxFSW44/9HdPxq9pcGHBT3u8dmeGcsxQhvnJMUMZ5ifHDOWYoYzZ8+McixYI9BwLasbSh4CVL7q3L30D+Z0vxHnPfgNdB5Jjw/Ddvech1M7amYiIiKitOMciyOi6joqKCtOuABAwjZed3boImYlRGNs7GQBQUlmHT34+uvQsM5RjhjLMT44ZyjA/OWYoxwxlzJ4fCwsL0DQNxcXFx72t+yktYygQ2dG9/dunwK4fcPM5WcbD//hup/HBY4ZyzFCG+ckxQxnmJ8cM5ZihjNnzY2FB1mULAc79f0d//uwuDEqPxsAu8QCA34oPnfBO3ERERETkOywsyNoG3QCkDXRvH9gK5afXm5y1ICIiIqL2x8LCAhRFQVRUlGlXAAgo1QZc8ByAI9ksn4txGS5kJLjvzP1dbil+Kaxghj7ADGWYnxwzlGF+csxQjhnKmD0/rgrVAlwVygI+uwtY86Z7u/fF+FfGI3jo480AgLOzO2L+TUNN+yEkIiIiMiuuChVkNE1DaWmpaSfqmMJ5DwKRie7tLR9jUoYDXTtGAgBW5h3EN9v2M0Mhvg9lmJ8cM5RhfnLMUI4Zypg9PxYWFqDrOkpLS027tJgpRHQARt1v/Biy8jncPbaH8fOTi7ehZP8BZijA96EM85NjhjLMT44ZyjFDGbPnx8KCgsfAPwJRSe7tLR/jgpRK9EuPA+BeIWr5zqoAdo6IiIgouLGwoOAREgGcPf3IDzrUlc/j/vE9jYff3VCGugZXYPpGREREFORYWFiAoiiIi4vj5OOWGHwjEB7v3v7lA5ydUIVRPToBAPZXO/HPH3YFrGtWx/ehDPOTY4YyzE+OGcoxQxmz58fCwgJUVUVqaipUlYfrpMJigLNuc2/rLuCHF3D/+J5Qj3z+Xl6eh4Kyw4Hrn4XxfSjD/OSYoQzzk2OGcsxQxuz5mbNX5EXTNBQVFZl2BQDTGXILEBrj3t7wv+gZVo7rzj4NAFDn1PA/izabdtKTmfF9KMP85JihDPOTY4ZyzFDG7PmxsLAAXdfhcDj4ZbilIhOAITe5t131wOf34M/nZqNjpA0A8PVv+7F0S0kAO2hNfB/KMD85ZijD/OSYoRwzlDF7fiwsKDiN+AsQk+rezl2CmPzP8KczE42H5yzajMP1zgB1joiIiCj4sLCg4BQeB4x/0vhRXfIAfpem45xuHQEA+xy1eHbp9kD1joiIiCjosLCwAEVRkJiYaNoVAEyr1x+A7uMBAEpVCbrkvo05F/dBqN39tv/n9/lYuaM0kD20FL4PZZifHDOUYX5yzFCOGcqYPb82FRYFBQUoLCw0fv7pp58wc+ZMvPHGGz7rGB2lqioSExNNuwKAaSkKMOFpICQKABDx6wJkVf+Ce8cdvSP33R/+DEdNQ6B6aCl8H8owPzlmKMP85JihHDOUMXt+berVNddcg+XLlwMAiouL8fvf/x4//fQTZs2ahUceecSnHST3CgAFBQWmXQHA1OIzgPNmHflBh/6fG3Fj/yicnX30kqjZH/8auP5ZCN+HMsxPjhnKMD85ZijHDGXMnl+bCotff/0VQ4YMAQB88MEHOP3007Fy5UrMnz8f8+bN82X/CO4VAKqrq027AoDpDfkT9K4jAABKVTHUhTfhmYmnIzbcDgD4eOM+fLxxbyB7aAl8H8owPzlmKMP85JihHDOUMXt+bSosGhoaEBYWBgD48ssv8Yc//AEA0LNnTxQVFfmud0S+YLNDu+wfaAg/sipU/gqkbXwej15yurHLPR/+gqWbiwPUQSIiIiLra1Nh0adPH7z22mv47rvvsGzZMpx//vkAgH379qFjx44+7SCRT0QnY9/Zj0FX3PeywIqncXHEL7h8UDoAoN6l4bb567Ho530B7CQRERGRdbWpsHjyySfx+uuvY9SoUbj66qvRv39/AMCiRYuMS6TId1RVRUpKimkn6liBqqqI6zceGP0/Rxs/vAF/HVyFSwd2BgC4NB1/fm8DPlxbEKBemhvfhzLMT44ZyjA/OWYoxwxlzJ6forfxIi2Xy4XKykp06NDBaNu1axciIyORlJTksw6aQWVlJeLi4uBwOBAbGxvo7pCErgP/uRHY/F/3z2Fx0K77FLN+BP79k7ugsKkKPpk+Ar3TeKyJiIjo1Naa78FtKndqampQV1dnFBW7d+/G888/j23btgVdUWEGmqZh586dpl0BwAqMDHUduPR1IPs89wN1DqjzJ+KJ30Xh2rO6AnCfuZj10SZomjknRgUK34cyzE+OGcowPzlmKMcMZcyeX5sKi4svvhjvvvsuAKCiogJDhw7Fs88+i0suuQSvvvqqTztI7hUA6uvrTbsCgBV4ZWgPBa76XyD9TPeD1fuhLLgSD/6+M7I6ue95sWFPBd5bw0uiGuP7UIb5yTFDGeYnxwzlmKGM2fNrU2Gxfv16nHPOOQCA//znP0hOTsbu3bvx7rvv4sUXX/RpB4naRWgUcM0HQKde7p8P7kDYp3fgsYv7GLv89YutKK2qC1AHiYiIiKylTYXF4cOHERMTAwBYunQpLrvsMqiqirPOOgu7d+/2aQeJ2k1kAnD1v4HwOPfPWxfh7AMf4LIjk7kra5144rOtAewgERERkXW0qbDo1q0bPvroIxQUFGDJkiUYO3YsAGD//v2c3NwOVFVFenq6aVcAsIJmM0zIdM+58Fj6EGb3dxg3z/vvhr34ckuJH3tqXnwfyjA/OWYow/zkmKEcM5Qxe35t6tXs2bNx991347TTTsOQIUMwbNgwAO6zFwMHDvRpBwlQFAXR0dFQFCXQXbGsE2bYYzww4k73tu5C/Kc345FR8cbDf/lgI3aVVvunoybG96EM85NjhjLMT44ZyjFDGbPn16bC4vLLL8eePXuwdu1aLFmyxGgfPXo0/va3v/msc+Tmcrmwfft2uFyuQHfFsk6a4bmzgNPc84ZQVYKLN9+Bib3cE7kP1Tox/V8/osZx0E+9NSe+D2WYnxwzlGF+csxQjhnKmD2/Np9HSUlJwcCBA7Fv3z4UFhYCAIYMGYKePXv6rHN0lFmXFbOSE2ZoswNXvAMkZAMAlAO/4SnXUxjcsR4z7f/B/Io/IvRvOdC3L/VTb82J70MZ5ifHDGWYnxwzlGOGMmbOr02FhaZpeOSRRxAXF4euXbuia9euiI+Px6OPPmrqwRKdUFRH4I//ASITAQC2PT/gw8M3Yqb9v4hTDsMGF/YtfT6wfSQiIiIyqTYVFrNmzcLLL7+Mv/71r9iwYQM2bNiAJ554Ai+99BIeeughX/eRyH8SstzL0NojAACK7l0odzqwGhtyufIZERER0bEUvQ132EhLS8Nrr72GP/zhD17tH3/8MW6//Xbs3bvXZx00g9bcyrw9eG6GEhoaatrJOmbX6gy3fQF8eAOgOYGBk7ElvwC9y74CAPw/252YOfM+JMWEt3OvzYXvQxnmJ8cMZZifHDOUY4YygcivNd+D23TGoqys7LhzKXr27ImysrK2PCWdhN1uD3QXLK9VGfYYD9y5Bbg3D7joBfQYP914aEj9j5i+YAMaXKfeZX98H8owPzlmKMP85JihHDOUMXN+bSos+vfvj5dffrlJ+8svv4x+/fqJO0XeNE1Dbm4u568ItCnDyATj5nm2rHOghbmr9PPUjVifvx8vfpXbHl01Lb4PZZifHDOUYX5yzFCOGcqYPb82lTxPPfUULrjgAnz55ZfGPSxWrVqFgoICfP755z7tIJEp2EKgdh8HbPoQscphDFW3Yt7KcEw7txvCQ2yB7h0RERFRwLXpjMXvfvc7bN++HZdeeikqKipQUVGByy67DJs3b8a//vUvX/eRyBx6TDA2x6prcajWiSWbiwPYISIiIiLzaPNFWmlpaXj88ce92n7++Wf885//xBtvvCHuGJHpdBsD2EIBVz3G2Nbjf5zX44O1Bbh4QOdA94yIiIgo4Np8gzzyH1VVkZOTA1Xl4Worn2QYHgtkjgQAdFYOoo+yCz/sOIiCssM+6qW58X0ow/zkmKEM85NjhnLMUMbs+ZmzV9SE0+kMdBcszycZ9rzA2BxrWwsA+M+6QvnzWgTfhzLMT44ZyjA/OWYoxwxlzJwfCwsL0DQN+fn5pl0BwAp8lmH38cbmGHU9AHdh4dJafTsYy+H7UIb5yTFDGeYnxwzlmKGM2fNr1RyLyy677ISPV1RUSPpCZH6xqUBKP6D4F/RRd6MjHNhbAazMK8U5OZ0C3TsiIiKigGlVYREXF3fSx6dMmSLqEJHpZZ8HFP8CABiubsYi7Wx8sLaQhQURERGd0lpVWLz99tvt1Q86CbNO0rESn2WYfS7ww/MAgNFhm7Go5mws2VyM2gZX0N/Tgu9DGeYnxwxlmJ8cM5RjhjJmzk/RdT34Lw4XqqysRFxcHBwOB2JjYwPdHQq0hlrgya6AsxYV9k4YUPU8AAX/vf1snNGlQ6B7R0REROQzrfkebN6Shwy6rqOqqgqsAdvOpxmGhANdzwYAxDsPIFvZBwDYuKdC/twmxvehDPOTY4YyzE+OGcoxQxmz58fCwgI0TUNhYaFpVwCwAp9nmHWusXmOugkAsKGgwjfPbVJ8H8owPzlmKMP85JihHDOUMXt+LCyI2iL7PGPzd/ZfAQAbC8oD1RsiIiKigGNhQdQWyX2AqCQAwFnqVtjhREFZDUqr6gLcMSIiIqLAYGFhAYqiIDQ0FIqiBLorluXzDBUFyBoFAIjQazBQ2QEguOdZ8H0ow/zkmKEM85NjhnLMUMbs+Zm+sNi7dy/++Mc/omPHjoiIiEDfvn2xdu1a43Fd1zF79mykpqYiIiICY8aMQW5urtdzlJWVYfLkyYiNjUV8fDymTp2Kqqoqfw+lzVRVRVZWlqmXFzO7dskwu9E8C5v7vhYbgvhyKL4PZZifHDOUYX5yzFCOGcqYPT9z9uqI8vJyDB8+HCEhIfjiiy+wZcsWPPvss+jQ4eiSnk899RRefPFFvPbaa1i9ejWioqIwbtw41NbWGvtMnjwZmzdvxrJly/Dpp59ixYoVuOWWWwIxpDbRdR0VFRWmXQHACtolwyNnLADgHNUzz6LCd89vMnwfyjA/OWYow/zkmKEcM5Qxe36mLiyefPJJZGRk4O2338aQIUOQmZmJsWPHIjs7G4A73Oeffx4PPvggLr74YvTr1w/vvvsu9u3bh48++ggAsHXrVixevBhvvvkmhg4dihEjRuCll17Ce++9h3379gVwdC2naRqKi4tNuwKAFbRLhrFpQKeeAIB+6k6Eow4/Fzjg0sz5YZfi+1CG+ckxQxnmJ8cM5ZihjNnza9Wdt/1t0aJFGDduHK644gp8++236Ny5M26//XbcfPPNAID8/HwUFxdjzJgxxu/ExcVh6NChWLVqFSZNmoRVq1YhPj4egwcPNvYZM2YMVFXF6tWrcemllzZ53bq6OtTVHZ2EW1lZCQBwuVxwuVwA3Ne4qaoKTdO8qsbm2lVVhaIozbZ7nrdxO+B+A7lcLuP/jdsbs9ls0HXdq93Tl+baW9r39hhTS9p9PSZPhj4dU9pAKAd+gw0aspV92FyXie3FDvRMjfPLmPx5nHRdh67rTfa38pj8+XnyfI41TYPNZguKMZ2s3ddjavxnYbCMyZ/HyfO7x+uLVcfk7+PkeQ8CCJoxefjrOB37nSYYxuTP4wSgyd/F7T2m1pwdMXVhsXPnTrz66qu488478f/+3//DmjVrcMcddyA0NBTXXXcdiouLAQDJyclev5ecnGw8VlxcjKSkJK/H7XY7EhISjH2ONXfuXMyZM6dJe15eHqKjowG4C5jU1FSUlJTA4XAY+yQmJiIxMRF79+5FdXW10Z6SkoL4+Hjs2rUL9fX1Rnt6ejqio6ORl5fn9WbIzMyE3W5Hbm4uNE1DWVkZduzYgR49esDpdCI/P9/YV1VVdO/eHdXV1SgsLDTaQ0NDkZWVBYfD4TXWqKgoZGRkoKysDKWlpUa7P8fUWE5OTruPaf/+/UaGqqr6bEzdOnY3PkTdlUJs1jOxZO12dL9gUNAdp6ysLLhcLiPDYBiTP957njF5PsdlZWVITk4OijH5+zjl5eUZn2O73R4UY/LncfJcRrxv3z7U1NQExZj8fZw0TUN5uXsuXbCMCfDvcTp06JDxOU5LSwuKMfnzOGVnZ6OhocHr7+L2HlNkZCRaStHNepEW3EENHjwYK1euNNruuOMOrFmzBqtWrcLKlSsxfPhw7Nu3D6mpqcY+V155JRRFwfvvv48nnngC77zzDrZt2+b13ElJSZgzZw5uu+22Jq97vDMWngPjuZW5PytYTdOwb98+pKWlwW63G+2NBWNV7ssxOZ1O7N27F2lpaUb/fDKmnV9DmX85AOA150X4q/NqXDU4HX+d2K/dx+Tv4wS4F1NITU019rH6mPz5r1yez3Hnzp1ht9uDYkwna/f1mJxOp/FnoaqqQTEmfx4nXddRVFSE1NRUrxVlrDwmfx8nz+c4IyPDeH6rj8nDn2csGn+nCYYx+fM4KYqCwsJCr7+L23tMVVVViI+Ph8PhML4HN8fUZyxSU1PRu3dvr7ZevXrh//7v/wC4q0IAKCkp8SosSkpKMGDAAGOf/fv3ez2H0+lEWVmZ8fvHCgsLQ1hYWJN2m80Gm83m1db4C5ak/djnPfY1u3btetL9FUVpVbuv+t6WMbW03VdjstvtTTI80f4tHlNSL2Ozh1oAAPi50GH8pR1sx6lLly7H3dfKY2qu3ddjOvZzHAxjkra3dkwhISFNPsdWH5O/j1NGRsZx9z3R85h9TG1pb+uYjv0cB8OYGvPHcVJVtcnn2Opjak27L8bU2r+LpX1v/A8RJ2PqydvDhw9vcqZh+/btxhsyMzMTKSkp+Oqrr4zHKysrsXr1agwbNgwAMGzYMFRUVGDdunXGPl9//TU0TcPQoUP9MAo5TdNQWlp63H9BppZptwxjOwNh7uq9t929GMD2kkOoqnP69nVMgO9DGeYnxwxlmJ8cM5RjhjJmz8/UhcVf/vIX/Pjjj3jiiSewY8cOLFiwAG+88QamTZsGwF1BzZw5E4899hgWLVqETZs2YcqUKUhLS8Mll1wCwH2G4/zzz8fNN9+Mn376CT/88AOmT5+OSZMmIS0tLYCjazld11FaWtqqyTPkrd0yVBTjrEWyth/ROAxNB37d6zjJL1oP34cyzE+OGcowPzlmKMcMZcyen6kLizPPPBMLFy7Ev//9b5x++ul49NFH8fzzz2Py5MnGPvfeey9mzJiBW265BWeeeSaqqqqwePFihIeHG/vMnz8fPXv2xOjRozFhwgSMGDECb7zxRiCGRMHoyJKzAJCj7AUA5JYcClRviIiIiALC1HMsAODCCy/EhRde2OzjiqLgkUcewSOPPNLsPgkJCViwYEF7dI8ISDo6DyhHLcQGVw5+K2ZhQURERKcWU5+xIDdFURAXF9eqyTPkrV0zbDyBW3Evabc9CM9Y8H0ow/zkmKEM85NjhnLMUMbs+Zn+jAW5Z+c3XvWKWq9dM2xUWPQN2Qs4gW3Fh6Drumk/+G3B96EM85NjhjLMT44ZyjFDGbPnxzMWFqBpGoqKiky7AoAVtGuGUZ2AyI4A3JdCAUBlrRMllXUn+i3L4ftQhvnJMUMZ5ifHDOWYoYzZ82NhYQG6rsPhcJh2BQAraNcMFcWYZ9HBVYZ4uC+D2hZkl0PxfSjD/OSYoQzzk2OGcsxQxuz5sbAg8oVGK0N198yz4ARuIiIiOoWwsCDyhUbzLLofuRwq2M5YEBEREZ0ICwsLUBQFiYmJQTUR2N/aPcNGS872UINzZSi+D2WYnxwzlGF+csxQjhnKmD0/rgplAaqqIjExMdDdsLR2zzDp6KVQfUP2AQ3uwkLTdKiqOT/8rcX3oQzzk2OGMsxPjhnKMUMZs+fHMxYWoGkaCgoKTLsCgBW0e4YRHYAY9/Jv3VAAQEdtg4aC8sPt83oBwPehDPOTY4YyzE+OGcoxQxmz58fCwgJ0XUd1dbVpVwCwAr9keGSeRbRWiU5wAHDfzyJY8H0ow/zkmKEM85NjhnLMUMbs+bGwIPKVRvMsequ7AQTfPAsiIiKi5rCwIPKVtIHGZn8lDwCwraQqUL0hIiIi8isWFhagqipSUlKgqjxcbeWXDDufYWwOtLkLi2C6lwXfhzLMT44ZyjA/OWYoxwxlzJ6fOXtFXhRFQXx8vGmXFrMCv2TYIROISADgKSx05B2oQr3TnBOsWovvQxnmJ8cMZZifHDOUY4YyZs+PhYUFaJqGnTt3mnYFACvwS4aKAnQeBACI1yuRrhyAU9Ox62B1+72mH/F9KMP85JihDPOTY4ZyzFDG7PmxsLAAXddRX19v2hUArMBvGR4pLABggGeeRZBcDsX3oQzzk2OGMsxPjhnKMUMZs+fHwoLIl9IHG5sD1B0AgF/3OgLVGyIiIiK/YWFB5EtpRydw91fdZyw2FFQEqDNERERE/sPCwgJUVUV6erppVwCwAr9lGNUR6HAaAKCfugt2OPFLYQWcLnNeC9kafB/KMD85ZijD/OSYoRwzlDF7fubsFXlRFAXR0dGmXQHACvyaYWf35VBhqEcPpRC1DRp+C4J5FnwfyjA/OWYow/zkmKEcM5Qxe34sLCzA5XJh+/btcLlcge6KZfk1w0YTuIPpcii+D2WYnxwzlGF+csxQjhnKmD0/FhYWYdZlxazEbxk2nsCtuCdwb9hT7p/Xbmd8H8owPzlmKMP85JihHDOUMXN+LCyIfC2lL6DaAQADjtyBe+OeigB2iIiIiKj9sbAg8rWQCCC5DwCgm7IX0TiMnaXVKK+uD3DHiIiIiNoPCwsLUFUVmZmZpl0BwAr8nuGRCdwqdPRV8wEAGwsr/PPa7YTvQxnmJ8cMZZifHDOUY4YyZs/PnL2iJux2e6C7YHl+zbDRBO6h6lYAwIYguByK70MZ5ifHDGWYnxwzlGOGMmbOj4WFBWiahtzcXFNP1jE7v2eYNcrYHKuuA2D9Cdx8H8owPzlmKMP85JihHDOUMXt+LCyI2kNcZ+OsRW91N7ooJdhYUAFN0wPcMSIiIqL2wcKCqL30usjYHKeuwaFaJ3aWVgWwQ0RERETth4UFUXvpebSwON+2BgCwPgjmWRAREREdj6LrOq/NOInKykrExcXB4XAgNjbW76+v6zo0TYOqqqa9hbvZBSzDvw8D9m8BAAytfRnnDRmAuZf19d/r+xDfhzLMT44ZyjA/OWYoxwxlApFfa74H84yFRTidzkB3wfICkmGjy6HG2tbit+JK//fBh/g+lGF+csxQhvnJMUM5Zihj5vxYWFiApmnIz8837QoAVhCwDHv9wdg8X12DHSVVsOpJQr4PZZifHDOUYX5yzFCOGcqYPT8WFkTtKbkP0CETgPt+Fva6MhRX1ga4U0RERES+x8KCqD0pinE5lF3RMMa2HrklXBmKiIiIgg8LC4sw663brSRgGTa6HGqk+gu2lxwKTD98gO9DGeYnxwxlmJ8cM5RjhjJmzo+rQrVAoFeFIotz1gGPJQEA1mk5+LD/W/jrxH4B7hQRERHRyXFVqCCj6zqqqqw76dcMApqhPQx6RAIAIAkVlj1jwfehDPOTY4YyzE+OGcoxQxmz58fCwgI0TUNhYaFpVwCwgkBnqMSkAACSlArk7j9k2j8QTiTQGVod85NjhjLMT44ZyjFDGbPnx8KCyB+ikwEAYUoD1FoHSirrAtwhIiIiIt9iYUHkDzGpxqbnrAURERFRMGFhYQGKoiA0NNRvt24PRgHPMCbZ2ExWyi255GzAM7Q45ifHDGWYnxwzlGOGMmbPzx7oDtDJqaqKrKysQHfD0gKeYeMzFii35BmLgGdoccxPjhnKMD85ZijHDGXMnh/PWFiAruuoqKiw5IRfswh4htFHz1gkKRWWPGMR8AwtjvnJMUMZ5ifHDOWYoYzZ82NhYQGapqG4uNi0KwBYQcAzbHTGIlkpx/YS660MFfAMLY75yTFDGeYnxwzlmKGM2fNjYUHkD43mWHRSKlBZ68SBQ1wZioiIiIIHCwsif4hOMTaTlXIAwHYLXg5FRERE1BwWFhagKAqioqJMuwKAFQQ8w5BwIDwegPvu2wAsN4E74BlaHPOTY4YyzE+OGcoxQxmz58dVoSxAVVVkZGQEuhuWZooMY1KA2gokKRUAdOTut9YZC1NkaGHMT44ZyjA/OWYoxwxlzJ4fz1hYgKZpKC0tNe1EHSswRYYx7suhIpR6xOIwckusdcbCFBlaGPOTY4YyzE+OGcoxQxmz58fCwgJ0XUdpaanlVhEyE1Nk2GieRSelwnJnLEyRoYUxPzlmKMP85JihHDOUMXt+LCyI/CXGewJ3xeEGlFfXB7BDRERERL7DwoLIXxoVFp4J3DtLrXXWgoiIiKg5LCwsQFEUxMXFmXYFACswRYZed992Lzmbd6A6UL1pNVNkaGHMT44ZyjA/OWYoxwxlzJ4fV4WyAFVVkZqaevIdqVmmyNDr7tsVAICdFiosTJGhhTE/OWYow/zkmKEcM5Qxe348Y2EBmqahqKjItCsAWIEpMoxpesZi5wHrXApligwtjPnJMUMZ5ifHDOWYoYzZ87NUYfHXv/4ViqJg5syZRlttbS2mTZuGjh07Ijo6GhMnTkRJSYnX7+3ZswcXXHABIiMjkZSUhHvuuQdOp9PPvW87XdfhcDhMuwKAFZgiw0arQqWoFQCA/FLrnLEwRYYWxvzkmKEM85NjhnLMUMbs+VmmsFizZg1ef/119OvXz6v9L3/5Cz755BN8+OGH+Pbbb7Fv3z5cdtllxuMulwsXXHAB6uvrsXLlSrzzzjuYN28eZs+e7e8h0KkuNBIIiwMApNkcAIDdBw/DpZnzDwciIiKi1rBEYVFVVYXJkyfjH//4Bzp06GC0OxwO/POf/8Rzzz2H8847D4MGDcLbb7+NlStX4scffwQALF26FFu2bMH//u//YsCAARg/fjweffRRvPLKK6iv51Kf5GdHLofqqJcD0FHv0lBYfjiwfSIiIiLyAUtM3p42bRouuOACjBkzBo899pjRvm7dOjQ0NGDMmDFGW8+ePdGlSxesWrUKZ511FlatWoW+ffsiOfno9e3jxo3Dbbfdhs2bN2PgwIFNXq+urg51dXXGz5WVlQDcZz9cLhcA96x8VVWhaZrX6ajm2lVVhaIozbZ7nrdxO+C+lk7TNCQkJEDTNK/2xmw2G3Rd92r39KW59pb2vT3G1JJ2X45J13Ujw4COKSYFKN2OcL0W0ahBFSKxY/8hdO0YZfrjBAAdO3bke6+NY/J8jj37BMOYTtbeHmNq/DkOljE11p5j0nUdiYmJ0HXdq59WHpO/j5PnPagoStCMycNfx+nY7zTBMCZ/HidFUZr8XdzeY2rNZVemLyzee+89rF+/HmvWrGnyWHFxMUJDQxEfH+/VnpycjOLiYmOfxkWF53HPY8czd+5czJkzp0l7Xl4eoqOjAQBxcXFITU1FSUkJHA6HsU9iYiISExOxd+9eVFcfvX4+JSUF8fHx2LVrl9eZkvT0dERHRyMvL8/rzZCZmQm73Y7c3FyjraysDDk5OXA6ncjPzzfaVVVF9+7dUV1djcLCQqM9NDQUWVlZcDgcXmONiopCRkYGysrKUFpaarQHYkwA/DKmAwcOwOFwoKysLLBjajTPIkmpQJUeiXXbCzG6V4oljlNsbCzy8vK8x8T3XqvGpKpq0I3J38eprKws6MYE+O84FRQUBN2Y/H2ckpKSUFVVFVRj8vdxKisrC7oxAf45ThEREV5/F7f3mCIjI9FSim7W2R8ACgoKMHjwYCxbtsyYWzFq1CgMGDAAzz//PBYsWIAbbrjB6+wCAAwZMgTnnnsunnzySdxyyy3YvXs3lixZYjx++PBhREVF4fPPP8f48eObvO7xzlh4DkxsbCwA/5+x2LdvH9LS0mC32432xoKxKvflmJxOJ/bu3Yu0tDSjfwEZ01cPAytfBABcXT8Lq7Q+uGZIBp64rJ/pjxMA7N27F6mpqcY+AN97rTljsW/fPnTu3Bl2uz0oxnSydl+Pyel0Gn8WqqoaFGPy9xmLoqIipKameq2Bb+UxBeKMxb59+5CRkWE8v9XH5OGv4+Ryuby+0wTDmPx9xqKwsNDr7+L2HlNVVRXi4+PhcDiM78HNMfUZi3Xr1mH//v0444wzjDaXy4UVK1bg5ZdfxpIlS1BfX4+KigqvsxYlJSVISXH/y3BKSgp++uknr+f1rBrl2edYYWFhCAsLa9Jus9lgs9m82hp/wZK0H/u8x7bX1NQYb8rm9lcUpVXtvup7W8fUknZfjUlRFCPDxr/n9zE1uvt2pyN3384vPWz00czHyeVy4fDhw00yBPjea2kfa2pqjM9wsIxJ0t7aMamq2uRzbPUx+fM4uVwuVFdXt/p5zDymtrZLxlRTUwNd14/7ZyFgzTF5+OM46bre5DuN1cfUmnbpmNryd7G0743/IeJkTD15e/To0di0aRM2btxo/Dd48GBMnjzZ2A4JCcFXX31l/M62bduwZ88eDBs2DAAwbNgwbNq0Cfv37zf2WbZsGWJjY9G7d2+/j4lOcY0Ki9PCDgEAdpZa514WRERERM0x9RmLmJgYnH766V5tUVFR6Nixo9E+depU3HnnnUhISEBsbCxmzJiBYcOG4ayzzgIAjB07Fr1798a1116Lp556CsXFxXjwwQcxbdq0456VIGpXjeZYdIuoAmqAkso6VNU5ER1m6o8jERER0QlZ/pvM3/72N6iqiokTJ6Kurg7jxo3D3//+d+Nxm82GTz/9FLfddhuGDRuGqKgoXHfddXjkkUcC2OvWUVUVKSkpzZ6yopMzTYaNzlhkhFQa27tKq3F657hA9KjFTJOhRTE/OWYow/zkmKEcM5Qxe36mnrxtFpWVlYiLi2vRpBWiE6qrAuZ2BgDsjRuE4SV3AQBemDQAFw/oHMieERERETXRmu/B5ix3yIumadi5c+dxV+mhljFNhmHRQGgMACDeddBo3nmgurnfMA3TZGhRzE+OGcowPzlmKMcMZcyeHwsLC9B1HfX19a26QQl5M1WG8V0AAJGHCxEO97LGO0vNX1iYKkMLYn5yzFCG+ckxQzlmKGP2/FhYEPlbZ/fyyYrmRH/bLgBAPleGIiIiIotjYUHkbxlDjM1zI9131Mw/UG3af30gIiIiagkWFhagqirS09NNuwKAFZgqw/Qzjc0z7XkAgOp6F/Y5agPVoxYxVYYWxPzkmKEM85NjhnLMUMbs+ZmzV+RFURRER0e36s6H5M1UGSb2AMLcS8v2cP4GwH2m4ueCisD1qQVMlaEFMT85ZijD/OSYoRwzlDF7fiwsLMDlcmH79u1wuVyB7oplmSpDVQXSBwEAohsOojNKAQDrdpcHslcnZaoMLYj5yTFDGeYnxwzlmKGM2fNjYWERZl1WzEpMlWGjy6HOUHMBAOv3mLuwAEyWoQUxPzlmKMP85JihHDOUMXN+LCyIAiH96ATuUVG7AACb91aizmnOf4EgIiIiOhkWFkSBcORSKAAYbHNP4K53afh1b2WgekREREQkwsLCAlRVRWZmpmlXALAC02UY0QFI7A4ASK/LRRjqAQDrTTzPwnQZWgzzk2OGMsxPjhnKMUMZs+dnzl5RE3a7PdBdsDzTZXjkciib7kQfZRcA88+zMF2GFsP85JihDPOTY4ZyzFDGzPmxsLAATdOQm5tr6sk6ZmfKDDOOTuAeFuq+HGr9nnLT3ijPlBlaCPOTY4YyzE+OGcoxQxmz58fCgihQGq0M9bsjd+AuqazD3oqaQPWIiIiIqM1YWBAFSqeeQGgMAKCXa7vRvH5PRYA6RERERNR2LCyIAkW1GatDxdTvR9qRG+WZeQI3ERERUXNYWFiAqqrIyckx7QoAVmDaDBvdz2KQ6j5rscGkE7hNm6FFMD85ZijD/OSYoRwzlDF7fubsFTXhdDoD3QXLM2WGXc4yNkdH7QQAbN5XidoGc94oz5QZWgjzk2OGMsxPjhnKMUMZM+fHwsICNE1Dfn6+aVcAsALTZph+JqC4P4Zn2nIBAE5Nxy+FjkD26rhMm6FFMD85ZijD/OSYoRwzlDF7fiwsiAIpPBZI6gMASK3NQzQOAwB+yj8YyF4RERERtRoLC6JA6zIUAKBCw0B1BwBgZR4LCyIiIrIWFhYWYdZJOlZi2gy7DDM2z41wz7NYt7vclPMsTJuhRTA/OWYow/zkmKEcM5Qxc36Kbtbb/JpIZWUl4uLi4HA4EBsbG+juULCpKACePx0AkBt1Bn5/8G4AwL9vPgvDsjsGsmdERER0imvN92Dzljxk0HUdVVVVYA3YdqbOMD4DiO0MAMis3Qob3GcqVu001+VQps7QApifHDOUYX5yzFCOGcqYPT8WFhagaRoKCwtNuwKAFZg+wwz3PAu7qwa9lN0AgFV5pYHsUROmz9DkmJ8cM5RhfnLMUI4Zypg9PxYWRGbQaJ7F+TG7AAAbCypwuN68a1UTERERNcbCgsgMjqwMBQAjI/IAAA0uHWt3mfMu3ERERETHYmFhAYqiIDQ0FIqiBLorlmX6DJP6AKHRAICcus0A3NdOmmmehekzNDnmJ8cMZZifHDOUY4YyZs+Pq0K1AFeFIr949xJg53IAwIi651GoJ6F/Rjw+njY8sP0iIiKiUxZXhQoyuq6joqLCtCsAWIElMuxylrH5h/hdAIBNhRWorG0IUIe8WSJDE2N+csxQhvnJMUM5Zihj9vxYWFiApmkoLi427QoAVmCJDLsePTPx+4htAABNB9bklwWqR14skaGJMT85ZijD/OSYoRwzlDF7fiwsiMwiYwhgjwAA9KzZAM88i+93mGvZWSIiIqLjYWFBZBb2MONyqIiaYmSrJQCAr3/bb9pTnkREREQeLCwsQFEUREVFmXYFACuwTIZZvzM2r+m0EwCw++Bh5B2oClSPDJbJ0KSYnxwzlGF+csxQjhnKmD0/FhYWoKoqMjIyoKo8XG1lmQwzRxqb54b9Zmx/uXV/IHrjxTIZmhTzk2OGMsxPjhnKMUMZs+dnzl6RF03TUFpaatqJOlZgmQxTBwDhcQCArpXroMDd36+2lgSwU26WydCkmJ8cM5RhfnLMUI4Zypg9PxYWFqDrOkpLS3mdvYBlMlRtwGnnAABsteUYm3AAALBudznKq+sD2TPrZGhSzE+OGcowPzlmKMcMZcyeHwsLIrPJPDrP4sqO7nkWmg4s3xb4y6GIiIiImsPCgshsGk3gHqRtMra/NMHlUERERETNYWFhAYqiIC4uzrQrAFiBpTJM7A5EpwAA4vavQaL71hZYsb0U9c7AXVNpqQxNiPnJMUMZ5ifHDOWYoYzZ82NhYQGqqiI1NdW0KwBYgaUyVBRjdSiloRrXdTkIAKiqc2J1/sGAdctSGZoQ85NjhjLMT44ZyjFDGbPnZ85ekRdN01BUVGTaFQCswHIZNrocakLYz8b2VwFcdtZyGZoM85NjhjLMT44ZyjFDGbPnx8LCAnRdh8PhMO0KAFZguQyzRwOqHQCQlf9vpNocANzzLAI1BstlaDLMT44ZyjA/OWYoxwxlzJ4fCwsiM4pNBQbdAMB9OdSj8Z8BAArLa7C9JPB34SYiIiI6FgsLIrP63X1AaDQA4LzDXyBL2QeAq0MRERGRObGwsABFUZCYmGjaFQCswJIZRncChv8ZAKDqLtxjfx9A4O7CbckMTYT5yTFDGeYnxwzlmKGM2fNTdLNepGUilZWViIuLg8PhQGxsbKC7Q6eS+mrgxYFAlbuYuKzuYWxAd6yZNQaJ0WEB7hwREREFu9Z8D+YZCwvQNA0FBQWmXQHACiybYWgUMOp+48fp9o+g68DXv/l/dSjLZmgSzE+OGcowPzlmKMcMZcyeHwsLC9B1HdXV1aZdAcAKLJ3hwClATCoA4Cx1K+xwBuRyKEtnaALMT44ZyjA/OWYoxwxlzJ4fCwsis7PZga7DAQCRSh16K7vxXW4pahtcAe4YERER0VEsLIisoOswY/NM9Tccrnfhx52Buws3ERER0bFYWFiAqqpISUkx7e3brcDyGXY5WlgMUbcB8P9duC2fYYAxPzlmKMP85JihHDOUMXt+5uwVeVEUBfHx8aZdWswKLJ9hp15AeBwA4Ex1GwAdX/n5LtyWzzDAmJ8cM5RhfnLMUI4Zypg9PxYWFqBpGnbu3GnaFQCswPIZqiqQcRYAIEE5hGxlH/Y5avFzocNvXbB8hgHG/OSYoQzzk2OGcsxQxuz5sbCwAF3XUV9fb9oVAKwgKDJsNM9isLodAPDpz/v89vJBkWEAMT85ZijD/OSYoRwzlDF7fiwsiKyi0TyLobbfAACfbSqCppnzDxciIiI6tZi6sJg7dy7OPPNMxMTEICkpCZdccgm2bdvmtU9tbS2mTZuGjh07Ijo6GhMnTkRJifca/3v27MEFF1yAyMhIJCUl4Z577oHT6fTnUIjk0gYCNvfdtkeE7gAAFDlqsaGgPJC9IiIiIgJg8sLi22+/xbRp0/Djjz9i2bJlaGhowNixY1FdXW3s85e//AWffPIJPvzwQ3z77bfYt28fLrvsMuNxl8uFCy64APX19Vi5ciXeeecdzJs3D7Nnzw7EkNpEVVWkp6ebdgUAKwiKDO1hQOdBAIAkZxGS4C4oPvm5yC8vHxQZBhDzk2OGMsxPjhnKMUMZs+en6Ga9SOs4Dhw4gKSkJHz77bcYOXIkHA4HOnXqhAULFuDyyy8HAPz222/o1asXVq1ahbPOOgtffPEFLrzwQuzbtw/JyckAgNdeew333XcfDhw4gNDQ0JO+bmVlJeLi4uBwOBAbG9uuYyQ6oa8eAb57FgDwZ+ef8bFzKJJiwvDjA6OhquZcIYKIiIisqzXfg81Z7jTD4XCvgJOQkAAAWLduHRoaGjBmzBhjn549e6JLly5YtWoVAGDVqlXo27evUVQAwLhx41BZWYnNmzf7sfdt53K5sH37drhcvNNyWwVNho3mWVzWYQf6KLswtHo5ft34Y7u/dNBkGCDMT44ZyjA/OWYoxwxlzJ6fPdAdaClN0zBz5kwMHz4cp59+OgCguLgYoaGhiI+P99o3OTkZxcXFxj6NiwrP457Hjqeurg51dXXGz5WVlQDcB9NzIBVFgaqq0DTNa2Z+c+2qqkJRlGbbj32DeE5xaZoGl8sFp9MJl8vl1d6YzWaDrute7Z6+NNfe0r63x5ha0u7rMXkytPSY0gZDhQIFOn536DP8LuwzAED9p29Cy1oLNT6j3cak67rXZ8BnY2pDu+mP03HaPZ9jTdNgs9mCYkwna/f1mBr/WRgsY/LncdI0zfjv2L5YdUz+Pk6e9yCAoBmTh7+O07HfaYJhTP48TgCa/F3c3mNqzcVNlikspk2bhl9//RXff/99u7/W3LlzMWfOnCbteXl5iI6OBgDExcUhNTUVJSUlxpkUAEhMTERiYiL27t3rNRckJSUF8fHx2LVrF+rr64329PR0REdHIy8vz+vNkJmZCbvdjtzcXGiahrKyMuzYsQM9evSA0+lEfn6+sa+qqujevTuqq6tRWFhotIeGhiIrKwsOh8OriIqKikJGRgbKyspQWlpqtPtzTI3l5OS0+5j2799vZKiqqqXHlN2xJ0IObvV6vVCtFo5fPkPcyFvbbUxZWVlwuVxGhu1xnILxvecZk+dzXFZWhuTk5KAYk7+PU15envE5ttvtQTEmfx6nDh06AAD27duHmpqaoBiTv4+TpmkoL3fPbwuWMQH+PU6HDh0yPsdpaWlBMSZ/Hqfs7Gw0NDR4/V3c3mOKjIxES1lijsX06dPx8ccfY8WKFcjMzDTav/76a4wePRrl5eVeZy26du2KmTNn4i9/+Qtmz56NRYsWYePGjcbj+fn5yMrKwvr16zFw4MAmr3e8MxaeA+O5tszfZyx27NiBbt26ISQkxGhvLBircl+OqaGhAbm5uejWrRtsNpu1x5T3FdTF90MPCceWygj0qVkLACjsfi3Sr3m5Xc9Y5ObmIjs7GzabzbdjCuL3XuMzFjt27EBOTg5CQkKCYkwna/f1mDx/mXo+x8EwJn+fscjLy0N2drbx+lYfUyDOWHj+kc/zulYfk4e/jpPT6fT6ThMMY/L3GYvt27d7/V3c3mOqqqpCfHx8i+ZYmLqw0HUdM2bMwMKFC/HNN98gJyfH63HP5O1///vfmDhxIgBg27Zt6NmzZ5PJ20VFRUhKSgIAvPHGG7jnnnuwf/9+hIWFnbQfgZ68revum6GEhoZCUThBty2CNcMv123FmE/cd+TOizgd2ff90G6vFawZ+gvzk2OGMsxPjhnKMUOZQOTXmu/Bpr4Uatq0aViwYAE+/vhjxMTEGKd34uLiEBERgbi4OEydOhV33nknEhISEBsbixkzZmDYsGE46yz3l62xY8eid+/euPbaa/HUU0+huLgYDz74IKZNm9aiosIs7HZTHypLCMYMR/bvgcJPkpCO/Ug9nIuKqhrER0e02+sFY4b+xPzkmKEM85NjhnLMUMbM+Zl6VahXX30VDocDo0aNQmpqqvHf+++/b+zzt7/9DRdeeCEmTpyIkSNHIiUlBf/973+Nx202Gz799FPYbDYMGzYMf/zjHzFlyhQ88sgjgRhSm2iaZsy1oLYJ1gxD7SoOdegDAIhU6vDNylXt9lrBmqG/MD85ZijD/OSYoRwzlDF7fuYtedCyWejh4eF45ZVX8MorrzS7T9euXfH555/7smtEppGYMxT4aTkAIHfj98DY8wLcIyIiIjoVmfqMBRGdXKfuZxrbHSq34Ne9jhPsTURERNQ+WFgQWV3qAGPzdHUXPlxbELi+EBER0SnL1KtCmYUZVoXSNM1YqoxaL9gz1J7rDbVyLw7pEThHmYcfZ/0e4SG2k/9iKwR7hu2N+ckxQxnmJ8cM5ZihTCDya833YJ6xsAjPnT6p7YI5Q/XIWYsYpQbxdYVYsvn4d5WXCuYM/YH5yTFDGeYnxwzlmKGMmfNjYWEBmqYhPz/ftCsAWEHQZ5g2wNjsq+Tj3VW7ff4SQZ9hO2N+csxQhvnJMUM5Zihj9vxYWBAFg9T+xmYfdRfW7S7HL4UVgesPERERnXJYWBAFg8YTuJV8AMC8lbsC0xciIiI6JbGwsAhV5aGSCuoMY5KB6BQAQF/bLgA6Pv25CKVVdT59maDO0A+YnxwzlGF+csxQjhnKmDk/rgrVAoFeFYqoReZfCeQuAQCMqHsehXoS7vp9d8wYnRPgjhEREZFVcVWoIKPrOqqqqlp0J3I6vlMiw0YTuPur7suh/nf1bjS4fDPB65TIsB0xPzlmKMP85JihHDOUMXt+LCwsQNM0FBYWmnYFACs4JTJMG2hsXh//MwCgpLIOi3/1zdKzp0SG7Yj5yTFDGeYnxwzlmKGM2fNjYUEULLLPA6I6AQAGH/4OaSgFALz5fb5p/2WDiIiIggcLC6JgYQ8DzrwJAKDoLvwl7lsAwM8FFfhxZ1kge0ZERESnABYWFqAoCkJDQ/126/ZgdMpkOPhGwBYKALhYW4YI1AIAXvs2T/zUp0yG7YT5yTFDGeYnxwzlmKGM2fPjqlAtwFWhyFI+uh3YOB8A8GzILXjp0CgAwOd3nIPeaXz/EhERUctxVaggo+s6KioqeJ28wCmV4dBbjc2bQpZAgXuC1xsrZGctTqkM2wHzk2OGMsxPjhnKMUMZs+fHwsICNE1DcXGxaVcAsIJTKsPUfsBp5wAA4g7vxoWRWwAAn/xShIKyw21+2lMqw3bA/OSYoQzzk2OGcsxQxuz5sbAgCkZn3W5szglbgEjUwqXpePO7nQHsFBEREQUzFhZEwaj7OCC1PwAgoWYX/hr6NgAdC37agx37DwW2b0RERBSUWFhYgKIoiIqKMu0KAFZwymWo2oDL3wZCYwAAf1C/w1W2b9Dg0jFr4a9tujbzlMvQx5ifHDOUYX5yzFCOGcqYPT+uCtUCXBWKLOvX/wP+cyMAoA6huLjuEfymd8HTl/fDFYMzAtw5IiIiMjuuChVkNE1DaWmpaSfqWMEpm+HpE4HBUwEAYajHCyEvwwYXnvh8K8qq61v1VKdshj7C/OSYoQzzk2OGcsxQxuz5sbCwAF3XUVpaatqlxazglM5w3BNASl8AQA+1ENfYvkL54QbM/Xxrq57mlM7QB5ifHDOUYX5yzFCOGcqYPT8WFkTBLiQcuOBvxo932f+DOFThw3WF+KWwInD9IiIioqDCwoLoVJBxJtD3SgBAvFKFP9v/CwB4cvFvgewVERERBREWFhagKAri4uJMuwKAFTBDAGP+B7BHAACm2JchW9mLH3YcxHe5B1r068xQhvnJMUMZ5ifHDOWYoYzZ8+OqUC3AVaEoaHzzV+CbuQCA5a7+uKHhPpzeORb/v717D5OiOvA+/j1VfZnpnhswwAwXuYtGlCgKIRpzgSjEJ9Fo4iVsRJPVmICamOQluust2Y2++r7qxjWYJ+slu+Y1Cdl4iSa63jBRERXFuwgIgjIDDjiXnpm+VNV5/2hobWeAkZLpHvl9nqefmT5V3X3q16er6nRVnb57wVE4TnmupERERKR0NCrUx0wQBDQ1NZXtCAADgTLc7tPnQc1IAD7vPs/h5jVeerude15s2u1DlWE4yi88ZRiO8gtPGYanDMMp9/zUsRgArLW0tbWV7QgAA4Ey3C6WgM//U+HueZE7APi//7OKrLfrlZQyDEf5hacMw1F+4SnD8JRhOOWenzoWIvuaQ06GujEAHO2+yFSzhje3drF46dri+ZpegEeugNYNJaikiIiIDDTqWIjsa9wofOaCwt3zoncC8IuHV7NyY2u+MNsFt50Ij14Jd5/b/3UUERGRAUcdiwHAGEN9fX3ZjgAwECjDD5j6DagZBcAs51kOMuvxA8sPfr+SrqwHL/4BOrePFvXmE5BLK8OQlF94yjAc5ReeMgxPGYZT7vmpYzEAOI5DfX09jqO3a08pww+IxOCo7xfu/lP1PQCsa+nk5/e+Ak/e+N68fhY2PacMQ1J+4SnDcJRfeMowPGUYTrnnV561kiJBELBx48ayHQFgIFCGvTj0m1DVAMCns08wI/oGAGufvg/eebV43o1PKsOQlF94yjAc5ReeMgxPGYZT7vmpYzEAWGvp7Ows2xEABgJl2ItoBRx5XuHuzcl/ZwhtfMu9r+e8G5Yrw5CUX3jKMBzlF54yDE8ZhlPu+aljIbIvO+IsGD0DgGS6mbsHXccs51kAmuxgOpztP4SzcTmU6UpMREREyoM6FiL7skgMTv7PwilRI7tX4Zh8B+K/vNk8mZuYn697G2xdXapaioiIyACgjsUA4DgODQ0NZXuhzkCgDHehugFO+S9wooWiwI1zX3wOK4L9C2XOW08pwxDUBsNThuEov/CUYXjKMJxyz688ayVFjDHU1dWV7dBiA4Ey3I3R0+G4/1O460w9leu+NYvnzQGFso3PP6IMQ1AbDE8ZhqP8wlOG4SnDcMo9P3UsBoAgCHjjjTfKdgSAgUAZ9sG0M+CEG+HI78Ox/8oho+r48tzjyNgIAMGbT7L02VeV4R5SGwxPGYaj/MJThuEpw3DKPT91LAYAay3ZbLZsRwAYCJRhH33yNPji5RCvBuC0T09iU2IyAGNNE9fc9xLt3dlS1nDAUhsMTxmGo/zCU4bhKcNwyj0/dSxEZKeMMYw65POF+w2plzn39pV4fnl+UyIiIiKlo46FiOxSdOzMwv/TnNf5+5qtXPbnl8v22xIREREpDXUsBgDHcRg1alTZjgAwECjDELb/zgXAEc7rANz25AZufnx9iSo0MKkNhqcMw1F+4SnD8JRhOOWeX3nWSooYY6iqqirbEQAGAmUYQtVQGDwBgMOc1Zzl3gNYfnbPK1z34OsEgY5c9IXaYHjKMBzlF54yDE8ZhlPu+aljMQD4vs/rr7+O7/ulrsqApQxDmja/8O8/Rf8fP4vcQoI0jz30Z/77+h/T/dgvwdNF3buiNhieMgxH+YWnDMNThuGUe36RUldA+qZchxUbSJRhCJ8+jyDbhfPolQB8M/Ig89yH8r/S/S7wIGx6+SHqz7ydWCxW2rqWMbXB8JRhOMovPGUYnjIMp5zz0xELEdk9Y7BH/y82feoy7PZf6HZM8SlQI5oe5OErT+SXD6+iM+OVopYiIiJSQupYiEiftY+dS/APf4Ihk2DIJFIHnsptifmFH9GbE/ydQQ8v4rh/+xsrN7aWtrIiIiLSr4zVmJG71d7eTm1tLW1tbdTU1PT76+/4MZRYLFa2F+uUO2UY3s4ytNby+t+WMPGRc3DJn/N5u/d5Lgu+zXlfPJBzPjsB11HmaoPhKcNwlF94yjA8ZRhOKfL7MPvBOmIxQEQiuhwmLGUYXm8ZGmOY/NmTcb/2H1iTX6WcFnmEf3ev4fr7X+Crv3ycp9dv6++qliW1wfCUYTjKLzxlGJ4yDKec81PHYgAIgoDVq1eX9cU65U4ZhrfbDKeciDnpPwrXYHzRfZbbY//K229t4Os3LmPBb59lzZZUP9a4vKgNhqcMw1F+4SnD8JRhOOWeX/l2eURk4JlyEiZRD7+bB9kODnXW8Fj8fO7yP81/vfRFZr/YxIGNNXxpSgNfOqSRCUOrSl1jERER+YioYyEiH63xn4Vv/RVu+xqkmqk0WU6NLOXUyFLWB8N5pWUMrzw8hjsehsMTzRwcfYvqmEP0uKswk2aXuvYiIiKyh9SxEJGPXsPBcPZSeOxaeP52yLQDMNbZzFg28yX3qfx8ue23LvB++3XuGXUBiZlncdSkehIxrZ5EREQGEo0K1QflMCpUEAQ4jqMRFPaQMgxvjzPMpODFJfkORtML4HUXTfatwX3fb2L8yjuOfzPzOHr/BuZMaWDamEGMqKvsdWQpP7AsX72Jtq3NTJtyEMNqKvZ4+fY2tcHwlGE4yi88ZRieMgynFPl9mP3gfapjccMNN3D11VfT3NzM1KlTuf7665k+ffpuH1cOHQsNzRaOMgzvI8kw8GHrWtj8IgDvVI7n3rcTDH3qKo5L/bEwW8pW8HwwgefsRJrtYLJOJcnqGirrGqkcPpFhjaNpf3sVyZf+k7neIwwyKf4nOIIXpl7MP8yeQUNt+XUw1AbDU4bhKL/wlGF4yjCcch9udp/pWPz+97/n9NNP58Ybb2TGjBlcd911LFmyhFWrVjFs2LBdPrbUHQvf91m9ejWTJk3Cdd1+f/2PA2UY3t7OMPvkr4ncvwjH+rudN22jVJhcj/I2m+Dn/ulsm/Q1PrP/UI6aWM+4+mRZbLzUBsNThuEov/CUYXjKMJxS5Pdh9oP3mZOYr7nmGs466yzOPPNMAG688Ubuvfdebr75Zn7yk5+UuHYiEvvUWdB4ECy/EfvWM5j2t3c67/s7FTmi5NwKEn4HtaaL/x25kcwbN9G0djBNdggbTYy4CzHX4BgLNsDagCwxtsVH0Z4cS3f1WLLRGjJOggCH8emXmZh6mlHtK/GiVTQPP5p3RszCDvsEQ+1WhnhbSPpt+G4cz60s3HJOJTm3gmhFFcmqWqqTlVREe1nxWwu5buh+N39Lt0FyKAwaC5FY8Xx+DhwXjAPv7yBZC13byLS8QXvzOtxEHdWjpxCtbSyer1SshWwnZFMQr4ZoIl+vIIDOd6D97fz95ND8LRIvdY3By8Km5/JH1OrGwKjDoXJQqWs18FkLbz4Oy3+V/ztkIsz4Dhz4FXCjpa5deQqC/OejHD7LYXVshlQzDJ4A8TIeCdDLQq4TKurC5e570PwCbHkVBo+HkdOK1+sfc/tExyKbzbJixQouvPDCQpnjOMyePZtly5aVsGYiUmTMp2HMpzEAbW9D0/P5ne5siiCTorNlI9l33sBtW0/gVhA79BSqpp9OFMjc82Pir/43AHGTY6zJXygOgAW8Xl6v61noAt7ZRZ0yMCi1hgPX3vyhFydjI7QSp4s43TaOi8NW00kdKWKmZ4V8a9jEUHJEqTUd1JIiwntjlXs4BDj4uBgsFWSJA0Pf9xxtVNFmaomZHDGbw8UnR4QsUTI2Qo4IORPFx8XFEiVHFA8w5EwUz0TI2vy8aeviWYe4E1DpeMSNj2tzhVuOKGknQcZJAJZk0EFV0EGNTVFN5/bn3RFjlA6boNakiNLzqFTKVNFqatlmakmRJGkyVNNFgm4862yvT4TAWl434BpLYFwyTiVZpxJjDIODbdT5W6kK2uk0SbZSxzu2Bs/EcF0X13WJG5940E2F7caxPp1UkrIVRGyGT/irqCBbVK+1jGazOxwnEsONRImZgEq/naTfQSzoxjNRsiaKR5SsiZEjSpYIFoOxAYYAYy2GAGx+Z9F3KwjcSowbIe51UOl3UOmnSBOj0yTopBJrXGKOJepYKoIuqrx3qfFbidk0HU4N7U4dHU4NOSIEGHxrwHFxnfxyOgQ41sOxHm7g4eDhBB6On2W9A47Nt6QcUbJOHM/EqAy6qQ3epTZ4F4Bt7jC2RobRHhmMQ0DUerh4RGwu/7zWI79kLp6J4OPgmwg+Lj4Ojs233RG5N9nPW/9eqF1bYeNyWpx6Xo5PxTUWF7v9b4BrAOvjZ9MEXhbr5zBOBBOJ4UZj4EQJtt8iNkfc6yDudxAJsvjW4OHiW4N1IuBEMI67vU4GHxeLwcHm99uxOGzfj7QWPwjwg4AgsEQciDiGiGMwWCwWrMXzPF53XCy28HjH2PyRUWvJr3Aga2JkTCVpUwnGEMEnn44HwY5bgO/E8J04GIc6v4XBuc3U+VvJmApaIg20RIbT6dbiOAbHGBwCyHVjvDQmyIETgUgM40aJ2PxnNGJzBMYha+LkTAyPCA4+jvUxNii8/8b629uKj2MDfJNfv3jb1xGBEyEwEYxxiNkMMZslYnN4gSVnDdnAgHFx3QhuJAKOi8XBGodk7l1GpF+n1s+3pwDD5shINkbH0RVE2BZxMYbt+b+Xpdn+3mAhF4BnwQsMjuPkP8eOQzzoIu51UOGnCDB0O1V0udV4JkrUZonZDK71yRClmyhpG8N1DBHHIeqS38ZYi7UBVX4bQ7Mbqfc24xDQYap5OzKaJreBCuNTRRdJuvFNfn2XNpXb2xH5L6oAs/19r/JbmZh5hUr73rWEaeK8GpvCu5FhRF1DzAXXgLEBYDFBjojXRczvJBJkyJgKMm6SjJvENYaI8cmvYTwin/sJ46Ye1dvmpmzsEx2LlpYWfN9n+PDhReXDhw/ntdde6zF/JpMhk8kU7re350e08X0f389vEI3JN/IgCHj/2WQ7K99xkc3Oync87/vLIf9DKDum+b5fVP5+rusWLuj5YF12Vt7Xuu+NZepL+Ue9TDsy/DgtU3++T9baXuffa8tU1YDZv7GoPAEkPrBM/vb5I1/7NXbNybDiFtLvrMNpf5u4H+4H+bpsnAqy+SMdeyBuPOJ41NG5fUu2a66xjGbLTqfnOxkBvfeS8mpJUWtTO/Zterezab2Vb9/A99IXyPN3Me194uSIm7adTq+yKapsilHs/EhVUT131HUnrx2zWQbxLhN3zB+QH4HsQ5rARib4G/Ovk9nt7H2zB/V4v4qghaFBy54/QR/eL4BGbyON3sY9f51evP80xvqghc92P7T7B+34KV+PXTX9nvq4nOWq0nYxOvcGo3Nv7H7m7O5nKTUHS6P3Fo3eW/mCj+rz9BGrth0ckHuFA3KvfCTPV0GGQ7MrPpL36JnN64GjemyL9/Z+xIe5amKf6Fh8WFdccQWXX355j/K1a9dSVZU/jFdbW0tjYyObN2+mre29jWV9fT319fW8/fbbdHZ2FsobGhqoq6tj/fr1ZLPvta5Ro0ZRVVXF2rVrixrDuHHjiEQirF69ulD2xhtvMGnSJDzPY926dYVyx3HYf//96ezs5K233iqUx2Ixxo8fT1tbG83NzYXyZDLJ6NGj2bZtGy0t722cSrFMQL8s046yN95442OzTKV4n8aOHVvIsDyX6SiqJs9h4+uv50fNyHWC9RkzZhyOG2HN2jdwXBeMwcmlGJX06Nj4Cq0bXiTidRHxu4j4GWiYwlu103g524jNpBj57nLGtj1JMkjRYgaz0RtEC7XEyZFwslRHfKJeJzG/iwqbJhKkiQUZYjZDNOimwqapsBkMASlTRcqppo0k7baKDpOki0qGmlZGBU00Bk04BLRRTZupImMq89+uWq/wLaOz/Zvwd90hbHWHkYoPJ+G3Mzz7JqP9t0jQRdrGyBDFtw4x4xEzPvnv03PE8HB2fKtK/iiGsTb/rZjZ/d5YzrrkTITY9u/L3y9FJe1U0WaTtJKk01ZQY7oZZDqpMZ202ySbGMJGL3+KUb1pZ4hpo5426k071ea9b/o869BFBQZLDI/49h1SH0NgnR51DayhhVq22WpqTSf1tO10eXxr8HGIvW96M4N5hoN4mQmMMZv5JK8z0a7rsYwAGRulkzhR8rnGezkC9WF02xgxckUjpL1fl43TQg1p4tSRYjDtRMye/fKuZ/PfSgeYHq+5zVbRYmsxwEjTQsJ8NHt/K4JJ/MY7lvuCI5jmvM633PuY5Ty7x5323qRtFIPNHxX4CJ+3v2211TTZIVTRzUjT0qfPZLl611bxcjCGZoYw0bzFAWZjr9fH7SnfGgyEbkcdtpJ1toGUrWSc00yj2bbHz7XZ1rE8OJCXgrFMdjZypPMyDebdUPXbobNtK67rMnLkyKJt8d7e5iYSiT7XcZ+4eDubzZJIJPjjH//ICSecUCifP38+ra2t3HXXXUXz93bEYscbs+Oilf781thaS1dXF4lEonChzsfpm/D++Hbf9306OztJJBIYYz4Wy9Tf75Mxhs7OTiorK4suhh7Iy9Sf79OOz3EymcR13X5Zph3PY4zpuUyBj3FcnA+W2wAT5HACjyCXxvo5cGPgRjGROE60gmD78mAteGlMNoXjuATxmvzpJ31YpsBCJpsrHHgwxlARdTFemqC7FRtNkjUVdOYCqiuixKMuvufR1d1d+By7xhBkU2Q62/E8D69iCJ6JgHGornCJGvKn0vlZsAFZzyPtO/jRJEQqcB2HhOsTD9IEfg6bqC+cW114/7Jd2EyKXC5De0cXGWswiSEQqcAx5E9NcQxRBxzrgZcm4mcxTv7x0UgUx3UJLGAcbOCR7u4k3d1FOp3GqawjkqwjHq8g5hgqTAbS7fieR9YaUjkgkqC6dhDxiCnkaAMfr6udmAtYH+t7eL5PdyZHdzaHNQ7WieC4Udwdp8lEY2SzOZLJJI5jtp9yZPOPz6XxnTi+yde18H51v4uf2oI1UXwnQuDEcCIxHDdGYFyMsfnTawIP1+ZPNjJeFoMPxs2/TxVVkKgvnApmIP/66XcJOreR8QN8a8hZg2/zp/sFGAbXVBOPx/Ntzwbg53i3I0Um3Y3vZfFyGRw3Rqx6CJGKamKxKPGIQ8QxuI5DEPikurroTmcLpwFFHYvBkPX8/PG/APzt7w3GkIg6JGIRIhEXz7d05iwdmRxBABHHwXENuUyGmuoqXDe/Psj5kAssfmBwXQcM2MDi+mkcrwsn14kBMtYhF7gYN0IsFqWiopJ4NEou00Uu042XyxAkhmJiSRzXwQYWAg+3sxmbSeFb8AIIAksimaQqWUU0Xonve3R2p+noSOEbd/tpUTEixuL4ach2Y6wHjotxIjiRCMaJ5E88chyMG8UYFzcSBeuDn8UEHvg5Aj8HgY/n+3gmhufE8Z0YVXGXmgqXqAnI5bJ0pbO0d+fw/fwJm4HvQTRJtG4ElfFI/rNtHHK5DPbdDXR3pojF49vrYcj5Qb7d8d774RiHuGupiBhiDmRyHhnPozvjYaMJIslBRCur85/DbAqbbsPm0gRunMCtwDoRkhGfuM3i+mmyfkBXNqArF2AcB8eAwWAqa3GSQ3Fdh1gkkj8FLteB07GJtInTZRJ0mSSBlyGSS+HmUrgE+TbsRoAd+xQGE6nArRuZbwfW5ts6Fqd9I7l0B905S0c6Ry7IPwbjEInGqEjWEE/UEK9I4Ge7yHW15dc7viXtG3JEyAQOIxuGU1dTTSqVKtoW7+1tbiqVoq6uTqNCvd+MGTOYPn06119/PZDf6O63334sXLhwtxdva1SogU8ZhqcMw1F+4SnDcJRfeMowPGUYjkaFKhMXXHAB8+fP5/DDD2f69Olcd911dHZ2FkaJEhERERGRPbfPdCxOOeUU3nnnHS655BKam5v55Cc/yX333dfjgm4REREREfnw9pmOBcDChQtZuHBhqavxoRlj9AuVISnD8JRhOMovPGUYjvILTxmGpwzDKff89plrLMIo9TUWIiIiIiKl8GH2g51dTpWyYK2ltbX1Q40jLMWUYXjKMBzlF54yDEf5hacMw1OG4ZR7fupYDABBENDc3NxjWEnpO2UYnjIMR/mFpwzDUX7hKcPwlGE45Z6fOhYiIiIiIhKaOhYiIiIiIhKaOhYDgDGGZDJZtiMADATKMDxlGI7yC08ZhqP8wlOG4SnDcMo9P40K1QcaFUpERERE9kUaFepjJggCWlpayvZCnYFAGYanDMNRfuEpw3CUX3jKMDxlGE6556eOxQBgraWlpaVshxYbCJRheMowHOUXnjIMR/mFpwzDU4bhlHt+6liIiIiIiEho6liIiIiIiEho6lgMAMYYamtry3YEgIFAGYanDMNRfuEpw3CUX3jKMDxlGE6556dRofpAo0KJiIiIyL5Io0J9zARBQFNTU9mOADAQKMPwlGE4yi88ZRiO8gtPGYanDMMp9/zUsRgArLW0tbWV7QgAA4EyDE8ZhqP8wlOG4Si/8JRheMownHLPTx0LEREREREJLVLqCgwEO3qF7e3tJXl93/dJpVK0t7fjum5J6jDQKcPwlGE4yi88ZRiO8gtPGYanDMMpRX479n/7cpREHYs+6OjoAGD06NElromIiIiISP/r6OigtrZ2l/NoVKg+CIKATZs2UV1dXZLhvdrb2xk9ejQbN27UqFR7SBmGpwzDUX7hKcNwlF94yjA8ZRhOKfKz1tLR0cGIESNwnF1fRaEjFn3gOA6jRo0qdTWoqanRhzAkZRieMgxH+YWnDMNRfuEpw/CUYTj9nd/ujlTsoIu3RUREREQkNHUsREREREQkNHUsBoB4PM6ll15KPB4vdVUGLGUYnjIMR/mFpwzDUX7hKcPwlGE45Z6fLt4WEREREZHQdMRCRERERERCU8dCRERERERCU8dCRERERERCU8diALjhhhsYO3YsFRUVzJgxg6eeeqrUVSpLV1xxBUcccQTV1dUMGzaME044gVWrVhXN87nPfQ5jTNHtnHPOKVGNy89ll13WI58DDjigMD2dTrNgwQKGDBlCVVUVJ510Eps3by5hjcvP2LFje2RojGHBggWA2uAH/e1vf+PLX/4yI0aMwBjDnXfeWTTdWssll1xCY2MjlZWVzJ49m9WrVxfNs23bNubNm0dNTQ11dXV8+9vfJpVK9eNSlNauMszlcixatIiDDz6YZDLJiBEjOP3009m0aVPRc/TWbq+88sp+XpLS2F0bPOOMM3pkM2fOnKJ51AZ3nWFv60RjDFdffXVhnn25DfZl/6Uv298NGzZw3HHHkUgkGDZsGD/+8Y/xPK8/F0Udi3L3+9//ngsuuIBLL72UZ599lqlTp3LssceyZcuWUlet7Dz66KMsWLCAJ598kgceeIBcLscxxxxDZ2dn0XxnnXUWTU1NhdtVV11VohqXp4MOOqgon8cee6ww7Qc/+AF//vOfWbJkCY8++iibNm3ixBNPLGFty8/TTz9dlN8DDzwAwNe//vXCPGqD7+ns7GTq1KnccMMNvU6/6qqr+MUvfsGNN97I8uXLSSaTHHvssaTT6cI88+bN4+WXX+aBBx7gnnvu4W9/+xtnn312fy1Cye0qw66uLp599lkuvvhinn32Wf70pz+xatUqvvKVr/SY96c//WlRuzz33HP7o/olt7s2CDBnzpyibG6//fai6WqDu87w/dk1NTVx8803Y4zhpJNOKppvX22Dfdl/2d321/d9jjvuOLLZLE888QS/+c1vuPXWW7nkkkv6d2GslLXp06fbBQsWFO77vm9HjBhhr7jiihLWamDYsmWLBeyjjz5aKPvsZz9rzz///NJVqsxdeumldurUqb1Oa21ttdFo1C5ZsqRQ9uqrr1rALlu2rJ9qOPCcf/75dsKECTYIAmut2uCuAPaOO+4o3A+CwDY0NNirr766UNba2mrj8bi9/fbbrbXWvvLKKxawTz/9dGGev/71r9YYY99+++1+q3u5+GCGvXnqqacsYN98881C2ZgxY+y11167dys3APSW3/z58+3xxx+/08eoDRbrSxs8/vjj7Re+8IWiMrXB93xw/6Uv29+//OUv1nEc29zcXJhn8eLFtqamxmYymX6ru45YlLFsNsuKFSuYPXt2ocxxHGbPns2yZctKWLOBoa2tDYDBgwcXlf/2t7+lvr6eKVOmcOGFF9LV1VWK6pWt1atXM2LECMaPH8+8efPYsGEDACtWrCCXyxW1xwMOOID99ttP7XEnstkst912G9/61rcwxhTK1Qb7Zt26dTQ3Nxe1udraWmbMmFFoc8uWLaOuro7DDz+8MM/s2bNxHIfly5f3e50Hgra2Nowx1NXVFZVfeeWVDBkyhEMPPZSrr76630+hKGdLly5l2LBhTJ48me9+97ts3bq1ME1t8MPZvHkz9957L9/+9rd7TFMbzPvg/ktftr/Lli3j4IMPZvjw4YV5jj32WNrb23n55Zf7re6Rfnsl+dBaWlrwfb+okQAMHz6c1157rUS1GhiCIOD73/8+Rx55JFOmTCmUf+Mb32DMmDGMGDGCF154gUWLFrFq1Sr+9Kc/lbC25WPGjBnceuutTJ48maamJi6//HI+85nP8NJLL9Hc3EwsFuuxMzJ8+HCam5tLU+Eyd+edd9La2soZZ5xRKFMb7Lsd7aq3deCOac3NzQwbNqxoeiQSYfDgwWqXvUin0yxatIjTTjuNmpqaQvl5553HYYcdxuDBg3niiSe48MILaWpq4pprrilhbcvDnDlzOPHEExk3bhxr167loosuYu7cuSxbtgzXddUGP6Tf/OY3VFdX9ziNVm0wr7f9l75sf5ubm3tdV+6Y1l/UsZCPpQULFvDSSy8VXR8AFJ3zevDBB9PY2MisWbNYu3YtEyZM6O9qlp25c+cW/j/kkEOYMWMGY8aM4Q9/+AOVlZUlrNnAdNNNNzF37lxGjBhRKFMblFLJ5XKcfPLJWGtZvHhx0bQLLrig8P8hhxxCLBbjO9/5DldccUXZ/sJvfzn11FML/x988MEccsghTJgwgaVLlzJr1qwS1mxguvnmm5k3bx4VFRVF5WqDeTvbfxkodCpUGauvr8d13R5X/W/evJmGhoYS1ar8LVy4kHvuuYdHHnmEUaNG7XLeGTNmALBmzZr+qNqAU1dXx/7778+aNWtoaGggm83S2tpaNI/aY+/efPNNHnzwQf7xH/9xl/OpDe7cjna1q3VgQ0NDj8EsPM9j27Ztapfvs6NT8eabb/LAAw8UHa3ozYwZM/A8j/Xr1/dPBQeQ8ePHU19fX/jMqg323d///ndWrVq12/Ui7JttcGf7L33Z/jY0NPS6rtwxrb+oY1HGYrEY06ZN46GHHiqUBUHAQw89xMyZM0tYs/JkrWXhwoXccccdPPzww4wbN263j1m5ciUAjY2Ne7l2A1MqlWLt2rU0NjYybdo0otFoUXtctWoVGzZsUHvsxS233MKwYcM47rjjdjmf2uDOjRs3joaGhqI2197ezvLlywttbubMmbS2trJixYrCPA8//DBBEBQ6bfu6HZ2K1atX8+CDDzJkyJDdPmblypU4jtPjFB+Bt956i61btxY+s2qDfXfTTTcxbdo0pk6dutt596U2uLv9l75sf2fOnMmLL75Y1Mnd8SXCJz7xif5ZENCoUOXud7/7nY3H4/bWW2+1r7zyij377LNtXV1d0VX/kvfd737X1tbW2qVLl9qmpqbCraury1pr7Zo1a+xPf/pT+8wzz9h169bZu+66y44fP94effTRJa55+fjhD39oly5datetW2cff/xxO3v2bFtfX2+3bNlirbX2nHPOsfvtt599+OGH7TPPPGNnzpxpZ86cWeJalx/f9+1+++1nFy1aVFSuNthTR0eHfe655+xzzz1nAXvNNdfY5557rjBi0ZVXXmnr6ursXXfdZV944QV7/PHH23Hjxtnu7u7Cc8yZM8ceeuihdvny5faxxx6zkyZNsqeddlqpFqnf7SrDbDZrv/KVr9hRo0bZlStXFq0bd4wU88QTT9hrr73Wrly50q5du9bedtttdujQofb0008v8ZL1j13l19HRYX/0ox/ZZcuW2XXr1tkHH3zQHnbYYXbSpEk2nU4XnkNtcNefY2utbWtrs4lEwi5evLjH4/f1Nri7/Rdrd7/99TzPTpkyxR5zzDF25cqV9r777rNDhw61F154Yb8uizoWA8D1119v99tvPxuLxez06dPtk08+WeoqlSWg19stt9xirbV2w4YN9uijj7aDBw+28XjcTpw40f74xz+2bW1tpa14GTnllFNsY2OjjcViduTIkfaUU06xa9asKUzv7u623/ve9+ygQYNsIpGwX/3qV21TU1MJa1ye7r//fgvYVatWFZWrDfb0yCOP9Pq5nT9/vrU2P+TsxRdfbIcPH27j8bidNWtWj1y3bt1qTzvtNFtVVWVramrsmWeeaTs6OkqwNKWxqwzXrVu303XjI488Yq21dsWKFXbGjBm2trbWVlRU2AMPPND+/Oc/L9px/jjbVX5dXV32mGOOsUOHDrXRaNSOGTPGnnXWWT2+3FMb3PXn2Fprf/WrX9nKykrb2tra4/H7ehvc3f6LtX3b/q5fv97OnTvXVlZW2vr6evvDH/7Q5nK5fl0Ws32BRERERERE9piusRARERERkdDUsRARERERkdDUsRARERERkdDUsRARERERkdDUsRARERERkdDUsRARERERkdDUsRARERERkdDUsRARERERkdDUsRARkY8lYwx33nlnqashIrLPUMdCREQ+cmeccQbGmB63OXPmlLpqIiKyl0RKXQEREfl4mjNnDrfccktRWTweL1FtRERkb9MRCxER2Svi8TgNDQ1Ft0GDBgH505QWL17M3LlzqaysZPz48fzxj38sevyLL77IF77wBSorKxkyZAhnn302qVSqaJ6bb76Zgw46iHg8TmNjIwsXLiya3tLSwle/+lUSiQSTJk3i7rvv3rsLLSKyD1PHQkRESuLiiy/mpJNO4vnnn2fevHmceuqpvPrqqwB0dnZy7LHHMmjQIJ5++mmWLFnCgw8+WNRxWLx4MQsWLODss8/mxRdf5O6772bixIlFr3H55Zdz8skn88ILL/ClL32JefPmsW3btn5dThGRfYWx1tpSV0JERD5ezjjjDG677TYqKiqKyi+66CIuuugijDGcc845LF68uDDtU5/6FIcddhi//OUv+fWvf82iRYvYuHEjyWQSgL/85S98+ctfZtOmTQwfPpyRI0dy5pln8i//8i+91sEYwz//8z/zs5/9DMh3VqqqqvjrX/+qaz1ERPYCXWMhIiJ7xec///mijgPA4MGDC//PnDmzaNrMmTNZuXIlAK+++ipTp04tdCoAjjzySIIgYNWqVRhj2LRpE7NmzdplHQ455JDC/8lkkpqaGrZs2bKniyQiIrugjoWIiOwVyWSyx6lJH5XKyso+zReNRovuG2MIgmBvVElEZJ+nayxERKQknnzyyR73DzzwQAAOPPBAnn/+eTo7OwvTH3/8cRzHYfLkyVRXVzN27Fgeeuihfq2ziIjsnI5YiIjIXpHJZGhubi4qi0Qi1NfXA7BkyRIOP/xwjjrqKH7729/y1FNPcdNNNwEwb948Lr30UubPn89ll13GO++8w7nnnss3v/lNhg8fDsBll13GOeecw7Bhw5g7dy4dHR08/vjjnHvuuf27oCIiAqhjISIie8l9991HY2NjUdnkyZN57bXXgPyITb/73e/43ve+R2NjI7fffjuf+MQnAEgkEtx///2cf/75HHHEESQSCU466SSuueaawnPNnz+fdDrNtddey49+9CPq6+v52te+1n8LKCIiRTQqlIiI9DtjDHfccQcnnHBCqasiIiIfEV1jISIiIiIioaljISIiIiIioekaCxER6Xc6C1dE5ONHRyxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCQ0dSxERERERCS0/w/xtX26rhOE7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
