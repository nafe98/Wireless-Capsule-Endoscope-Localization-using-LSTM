{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 20ms/step - loss: 1375.1863 - val_loss: 1283.5806\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1210.8633 - val_loss: 1180.6276\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 1124.1123 - val_loss: 1106.4701\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1060.3868 - val_loss: 1050.9153\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1013.1277 - val_loss: 1010.3083\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 978.8282 - val_loss: 980.8889\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 954.6014 - val_loss: 960.5726\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 938.2953 - val_loss: 947.3573\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 928.0504 - val_loss: 939.1447\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 922.0922 - val_loss: 934.6708\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 918.9131 - val_loss: 932.2861\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 917.4110 - val_loss: 931.2054\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.7755 - val_loss: 930.7690\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5807 - val_loss: 930.5767\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5264 - val_loss: 930.5223\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.4689 - val_loss: 930.5122\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 916.4451 - val_loss: 930.5353\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 916.4468 - val_loss: 930.5115\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 916.4443 - val_loss: 930.5216\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.4861 - val_loss: 930.5072\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.4849 - val_loss: 930.5087\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.4895 - val_loss: 930.5222\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 916.4433 - val_loss: 930.4671\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 917.4808 - val_loss: 930.7764\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 916.3725 - val_loss: 929.7678\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 904.2392 - val_loss: 899.2192\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 876.5659 - val_loss: 876.9586\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 844.6725 - val_loss: 840.5356\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 809.6393 - val_loss: 808.3513\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 783.8255 - val_loss: 786.9196\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 762.6729 - val_loss: 766.7216\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 745.1023 - val_loss: 749.2749\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 727.5707 - val_loss: 732.7022\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 709.9941 - val_loss: 715.2161\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 694.6898 - val_loss: 698.9669\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 676.0209 - val_loss: 677.6451\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 651.3193 - val_loss: 650.7965\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 618.1985 - val_loss: 607.3038\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 581.1591 - val_loss: 567.8101\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 536.8777 - val_loss: 530.2272\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 497.3260 - val_loss: 486.3637\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 454.2946 - val_loss: 446.6689\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 405.4529 - val_loss: 394.7429\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 367.3072 - val_loss: 362.6989\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 335.6844 - val_loss: 328.1363\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 304.3496 - val_loss: 298.5858\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 277.5027 - val_loss: 272.9746\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 251.1575 - val_loss: 245.9493\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 227.4020 - val_loss: 245.0723\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 204.3260 - val_loss: 200.5581\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 183.5040 - val_loss: 180.7637\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 165.1428 - val_loss: 162.1225\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 147.2479 - val_loss: 148.0417\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 132.6107 - val_loss: 130.4088\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 118.3920 - val_loss: 115.0724\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 104.6616 - val_loss: 104.9500\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 92.9237 - val_loss: 93.5925\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 82.3091 - val_loss: 81.5483\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 72.9757 - val_loss: 71.6025\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.7978 - val_loss: 61.8294\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 56.0445 - val_loss: 54.2643\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 49.6354 - val_loss: 48.1853\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 43.8301 - val_loss: 44.9233\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 38.3085 - val_loss: 36.8760\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.3432 - val_loss: 32.4187\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.8284 - val_loss: 27.9224\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.8120 - val_loss: 25.3177\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.5829 - val_loss: 23.6685\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 22.0242 - val_loss: 19.7684\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.6822 - val_loss: 18.2043\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.6954 - val_loss: 18.0416\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.5216 - val_loss: 15.0036\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.4465 - val_loss: 14.0041\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.4362 - val_loss: 12.6520\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.5098 - val_loss: 12.8215\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.2553 - val_loss: 11.7684\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.1856 - val_loss: 12.4560\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6211 - val_loss: 10.8264\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.5929 - val_loss: 10.8328\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 9.4081 - val_loss: 10.3889\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.5351 - val_loss: 10.0239\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0047 - val_loss: 8.9298\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 8.0317 - val_loss: 9.6312\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9465 - val_loss: 9.7202\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8156 - val_loss: 7.9716\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3875 - val_loss: 9.0289\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4271 - val_loss: 6.9372\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4927 - val_loss: 6.5854\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.7403 - val_loss: 7.4873\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.3257 - val_loss: 6.7150\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4285 - val_loss: 7.5117\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4110 - val_loss: 7.5159\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7194 - val_loss: 8.2457\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8336 - val_loss: 8.5890\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4659 - val_loss: 7.7244\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9653 - val_loss: 6.4232\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6372 - val_loss: 6.0411\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2305 - val_loss: 12.5283\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 6.1481 - val_loss: 7.6577\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.9140 - val_loss: 6.4016\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7126 - val_loss: 6.7631\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8165 - val_loss: 6.7530\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7774 - val_loss: 8.2225\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0924 - val_loss: 7.3945\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2166 - val_loss: 6.8168\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8666 - val_loss: 7.2687\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9491 - val_loss: 6.5627\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5130 - val_loss: 5.8975\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9966 - val_loss: 6.3629\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4909 - val_loss: 6.5830\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9276 - val_loss: 6.1709\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6548 - val_loss: 6.2673\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5080 - val_loss: 5.7146\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2286 - val_loss: 5.8027\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0143 - val_loss: 5.7557\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6644 - val_loss: 7.0858\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6962 - val_loss: 6.2942\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4347 - val_loss: 13.4608\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5325 - val_loss: 5.3489\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.8660 - val_loss: 5.4256\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.8141 - val_loss: 5.6649\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.6854 - val_loss: 6.3194\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9371 - val_loss: 6.4489\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5380 - val_loss: 6.2934\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5607 - val_loss: 5.3193\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.8044 - val_loss: 5.3912\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.6529 - val_loss: 8.2216\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0027 - val_loss: 6.1313\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6815 - val_loss: 5.9003\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7757 - val_loss: 13.7133\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5301 - val_loss: 4.7453\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2831 - val_loss: 5.9496\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8559 - val_loss: 6.0360\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2166 - val_loss: 5.5885\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2962 - val_loss: 5.8959\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4586 - val_loss: 5.2015\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1740 - val_loss: 8.3234\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9521 - val_loss: 6.3844\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9122 - val_loss: 6.3960\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.1876 - val_loss: 5.6412\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1111 - val_loss: 5.7409\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8771 - val_loss: 5.3567\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3741 - val_loss: 5.6438\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5580 - val_loss: 7.7259\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0361 - val_loss: 5.1524\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8954 - val_loss: 5.2539\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6376 - val_loss: 5.0325\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8778 - val_loss: 5.2323\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7023 - val_loss: 5.1950\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8264 - val_loss: 5.7098\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7252 - val_loss: 6.2972\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2146 - val_loss: 5.2810\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7944 - val_loss: 5.6363\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8744 - val_loss: 5.0971\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8983 - val_loss: 5.3668\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6714 - val_loss: 5.3328\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0781 - val_loss: 5.2679\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7044 - val_loss: 5.0196\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6229 - val_loss: 5.0986\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9038 - val_loss: 5.3937\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0364 - val_loss: 5.3936\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5149 - val_loss: 6.0444\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7657 - val_loss: 4.8953\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6577 - val_loss: 6.3334\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.9233 - val_loss: 8.0347\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0594 - val_loss: 5.2957\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6317 - val_loss: 5.7805\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3347 - val_loss: 5.1316\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2520 - val_loss: 4.6866\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2233 - val_loss: 6.7619\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3097 - val_loss: 5.0649\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4613 - val_loss: 8.7740\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9148 - val_loss: 5.9188\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1588 - val_loss: 5.3704\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1991 - val_loss: 5.0184\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5648 - val_loss: 7.1377\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.5325 - val_loss: 5.3828\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.4430 - val_loss: 5.1523\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2540 - val_loss: 5.8891\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.8689 - val_loss: 4.9576\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.8942 - val_loss: 5.1163\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1374 - val_loss: 5.3909\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8937 - val_loss: 5.2567\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9993 - val_loss: 5.3126\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8547 - val_loss: 4.8974\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9562 - val_loss: 5.7083\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9034 - val_loss: 5.9646\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9424 - val_loss: 6.3335\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2566 - val_loss: 5.9071\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8925 - val_loss: 5.1581\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4760 - val_loss: 5.4125\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2084 - val_loss: 5.4882\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6588 - val_loss: 5.1704\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7548 - val_loss: 5.4915\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9564 - val_loss: 5.5182\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6509 - val_loss: 5.5718\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6408 - val_loss: 5.1894\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.8778 - val_loss: 5.3300\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6479 - val_loss: 6.0309\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.0250 - val_loss: 5.4588\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 5.458808223295775\n",
      "Mean Absolute Error (MAE): 1.570499430582692\n",
      "Root Mean Squared Error (RMSE): 2.336409258519529\n",
      "Time taken: 1260.0751502513885\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1358.0076 - val_loss: 1276.9456\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1196.4326 - val_loss: 1173.6903\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1110.9496 - val_loss: 1100.0209\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1049.3108 - val_loss: 1045.4705\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1004.4958 - val_loss: 1005.8429\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 972.2317 - val_loss: 977.2983\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 949.8423 - val_loss: 957.4413\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 935.3359 - val_loss: 944.9407\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.4005 - val_loss: 937.1077\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.4312 - val_loss: 932.6500\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.9452 - val_loss: 930.3568\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.7786 - val_loss: 929.1531\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.3140 - val_loss: 928.6198\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.1356 - val_loss: 928.3729\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1042 - val_loss: 928.2422\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.0986 - val_loss: 928.1931\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.0952 - val_loss: 928.2283\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1224 - val_loss: 928.2332\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.0811 - val_loss: 928.1062\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1124 - val_loss: 928.0694\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1207 - val_loss: 928.0920\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1143 - val_loss: 928.1506\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.3078 - val_loss: 928.5093\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1164 - val_loss: 928.3639\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.0883 - val_loss: 928.2325\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 917.1098 - val_loss: 928.1942\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.0333 - val_loss: 928.1782\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 916.9772 - val_loss: 930.9411\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 895.9520 - val_loss: 885.7156\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 858.4719 - val_loss: 856.7747\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 832.9739 - val_loss: 829.9315\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 804.6419 - val_loss: 792.0665\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 760.9586 - val_loss: 748.2120\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 722.9020 - val_loss: 707.3329\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 678.5276 - val_loss: 666.4331\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 636.2365 - val_loss: 622.8339\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 590.7805 - val_loss: 575.1158\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 544.3797 - val_loss: 529.5988\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 500.6607 - val_loss: 486.5704\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 456.1688 - val_loss: 444.7651\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 415.4406 - val_loss: 404.2916\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 379.1613 - val_loss: 371.8858\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 348.7760 - val_loss: 340.0806\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 317.6333 - val_loss: 307.9196\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 289.1108 - val_loss: 281.5521\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 263.0903 - val_loss: 255.9850\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 240.2523 - val_loss: 231.5746\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 217.8708 - val_loss: 211.4267\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 199.3477 - val_loss: 194.8991\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 179.0672 - val_loss: 172.7429\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 160.3815 - val_loss: 156.3899\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 145.6391 - val_loss: 140.1396\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 130.5806 - val_loss: 126.3703\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 116.6845 - val_loss: 114.9094\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 105.2166 - val_loss: 101.5029\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 93.6537 - val_loss: 94.2866\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 84.7237 - val_loss: 80.1100\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.9275 - val_loss: 72.3451\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 67.5140 - val_loss: 66.4763\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 60.7983 - val_loss: 61.5055\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 54.5226 - val_loss: 54.4938\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 48.4753 - val_loss: 47.8287\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.9538 - val_loss: 42.7678\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.5154 - val_loss: 46.7022\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.1094 - val_loss: 36.8481\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.2171 - val_loss: 33.0345\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.1532 - val_loss: 31.8114\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.0345 - val_loss: 27.8112\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.7604 - val_loss: 23.7339\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.4281 - val_loss: 23.2548\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.0057 - val_loss: 21.0881\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.6026 - val_loss: 20.3371\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.2467 - val_loss: 18.9218\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 16.6561 - val_loss: 18.1200\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.0701 - val_loss: 16.9838\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.8624 - val_loss: 16.6333\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.7099 - val_loss: 20.8675\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 13.1135 - val_loss: 13.7501\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.5918 - val_loss: 14.7145\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.4079 - val_loss: 14.0600\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.9967 - val_loss: 13.8660\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.8413 - val_loss: 14.4086\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.4445 - val_loss: 12.5407\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.1886 - val_loss: 12.1020\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.5787 - val_loss: 15.3771\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9737 - val_loss: 13.0356\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5552 - val_loss: 13.0841\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6660 - val_loss: 11.3377\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7409 - val_loss: 12.4521\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9821 - val_loss: 10.3680\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4674 - val_loss: 9.8708\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.2096 - val_loss: 10.1317\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4656 - val_loss: 9.9309\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.6501 - val_loss: 13.3556\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5712 - val_loss: 12.3012\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.8922 - val_loss: 9.6266\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 7.4789 - val_loss: 8.3810\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.7636 - val_loss: 8.5005\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6022 - val_loss: 8.2453\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9420 - val_loss: 8.3394\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3789 - val_loss: 8.3422\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6120 - val_loss: 8.8136\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0781 - val_loss: 8.9737\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7515 - val_loss: 8.4482\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2696 - val_loss: 7.9767\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5218 - val_loss: 8.2456\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.9580 - val_loss: 7.4438\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.7396 - val_loss: 9.1922\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7504 - val_loss: 7.6468\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5770 - val_loss: 9.7218\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4890 - val_loss: 6.9048\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2242 - val_loss: 7.4294\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2871 - val_loss: 8.1226\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2412 - val_loss: 6.4745\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 7.5017 - val_loss: 7.4134\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8328 - val_loss: 7.4936\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4643 - val_loss: 7.6183\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5266 - val_loss: 6.8010\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4772 - val_loss: 6.8929\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4785 - val_loss: 6.6208\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3839 - val_loss: 6.2355\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5992 - val_loss: 6.6041\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3175 - val_loss: 6.9098\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7301 - val_loss: 6.2106\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.2111 - val_loss: 6.6151\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4188 - val_loss: 6.3783\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2794 - val_loss: 8.2003\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1315 - val_loss: 6.6898\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9244 - val_loss: 6.4890\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0845 - val_loss: 7.8173\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2728 - val_loss: 8.7647\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.3115 - val_loss: 7.1912\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9364 - val_loss: 7.0961\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3974 - val_loss: 6.4166\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.6784 - val_loss: 5.9072\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6005 - val_loss: 6.7280\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6211 - val_loss: 5.6615\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5773 - val_loss: 6.7541\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3836 - val_loss: 5.6815\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7983 - val_loss: 6.1722\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6481 - val_loss: 6.7831\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6390 - val_loss: 6.2480\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8743 - val_loss: 10.4451\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9146 - val_loss: 7.6710\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2083 - val_loss: 5.6701\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0114 - val_loss: 5.7058\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9149 - val_loss: 5.3743\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2442 - val_loss: 6.6736\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3607 - val_loss: 6.7458\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0407 - val_loss: 6.3989\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0633 - val_loss: 6.3082\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0956 - val_loss: 6.3638\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0452 - val_loss: 5.9654\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4146 - val_loss: 6.1161\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6469 - val_loss: 10.8085\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.5694 - val_loss: 6.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7683 - val_loss: 6.7099\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7275 - val_loss: 6.1362\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7368 - val_loss: 6.8397\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1668 - val_loss: 6.3126\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7801 - val_loss: 5.8782\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8655 - val_loss: 6.0475\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4496 - val_loss: 5.4656\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8029 - val_loss: 5.7290\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4160 - val_loss: 5.5801\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8725 - val_loss: 5.9410\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3028 - val_loss: 8.2619\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7453 - val_loss: 6.8359\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3173 - val_loss: 6.4909\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3730 - val_loss: 5.8170\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8701 - val_loss: 6.6405\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1356 - val_loss: 5.3607\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3612 - val_loss: 5.5911\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1295 - val_loss: 5.3599\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3358 - val_loss: 6.2032\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3645 - val_loss: 7.7373\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4706 - val_loss: 5.5904\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1559 - val_loss: 6.7628\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3620 - val_loss: 6.7582\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3585 - val_loss: 6.3044\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0751 - val_loss: 5.5540\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7612 - val_loss: 5.7585\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7723 - val_loss: 5.6817\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6976 - val_loss: 5.2898\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8815 - val_loss: 5.3431\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1729 - val_loss: 6.8010\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0823 - val_loss: 6.1440\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0907 - val_loss: 7.5069\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0629 - val_loss: 6.6160\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9086 - val_loss: 6.7133\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9575 - val_loss: 5.8593\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 2.2978 - val_loss: 5.9928\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9698 - val_loss: 5.6881\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.1360 - val_loss: 9.2429\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2678 - val_loss: 5.8014\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4993 - val_loss: 6.9579\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6046 - val_loss: 6.0113\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5346 - val_loss: 5.9431\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6888 - val_loss: 5.2951\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7883 - val_loss: 9.6205\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 9.620460729197449\n",
      "Mean Absolute Error (MAE): 2.0731687644950094\n",
      "Root Mean Squared Error (RMSE): 3.101686755492477\n",
      "Time taken: 1237.052337884903\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1405.5044 - val_loss: 1242.4393\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1248.2927 - val_loss: 1142.3206\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1157.4807 - val_loss: 1068.4919\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1089.3207 - val_loss: 1012.1583\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1037.0879 - val_loss: 970.3586\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 998.9019 - val_loss: 940.7317\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 971.6658 - val_loss: 920.1050\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 952.8474 - val_loss: 906.7488\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 940.7327 - val_loss: 898.7943\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 933.4671 - val_loss: 894.6829\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 929.4650 - val_loss: 892.6970\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 927.4546 - val_loss: 891.9960\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.5956 - val_loss: 891.9114\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.2430 - val_loss: 891.9959\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1054 - val_loss: 892.1815\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1336 - val_loss: 892.1840\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1089 - val_loss: 892.1183\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.0975 - val_loss: 892.1790\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.1271 - val_loss: 892.2366\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.0815 - val_loss: 892.3192\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1035 - val_loss: 892.2425\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.5419 - val_loss: 892.0152\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 926.2819 - val_loss: 892.0859\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1331 - val_loss: 892.1476\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1415 - val_loss: 892.1680\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.1255 - val_loss: 891.8103\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 925.9351 - val_loss: 885.6418\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.3234 - val_loss: 890.7567\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 926.3007 - val_loss: 891.6398\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 925.8152 - val_loss: 886.4024\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 927.2156 - val_loss: 883.7292\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.8018 - val_loss: 888.6276\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 896.9957 - val_loss: 816.5566\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 825.7545 - val_loss: 771.7385\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 776.7462 - val_loss: 707.0507\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 698.5316 - val_loss: 630.6720\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 625.8620 - val_loss: 572.9678\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 575.7218 - val_loss: 527.4231\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 529.8047 - val_loss: 486.3067\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 487.1595 - val_loss: 446.6109\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 447.0643 - val_loss: 410.1155\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 409.6600 - val_loss: 377.6125\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 373.9240 - val_loss: 346.4109\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 342.0196 - val_loss: 315.3556\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 311.8249 - val_loss: 290.5912\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 282.7675 - val_loss: 260.9457\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 256.2103 - val_loss: 234.3454\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 231.7863 - val_loss: 219.7956\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 209.0690 - val_loss: 195.3873\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 190.8048 - val_loss: 177.6157\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 168.5310 - val_loss: 155.7764\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 151.9432 - val_loss: 141.1628\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 135.7218 - val_loss: 129.2091\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 121.3266 - val_loss: 112.5306\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 107.4904 - val_loss: 99.9459\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 94.8444 - val_loss: 93.7808\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 84.4378 - val_loss: 83.0473\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.3706 - val_loss: 70.3544\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.9962 - val_loss: 60.8283\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.4531 - val_loss: 54.2702\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 50.1879 - val_loss: 48.8648\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.6679 - val_loss: 43.3080\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.1423 - val_loss: 37.3517\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.5937 - val_loss: 40.0927\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.3778 - val_loss: 30.0821\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.8217 - val_loss: 26.6632\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.9685 - val_loss: 27.6672\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 21.8187 - val_loss: 21.6765\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.6830 - val_loss: 22.1837\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 17.6646 - val_loss: 18.7704\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.5778 - val_loss: 18.6545\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.3295 - val_loss: 16.2721\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 13.9614 - val_loss: 15.4504\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.6461 - val_loss: 13.2607\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.1704 - val_loss: 16.5234\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.0889 - val_loss: 12.4386\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.1767 - val_loss: 12.5566\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9810 - val_loss: 12.5236\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.3451 - val_loss: 10.8734\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2571 - val_loss: 11.0871\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.5885 - val_loss: 10.1581\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.2203 - val_loss: 10.0295\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.4379 - val_loss: 10.7308\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8617 - val_loss: 9.0575\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3611 - val_loss: 10.3582\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.9366 - val_loss: 8.1513\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3613 - val_loss: 10.3541\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 6.3571 - val_loss: 8.3767\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 5.6120 - val_loss: 7.9031\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2764 - val_loss: 8.6390\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5131 - val_loss: 8.2892\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2603 - val_loss: 7.7237\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0079 - val_loss: 8.0019\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.8628 - val_loss: 7.6615\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8927 - val_loss: 8.3971\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.7647 - val_loss: 7.6887\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 6.6923 - val_loss: 10.4346\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0311 - val_loss: 6.9033\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5381 - val_loss: 7.4419\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3809 - val_loss: 7.7207\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4880 - val_loss: 15.9238\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3767 - val_loss: 7.1878\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0741 - val_loss: 7.3852\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3771 - val_loss: 7.5261\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3625 - val_loss: 7.0642\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6969 - val_loss: 7.4559\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.6855 - val_loss: 7.1740\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9524 - val_loss: 7.1951\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6626 - val_loss: 7.2128\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6074 - val_loss: 6.4379\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5271 - val_loss: 6.7805\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 3.7974 - val_loss: 6.9946\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.4058 - val_loss: 7.7487\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6118 - val_loss: 6.5836\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0664 - val_loss: 6.1965\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1997 - val_loss: 6.1419\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2217 - val_loss: 6.4641\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2435 - val_loss: 7.4753\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3120 - val_loss: 6.8802\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4818 - val_loss: 6.8202\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2731 - val_loss: 6.4589\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3757 - val_loss: 7.1386\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4458 - val_loss: 6.4186\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5335 - val_loss: 7.6948\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8507 - val_loss: 6.5627\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0148 - val_loss: 6.8568\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8457 - val_loss: 6.7379\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 4.9974 - val_loss: 6.4351\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6571 - val_loss: 6.0055\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.2812 - val_loss: 5.5776\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1785 - val_loss: 5.8356\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2810 - val_loss: 5.8487\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3698 - val_loss: 5.9501\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5959 - val_loss: 6.2203\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0678 - val_loss: 5.9150\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5199 - val_loss: 6.1858\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2478 - val_loss: 6.3688\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.8970 - val_loss: 7.3991\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4015 - val_loss: 5.9846\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9348 - val_loss: 6.1166\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9642 - val_loss: 6.0654\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0610 - val_loss: 9.6167\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2850 - val_loss: 6.4042\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1316 - val_loss: 6.9902\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8881 - val_loss: 6.0320\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7827 - val_loss: 6.5370\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7853 - val_loss: 5.7861\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.9125 - val_loss: 6.7200\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7802 - val_loss: 7.1873\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2435 - val_loss: 6.1814\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9918 - val_loss: 7.1185\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1543 - val_loss: 6.2525\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1803 - val_loss: 6.5809\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1824 - val_loss: 6.5125\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7861 - val_loss: 6.4351\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8413 - val_loss: 6.3962\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8597 - val_loss: 6.0794\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8106 - val_loss: 6.1948\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8869 - val_loss: 5.9544\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5065 - val_loss: 7.1603\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8373 - val_loss: 7.2824\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.6468 - val_loss: 6.0831\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2807 - val_loss: 6.4507\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8160 - val_loss: 5.7527\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9628 - val_loss: 9.5382\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6867 - val_loss: 5.8164\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2742 - val_loss: 5.9103\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2166 - val_loss: 5.6902\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3999 - val_loss: 6.5442\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3638 - val_loss: 5.5466\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2651 - val_loss: 5.9853\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2143 - val_loss: 5.7030\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2247 - val_loss: 6.4820\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4149 - val_loss: 6.5413\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2869 - val_loss: 6.5795\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7653 - val_loss: 6.7846\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4624 - val_loss: 5.9884\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2750 - val_loss: 5.9668\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4917 - val_loss: 6.6260\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0283 - val_loss: 5.8316\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8934 - val_loss: 6.5701\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2528 - val_loss: 6.0093\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1413 - val_loss: 5.8481\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.0053 - val_loss: 6.0171\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2666 - val_loss: 6.1741\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1928 - val_loss: 6.0341\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.1044 - val_loss: 6.2756\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.0550 - val_loss: 6.0950\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2340 - val_loss: 5.9236\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0254 - val_loss: 5.7145\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.2260 - val_loss: 6.0704\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9387 - val_loss: 6.5015\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2103 - val_loss: 5.4913\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8314 - val_loss: 6.1760\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.9134 - val_loss: 6.1286\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8283 - val_loss: 5.8391\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.8605 - val_loss: 6.3193\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.8306 - val_loss: 5.9206\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9578 - val_loss: 10.7244\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0901 - val_loss: 6.3116\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 6.311615066036061\n",
      "Mean Absolute Error (MAE): 1.6898851355621505\n",
      "Root Mean Squared Error (RMSE): 2.512292790666737\n",
      "Time taken: 1229.6423280239105\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1403.9456 - val_loss: 1290.9456\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1241.5060 - val_loss: 1184.7649\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1151.0024 - val_loss: 1105.9165\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1083.1853 - val_loss: 1046.6304\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1032.0933 - val_loss: 1002.2562\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 994.2233 - val_loss: 969.8002\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 966.9748 - val_loss: 946.9741\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 948.3040 - val_loss: 931.8705\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 936.1844 - val_loss: 922.3355\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 928.8087 - val_loss: 916.7966\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 924.7212 - val_loss: 913.8741\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 922.6597 - val_loss: 912.5093\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.7757 - val_loss: 911.9840\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.4024 - val_loss: 911.7332\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2972 - val_loss: 911.6630\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 921.2464 - val_loss: 911.6364\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2213 - val_loss: 911.6209\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 921.2224 - val_loss: 911.5995\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2000 - val_loss: 911.6430\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2231 - val_loss: 911.7264\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2367 - val_loss: 911.6750\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2388 - val_loss: 911.7067\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2390 - val_loss: 911.6574\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2110 - val_loss: 911.6551\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2266 - val_loss: 911.6578\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2585 - val_loss: 911.6631\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.2254 - val_loss: 911.7143\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.9963 - val_loss: 912.6189\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.3959 - val_loss: 911.9702\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 921.2541 - val_loss: 911.7561\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.6285 - val_loss: 905.9603\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 900.8154 - val_loss: 876.4982\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 879.8602 - val_loss: 854.1586\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 849.4695 - val_loss: 819.1159\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 823.6642 - val_loss: 822.0640\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 812.1879 - val_loss: 783.9310\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 779.9703 - val_loss: 752.5382\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 759.8315 - val_loss: 729.0485\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 735.5145 - val_loss: 707.7977\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 702.3234 - val_loss: 673.0316\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 662.5581 - val_loss: 631.3969\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 620.8163 - val_loss: 606.2182\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 576.5660 - val_loss: 539.1749\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 527.8395 - val_loss: 497.6010\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 491.2722 - val_loss: 460.3084\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 449.3625 - val_loss: 416.7296\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 420.7738 - val_loss: 385.3452\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 378.4192 - val_loss: 352.3293\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 345.2352 - val_loss: 321.4420\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 317.7591 - val_loss: 291.9474\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 287.4133 - val_loss: 263.9432\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 261.5249 - val_loss: 243.0367\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 245.5587 - val_loss: 233.7772\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 220.4277 - val_loss: 199.1453\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 197.1258 - val_loss: 183.0478\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 179.0747 - val_loss: 158.7834\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 163.0667 - val_loss: 143.6111\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 145.0511 - val_loss: 130.9633\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 131.1904 - val_loss: 115.2976\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 117.5870 - val_loss: 107.1367\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 105.2624 - val_loss: 92.3870\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 96.0616 - val_loss: 83.1003\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 87.0448 - val_loss: 80.7223\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 76.6099 - val_loss: 69.6087\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 69.5895 - val_loss: 58.1653\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 61.0640 - val_loss: 52.6677\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 55.0033 - val_loss: 49.4457\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.3353 - val_loss: 66.9864\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.3792 - val_loss: 46.5060\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.0447 - val_loss: 39.6662\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.0707 - val_loss: 35.4494\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.5184 - val_loss: 29.1979\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.3525 - val_loss: 32.7785\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.0573 - val_loss: 26.8996\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 28.0949 - val_loss: 27.2778\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 25.9539 - val_loss: 25.3307\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.2768 - val_loss: 25.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 22.6659 - val_loss: 20.6827\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.0982 - val_loss: 18.6888\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.0082 - val_loss: 18.2392\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.7074 - val_loss: 16.6114\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.3174 - val_loss: 17.4865\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3189 - val_loss: 15.8503\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.2297 - val_loss: 17.2805\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7585 - val_loss: 15.6338\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.8112 - val_loss: 18.7096\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.9758 - val_loss: 13.2654\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.6444 - val_loss: 14.9006\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.8896 - val_loss: 12.4153\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.2300 - val_loss: 19.6881\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1516 - val_loss: 12.6327\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.3283 - val_loss: 14.1620\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.8494 - val_loss: 14.7058\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.2752 - val_loss: 11.7674\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 11.6221 - val_loss: 12.3413\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.5072 - val_loss: 11.4355\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.3335 - val_loss: 10.8848\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.2964 - val_loss: 10.5349\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.6103 - val_loss: 12.6207\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5805 - val_loss: 11.8423\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.6441 - val_loss: 17.4386\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5702 - val_loss: 10.7719\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7215 - val_loss: 14.3182\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.5449 - val_loss: 11.5673\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7978 - val_loss: 9.2945\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.0606 - val_loss: 10.7195\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4164 - val_loss: 9.2414\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.4815 - val_loss: 9.2637\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.8906 - val_loss: 10.2678\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.6074 - val_loss: 9.0242\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8756 - val_loss: 8.9705\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.3016 - val_loss: 10.6336\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.5166 - val_loss: 10.4697\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.5947 - val_loss: 8.6789\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0535 - val_loss: 8.6459\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0334 - val_loss: 8.1810\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8668 - val_loss: 8.9719\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8660 - val_loss: 8.1906\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.3915 - val_loss: 8.8793\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.0479 - val_loss: 8.0670\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.7387 - val_loss: 8.3686\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.8313 - val_loss: 7.7008\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4047 - val_loss: 7.4087\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2853 - val_loss: 9.6245\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.4970 - val_loss: 9.0996\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.5907 - val_loss: 10.8502\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9284 - val_loss: 11.5942\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4430 - val_loss: 8.1157\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8849 - val_loss: 8.6151\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9510 - val_loss: 8.5299\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5805 - val_loss: 8.5419\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4214 - val_loss: 8.5890\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5228 - val_loss: 7.1821\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0347 - val_loss: 8.0339\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4060 - val_loss: 9.4520\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.6060 - val_loss: 7.5421\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 5.5415 - val_loss: 8.7380\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.9965 - val_loss: 7.1385\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3770 - val_loss: 10.5853\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2546 - val_loss: 6.6976\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1270 - val_loss: 9.2908\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0632 - val_loss: 6.7130\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0209 - val_loss: 7.8856\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9725 - val_loss: 6.6225\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9031 - val_loss: 6.5687\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3461 - val_loss: 10.3641\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.2181 - val_loss: 6.6002\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.6112 - val_loss: 6.5530\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.4838 - val_loss: 5.9971\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.1731 - val_loss: 8.3150\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.6976 - val_loss: 7.8278\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3013 - val_loss: 8.6500\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4539 - val_loss: 7.1864\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1213 - val_loss: 6.3276\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4994 - val_loss: 7.0776\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5411 - val_loss: 8.6502\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2957 - val_loss: 7.1575\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.4477 - val_loss: 6.2906\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 4.1294 - val_loss: 6.2186\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.1826 - val_loss: 7.7152\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0613 - val_loss: 6.2413\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3929 - val_loss: 8.8052\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9406 - val_loss: 7.7070\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0479 - val_loss: 6.1145\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0479 - val_loss: 5.3045\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8768 - val_loss: 8.2325\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.4550 - val_loss: 6.9564\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4956 - val_loss: 6.0156\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4696 - val_loss: 5.5575\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3436 - val_loss: 6.0221\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4810 - val_loss: 6.1583\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6194 - val_loss: 6.0127\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3865 - val_loss: 6.8610\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6275 - val_loss: 7.0213\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4492 - val_loss: 6.0165\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3448 - val_loss: 5.9899\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5043 - val_loss: 6.2905\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9595 - val_loss: 7.1084\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4358 - val_loss: 6.4400\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1249 - val_loss: 6.3422\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2465 - val_loss: 6.5730\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2310 - val_loss: 6.0086\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1321 - val_loss: 6.1345\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9245 - val_loss: 6.2667\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.1401 - val_loss: 6.4655\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1158 - val_loss: 6.4892\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4317 - val_loss: 6.5390\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8895 - val_loss: 5.5613\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.7332 - val_loss: 6.8079\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3425 - val_loss: 6.1300\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8312 - val_loss: 4.9506\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9472 - val_loss: 6.4926\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7860 - val_loss: 6.7616\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6333 - val_loss: 6.8328\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7968 - val_loss: 5.9048\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6404 - val_loss: 5.4948\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7722 - val_loss: 6.1792\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5671 - val_loss: 6.0439\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.9023 - val_loss: 6.0879\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4218 - val_loss: 6.1956\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 6.195483621164358\n",
      "Mean Absolute Error (MAE): 1.6815737298604747\n",
      "Root Mean Squared Error (RMSE): 2.48907284368384\n",
      "Time taken: 1187.1779763698578\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 18ms/step - loss: 1387.4172 - val_loss: 1297.9346\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1223.0923 - val_loss: 1194.6936\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1135.1219 - val_loss: 1118.4871\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1068.5446 - val_loss: 1060.4209\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1018.7100 - val_loss: 1017.9047\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 982.4276 - val_loss: 987.1097\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 956.6348 - val_loss: 965.8223\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 939.3141 - val_loss: 952.2750\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 928.3104 - val_loss: 943.8018\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 921.7214 - val_loss: 939.0051\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.2026 - val_loss: 936.6641\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 916.5435 - val_loss: 935.6038\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.8154 - val_loss: 935.2407\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.5448 - val_loss: 935.1722\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.4261 - val_loss: 935.1628\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3812 - val_loss: 935.0843\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.4329 - val_loss: 935.1083\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.4063 - val_loss: 935.1039\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3904 - val_loss: 935.0350\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3781 - val_loss: 935.0480\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 915.3868 - val_loss: 935.0209\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 908.0552 - val_loss: 903.0789\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 868.4165 - val_loss: 878.3894\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 847.3337 - val_loss: 861.8392\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 831.1833 - val_loss: 849.0146\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 815.5203 - val_loss: 828.3896\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 794.7097 - val_loss: 793.2017\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 761.9513 - val_loss: 769.0197\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 741.2767 - val_loss: 752.7149\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 724.5309 - val_loss: 736.4734\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 708.4421 - val_loss: 723.2512\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 684.1640 - val_loss: 681.1632\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 636.0027 - val_loss: 619.8026\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 580.5988 - val_loss: 569.4083\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 534.7448 - val_loss: 522.8962\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 491.6826 - val_loss: 478.6924\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 450.6251 - val_loss: 437.8605\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 412.8685 - val_loss: 404.3526\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 376.5657 - val_loss: 366.5161\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 342.8083 - val_loss: 337.0513\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 313.9890 - val_loss: 309.0858\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 287.0628 - val_loss: 276.5863\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 260.0170 - val_loss: 252.5469\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 236.5599 - val_loss: 227.8004\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 214.1560 - val_loss: 208.3937\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 194.1293 - val_loss: 187.7068\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 175.7182 - val_loss: 168.3137\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 157.0238 - val_loss: 152.6444\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 139.9769 - val_loss: 136.8959\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 125.0339 - val_loss: 128.9039\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 110.8567 - val_loss: 108.7820\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 98.3244 - val_loss: 95.6179\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 87.7985 - val_loss: 84.5981\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 77.8431 - val_loss: 77.3550\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 69.8344 - val_loss: 68.5716\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 61.8453 - val_loss: 70.3761\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 53.8442 - val_loss: 54.2126\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 46.7342 - val_loss: 46.5993\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 42.1576 - val_loss: 42.6155\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.6775 - val_loss: 38.3624\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.1273 - val_loss: 34.7494\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.5745 - val_loss: 30.3641\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 26.6455 - val_loss: 27.9462\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.0248 - val_loss: 27.9287\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 21.3695 - val_loss: 23.2986\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.8151 - val_loss: 22.0060\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.6530 - val_loss: 19.3038\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.7144 - val_loss: 19.1376\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.8451 - val_loss: 19.5680\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 14.5449 - val_loss: 17.0052\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.9673 - val_loss: 15.0885\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.4534 - val_loss: 14.8287\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1851 - val_loss: 14.3390\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.9093 - val_loss: 12.7139\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.6212 - val_loss: 12.6304\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.7894 - val_loss: 11.2512\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2100 - val_loss: 11.1206\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 9.2905 - val_loss: 10.5555\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.6729 - val_loss: 9.9568\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.4747 - val_loss: 12.6812\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.8989 - val_loss: 10.7488\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8726 - val_loss: 9.8861\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 8.0032 - val_loss: 14.9706\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2342 - val_loss: 10.5235\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.1807 - val_loss: 9.5122\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8844 - val_loss: 8.5638\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2481 - val_loss: 8.6889\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9284 - val_loss: 8.4462\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.3526 - val_loss: 8.5903\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.6223 - val_loss: 9.1359\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.9542 - val_loss: 9.0130\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0739 - val_loss: 9.4836\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.3090 - val_loss: 10.4206\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2304 - val_loss: 7.5253\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.0033 - val_loss: 12.1267\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8119 - val_loss: 8.1525\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6420 - val_loss: 7.2069\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.8537 - val_loss: 8.1413\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.5177 - val_loss: 7.4331\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9514 - val_loss: 7.5150\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0656 - val_loss: 12.7635\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.5930 - val_loss: 7.5226\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9563 - val_loss: 7.8237\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3733 - val_loss: 6.9035\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.6697 - val_loss: 6.7495\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8156 - val_loss: 8.1513\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0272 - val_loss: 7.3487\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3892 - val_loss: 7.5935\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.8960 - val_loss: 7.0989\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.0333 - val_loss: 7.0308\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7718 - val_loss: 7.0983\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4218 - val_loss: 7.5075\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5119 - val_loss: 7.8990\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5513 - val_loss: 7.1261\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3782 - val_loss: 6.0688\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3885 - val_loss: 6.7003\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2149 - val_loss: 6.4506\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3145 - val_loss: 6.9392\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4713 - val_loss: 6.4095\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4965 - val_loss: 6.3876\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7299 - val_loss: 6.2115\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.9489 - val_loss: 5.8840\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4429 - val_loss: 11.9244\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.4952 - val_loss: 7.1251\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9319 - val_loss: 6.0409\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9382 - val_loss: 6.9442\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.0741 - val_loss: 6.3959\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.5091 - val_loss: 8.5649\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7667 - val_loss: 7.3848\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4933 - val_loss: 7.3038\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.5153 - val_loss: 7.5908\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.7189 - val_loss: 7.4509\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9608 - val_loss: 5.6169\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.9402 - val_loss: 9.0303\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5702 - val_loss: 6.2979\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.2327 - val_loss: 5.5283\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3701 - val_loss: 6.8072\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9881 - val_loss: 6.6603\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0669 - val_loss: 5.4432\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0966 - val_loss: 7.1717\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5491 - val_loss: 6.7096\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.8707 - val_loss: 6.1047\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0053 - val_loss: 6.1107\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 3.7415 - val_loss: 6.7437\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 2.9682 - val_loss: 5.6601\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0785 - val_loss: 6.5465\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1932 - val_loss: 6.4380\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0647 - val_loss: 6.3623\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.8631 - val_loss: 6.9192\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9900 - val_loss: 7.1458\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3843 - val_loss: 6.0257\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0454 - val_loss: 6.3115\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8282 - val_loss: 5.6600\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7663 - val_loss: 5.9168\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0415 - val_loss: 6.4131\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0335 - val_loss: 6.6348\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 2.6000 - val_loss: 5.6149\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8968 - val_loss: 5.7317\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7881 - val_loss: 5.9883\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7883 - val_loss: 5.5648\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5983 - val_loss: 5.9675\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2112 - val_loss: 8.5958\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7093 - val_loss: 5.7917\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4333 - val_loss: 6.3032\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3870 - val_loss: 5.5739\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5215 - val_loss: 5.4009\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2846 - val_loss: 5.5136\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4305 - val_loss: 5.6462\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.3185 - val_loss: 6.3642\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6660 - val_loss: 5.8957\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2653 - val_loss: 5.2806\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4223 - val_loss: 5.9391\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2128 - val_loss: 5.7052\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2933 - val_loss: 5.8428\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4820 - val_loss: 5.8972\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1682 - val_loss: 5.1145\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2454 - val_loss: 5.6131\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5770 - val_loss: 6.6022\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2391 - val_loss: 6.4830\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3526 - val_loss: 5.8675\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2128 - val_loss: 5.8788\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2512 - val_loss: 5.3063\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2801 - val_loss: 5.4882\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9307 - val_loss: 6.9171\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8796 - val_loss: 5.0505\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4903 - val_loss: 5.1627\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1239 - val_loss: 5.6425\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6351 - val_loss: 9.0336\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9587 - val_loss: 5.6267\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6390 - val_loss: 5.0233\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8920 - val_loss: 5.3321\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7983 - val_loss: 5.7075\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8962 - val_loss: 5.3839\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8659 - val_loss: 4.9801\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8532 - val_loss: 5.5817\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6753 - val_loss: 5.9164\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9681 - val_loss: 5.4192\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7178 - val_loss: 5.9088\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9321 - val_loss: 7.7262\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8900 - val_loss: 6.1302\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 6.130233804751605\n",
      "Mean Absolute Error (MAE): 1.6269318013821996\n",
      "Root Mean Squared Error (RMSE): 2.4759308966026508\n",
      "Time taken: 1193.47589635849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_16116\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  5.458808  1.570499  2.336409  1260.075150\n",
      "1        2  9.620461  2.073169  3.101687  1237.052338\n",
      "2        3  6.311615  1.689885  2.512293  1229.642328\n",
      "3        4  6.195484  1.681574  2.489073  1187.177976\n",
      "4        5  6.130234  1.626932  2.475931  1193.475896\n",
      "5  Average  6.743320  1.728412  2.583079  1221.484738\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
