{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 33ms/step - loss: 1365.3563 - val_loss: 1242.7837\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1203.6484 - val_loss: 1148.5385\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1118.4506 - val_loss: 1080.0055\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1055.7383 - val_loss: 1029.6089\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 1009.3607 - val_loss: 993.0695\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 975.8963 - val_loss: 967.5344\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 952.5790 - val_loss: 950.4421\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 937.0168 - val_loss: 939.7482\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 927.3164 - val_loss: 933.7030\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 921.7200 - val_loss: 930.7725\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 918.8745 - val_loss: 929.5711\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 917.5189 - val_loss: 929.2599\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 917.0171 - val_loss: 929.2805\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 916.8609 - val_loss: 929.3769\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 916.7822 - val_loss: 929.5064\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 916.7756 - val_loss: 929.5967\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 916.7535 - val_loss: 929.6760\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 916.7421 - val_loss: 929.6104\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 916.7929 - val_loss: 929.7203\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 916.7969 - val_loss: 929.6154\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 916.7227 - val_loss: 929.5644\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 916.7720 - val_loss: 929.6140\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 916.7604 - val_loss: 929.6741\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 916.7791 - val_loss: 929.7402\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 916.7938 - val_loss: 929.7167\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 916.7598 - val_loss: 929.6315\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 908.7062 - val_loss: 886.8543\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 869.0501 - val_loss: 866.6707\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 850.3423 - val_loss: 843.3708\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 825.4021 - val_loss: 818.0522\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 793.7278 - val_loss: 779.2737\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 752.3410 - val_loss: 736.6402\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 708.7657 - val_loss: 688.9853\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 660.1586 - val_loss: 638.3780\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 610.1906 - val_loss: 587.8931\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 560.7342 - val_loss: 542.0255\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 514.7324 - val_loss: 507.4284\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 474.4225 - val_loss: 458.8428\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 435.2216 - val_loss: 427.9199\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 400.2632 - val_loss: 387.7892\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 365.0221 - val_loss: 359.2425\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 335.0190 - val_loss: 323.1669\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 304.2629 - val_loss: 296.2658\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 275.0391 - val_loss: 271.0010\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 250.2519 - val_loss: 240.9294\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 225.3722 - val_loss: 218.2408\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 202.2272 - val_loss: 200.7491\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 181.8962 - val_loss: 175.3987\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 160.9576 - val_loss: 154.6461\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 143.4719 - val_loss: 148.3659\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 127.0859 - val_loss: 122.0944\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 112.7870 - val_loss: 110.5399\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 100.0039 - val_loss: 94.6670\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 88.0892 - val_loss: 83.7572\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 77.8522 - val_loss: 78.0096\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 68.8918 - val_loss: 66.3723\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 60.2849 - val_loss: 60.2953\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 54.0590 - val_loss: 56.6693\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 47.2540 - val_loss: 44.6347\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 42.8979 - val_loss: 39.9634\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 37.1495 - val_loss: 35.3351\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 33.7783 - val_loss: 35.0649\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 29.8649 - val_loss: 28.1946\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 26.2517 - val_loss: 28.5455\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 23.4862 - val_loss: 24.9497\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 21.0746 - val_loss: 21.5788\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 19.8118 - val_loss: 19.8619\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 17.8265 - val_loss: 21.3955\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 16.4937 - val_loss: 16.2629\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 14.6544 - val_loss: 16.6217\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 13.8949 - val_loss: 15.7919\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 13.4540 - val_loss: 14.8115\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 12.3874 - val_loss: 13.1612\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 11.3821 - val_loss: 12.2433\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 11.2938 - val_loss: 12.4233\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 10.5955 - val_loss: 11.2120\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 27ms/step - loss: 9.5366 - val_loss: 11.3619\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 9.5521 - val_loss: 13.3532\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 9.2405 - val_loss: 9.6228\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 8.4464 - val_loss: 10.9666\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 9.1937 - val_loss: 9.7273\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 7.3427 - val_loss: 10.7703\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 7.6378 - val_loss: 8.5730\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 7.9969 - val_loss: 9.9318\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 7.8104 - val_loss: 8.4264\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 7.3790 - val_loss: 8.8922\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 7.5238 - val_loss: 7.6578\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 6.5079 - val_loss: 8.4454\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 6.3444 - val_loss: 9.3387\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 6.4877 - val_loss: 9.6713\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 6.0569 - val_loss: 8.6430\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 6.1021 - val_loss: 9.0016\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 5.5456 - val_loss: 7.6666\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 5.5825 - val_loss: 7.9655\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 6.3217 - val_loss: 6.9818\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 5.4769 - val_loss: 8.6530\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 5.2438 - val_loss: 8.3639\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 5.8592 - val_loss: 13.1952\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 5.2056 - val_loss: 7.4845\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 6.3330 - val_loss: 9.6208\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 4.7386 - val_loss: 7.0385\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 4.8847 - val_loss: 6.5147\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 5.0245 - val_loss: 7.0203\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 4.8252 - val_loss: 6.4897\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 5.2109 - val_loss: 12.3018\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 4.8084 - val_loss: 7.9859\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 5.0282 - val_loss: 7.0976\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 4.5766 - val_loss: 6.5582\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 10s 25ms/step - loss: 4.3426 - val_loss: 6.8699\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 4.6584 - val_loss: 8.3152\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 4.5990 - val_loss: 5.8992\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.9900 - val_loss: 7.1588\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 5.1063 - val_loss: 6.3944\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 4.0715 - val_loss: 7.8050\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 4.0334 - val_loss: 6.7405\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 4.1857 - val_loss: 7.6593\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 4.0191 - val_loss: 6.5357\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 4.0084 - val_loss: 5.7775\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 3.8334 - val_loss: 6.2921\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 3.6168 - val_loss: 6.3210\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 10s 25ms/step - loss: 3.8904 - val_loss: 5.6199\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 4.2728 - val_loss: 6.4501\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 3.7013 - val_loss: 6.0408\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 3.6795 - val_loss: 7.1290\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.7019 - val_loss: 6.4854\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.4017 - val_loss: 5.8080\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.5626 - val_loss: 6.1428\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 3.4628 - val_loss: 6.2227\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 3.4604 - val_loss: 5.6218\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 3.7176 - val_loss: 8.2320\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.8092 - val_loss: 8.8014\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 7.2296 - val_loss: 6.7001\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 3.4782 - val_loss: 6.0583\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 3.0204 - val_loss: 6.0719\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 3.0450 - val_loss: 7.2449\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.0773 - val_loss: 5.8832\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 3.1400 - val_loss: 5.2556\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 2.9981 - val_loss: 5.6732\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 3.0987 - val_loss: 7.0146\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 3.2108 - val_loss: 10.3918\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 4.1683 - val_loss: 5.7589\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 2.7407 - val_loss: 5.4390\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.8660 - val_loss: 5.8722\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.7704 - val_loss: 5.6777\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 2.8675 - val_loss: 5.6665\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.7292 - val_loss: 5.8762\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.7369 - val_loss: 5.7444\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 3.5581 - val_loss: 5.9591\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.8641 - val_loss: 6.3172\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 2.6502 - val_loss: 6.5440\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 4.1238 - val_loss: 6.8549\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.6981 - val_loss: 6.6120\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.6369 - val_loss: 6.0275\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.4995 - val_loss: 5.5085\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 17s 45ms/step - loss: 2.4760 - val_loss: 6.6200\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.5655 - val_loss: 5.7413\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 2.6485 - val_loss: 7.4243\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.7030 - val_loss: 5.3748\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.5445 - val_loss: 6.1159\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.4882 - val_loss: 5.6339\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.6755 - val_loss: 5.9195\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.4390 - val_loss: 6.1026\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.4676 - val_loss: 6.8447\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.4360 - val_loss: 5.8183\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 2.2360 - val_loss: 5.2502\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 2.3896 - val_loss: 6.7807\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 3.3692 - val_loss: 12.9100\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 3.0950 - val_loss: 5.4851\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.9941 - val_loss: 5.8996\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.9558 - val_loss: 5.6062\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.2085 - val_loss: 6.4818\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.3196 - val_loss: 5.6801\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.3018 - val_loss: 6.5283\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 2.3322 - val_loss: 5.8393\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.9673 - val_loss: 5.7353\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1.9926 - val_loss: 5.6547\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1.9076 - val_loss: 5.7182\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.9489 - val_loss: 5.6483\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.0725 - val_loss: 6.0654\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 2.4076 - val_loss: 5.8154\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.0120 - val_loss: 5.1981\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.0773 - val_loss: 5.6353\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.8671 - val_loss: 5.9134\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.3565 - val_loss: 6.3076\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 1.7446 - val_loss: 5.3303\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.9865 - val_loss: 5.8581\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.9509 - val_loss: 5.7846\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.7070 - val_loss: 5.8229\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.8370 - val_loss: 5.5486\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.6551 - val_loss: 5.9267\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 1.6034 - val_loss: 5.7694\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.8054 - val_loss: 6.1004\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.6120 - val_loss: 5.3750\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 1.7657 - val_loss: 6.9070\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.7626 - val_loss: 5.5788\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.9672 - val_loss: 6.6536\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 2.5908 - val_loss: 7.8693\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.9609 - val_loss: 5.5362\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1.3833 - val_loss: 5.3242\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 1.9626 - val_loss: 6.2195\n",
      "16/16 [==============================] - 2s 13ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 6.219511805428816\n",
      "Mean Absolute Error (MAE): 1.6942860586538486\n",
      "Root Mean Squared Error (RMSE): 2.4938949066528076\n",
      "Time taken: 2641.3328680992126\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 47ms/step - loss: 1385.0696 - val_loss: 1245.6624\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1219.2296 - val_loss: 1140.8441\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 1133.5216 - val_loss: 1064.5195\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1070.1349 - val_loss: 1007.7083\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1023.2072 - val_loss: 965.8738\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 989.0688 - val_loss: 935.9573\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 965.2293 - val_loss: 915.6709\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 949.2375 - val_loss: 902.2538\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 939.1827 - val_loss: 894.0905\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 933.4136 - val_loss: 889.5500\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 930.3895 - val_loss: 887.1958\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.9711 - val_loss: 885.9359\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 928.3915 - val_loss: 885.7100\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.2522 - val_loss: 885.4507\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.2222 - val_loss: 885.4044\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.1726 - val_loss: 885.3747\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 928.1182 - val_loss: 885.3367\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 928.1120 - val_loss: 885.3818\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.1269 - val_loss: 885.4488\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 928.1150 - val_loss: 885.5239\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.1414 - val_loss: 885.3876\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 928.1421 - val_loss: 885.4717\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.1646 - val_loss: 885.3824\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.1273 - val_loss: 885.2927\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.1738 - val_loss: 885.3057\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.1519 - val_loss: 885.2331\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.9318 - val_loss: 885.5811\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 928.3356 - val_loss: 885.4208\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.2186 - val_loss: 885.4392\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 928.1763 - val_loss: 885.5056\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.1650 - val_loss: 885.2193\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 928.1336 - val_loss: 885.2037\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 927.5681 - val_loss: 878.0290\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 885.8751 - val_loss: 833.5505\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 863.9810 - val_loss: 816.7429\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 849.8990 - val_loss: 801.2142\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 834.2477 - val_loss: 785.4290\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 809.5342 - val_loss: 752.3834\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 777.8679 - val_loss: 727.5839\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 756.5029 - val_loss: 699.4100\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 728.4018 - val_loss: 664.5076\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 680.9844 - val_loss: 610.1510\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 625.7836 - val_loss: 556.7690\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 575.8633 - val_loss: 512.3809\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 524.8856 - val_loss: 480.5743\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 485.3387 - val_loss: 431.7932\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 446.5949 - val_loss: 396.8195\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 411.2459 - val_loss: 365.0720\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 380.0477 - val_loss: 335.3038\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 344.4037 - val_loss: 301.7775\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 316.6101 - val_loss: 292.8408\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 286.9231 - val_loss: 250.5292\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 260.2753 - val_loss: 226.9867\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 235.3265 - val_loss: 206.5133\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 213.7463 - val_loss: 195.7521\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 194.5117 - val_loss: 167.1407\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 172.5382 - val_loss: 151.3389\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 153.9464 - val_loss: 133.0506\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 139.8863 - val_loss: 192.6902\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 124.2288 - val_loss: 113.1542\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 107.0719 - val_loss: 94.5220\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 94.5220 - val_loss: 82.3038\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 19s 47ms/step - loss: 83.7000 - val_loss: 77.0468\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 74.6177 - val_loss: 69.4418\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 66.4008 - val_loss: 62.8966\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 58.7852 - val_loss: 54.4693\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 51.9536 - val_loss: 48.9274\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 47.2135 - val_loss: 40.4037\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 41.5017 - val_loss: 37.5977\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 37.5731 - val_loss: 35.0955\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 33.6976 - val_loss: 35.2421\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 30.2519 - val_loss: 25.3149\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 26.5828 - val_loss: 27.3417\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 26.9387 - val_loss: 22.4868\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 23.2677 - val_loss: 20.3815\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 19.9861 - val_loss: 19.7067\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 16s 42ms/step - loss: 19.2048 - val_loss: 19.9129\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 18.1936 - val_loss: 17.2832\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 15.8039 - val_loss: 14.9267\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 16.3420 - val_loss: 16.1148\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 14.1797 - val_loss: 15.1927\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 14.2450 - val_loss: 15.8673\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 12.7410 - val_loss: 12.1354\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 12.6620 - val_loss: 12.4126\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 11.5039 - val_loss: 15.6737\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 11.4018 - val_loss: 10.5115\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 10.2325 - val_loss: 14.4134\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 10.4481 - val_loss: 11.5470\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 9.7941 - val_loss: 10.3290\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 9.4400 - val_loss: 10.0359\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 8.7203 - val_loss: 12.6594\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 8.6647 - val_loss: 9.6455\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 8.4267 - val_loss: 9.4361\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 8.5594 - val_loss: 9.1801\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 8.5655 - val_loss: 7.7055\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 7.7801 - val_loss: 9.3450\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 7.9815 - val_loss: 9.5282\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 7.6847 - val_loss: 7.9386\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 6.8750 - val_loss: 7.7875\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 7.0941 - val_loss: 11.2789\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 6.7368 - val_loss: 9.0555\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 6.4966 - val_loss: 7.5536\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 6.8361 - val_loss: 9.1132\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 6.4339 - val_loss: 8.3657\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 6.7941 - val_loss: 7.7061\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 5.8147 - val_loss: 8.4030\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 6.0275 - val_loss: 7.4972\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 6.1262 - val_loss: 7.1714\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 5.7568 - val_loss: 8.8855\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 5.5630 - val_loss: 6.3298\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 5.8838 - val_loss: 7.7949\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 5.3843 - val_loss: 6.7450\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 5.0005 - val_loss: 8.1111\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 5.6505 - val_loss: 6.6206\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 5.5471 - val_loss: 8.2317\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.9191 - val_loss: 6.8832\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 5.0176 - val_loss: 10.1292\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.9186 - val_loss: 6.3043\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 5.2337 - val_loss: 10.1000\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 5.1336 - val_loss: 7.2161\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.6748 - val_loss: 6.7048\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.7090 - val_loss: 7.2120\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.6165 - val_loss: 7.7183\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 4.8104 - val_loss: 8.2122\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.3610 - val_loss: 7.0473\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 4.4202 - val_loss: 6.7971\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 5.5667 - val_loss: 6.5689\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 4.2187 - val_loss: 6.2354\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 3.9068 - val_loss: 6.2307\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 3.8405 - val_loss: 6.6426\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 4.4186 - val_loss: 6.5035\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 3.9515 - val_loss: 7.5785\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.8875 - val_loss: 6.3117\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 4.2238 - val_loss: 8.2623\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 4.0362 - val_loss: 5.7755\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 4.3407 - val_loss: 5.9404\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 3.6849 - val_loss: 7.4409\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.6387 - val_loss: 5.8629\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 3.6457 - val_loss: 5.2475\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 3.7597 - val_loss: 6.6900\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 3.8712 - val_loss: 8.7888\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.1202 - val_loss: 5.8463\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 3.4544 - val_loss: 5.3221\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 3.4161 - val_loss: 6.0312\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 4.7865 - val_loss: 6.7761\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 3.4969 - val_loss: 6.8573\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 3.1552 - val_loss: 5.2428\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 53s 136ms/step - loss: 3.0740 - val_loss: 6.7506\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 57s 146ms/step - loss: 3.0609 - val_loss: 6.2456\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 3.2888 - val_loss: 5.6952\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 3.9892 - val_loss: 5.7732\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 3.0693 - val_loss: 6.3971\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 36s 91ms/step - loss: 3.1430 - val_loss: 6.0904\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 36s 91ms/step - loss: 3.6937 - val_loss: 11.8984\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 36s 92ms/step - loss: 9.9277 - val_loss: 6.0296\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 3.3440 - val_loss: 5.7643\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 2.9679 - val_loss: 5.6036\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 2.7293 - val_loss: 5.2718\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.6148 - val_loss: 5.5959\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.7337 - val_loss: 5.5772\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.7872 - val_loss: 5.8687\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.1677 - val_loss: 5.8802\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 3.0685 - val_loss: 7.0873\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.8292 - val_loss: 5.6294\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.7536 - val_loss: 5.9873\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.9020 - val_loss: 6.3593\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 3.1476 - val_loss: 5.5515\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.7926 - val_loss: 7.0044\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 2.9055 - val_loss: 5.8555\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 4.1294 - val_loss: 5.6611\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.4888 - val_loss: 5.3667\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.3625 - val_loss: 6.4465\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.9761 - val_loss: 5.6078\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.6633 - val_loss: 6.0258\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.5832 - val_loss: 5.7443\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.9308 - val_loss: 5.2026\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 2.3730 - val_loss: 5.9149\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 2.4388 - val_loss: 5.5515\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 2.4671 - val_loss: 5.7009\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 2.5448 - val_loss: 6.3333\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.5207 - val_loss: 5.5251\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 2.4159 - val_loss: 6.5814\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.5901 - val_loss: 5.6609\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 2.2235 - val_loss: 5.8623\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.5318 - val_loss: 5.6272\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 2.4261 - val_loss: 6.2544\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 2.1807 - val_loss: 5.5545\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.3582 - val_loss: 6.0839\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 2.6411 - val_loss: 5.5878\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.0696 - val_loss: 5.4358\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.0560 - val_loss: 6.3145\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 2.0467 - val_loss: 5.4146\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 2.1902 - val_loss: 5.4326\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 2.2291 - val_loss: 5.4496\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 2.2320 - val_loss: 5.2923\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 4.8440 - val_loss: 6.7448\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 2.7140 - val_loss: 5.3736\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 1.7937 - val_loss: 4.9996\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 1.7539 - val_loss: 5.1599\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7420 - val_loss: 5.1271\n",
      "16/16 [==============================] - 2s 18ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 5.127058532685797\n",
      "Mean Absolute Error (MAE): 1.5215414234475657\n",
      "Root Mean Squared Error (RMSE): 2.2643008927008346\n",
      "Time taken: 3691.7601273059845\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 41ms/step - loss: 1404.3635 - val_loss: 1293.3149\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 1238.8912 - val_loss: 1189.3121\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 1147.8895 - val_loss: 1111.2737\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 1080.2255 - val_loss: 1053.0862\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1029.4275 - val_loss: 1009.2922\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 991.7151 - val_loss: 977.0766\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 964.6749 - val_loss: 954.5372\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 946.1397 - val_loss: 939.3890\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 934.1407 - val_loss: 929.7863\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 926.8647 - val_loss: 924.1543\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 922.8454 - val_loss: 921.2101\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 920.8533 - val_loss: 919.7884\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 919.9564 - val_loss: 919.1636\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 919.6344 - val_loss: 918.8456\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 125s 320ms/step - loss: 919.4881 - val_loss: 918.7389\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 63s 159ms/step - loss: 919.4518 - val_loss: 918.6886\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 919.4683 - val_loss: 918.7030\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 919.4623 - val_loss: 918.6586\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 919.5025 - val_loss: 918.7495\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 919.4604 - val_loss: 918.6896\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 919.4813 - val_loss: 918.7871\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 919.4459 - val_loss: 918.7952\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 920.0973 - val_loss: 918.4964\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 919.0692 - val_loss: 917.6247\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 884.6541 - val_loss: 854.2521\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 836.0069 - val_loss: 816.3025\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 812.4190 - val_loss: 798.0801\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 789.3814 - val_loss: 774.9957\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 768.1873 - val_loss: 759.1255\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 749.3496 - val_loss: 725.8649\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 719.7077 - val_loss: 696.4692\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 22s 58ms/step - loss: 686.5135 - val_loss: 663.2912\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 636.5387 - val_loss: 608.4040\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 22s 58ms/step - loss: 570.8231 - val_loss: 535.1584\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 507.6248 - val_loss: 476.0086\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 455.1363 - val_loss: 430.3059\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 406.6454 - val_loss: 394.4297\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 366.1433 - val_loss: 345.5757\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 327.2654 - val_loss: 307.6980\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 291.8949 - val_loss: 274.5475\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 263.3306 - val_loss: 245.6931\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 232.4313 - val_loss: 216.7825\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 206.8766 - val_loss: 193.3689\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 181.8895 - val_loss: 170.3721\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 161.7677 - val_loss: 154.0392\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 140.0915 - val_loss: 138.7268\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 125.1964 - val_loss: 114.9166\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 108.1135 - val_loss: 106.7584\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 95.5702 - val_loss: 86.4843\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 17s 42ms/step - loss: 82.7922 - val_loss: 79.1622\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 70.4679 - val_loss: 69.3867\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 16s 42ms/step - loss: 61.0199 - val_loss: 56.0972\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 54.8668 - val_loss: 58.5213\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 46.4981 - val_loss: 41.6213\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 39.1817 - val_loss: 38.1431\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 34.1769 - val_loss: 31.6106\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 29.4885 - val_loss: 29.2398\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 26.4734 - val_loss: 24.3450\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 22.8084 - val_loss: 23.7330\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 20.4236 - val_loss: 19.5691\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 18.0176 - val_loss: 16.5045\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 16.4437 - val_loss: 15.7241\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 14.9407 - val_loss: 15.7123\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 16s 40ms/step - loss: 13.2371 - val_loss: 16.8009\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 12.8070 - val_loss: 11.8212\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 12.4913 - val_loss: 11.6982\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 15s 39ms/step - loss: 11.3314 - val_loss: 10.8427\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 16s 41ms/step - loss: 9.6748 - val_loss: 10.0873\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 8.7443 - val_loss: 12.0051\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 9.8635 - val_loss: 10.0780\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 8.4093 - val_loss: 9.8191\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 7.4501 - val_loss: 9.5116\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 8.1948 - val_loss: 10.2220\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 7.7990 - val_loss: 8.0884\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 6.6680 - val_loss: 8.6139\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 6.0281 - val_loss: 8.2224\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 34ms/step - loss: 6.5783 - val_loss: 8.3399\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 6.6989 - val_loss: 7.5605\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 5.8907 - val_loss: 7.4761\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 5.6468 - val_loss: 6.3166\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 6.5437 - val_loss: 9.5752\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 5.7456 - val_loss: 6.2833\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 4.9604 - val_loss: 6.1379\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.8503 - val_loss: 6.5562\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 6.3333 - val_loss: 7.7907\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.8817 - val_loss: 7.8277\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.4709 - val_loss: 17.2281\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.9230 - val_loss: 6.5995\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.9968 - val_loss: 8.7316\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.1789 - val_loss: 8.1468\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.9394 - val_loss: 6.6618\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.1253 - val_loss: 8.1563\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.5928 - val_loss: 5.9388\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.4139 - val_loss: 6.7568\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.8028 - val_loss: 5.9752\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.1259 - val_loss: 5.9650\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.3216 - val_loss: 6.6340\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.4953 - val_loss: 5.8986\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.5473 - val_loss: 5.4857\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.7533 - val_loss: 7.0685\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.5359 - val_loss: 5.7526\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.7537 - val_loss: 6.0328\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.3780 - val_loss: 7.0126\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.3894 - val_loss: 7.8034\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.6699 - val_loss: 6.3103\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.7605 - val_loss: 6.7023\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.9444 - val_loss: 5.8371\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 3.0573 - val_loss: 6.1780\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.0755 - val_loss: 5.8070\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 5.1685 - val_loss: 6.6331\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.1176 - val_loss: 6.4093\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.8599 - val_loss: 5.9018\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.7440 - val_loss: 5.9560\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.7808 - val_loss: 6.0841\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.0695 - val_loss: 5.8820\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.2303 - val_loss: 6.2535\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.8463 - val_loss: 6.2857\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.8492 - val_loss: 11.2566\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 3.3769 - val_loss: 5.6693\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4412 - val_loss: 5.7586\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4894 - val_loss: 5.1673\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4244 - val_loss: 5.3528\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.5095 - val_loss: 8.2989\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 5.6015 - val_loss: 5.9244\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 2.5865 - val_loss: 5.4015\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.2219 - val_loss: 5.0216\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.2750 - val_loss: 6.4954\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.5280 - val_loss: 5.8778\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.2821 - val_loss: 5.2606\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4888 - val_loss: 7.3814\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1579 - val_loss: 5.4411\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.6658 - val_loss: 7.5390\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.2929 - val_loss: 5.1807\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0012 - val_loss: 5.8070\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1717 - val_loss: 5.1702\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1161 - val_loss: 5.1443\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.7876 - val_loss: 6.1769\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1277 - val_loss: 5.4920\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1688 - val_loss: 6.7870\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.1823 - val_loss: 5.9080\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4576 - val_loss: 7.2325\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0814 - val_loss: 6.4473\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0855 - val_loss: 5.6889\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7796 - val_loss: 5.4469\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4393 - val_loss: 19.2681\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.6515 - val_loss: 6.1494\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.4843 - val_loss: 8.1337\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.9032 - val_loss: 4.8906\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.4159 - val_loss: 4.6636\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.9089 - val_loss: 4.9738\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.4591 - val_loss: 5.2933\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7272 - val_loss: 5.0509\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.7716 - val_loss: 4.8616\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.9412 - val_loss: 5.0377\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 34ms/step - loss: 5.2655 - val_loss: 5.5888\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5081 - val_loss: 5.2285\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3344 - val_loss: 4.9561\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1269 - val_loss: 5.3369\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5152 - val_loss: 5.4463\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5566 - val_loss: 4.8698\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.5196 - val_loss: 5.3067\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.0482 - val_loss: 12.9033\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 4.7205 - val_loss: 12.7629\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 2.6948 - val_loss: 5.3433\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3250 - val_loss: 5.7404\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1655 - val_loss: 5.1339\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1667 - val_loss: 6.2142\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.4077 - val_loss: 5.5895\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3889 - val_loss: 5.6363\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3170 - val_loss: 5.3012\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 1.6797 - val_loss: 5.4501\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 13s 33ms/step - loss: 1.7371 - val_loss: 5.2414\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3951 - val_loss: 5.3317\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.8327 - val_loss: 5.4647\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.8701 - val_loss: 6.6257\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.8058 - val_loss: 5.1609\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.0217 - val_loss: 5.0095\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9317 - val_loss: 5.2500\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9871 - val_loss: 5.3418\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1137 - val_loss: 5.5748\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.2334 - val_loss: 5.3971\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.4240 - val_loss: 5.1997\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 13s 35ms/step - loss: 1.1068 - val_loss: 5.3427\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9330 - val_loss: 5.7858\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.3806 - val_loss: 5.3010\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 15s 38ms/step - loss: 1.1292 - val_loss: 5.8950\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.6785 - val_loss: 8.7689\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 3.3930 - val_loss: 5.3646\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9191 - val_loss: 5.1723\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7801 - val_loss: 5.1591\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8000 - val_loss: 5.2923\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9137 - val_loss: 5.1962\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0393 - val_loss: 5.2577\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0608 - val_loss: 5.6159\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9794 - val_loss: 5.4194\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1688 - val_loss: 5.5235\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.1749 - val_loss: 5.7172\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.0331 - val_loss: 4.8877\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 1.0619 - val_loss: 5.5769\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.9812 - val_loss: 5.3514\n",
      "16/16 [==============================] - 2s 26ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 5.351432562116113\n",
      "Mean Absolute Error (MAE): 1.5351583501187804\n",
      "Root Mean Squared Error (RMSE): 2.313316355822548\n",
      "Time taken: 3095.327966928482\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 50ms/step - loss: 1352.6730 - val_loss: 1284.0726\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 1195.9229 - val_loss: 1182.6229\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1110.4111 - val_loss: 1109.1064\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1048.2122 - val_loss: 1055.5654\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 1002.5696 - val_loss: 1016.5182\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 969.6910 - val_loss: 988.9949\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 946.8632 - val_loss: 970.5480\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 931.8860 - val_loss: 959.0377\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 922.7679 - val_loss: 952.3226\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 917.4432 - val_loss: 948.5912\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 914.7347 - val_loss: 946.8817\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 913.5175 - val_loss: 946.2258\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 18s 46ms/step - loss: 912.9937 - val_loss: 945.9933\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 912.8237 - val_loss: 945.8281\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 912.7469 - val_loss: 945.8710\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7739 - val_loss: 945.8672\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7068 - val_loss: 945.9255\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7580 - val_loss: 945.9047\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7449 - val_loss: 946.0963\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 912.7244 - val_loss: 945.9905\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7385 - val_loss: 945.9022\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7687 - val_loss: 945.8667\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7313 - val_loss: 945.8547\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7272 - val_loss: 945.8380\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7704 - val_loss: 945.8788\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7525 - val_loss: 945.8987\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7759 - val_loss: 945.8935\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7585 - val_loss: 945.9269\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 912.7792 - val_loss: 947.0343\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 912.6908 - val_loss: 947.4708\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 885.2347 - val_loss: 892.2902\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 843.5537 - val_loss: 850.1669\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 790.1924 - val_loss: 790.5394\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 739.5851 - val_loss: 739.2578\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 691.6746 - val_loss: 696.0502\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 643.7811 - val_loss: 645.4450\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 600.9667 - val_loss: 601.3735\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 561.2504 - val_loss: 563.6176\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 524.9877 - val_loss: 522.1856\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 486.4300 - val_loss: 487.7404\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 447.9893 - val_loss: 444.6002\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 412.8556 - val_loss: 408.1125\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 378.3824 - val_loss: 382.7662\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 345.8320 - val_loss: 343.5266\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 315.9435 - val_loss: 313.0364\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 288.1658 - val_loss: 286.6251\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 263.4443 - val_loss: 264.2879\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 238.3355 - val_loss: 237.2471\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 217.1098 - val_loss: 216.5895\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 195.8594 - val_loss: 194.4976\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 177.7051 - val_loss: 179.3617\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 160.8747 - val_loss: 158.9427\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 144.5769 - val_loss: 143.7735\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 129.8612 - val_loss: 130.4698\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 117.5608 - val_loss: 114.0908\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 106.7488 - val_loss: 103.6123\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 94.2030 - val_loss: 93.0163\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 83.7945 - val_loss: 83.8409\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 75.1570 - val_loss: 78.3894\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 67.5355 - val_loss: 69.8807\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 60.1515 - val_loss: 59.1020\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 53.6864 - val_loss: 53.8262\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 47.1902 - val_loss: 47.8355\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 42.9844 - val_loss: 41.2977\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 38.8022 - val_loss: 37.4201\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 33.9993 - val_loss: 35.6603\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 17s 45ms/step - loss: 30.6205 - val_loss: 29.8902\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 27.3760 - val_loss: 29.1919\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 24.8610 - val_loss: 24.9353\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 22.1659 - val_loss: 21.7332\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 20.3083 - val_loss: 20.5515\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 19.0532 - val_loss: 17.8770\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 17.5694 - val_loss: 19.3866\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 16.2009 - val_loss: 15.8255\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 14.7750 - val_loss: 16.7331\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 14.0665 - val_loss: 15.6115\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 28ms/step - loss: 13.0339 - val_loss: 14.6074\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 12.4409 - val_loss: 12.9809\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 11.0705 - val_loss: 13.7694\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 11.6758 - val_loss: 11.9429\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 10.8323 - val_loss: 13.1017\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 10.0472 - val_loss: 12.7844\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 10.4959 - val_loss: 10.4693\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 9.2151 - val_loss: 9.5633\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 9.2726 - val_loss: 11.5154\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 9.5607 - val_loss: 10.2875\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 9.0434 - val_loss: 10.0480\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 7.8857 - val_loss: 9.0564\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 7.7053 - val_loss: 10.2963\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 7.4526 - val_loss: 10.6134\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 7.9344 - val_loss: 8.3786\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 8.8410 - val_loss: 9.6582\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 7.5478 - val_loss: 9.8385\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 7.0933 - val_loss: 10.5490\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 6.8523 - val_loss: 7.6850\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 6.5201 - val_loss: 8.6988\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 6.8678 - val_loss: 12.1173\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 6.9525 - val_loss: 11.7389\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 6.5538 - val_loss: 7.5822\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 5.9624 - val_loss: 7.8180\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 6.3856 - val_loss: 7.7310\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 5.9038 - val_loss: 8.3031\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 5.8896 - val_loss: 8.2467\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 6.0674 - val_loss: 8.0621\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 15s 40ms/step - loss: 5.7358 - val_loss: 8.5263\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 5.6830 - val_loss: 7.4921\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.4257 - val_loss: 8.5832\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.2362 - val_loss: 7.5847\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.6790 - val_loss: 8.4521\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 6.8022 - val_loss: 7.4671\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.7476 - val_loss: 7.2589\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.8148 - val_loss: 8.4072\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.1204 - val_loss: 8.1769\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.0030 - val_loss: 7.7834\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.9947 - val_loss: 8.6660\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 6.0920 - val_loss: 7.1752\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.3860 - val_loss: 7.0081\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.3810 - val_loss: 7.8566\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.1120 - val_loss: 9.2469\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.6125 - val_loss: 7.6009\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 6.8730 - val_loss: 7.3596\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.3871 - val_loss: 6.4828\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.5158 - val_loss: 7.7854\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.2636 - val_loss: 6.3651\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.9662 - val_loss: 7.2360\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.9711 - val_loss: 7.6481\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.1657 - val_loss: 9.5228\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.0679 - val_loss: 6.7658\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.9019 - val_loss: 6.5306\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.1861 - val_loss: 7.4084\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.0172 - val_loss: 6.4848\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.9801 - val_loss: 6.7814\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.0033 - val_loss: 7.3970\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 6.0027 - val_loss: 6.8684\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.2475 - val_loss: 8.7397\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.5516 - val_loss: 6.4596\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.0486 - val_loss: 8.1339\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.8115 - val_loss: 8.1935\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.5188 - val_loss: 6.3469\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 3.7283 - val_loss: 6.8885\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.6929 - val_loss: 7.1534\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.6292 - val_loss: 6.9076\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.9217 - val_loss: 6.6783\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.5275 - val_loss: 6.0473\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.5345 - val_loss: 8.4123\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.8160 - val_loss: 6.5895\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.3882 - val_loss: 6.4046\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.3174 - val_loss: 6.8339\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 3.4069 - val_loss: 6.3536\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.2041 - val_loss: 7.8330\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.2457 - val_loss: 6.3606\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.4157 - val_loss: 8.4925\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.4884 - val_loss: 7.1972\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.3904 - val_loss: 6.6898\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 17s 44ms/step - loss: 4.3403 - val_loss: 6.1605\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 3.2153 - val_loss: 6.6641\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.0873 - val_loss: 6.2564\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.9564 - val_loss: 5.9122\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.0319 - val_loss: 6.5249\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.9736 - val_loss: 6.2298\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 5.6549 - val_loss: 6.4014\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.7527 - val_loss: 8.8018\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 3.0482 - val_loss: 5.9213\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.7077 - val_loss: 5.8564\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.5765 - val_loss: 5.9842\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.2667 - val_loss: 5.7608\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.8735 - val_loss: 5.5482\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.5831 - val_loss: 6.1665\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.9044 - val_loss: 6.7961\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 3.3836 - val_loss: 7.2251\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.9308 - val_loss: 7.1374\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.7519 - val_loss: 6.3954\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.6989 - val_loss: 7.4973\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.6962 - val_loss: 6.5618\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.0092 - val_loss: 6.1320\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.7733 - val_loss: 6.8882\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.7570 - val_loss: 6.2770\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.5704 - val_loss: 6.7734\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.6803 - val_loss: 6.5328\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.5271 - val_loss: 6.2395\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.5071 - val_loss: 7.8468\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.4817 - val_loss: 6.7744\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.3009 - val_loss: 6.3911\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 4.8153 - val_loss: 6.3571\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.5000 - val_loss: 7.6678\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 17s 43ms/step - loss: 2.5127 - val_loss: 6.9030\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.3289 - val_loss: 6.3986\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.3205 - val_loss: 6.0840\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.3391 - val_loss: 6.5526\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.1315 - val_loss: 5.5398\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.0732 - val_loss: 8.5096\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.4407 - val_loss: 6.2419\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.0064 - val_loss: 6.1002\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.9307 - val_loss: 6.3135\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 2.2530 - val_loss: 7.1369\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 3.8417 - val_loss: 5.9902\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 17s 44ms/step - loss: 1.9680 - val_loss: 6.1364\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 1.9566 - val_loss: 6.0352\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 2.4955 - val_loss: 6.7390\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 1.9920 - val_loss: 5.7632\n",
      "16/16 [==============================] - 2s 39ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 5.763029450060151\n",
      "Mean Absolute Error (MAE): 1.628912151200427\n",
      "Root Mean Squared Error (RMSE): 2.4006310524651955\n",
      "Time taken: 3261.6324558258057\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 15s 27ms/step - loss: 1369.4293 - val_loss: 1272.1455\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1203.6768 - val_loss: 1170.4330\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1117.8582 - val_loss: 1096.9109\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1055.1246 - val_loss: 1042.4749\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1009.3065 - val_loss: 1002.3085\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 976.1516 - val_loss: 973.8104\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 953.1008 - val_loss: 953.9283\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 937.9405 - val_loss: 940.9376\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 928.5804 - val_loss: 933.1148\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 923.2750 - val_loss: 928.7692\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 920.5476 - val_loss: 926.4357\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 919.2621 - val_loss: 925.3572\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.7531 - val_loss: 924.6828\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5684 - val_loss: 924.4591\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.5303 - val_loss: 924.2739\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4688 - val_loss: 924.3915\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4615 - val_loss: 924.2836\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.5140 - val_loss: 924.2231\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4573 - val_loss: 924.3694\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.5167 - val_loss: 924.0872\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4916 - val_loss: 924.1390\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 918.4885 - val_loss: 924.1378\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4769 - val_loss: 924.3112\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.4974 - val_loss: 924.3140\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 918.6030 - val_loss: 924.1940\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.9265 - val_loss: 911.3510\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 877.2922 - val_loss: 872.3197\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 855.1863 - val_loss: 855.4099\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 836.0374 - val_loss: 833.1114\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 810.7705 - val_loss: 802.9337\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 771.0826 - val_loss: 770.4189\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 729.6644 - val_loss: 723.3053\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 690.0608 - val_loss: 680.4037\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 640.5942 - val_loss: 635.6159\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 590.4214 - val_loss: 571.6652\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 534.6793 - val_loss: 517.3826\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 487.0404 - val_loss: 473.7489\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 443.7473 - val_loss: 432.8505\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 406.1464 - val_loss: 395.8576\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 369.3230 - val_loss: 357.7906\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 335.2867 - val_loss: 325.0815\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 304.6920 - val_loss: 293.1109\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 274.9739 - val_loss: 266.7895\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 249.5353 - val_loss: 244.1403\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 222.3022 - val_loss: 212.4840\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 199.1828 - val_loss: 191.7096\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 178.2329 - val_loss: 171.4388\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 160.1738 - val_loss: 155.9240\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 142.2575 - val_loss: 135.5562\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 125.8657 - val_loss: 120.6627\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 109.7471 - val_loss: 104.7419\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 96.8173 - val_loss: 93.2760\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 85.4933 - val_loss: 81.7989\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 74.5618 - val_loss: 73.4194\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 65.0667 - val_loss: 65.7125\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 56.6929 - val_loss: 55.7592\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 49.9197 - val_loss: 55.6094\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.4723 - val_loss: 43.5037\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.8262 - val_loss: 40.1108\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.4072 - val_loss: 34.7641\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.5773 - val_loss: 31.7642\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.4652 - val_loss: 29.6445\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.3148 - val_loss: 32.9168\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.2973 - val_loss: 23.3034\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.0549 - val_loss: 23.2737\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.6305 - val_loss: 20.3869\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.6813 - val_loss: 21.4757\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.5984 - val_loss: 20.2842\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 14.5877 - val_loss: 16.7759\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 13.6866 - val_loss: 15.3648\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 12.1421 - val_loss: 13.3505\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 12.4612 - val_loss: 15.7817\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 10.7420 - val_loss: 13.6387\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 10.4337 - val_loss: 14.4008\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 9.6389 - val_loss: 11.9710\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 9.9768 - val_loss: 13.2080\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7555 - val_loss: 10.6664\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 15ms/step - loss: 8.7217 - val_loss: 11.3583\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.6148 - val_loss: 9.5453\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.9815 - val_loss: 9.4737\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.8878 - val_loss: 9.0246\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.1115 - val_loss: 12.4784\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.0225 - val_loss: 9.6216\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 7.2334 - val_loss: 9.8717\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.6729 - val_loss: 9.2243\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4169 - val_loss: 8.2076\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.2807 - val_loss: 10.5320\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2365 - val_loss: 9.1219\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4805 - val_loss: 10.9707\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.4930 - val_loss: 10.6868\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0559 - val_loss: 8.6381\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.7489 - val_loss: 8.4785\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.0243 - val_loss: 7.9886\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3911 - val_loss: 7.9379\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.4645 - val_loss: 7.6112\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9472 - val_loss: 10.1540\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3824 - val_loss: 8.5086\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.9762 - val_loss: 9.7550\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.9099 - val_loss: 8.3354\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.2965 - val_loss: 7.6665\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.9413 - val_loss: 7.7339\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7530 - val_loss: 6.9634\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.3586 - val_loss: 6.9096\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0097 - val_loss: 8.4304\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.7959 - val_loss: 9.5662\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0724 - val_loss: 23.9842\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 5.0667 - val_loss: 6.9558\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.1810 - val_loss: 9.4992\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 4.3997 - val_loss: 7.0303\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.2507 - val_loss: 6.2629\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 6.8556 - val_loss: 8.0373\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3360 - val_loss: 7.0414\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0873 - val_loss: 6.4825\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 5.3521 - val_loss: 7.3920\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0284 - val_loss: 7.1037\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8402 - val_loss: 7.2466\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.8096 - val_loss: 6.6749\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5804 - val_loss: 7.3151\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.3467 - val_loss: 6.7514\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.5504 - val_loss: 7.2961\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.9609 - val_loss: 7.4719\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.4070 - val_loss: 7.4389\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.6901 - val_loss: 6.6024\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4695 - val_loss: 6.7241\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4401 - val_loss: 6.0991\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7664 - val_loss: 7.1157\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.6495 - val_loss: 7.6187\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2935 - val_loss: 7.4644\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4515 - val_loss: 7.9971\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4831 - val_loss: 6.6842\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.2902 - val_loss: 8.8480\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.3259 - val_loss: 5.8299\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.0309 - val_loss: 6.2790\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 3.0543 - val_loss: 9.7782\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.7733 - val_loss: 7.3159\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.4647 - val_loss: 15.2707\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1098 - val_loss: 5.6837\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.7460 - val_loss: 6.1698\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.8484 - val_loss: 6.2088\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.9125 - val_loss: 5.9296\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6592 - val_loss: 6.2324\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 4.5992 - val_loss: 6.7034\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6893 - val_loss: 6.0033\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.6944 - val_loss: 6.1576\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6082 - val_loss: 6.1425\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6605 - val_loss: 6.4722\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.6486 - val_loss: 6.1061\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5634 - val_loss: 6.8794\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5660 - val_loss: 7.8059\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5731 - val_loss: 6.5283\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.9199 - val_loss: 6.8962\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.0185 - val_loss: 6.8878\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1945 - val_loss: 6.1228\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5398 - val_loss: 5.5061\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4533 - val_loss: 6.5360\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.4338 - val_loss: 5.7356\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1706 - val_loss: 6.9900\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.3387 - val_loss: 7.1791\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5961 - val_loss: 6.1135\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.4590 - val_loss: 6.3864\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.5075 - val_loss: 6.0093\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2360 - val_loss: 6.1698\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.5913 - val_loss: 6.3155\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9935 - val_loss: 5.8707\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.3990 - val_loss: 7.1142\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.1401 - val_loss: 5.6702\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.8825 - val_loss: 5.8895\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0524 - val_loss: 6.0176\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8075 - val_loss: 7.0969\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9950 - val_loss: 6.2639\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8625 - val_loss: 6.1451\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.2180 - val_loss: 7.1672\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.8649 - val_loss: 6.0390\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9908 - val_loss: 6.2695\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9185 - val_loss: 7.0769\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0693 - val_loss: 6.1384\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7824 - val_loss: 5.9998\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9118 - val_loss: 5.8956\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.1584 - val_loss: 5.5530\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.7401 - val_loss: 6.4293\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.9695 - val_loss: 6.3977\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.7111 - val_loss: 6.3248\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6710 - val_loss: 6.5837\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.0040 - val_loss: 5.7094\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8797 - val_loss: 6.1702\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6484 - val_loss: 6.1861\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.8702 - val_loss: 7.2619\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5267 - val_loss: 5.8785\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 2.0041 - val_loss: 5.8902\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.5843 - val_loss: 6.1041\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6258 - val_loss: 5.8677\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4249 - val_loss: 6.5337\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.6655 - val_loss: 7.2151\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.5887 - val_loss: 6.1777\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 6.2648 - val_loss: 6.9679\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 2.2443 - val_loss: 5.9072\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.4304 - val_loss: 6.6724\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.3029 - val_loss: 5.4427\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1.1723 - val_loss: 5.9954\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2547 - val_loss: 5.7260\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 5.726031552941369\n",
      "Mean Absolute Error (MAE): 1.6423588034837746\n",
      "Root Mean Squared Error (RMSE): 2.3929127758740747\n",
      "Time taken: 1231.3451263904572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_7392\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  6.219512  1.694286  2.493895  2641.332868\n",
      "1        2  5.127059  1.521541  2.264301  3691.760127\n",
      "2        3  5.351433  1.535158  2.313316  3095.327967\n",
      "3        4  5.763029  1.628912  2.400631  3261.632456\n",
      "4        5  5.726032  1.642359  2.392913  1231.345126\n",
      "5  Average  5.637413  1.604451  2.373011  2784.279709\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
