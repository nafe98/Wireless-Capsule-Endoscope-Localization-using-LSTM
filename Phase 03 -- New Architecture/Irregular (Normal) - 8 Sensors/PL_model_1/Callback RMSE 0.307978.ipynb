{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 998.9116 - val_loss: 695.8474\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 527.2133 - val_loss: 412.1731\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 317.2022 - val_loss: 259.1570\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 199.0733 - val_loss: 152.0611\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 96.8037 - val_loss: 61.1569\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 44.1547 - val_loss: 35.5760\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.4094 - val_loss: 10.3479\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.7151 - val_loss: 8.0661\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.0621 - val_loss: 5.4092\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1443 - val_loss: 11.1392\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4616 - val_loss: 2.6266\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4248 - val_loss: 1.1428\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9444 - val_loss: 1.2662\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5611 - val_loss: 2.1962\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9543 - val_loss: 1.7234\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2883 - val_loss: 1.7420\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2070 - val_loss: 1.5621\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1444 - val_loss: 2.0434\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7979 - val_loss: 0.9196\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8811 - val_loss: 1.6548\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4635 - val_loss: 0.9166\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3261 - val_loss: 1.5634\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7861 - val_loss: 0.9575\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1461 - val_loss: 0.6718\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1521 - val_loss: 0.9935\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2168 - val_loss: 1.7047\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7222 - val_loss: 1.1375\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2194 - val_loss: 4.2459\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2649 - val_loss: 1.2784\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8587 - val_loss: 1.0946\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8797 - val_loss: 0.5377\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9112 - val_loss: 0.9406\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7975 - val_loss: 0.7524\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7831 - val_loss: 0.7931\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7179 - val_loss: 0.8301\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8513 - val_loss: 0.7999\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7173 - val_loss: 0.4627\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8257 - val_loss: 1.5753\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7127 - val_loss: 3.2286\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7009 - val_loss: 0.6878\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5918 - val_loss: 0.8382\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6072 - val_loss: 0.6284\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8393 - val_loss: 0.9531\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6848 - val_loss: 9.1532\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9357 - val_loss: 0.3959\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3656 - val_loss: 0.3682\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4379 - val_loss: 0.3799\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5320 - val_loss: 0.7130\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5017 - val_loss: 0.6427\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5168 - val_loss: 0.3956\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4653 - val_loss: 0.3253\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7683 - val_loss: 2.2312\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4691 - val_loss: 0.8109\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7332 - val_loss: 0.6673\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5864 - val_loss: 1.6300\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0809 - val_loss: 0.7781\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5114 - val_loss: 0.2337\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3361 - val_loss: 0.2080\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3320 - val_loss: 0.4480\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4118 - val_loss: 0.4905\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5333 - val_loss: 0.6303\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4824 - val_loss: 0.4248\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5412 - val_loss: 0.2522\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5281 - val_loss: 0.5300\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3724 - val_loss: 0.2548\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3933 - val_loss: 0.6634\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3939 - val_loss: 0.4201\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6358 - val_loss: 1.0226\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1463 - val_loss: 1.0537\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4544 - val_loss: 0.3380\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4384 - val_loss: 0.3386\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5408 - val_loss: 0.6232\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2747 - val_loss: 0.3340\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3123 - val_loss: 0.2159\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2432 - val_loss: 0.4450\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4088 - val_loss: 0.4669\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4338 - val_loss: 0.5494\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4235 - val_loss: 0.9731\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3155 - val_loss: 0.1897\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2854 - val_loss: 0.1939\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5990 - val_loss: 0.6739\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3908 - val_loss: 0.3925\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4008 - val_loss: 1.0649\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3101 - val_loss: 1.0191\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3092 - val_loss: 0.4666\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2457 - val_loss: 0.1704\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2485 - val_loss: 0.1588\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1978 - val_loss: 0.2417\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3783 - val_loss: 0.5579\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2379 - val_loss: 0.2602\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2371 - val_loss: 0.2320\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2593 - val_loss: 0.3808\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4828 - val_loss: 0.2619\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2834 - val_loss: 0.4716\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5207 - val_loss: 0.4199\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4281 - val_loss: 0.4047\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1792 - val_loss: 0.5203\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2329 - val_loss: 0.2263\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2123 - val_loss: 0.1782\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3162 - val_loss: 0.2962\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1908 - val_loss: 0.1668\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2649 - val_loss: 0.4496\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4759 - val_loss: 0.1865\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2188 - val_loss: 0.2200\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5009 - val_loss: 0.5664\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3890 - val_loss: 0.2988\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1725 - val_loss: 0.2914\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2839 - val_loss: 0.1966\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3464 - val_loss: 0.6346\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3291 - val_loss: 0.2309\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2819 - val_loss: 0.2724\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2496 - val_loss: 0.2406\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3237 - val_loss: 0.2298\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4752 - val_loss: 1.0845\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4000 - val_loss: 0.1232\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3584 - val_loss: 0.1643\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2306 - val_loss: 0.2243\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5073 - val_loss: 0.1586\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2128 - val_loss: 0.1577\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2410 - val_loss: 0.2541\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2488 - val_loss: 0.3701\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3322 - val_loss: 0.3197\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2817 - val_loss: 0.2816\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2642 - val_loss: 0.4050\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2584 - val_loss: 0.2891\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2651 - val_loss: 0.3964\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3430 - val_loss: 0.6581\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3330 - val_loss: 0.3879\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5239 - val_loss: 0.4045\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2544 - val_loss: 0.2441\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2578 - val_loss: 0.4495\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3601 - val_loss: 0.3341\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2523 - val_loss: 0.1368\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2226 - val_loss: 0.2044\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2069 - val_loss: 0.1906\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3610 - val_loss: 0.4689\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1742 - val_loss: 0.2244\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2484 - val_loss: 0.1194\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2466 - val_loss: 0.3598\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2430 - val_loss: 0.4460\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2992 - val_loss: 0.4141\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2541 - val_loss: 0.2975\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3656 - val_loss: 0.8324\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2217 - val_loss: 0.2345\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2215 - val_loss: 0.2273\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4294 - val_loss: 11.8167\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5129 - val_loss: 0.1728\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1436 - val_loss: 0.4213\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1461 - val_loss: 0.1751\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1696 - val_loss: 0.1505\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1364 - val_loss: 0.0939\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1680 - val_loss: 0.1273\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1707 - val_loss: 0.1935\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2405 - val_loss: 0.2309\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1635 - val_loss: 0.1912\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2394 - val_loss: 0.2032\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2555 - val_loss: 0.3556\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2374 - val_loss: 0.1243\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1670 - val_loss: 0.4222\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2363 - val_loss: 0.4287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1817 - val_loss: 0.0869\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2532 - val_loss: 0.2844\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1826 - val_loss: 0.1789\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2517 - val_loss: 0.1786\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2803 - val_loss: 0.6159\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2002 - val_loss: 0.1676\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2404 - val_loss: 0.3948\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1712 - val_loss: 0.2430\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1691 - val_loss: 0.1532\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2849 - val_loss: 0.1367\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2402 - val_loss: 0.1438\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2229 - val_loss: 0.2229\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2752 - val_loss: 0.3427\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1715 - val_loss: 0.4170\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1578 - val_loss: 0.2077\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4628 - val_loss: 0.5171\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2010 - val_loss: 0.2518\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2262 - val_loss: 0.1646\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1582 - val_loss: 0.1505\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1346 - val_loss: 0.5143\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2096 - val_loss: 0.4291\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1751 - val_loss: 0.1862\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1461 - val_loss: 0.1655\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3284 - val_loss: 0.1285\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1738 - val_loss: 0.2291\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1060 - val_loss: 0.2767\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1736 - val_loss: 0.1839\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1789 - val_loss: 0.1943\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3233 - val_loss: 0.4016\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8969 - val_loss: 0.5800\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1220 - val_loss: 0.1040\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.08784285356479497\n",
      "Mean Absolute Error (MAE): 0.2149509400336199\n",
      "Root Mean Squared Error (RMSE): 0.2963829508672774\n",
      "Time taken: 539.9419639110565\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1030.0710 - val_loss: 677.7444\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 549.2709 - val_loss: 429.7796\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 341.5134 - val_loss: 244.2189\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 187.8492 - val_loss: 121.7198\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 86.3814 - val_loss: 46.2174\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 37.5085 - val_loss: 21.1151\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 19.6092 - val_loss: 16.8858\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.1579 - val_loss: 9.7543\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.9269 - val_loss: 16.7499\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.2835 - val_loss: 3.5747\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.1876 - val_loss: 6.5874\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4906 - val_loss: 2.8430\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.9233 - val_loss: 2.1981\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2839 - val_loss: 2.7769\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6115 - val_loss: 3.3322\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9959 - val_loss: 2.3945\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8868 - val_loss: 0.8940\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7849 - val_loss: 0.9796\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8581 - val_loss: 6.9415\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4204 - val_loss: 3.0593\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0662 - val_loss: 2.1282\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4179 - val_loss: 1.2333\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3056 - val_loss: 2.3748\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4053 - val_loss: 1.7523\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5544 - val_loss: 0.8085\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5699 - val_loss: 1.8050\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8858 - val_loss: 1.0267\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6311 - val_loss: 1.1047\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3127 - val_loss: 0.7227\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0651 - val_loss: 1.1132\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4437 - val_loss: 1.0509\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8950 - val_loss: 0.6224\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8734 - val_loss: 0.9650\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6847 - val_loss: 0.8722\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8386 - val_loss: 0.7861\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4217 - val_loss: 2.8683\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9866 - val_loss: 0.6900\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3561 - val_loss: 1.4285\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8719 - val_loss: 0.4713\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3479 - val_loss: 1.2711\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6939 - val_loss: 0.8329\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5461 - val_loss: 0.3017\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4883 - val_loss: 1.0583\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7092 - val_loss: 0.5694\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4939 - val_loss: 0.4948\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6625 - val_loss: 0.6957\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5547 - val_loss: 0.3844\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5025 - val_loss: 0.4350\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7422 - val_loss: 1.6181\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7073 - val_loss: 0.6513\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8840 - val_loss: 0.8235\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3850 - val_loss: 0.3556\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8147 - val_loss: 0.9991\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1232 - val_loss: 0.6110\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6792 - val_loss: 0.4347\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6378 - val_loss: 0.6396\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5078 - val_loss: 0.4996\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5504 - val_loss: 1.0646\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8680 - val_loss: 0.4034\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4066 - val_loss: 0.5603\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7507 - val_loss: 0.4200\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7199 - val_loss: 0.3260\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6619 - val_loss: 0.3464\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3695 - val_loss: 0.5987\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3961 - val_loss: 0.8951\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5765 - val_loss: 0.9911\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4420 - val_loss: 0.4612\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5552 - val_loss: 1.5711\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3511 - val_loss: 0.6423\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4547 - val_loss: 0.2421\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3062 - val_loss: 0.3376\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3559 - val_loss: 0.4427\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4066 - val_loss: 0.4274\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5661 - val_loss: 3.3365\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5225 - val_loss: 0.7138\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8192 - val_loss: 1.0953\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3542 - val_loss: 0.5449\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3439 - val_loss: 0.2706\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6320 - val_loss: 5.0058\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7843 - val_loss: 0.3502\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2173 - val_loss: 0.3548\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3628 - val_loss: 0.2356\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3091 - val_loss: 0.5704\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5813 - val_loss: 0.5354\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3542 - val_loss: 0.3561\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3781 - val_loss: 0.2322\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2831 - val_loss: 0.8138\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6209 - val_loss: 0.5067\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5736 - val_loss: 0.5384\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6398 - val_loss: 0.2311\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2733 - val_loss: 0.1818\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2819 - val_loss: 0.3052\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3069 - val_loss: 0.9549\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3881 - val_loss: 0.2648\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3582 - val_loss: 0.2078\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5905 - val_loss: 0.4369\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3333 - val_loss: 0.2057\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5445 - val_loss: 0.3365\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5275 - val_loss: 0.4172\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2236 - val_loss: 0.1215\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3180 - val_loss: 0.2575\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3842 - val_loss: 0.5711\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5251 - val_loss: 1.8766\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3477 - val_loss: 0.3430\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2109 - val_loss: 0.2363\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5737 - val_loss: 0.3434\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2447 - val_loss: 0.2718\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2789 - val_loss: 0.3761\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3641 - val_loss: 0.2206\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7879 - val_loss: 0.7098\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2479 - val_loss: 0.1868\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2689 - val_loss: 0.3925\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1978 - val_loss: 0.6057\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2791 - val_loss: 0.1902\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3466 - val_loss: 0.2951\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2275 - val_loss: 0.2359\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1937 - val_loss: 0.1301\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1459 - val_loss: 0.2860\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2324 - val_loss: 0.1435\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2354 - val_loss: 0.5427\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3996 - val_loss: 0.4136\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2334 - val_loss: 0.2761\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3525 - val_loss: 0.3203\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2731 - val_loss: 0.1610\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2235 - val_loss: 0.1414\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4245 - val_loss: 0.5022\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1756 - val_loss: 0.1995\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3290 - val_loss: 0.1117\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2047 - val_loss: 0.5182\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3797 - val_loss: 0.3800\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3877 - val_loss: 0.1563\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2226 - val_loss: 0.2713\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3957 - val_loss: 0.1567\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2038 - val_loss: 0.2352\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1929 - val_loss: 0.1881\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4308 - val_loss: 0.5979\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2479 - val_loss: 0.1695\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2343 - val_loss: 0.5339\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2160 - val_loss: 0.3962\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3107 - val_loss: 0.2308\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1763 - val_loss: 0.2475\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3894 - val_loss: 0.2545\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2343 - val_loss: 0.2426\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4167 - val_loss: 0.6096\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1821 - val_loss: 0.1228\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1706 - val_loss: 0.3615\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2363 - val_loss: 0.3918\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2435 - val_loss: 0.1484\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2269 - val_loss: 0.2849\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1990 - val_loss: 0.4028\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4046 - val_loss: 0.4665\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1680 - val_loss: 0.2002\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2314 - val_loss: 0.4109\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2324 - val_loss: 0.1189\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2383 - val_loss: 0.5637\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3714 - val_loss: 0.6212\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1989 - val_loss: 0.1044\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1505 - val_loss: 0.2032\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1752 - val_loss: 0.2674\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1972 - val_loss: 0.6298\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2431 - val_loss: 0.1705\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2595 - val_loss: 0.1793\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3956 - val_loss: 1.8601\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5820 - val_loss: 0.1314\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0944 - val_loss: 0.0443\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1001 - val_loss: 0.0478\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0841 - val_loss: 0.1055\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1276 - val_loss: 0.0900\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1203 - val_loss: 0.1773\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1777 - val_loss: 0.2350\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1643 - val_loss: 0.1069\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2003 - val_loss: 0.4221\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3505 - val_loss: 0.4471\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1496 - val_loss: 0.1136\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1068 - val_loss: 0.1371\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2842 - val_loss: 0.4705\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1684 - val_loss: 0.1356\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1193 - val_loss: 0.6211\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1866 - val_loss: 0.4746\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3487 - val_loss: 0.5630\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1921 - val_loss: 0.3447\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1520 - val_loss: 0.2056\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1900 - val_loss: 0.4478\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3481 - val_loss: 0.3162\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3746 - val_loss: 0.0904\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1393 - val_loss: 0.2545\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1706 - val_loss: 0.1868\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1894 - val_loss: 0.1327\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3605 - val_loss: 0.2505\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1427 - val_loss: 0.1861\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1367 - val_loss: 1.1739\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2498 - val_loss: 0.1729\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1171 - val_loss: 0.0853\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0941 - val_loss: 0.1107\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1122 - val_loss: 0.1035\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.04340187008549696\n",
      "Mean Absolute Error (MAE): 0.1548725742711536\n",
      "Root Mean Squared Error (RMSE): 0.20833115486046958\n",
      "Time taken: 528.2452960014343\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1049.8456 - val_loss: 788.1287\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 594.6522 - val_loss: 510.6976\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 324.8636 - val_loss: 241.3105\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 150.3424 - val_loss: 104.8351\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 60.8491 - val_loss: 41.0018\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.7461 - val_loss: 16.8483\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 14.2652 - val_loss: 9.1719\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.9568 - val_loss: 8.9614\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 6.8002 - val_loss: 3.9636\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.9196 - val_loss: 3.4319\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.9048 - val_loss: 3.4508\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.7177 - val_loss: 2.3017\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2887 - val_loss: 2.3669\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.5664 - val_loss: 1.6841\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.6373 - val_loss: 5.1683\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.1479 - val_loss: 1.6935\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.7867 - val_loss: 1.0258\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.7565 - val_loss: 1.6299\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6269 - val_loss: 0.9240\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9233 - val_loss: 3.9828\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5498 - val_loss: 0.9289\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4808 - val_loss: 1.2776\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.8176 - val_loss: 1.2658\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6836 - val_loss: 1.6250\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2199 - val_loss: 1.1513\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8415 - val_loss: 0.9097\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2195 - val_loss: 0.6334\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3792 - val_loss: 0.7785\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1603 - val_loss: 3.1194\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9655 - val_loss: 0.3921\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7071 - val_loss: 0.7431\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0929 - val_loss: 1.4693\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1963 - val_loss: 1.8418\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4964 - val_loss: 2.0322\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7681 - val_loss: 1.2979\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7981 - val_loss: 0.8330\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6643 - val_loss: 0.4059\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2099 - val_loss: 0.8411\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7168 - val_loss: 0.4814\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2255 - val_loss: 0.8133\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6454 - val_loss: 0.8051\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7394 - val_loss: 0.7357\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6696 - val_loss: 1.1543\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2302 - val_loss: 1.4149\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6965 - val_loss: 0.3360\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6413 - val_loss: 1.2337\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7824 - val_loss: 0.4838\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5739 - val_loss: 3.0047\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8898 - val_loss: 0.6412\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4873 - val_loss: 0.5180\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4365 - val_loss: 0.3986\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7626 - val_loss: 0.7619\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4203 - val_loss: 0.3119\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3536 - val_loss: 0.2629\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3438 - val_loss: 0.1910\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4871 - val_loss: 0.2364\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3904 - val_loss: 0.5104\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8169 - val_loss: 0.9715\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5224 - val_loss: 0.2070\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4749 - val_loss: 0.4733\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7280 - val_loss: 0.9066\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5099 - val_loss: 0.3393\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6216 - val_loss: 0.3514\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5988 - val_loss: 0.3733\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3506 - val_loss: 0.6036\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7103 - val_loss: 1.4508\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7549 - val_loss: 1.8816\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7867 - val_loss: 1.0536\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3875 - val_loss: 0.3545\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3990 - val_loss: 0.4513\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4054 - val_loss: 0.4872\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2579 - val_loss: 0.1339\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4286 - val_loss: 0.4545\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8174 - val_loss: 0.8968\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4224 - val_loss: 1.8432\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5443 - val_loss: 0.1643\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2806 - val_loss: 0.1665\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0701 - val_loss: 0.3606\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1842 - val_loss: 0.1513\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2016 - val_loss: 0.2049\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1832 - val_loss: 0.2498\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3259 - val_loss: 0.6506\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5297 - val_loss: 0.7729\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2689 - val_loss: 0.5843\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3711 - val_loss: 0.3335\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3903 - val_loss: 0.6540\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5262 - val_loss: 0.3599\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3959 - val_loss: 0.5642\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5569 - val_loss: 0.3714\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3013 - val_loss: 0.1738\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2437 - val_loss: 0.1962\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2897 - val_loss: 1.0551\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4225 - val_loss: 0.5007\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4354 - val_loss: 0.8605\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3028 - val_loss: 0.3746\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2941 - val_loss: 0.4454\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5145 - val_loss: 0.2164\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2229 - val_loss: 0.3625\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7924 - val_loss: 0.7330\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4130 - val_loss: 0.1953\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1545 - val_loss: 0.1623\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3383 - val_loss: 0.3836\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.13464473067638624\n",
      "Mean Absolute Error (MAE): 0.27485054568159767\n",
      "Root Mean Squared Error (RMSE): 0.3669396826133503\n",
      "Time taken: 293.99893450737\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 986.8035 - val_loss: 632.3146\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 493.6151 - val_loss: 338.5167\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 282.2793 - val_loss: 187.6338\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 143.4003 - val_loss: 79.3192\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 60.9549 - val_loss: 37.0131\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 27.3910 - val_loss: 15.2448\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.1210 - val_loss: 9.6121\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.7354 - val_loss: 5.2335\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.1858 - val_loss: 5.8060\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.6876 - val_loss: 4.1609\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0027 - val_loss: 2.0785\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8109 - val_loss: 2.9023\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4239 - val_loss: 3.0912\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7852 - val_loss: 1.2485\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6165 - val_loss: 1.0733\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4990 - val_loss: 0.8411\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0536 - val_loss: 0.8358\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8864 - val_loss: 2.0944\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2693 - val_loss: 2.1089\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8400 - val_loss: 1.6906\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6405 - val_loss: 2.0632\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5157 - val_loss: 1.4638\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4314 - val_loss: 1.6074\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0967 - val_loss: 1.6474\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2252 - val_loss: 1.1953\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0911 - val_loss: 1.8395\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.4876 - val_loss: 2.4502\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.0335 - val_loss: 1.1563\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.0949 - val_loss: 0.8302\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 1.1873 - val_loss: 1.0311\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.8454 - val_loss: 0.5676\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.0395 - val_loss: 2.5086\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9619 - val_loss: 1.9214\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8859 - val_loss: 1.2128\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8993 - val_loss: 0.3730\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.9806 - val_loss: 0.6833\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.8943 - val_loss: 2.1569\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9938 - val_loss: 1.4217\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 2.5439 - val_loss: 0.7183\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6658 - val_loss: 0.4980\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4407 - val_loss: 0.5974\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6180 - val_loss: 0.6235\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.7296 - val_loss: 0.7371\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.7723 - val_loss: 0.4646\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5189 - val_loss: 0.7898\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.8634 - val_loss: 0.9982\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.7456 - val_loss: 1.9615\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8539 - val_loss: 0.8787\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6417 - val_loss: 0.4193\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6299 - val_loss: 0.9933\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7471 - val_loss: 0.8941\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5406 - val_loss: 0.4753\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5630 - val_loss: 0.6608\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6618 - val_loss: 0.5790\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.6451 - val_loss: 0.3823\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.6740 - val_loss: 0.3419\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.6740 - val_loss: 0.4362\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.6968 - val_loss: 0.3255\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5139 - val_loss: 0.6079\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 8s 23ms/step - loss: 0.5214 - val_loss: 0.6276\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3987 - val_loss: 1.0183\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.5753 - val_loss: 0.5055\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4255 - val_loss: 1.3308\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 0.7706 - val_loss: 0.4166\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3434 - val_loss: 0.3621\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 8s 23ms/step - loss: 1.6328 - val_loss: 15.7297\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 1.8215 - val_loss: 0.2883\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 0.3597 - val_loss: 0.4273\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3449 - val_loss: 0.2255\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.2552 - val_loss: 0.1782\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2957 - val_loss: 0.1123\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3107 - val_loss: 1.0956\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2597 - val_loss: 0.1738\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3786 - val_loss: 0.2486\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4333 - val_loss: 0.5448\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.4005 - val_loss: 1.3440\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5193 - val_loss: 1.5144\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.5267 - val_loss: 0.6191\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5222 - val_loss: 0.6337\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5522 - val_loss: 1.3394\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5415 - val_loss: 0.4628\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 8s 24ms/step - loss: 0.2533 - val_loss: 0.4089\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2550 - val_loss: 0.2428\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.4186 - val_loss: 0.2442\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.6105 - val_loss: 0.3160\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.4451 - val_loss: 0.5680\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 8s 23ms/step - loss: 0.3491 - val_loss: 0.9824\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 7s 23ms/step - loss: 0.2251 - val_loss: 0.3657\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4065 - val_loss: 0.3206\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 0.4579 - val_loss: 0.7527\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.4786 - val_loss: 0.3359\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3897 - val_loss: 0.2355\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3754 - val_loss: 0.4741\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3400 - val_loss: 0.2825\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4265 - val_loss: 0.4340\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2527 - val_loss: 0.3269\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 8s 23ms/step - loss: 0.3331 - val_loss: 0.4790\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.8034 - val_loss: 0.6559\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2863 - val_loss: 0.1206\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.1764 - val_loss: 0.2991\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2631 - val_loss: 0.4146\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.11150839626578714\n",
      "Mean Absolute Error (MAE): 0.255068767091684\n",
      "Root Mean Squared Error (RMSE): 0.3339287293207746\n",
      "Time taken: 586.9092056751251\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 23ms/step - loss: 1039.2572 - val_loss: 795.9542\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 593.9728 - val_loss: 482.2775\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 314.0672 - val_loss: 226.0395\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 139.8003 - val_loss: 94.6330\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 53.8668 - val_loss: 32.3312\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 25.2666 - val_loss: 18.0110\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 13.9176 - val_loss: 7.0467\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 9.5700 - val_loss: 10.2700\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 6.7582 - val_loss: 10.6943\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 5.0731 - val_loss: 3.0458\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 5.2408 - val_loss: 5.4487\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.4792 - val_loss: 2.0924\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.8301 - val_loss: 2.2594\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.7890 - val_loss: 1.7889\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.5779 - val_loss: 8.8125\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.6105 - val_loss: 1.4915\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7830 - val_loss: 1.8317\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.6409 - val_loss: 0.9882\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.7891 - val_loss: 1.7583\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.9341 - val_loss: 1.9347\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.0329 - val_loss: 0.9385\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.7007 - val_loss: 1.6291\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.1966 - val_loss: 0.8643\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4958 - val_loss: 0.9844\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.5561 - val_loss: 3.8400\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2677 - val_loss: 4.7622\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 5.3138 - val_loss: 2.2881\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9722 - val_loss: 0.8084\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8425 - val_loss: 0.5623\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8006 - val_loss: 0.5247\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8813 - val_loss: 0.7177\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3176 - val_loss: 4.0562\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1142 - val_loss: 0.5791\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9561 - val_loss: 1.7628\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9610 - val_loss: 2.6837\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3774 - val_loss: 0.9553\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8546 - val_loss: 0.8277\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1174 - val_loss: 1.2743\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9434 - val_loss: 1.7527\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.0051 - val_loss: 0.6462\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5033 - val_loss: 0.4426\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3696 - val_loss: 1.9753\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1557 - val_loss: 4.1053\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4853 - val_loss: 0.5428\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6839 - val_loss: 1.2613\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6411 - val_loss: 0.7743\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9013 - val_loss: 0.7850\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6590 - val_loss: 0.5237\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5031 - val_loss: 1.8535\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6238 - val_loss: 1.5903\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.8571 - val_loss: 0.7811\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.6717 - val_loss: 0.8758\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.7905 - val_loss: 10.4047\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.0835 - val_loss: 0.5266\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.5086 - val_loss: 0.3966\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.3685 - val_loss: 0.4837\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6611 - val_loss: 0.3855\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.4282 - val_loss: 0.4846\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.3659 - val_loss: 0.2454\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7506 - val_loss: 1.9358\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.8243 - val_loss: 0.9169\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.6062 - val_loss: 0.3551\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3440 - val_loss: 0.4303\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3813 - val_loss: 0.7098\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5418 - val_loss: 0.6046\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.7359 - val_loss: 1.0778\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.7817 - val_loss: 0.9073\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4241 - val_loss: 0.6558\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4643 - val_loss: 0.3146\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4555 - val_loss: 0.1807\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.5225 - val_loss: 0.5399\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4326 - val_loss: 1.4368\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.5088 - val_loss: 1.6622\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9636 - val_loss: 0.4507\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.4807 - val_loss: 0.7358\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.6064 - val_loss: 0.2582\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3570 - val_loss: 0.9387\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.4451 - val_loss: 0.5279\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4823 - val_loss: 0.3545\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4721 - val_loss: 0.2893\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2859 - val_loss: 0.4013\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.4304 - val_loss: 2.5776\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.5760 - val_loss: 0.5822\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2532 - val_loss: 0.3477\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.1952 - val_loss: 0.1159\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2906 - val_loss: 0.2315\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3472 - val_loss: 0.3285\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2391 - val_loss: 0.3139\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2919 - val_loss: 0.3966\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5886 - val_loss: 0.3472\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.7097 - val_loss: 0.2039\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3131 - val_loss: 0.1604\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6345 - val_loss: 2.1690\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3492 - val_loss: 0.3116\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2501 - val_loss: 0.7386\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.4404 - val_loss: 0.7197\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3093 - val_loss: 0.1887\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3740 - val_loss: 0.5595\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5463 - val_loss: 0.8842\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2714 - val_loss: 0.2485\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6801 - val_loss: 0.4768\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3000 - val_loss: 0.4663\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2311 - val_loss: 0.1121\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2706 - val_loss: 0.4265\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4728 - val_loss: 0.6989\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3774 - val_loss: 0.3067\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2135 - val_loss: 0.3180\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5302 - val_loss: 0.4058\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3568 - val_loss: 0.5979\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2910 - val_loss: 0.5373\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4322 - val_loss: 0.7128\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2370 - val_loss: 0.1792\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2832 - val_loss: 0.1979\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.2154 - val_loss: 0.2323\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 8s 26ms/step - loss: 0.3571 - val_loss: 0.2990\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4834 - val_loss: 0.7469\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.2388 - val_loss: 0.2761\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2712 - val_loss: 1.3338\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3866 - val_loss: 0.3287\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.2359 - val_loss: 0.2491\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2272 - val_loss: 0.3482\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3180 - val_loss: 0.2010\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3682 - val_loss: 0.1402\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3239 - val_loss: 0.3155\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3756 - val_loss: 0.2586\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.1821 - val_loss: 0.2017\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3605 - val_loss: 0.8019\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.3000 - val_loss: 0.2163\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.3339 - val_loss: 0.3406\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2378 - val_loss: 0.2886\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.1853 - val_loss: 0.2636\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5925 - val_loss: 0.3644\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2672 - val_loss: 0.1894\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.11176221072912507\n",
      "Mean Absolute Error (MAE): 0.24370620427842873\n",
      "Root Mean Squared Error (RMSE): 0.33430855617097965\n",
      "Time taken: 865.3228409290314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_7564\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.087843  0.214951  0.296383  539.941964\n",
      "1        2  0.043402  0.154873  0.208331  528.245296\n",
      "2        3  0.134645  0.274851  0.366940  293.998935\n",
      "3        4  0.111508  0.255069  0.333929  586.909206\n",
      "4        5  0.111762  0.243706  0.334309  865.322841\n",
      "5  Average  0.097832  0.228690  0.307978  562.883648\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ca437",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
