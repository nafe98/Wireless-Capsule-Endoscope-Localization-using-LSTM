{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 10ms/step - loss: 1002.5510 - val_loss: 659.8964\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 486.9336 - val_loss: 337.4340\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 234.9415 - val_loss: 161.9333\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 100.8110 - val_loss: 69.8448\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 37.9335 - val_loss: 33.9716\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.8785 - val_loss: 10.2804\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.7575 - val_loss: 5.3859\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.8694 - val_loss: 10.5822\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.3230 - val_loss: 6.7654\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.5471 - val_loss: 2.0711\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.0103 - val_loss: 2.0547\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0796 - val_loss: 3.4810\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 8.7497 - val_loss: 2.0261\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7899 - val_loss: 2.1438\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8137 - val_loss: 1.5196\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.8245 - val_loss: 1.7454\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1233 - val_loss: 1.5053\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7859 - val_loss: 0.9227\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3124 - val_loss: 3.8190\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6640 - val_loss: 1.8071\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1316 - val_loss: 2.4402\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.5461 - val_loss: 2.5482\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3316 - val_loss: 1.9236\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.9827 - val_loss: 2.2146\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1432 - val_loss: 0.7148\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.2107 - val_loss: 1.7544\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4246 - val_loss: 1.0741\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.9925 - val_loss: 0.6462\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.3835 - val_loss: 1.8674\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.3397 - val_loss: 0.7101\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.4139 - val_loss: 0.6955\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8907 - val_loss: 0.3844\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8059 - val_loss: 1.7259\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 3.4668 - val_loss: 0.4020\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6249 - val_loss: 0.5168\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5376 - val_loss: 1.0925\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5462 - val_loss: 0.9708\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6044 - val_loss: 0.5378\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7371 - val_loss: 0.5454\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5294 - val_loss: 0.6666\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.9678 - val_loss: 2.6334\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.6893 - val_loss: 0.4144\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.7678 - val_loss: 2.2667\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6988 - val_loss: 0.4273\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5391 - val_loss: 0.6549\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8890 - val_loss: 1.3617\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4821 - val_loss: 0.5508\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3873 - val_loss: 0.3356\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2696 - val_loss: 0.3532\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4522 - val_loss: 0.2738\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7234 - val_loss: 0.8993\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5498 - val_loss: 0.5825\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5342 - val_loss: 0.3890\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6995 - val_loss: 0.5629\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4408 - val_loss: 0.3571\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6909 - val_loss: 4.0699\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6405 - val_loss: 0.7344\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7344 - val_loss: 0.7408\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5280 - val_loss: 0.9136\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5279 - val_loss: 0.5723\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4298 - val_loss: 0.6362\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4165 - val_loss: 0.6183\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.0635 - val_loss: 0.6466\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4705 - val_loss: 0.2657\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3595 - val_loss: 0.7082\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6048 - val_loss: 1.0435\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4464 - val_loss: 0.9341\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4508 - val_loss: 0.9112\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3887 - val_loss: 0.3985\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5079 - val_loss: 0.7429\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7985 - val_loss: 1.2387\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5121 - val_loss: 0.6204\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4940 - val_loss: 0.8377\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3957 - val_loss: 0.3862\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2924 - val_loss: 0.2026\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4840 - val_loss: 0.5933\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6937 - val_loss: 0.3122\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3875 - val_loss: 0.5808\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4674 - val_loss: 0.3947\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2924 - val_loss: 0.2688\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3266 - val_loss: 0.9483\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5699 - val_loss: 0.2465\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2602 - val_loss: 0.3899\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5058 - val_loss: 0.3374\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7264 - val_loss: 0.1965\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2387 - val_loss: 0.2082\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3437 - val_loss: 0.6777\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3240 - val_loss: 0.2614\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3789 - val_loss: 0.4408\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4427 - val_loss: 0.8045\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3533 - val_loss: 0.2716\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3242 - val_loss: 0.8924\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3504 - val_loss: 0.4426\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2609 - val_loss: 0.1963\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2999 - val_loss: 0.4379\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3401 - val_loss: 0.9402\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4400 - val_loss: 0.4802\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2882 - val_loss: 0.2820\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3014 - val_loss: 0.2859\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3244 - val_loss: 0.1479\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4517 - val_loss: 0.3381\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2769 - val_loss: 0.1360\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3146 - val_loss: 0.4638\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3052 - val_loss: 0.3476\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1836 - val_loss: 0.4276\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5407 - val_loss: 0.6310\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2283 - val_loss: 0.2361\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2445 - val_loss: 0.2352\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3608 - val_loss: 0.1702\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1822 - val_loss: 0.3231\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3736 - val_loss: 0.6160\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2224 - val_loss: 0.3120\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2803 - val_loss: 0.4090\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4854 - val_loss: 3.7639\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2505 - val_loss: 0.1374\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2471 - val_loss: 0.2360\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2997 - val_loss: 0.3977\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1733 - val_loss: 0.4781\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2952 - val_loss: 0.1875\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2536 - val_loss: 0.2774\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2870 - val_loss: 0.5939\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2211 - val_loss: 0.2255\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3253 - val_loss: 0.3279\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2295 - val_loss: 0.2054\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1939 - val_loss: 0.5632\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3454 - val_loss: 0.3152\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4530 - val_loss: 0.4392\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1954 - val_loss: 0.0924\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2398 - val_loss: 0.2949\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2293 - val_loss: 0.9349\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2212 - val_loss: 0.1398\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3577 - val_loss: 0.6179\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2278 - val_loss: 0.2104\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2357 - val_loss: 0.3337\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2235 - val_loss: 0.2500\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3699 - val_loss: 0.2190\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2220 - val_loss: 0.1729\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2962 - val_loss: 0.3604\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3711 - val_loss: 0.1397\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1581 - val_loss: 0.1798\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2121 - val_loss: 0.4272\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1770 - val_loss: 0.3453\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3441 - val_loss: 0.1445\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1977 - val_loss: 0.2858\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2539 - val_loss: 0.2467\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2969 - val_loss: 0.1667\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1774 - val_loss: 0.3747\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2774 - val_loss: 0.0946\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2202 - val_loss: 0.5949\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2739 - val_loss: 0.1625\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2588 - val_loss: 0.3025\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2231 - val_loss: 0.1793\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2180 - val_loss: 1.2310\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2469 - val_loss: 0.1619\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1684 - val_loss: 0.1222\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3456 - val_loss: 2.3852\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2483 - val_loss: 0.1431\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1445 - val_loss: 0.3182\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2529 - val_loss: 0.2945\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1768 - val_loss: 0.1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1957 - val_loss: 0.2376\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3265 - val_loss: 0.3323\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1395 - val_loss: 0.1278\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3061 - val_loss: 0.1898\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1969 - val_loss: 0.1206\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2313 - val_loss: 0.3352\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1558 - val_loss: 0.2089\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2327 - val_loss: 0.1277\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2018 - val_loss: 0.3007\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1481 - val_loss: 0.2048\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1698 - val_loss: 0.2875\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1740 - val_loss: 0.1949\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2526 - val_loss: 0.1954\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2425 - val_loss: 0.1501\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1239 - val_loss: 0.3839\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1934 - val_loss: 0.2170\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2842 - val_loss: 0.1771\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1888 - val_loss: 0.2876\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1613 - val_loss: 0.1542\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2709 - val_loss: 0.3103\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1999 - val_loss: 0.0986\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1428 - val_loss: 0.4712\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2169 - val_loss: 3.6570\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2893 - val_loss: 0.3094\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2638 - val_loss: 0.1536\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1473 - val_loss: 0.3083\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2113 - val_loss: 0.1241\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1119 - val_loss: 0.3287\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1890 - val_loss: 0.2413\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2360 - val_loss: 0.2389\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2523 - val_loss: 0.3425\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2021 - val_loss: 0.1469\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1242 - val_loss: 0.1228\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1987 - val_loss: 0.1199\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1099 - val_loss: 0.1341\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2370 - val_loss: 0.1230\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1392 - val_loss: 0.3421\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2018 - val_loss: 0.3631\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1836 - val_loss: 0.2416\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2135 - val_loss: 0.2177\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.21709359185659752\n",
      "Mean Absolute Error (MAE): 0.32735298004257823\n",
      "Root Mean Squared Error (RMSE): 0.465933033661059\n",
      "Time taken: 660.9674608707428\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 11ms/step - loss: 949.1890 - val_loss: 651.2795\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 486.2234 - val_loss: 397.8288\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 260.0258 - val_loss: 197.4357\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 118.5357 - val_loss: 75.3913\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 46.1341 - val_loss: 41.0614\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 20.4702 - val_loss: 13.9257\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 12.8007 - val_loss: 5.6632\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 7.7760 - val_loss: 3.7364\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 5.8097 - val_loss: 3.9785\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.8771 - val_loss: 2.9891\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.7273 - val_loss: 1.5408\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.9483 - val_loss: 1.3673\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7142 - val_loss: 3.8243\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7031 - val_loss: 5.9972\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.0332 - val_loss: 0.9131\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.8073 - val_loss: 3.2813\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.9002 - val_loss: 1.2758\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.5466 - val_loss: 1.1077\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8664 - val_loss: 1.3969\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.5230 - val_loss: 1.0353\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0711 - val_loss: 2.8484\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.9382 - val_loss: 3.6133\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2808 - val_loss: 1.5865\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.5459 - val_loss: 0.7420\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.1499 - val_loss: 0.8873\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9952 - val_loss: 1.3422\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2853 - val_loss: 1.2888\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7773 - val_loss: 0.8238\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8781 - val_loss: 0.5414\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4142 - val_loss: 1.4982\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1544 - val_loss: 0.4676\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2085 - val_loss: 0.2964\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8237 - val_loss: 1.0430\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2497 - val_loss: 1.3375\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0794 - val_loss: 0.5872\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.0219 - val_loss: 9.8319\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6923 - val_loss: 0.3912\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4556 - val_loss: 0.5711\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6047 - val_loss: 1.3083\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5563 - val_loss: 0.5035\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7810 - val_loss: 1.4106\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9352 - val_loss: 1.7566\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9040 - val_loss: 0.4657\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5301 - val_loss: 0.5375\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.9130 - val_loss: 1.0059\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6842 - val_loss: 0.7212\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6382 - val_loss: 0.5548\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5794 - val_loss: 0.5890\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7865 - val_loss: 0.7962\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7904 - val_loss: 0.5441\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7200 - val_loss: 0.2785\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5751 - val_loss: 1.7678\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8122 - val_loss: 0.7158\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7756 - val_loss: 0.3687\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4723 - val_loss: 0.7037\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.9675 - val_loss: 5.1593\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7312 - val_loss: 0.4362\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2779 - val_loss: 0.3435\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3942 - val_loss: 0.4735\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4812 - val_loss: 0.6655\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4358 - val_loss: 0.4361\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4223 - val_loss: 0.5094\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4459 - val_loss: 0.6953\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6832 - val_loss: 0.6696\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5932 - val_loss: 0.2912\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5345 - val_loss: 0.3530\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4677 - val_loss: 0.6049\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4186 - val_loss: 0.7336\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6893 - val_loss: 1.4920\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3693 - val_loss: 0.7580\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7530 - val_loss: 2.2383\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7973 - val_loss: 0.7225\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3249 - val_loss: 0.2582\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2376 - val_loss: 0.2070\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2772 - val_loss: 0.3824\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3394 - val_loss: 0.2315\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2757 - val_loss: 0.4579\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5390 - val_loss: 0.3699\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5829 - val_loss: 0.4104\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3709 - val_loss: 0.2782\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5064 - val_loss: 0.2441\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3592 - val_loss: 0.2307\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4306 - val_loss: 0.4404\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4572 - val_loss: 0.3882\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4144 - val_loss: 0.3032\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3027 - val_loss: 0.6795\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4054 - val_loss: 0.7169\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4426 - val_loss: 0.3612\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3365 - val_loss: 0.7820\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4474 - val_loss: 9.9553\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6703 - val_loss: 0.2115\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1973 - val_loss: 0.1315\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2520 - val_loss: 0.1924\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2126 - val_loss: 0.2591\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3194 - val_loss: 0.3730\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3081 - val_loss: 0.3661\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2990 - val_loss: 0.2645\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5455 - val_loss: 0.4386\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4336 - val_loss: 0.7932\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3905 - val_loss: 0.2071\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2580 - val_loss: 0.5548\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4537 - val_loss: 0.2713\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2591 - val_loss: 0.2043\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3328 - val_loss: 0.3162\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2051 - val_loss: 0.4693\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3216 - val_loss: 0.2254\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2903 - val_loss: 1.2386\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5822 - val_loss: 0.7739\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3211 - val_loss: 0.0997\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1887 - val_loss: 0.1863\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3581 - val_loss: 0.5888\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4066 - val_loss: 0.1807\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1988 - val_loss: 0.1613\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3133 - val_loss: 0.3528\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3462 - val_loss: 0.6949\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4345 - val_loss: 0.2407\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1840 - val_loss: 0.2093\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2712 - val_loss: 1.4403\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3778 - val_loss: 1.4678\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2479 - val_loss: 0.2394\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2366 - val_loss: 0.1701\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4639 - val_loss: 2.7347\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2736 - val_loss: 0.2073\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3262 - val_loss: 0.7485\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1873 - val_loss: 0.8099\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2616 - val_loss: 0.4620\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2904 - val_loss: 0.1570\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2829 - val_loss: 0.5611\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3314 - val_loss: 0.3050\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1852 - val_loss: 0.2727\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2228 - val_loss: 0.3992\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3140 - val_loss: 0.2483\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2931 - val_loss: 0.1893\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1730 - val_loss: 0.2011\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6255 - val_loss: 0.1383\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1896 - val_loss: 0.1559\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2414 - val_loss: 0.2055\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1658 - val_loss: 0.3257\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2983 - val_loss: 0.2750\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2809 - val_loss: 0.1481\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1584 - val_loss: 0.4158\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3540 - val_loss: 0.2414\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2463 - val_loss: 0.2053\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2556 - val_loss: 0.2587\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2765 - val_loss: 0.2443\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2242 - val_loss: 0.3315\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2050 - val_loss: 0.2792\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2553 - val_loss: 0.4340\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3885 - val_loss: 0.2864\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2126 - val_loss: 0.1169\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1803 - val_loss: 0.2212\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2079 - val_loss: 0.1791\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2382 - val_loss: 0.1933\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6254 - val_loss: 0.9159\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1786 - val_loss: 0.1299\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0929 - val_loss: 0.0760\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1075 - val_loss: 0.1154\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1184 - val_loss: 0.2335\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2603 - val_loss: 0.2654\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1165 - val_loss: 0.0694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1954 - val_loss: 0.3174\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2252 - val_loss: 0.2934\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1840 - val_loss: 0.9672\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3581 - val_loss: 0.1971\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2417 - val_loss: 0.1451\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1368 - val_loss: 0.1252\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1710 - val_loss: 0.6629\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3842 - val_loss: 0.3079\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2231 - val_loss: 0.1325\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8389 - val_loss: 1.2594\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1800 - val_loss: 0.2679\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1053 - val_loss: 0.1426\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1593 - val_loss: 0.1475\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1076 - val_loss: 0.1024\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1712 - val_loss: 0.7079\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2335 - val_loss: 0.2068\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1645 - val_loss: 0.4548\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3194 - val_loss: 0.2248\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1659 - val_loss: 0.2537\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2951 - val_loss: 0.3430\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2432 - val_loss: 0.3271\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1421 - val_loss: 0.1270\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3798 - val_loss: 0.3297\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1964 - val_loss: 0.5560\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2545 - val_loss: 0.1243\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1105 - val_loss: 0.2044\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2789 - val_loss: 0.7155\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1720 - val_loss: 0.1416\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1545 - val_loss: 0.3222\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2226 - val_loss: 0.3350\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2965 - val_loss: 0.1892\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1392 - val_loss: 0.3767\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1739 - val_loss: 0.0981\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2838 - val_loss: 0.1804\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1607 - val_loss: 0.1218\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1269 - val_loss: 0.4682\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1966 - val_loss: 0.3249\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1652 - val_loss: 0.0875\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1829 - val_loss: 0.1575\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2294 - val_loss: 1.8095\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 1.8165383902828225\n",
      "Mean Absolute Error (MAE): 0.9464018388443373\n",
      "Root Mean Squared Error (RMSE): 1.3477901877825134\n",
      "Time taken: 667.4296963214874\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 11ms/step - loss: 921.4457 - val_loss: 552.3377\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 410.5737 - val_loss: 346.2393\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 232.6226 - val_loss: 185.0632\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 103.5469 - val_loss: 72.2653\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 45.0265 - val_loss: 33.0159\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 18.6251 - val_loss: 13.3151\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 11.6537 - val_loss: 6.2018\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.2658 - val_loss: 7.2502\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.2533 - val_loss: 4.2026\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.0744 - val_loss: 2.7710\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.8406 - val_loss: 2.0999\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5246 - val_loss: 1.7726\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7066 - val_loss: 2.8552\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.2183 - val_loss: 2.8319\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3133 - val_loss: 2.9077\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1589 - val_loss: 1.5710\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7373 - val_loss: 1.2668\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7284 - val_loss: 2.7222\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.2427 - val_loss: 5.8138\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.6764 - val_loss: 1.4545\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4388 - val_loss: 0.8208\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0780 - val_loss: 1.3393\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.4485 - val_loss: 1.7437\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.7370 - val_loss: 1.8587\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3749 - val_loss: 2.0390\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0404 - val_loss: 1.0992\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1506 - val_loss: 1.4119\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1240 - val_loss: 1.3090\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8830 - val_loss: 3.6658\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.8801 - val_loss: 1.2754\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6646 - val_loss: 0.4623\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6100 - val_loss: 0.6120\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6880 - val_loss: 0.7003\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9441 - val_loss: 0.7425\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8124 - val_loss: 0.5451\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8934 - val_loss: 1.2550\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7267 - val_loss: 0.5903\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6955 - val_loss: 0.8851\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0888 - val_loss: 0.8297\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.5122 - val_loss: 3.2575\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6364 - val_loss: 0.3470\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8053 - val_loss: 0.4211\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8111 - val_loss: 1.2223\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6389 - val_loss: 0.8443\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8840 - val_loss: 1.9784\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9553 - val_loss: 0.4898\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6180 - val_loss: 0.4601\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7652 - val_loss: 2.4839\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8645 - val_loss: 1.4387\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.5106 - val_loss: 2.6610\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8019 - val_loss: 0.7655\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3564 - val_loss: 0.4969\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4538 - val_loss: 0.6635\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5559 - val_loss: 0.4631\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5740 - val_loss: 0.5329\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6279 - val_loss: 0.6515\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6077 - val_loss: 0.7577\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6011 - val_loss: 1.3943\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1680 - val_loss: 0.9565\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4767 - val_loss: 0.8057\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6412 - val_loss: 0.5311\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4695 - val_loss: 0.3050\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6990 - val_loss: 0.9809\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4565 - val_loss: 0.2267\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5588 - val_loss: 0.4055\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4474 - val_loss: 0.8668\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9212 - val_loss: 1.5333\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3977 - val_loss: 0.8589\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3247 - val_loss: 0.1519\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4619 - val_loss: 0.4077\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4067 - val_loss: 0.2301\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.9156 - val_loss: 0.5947\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3998 - val_loss: 0.4084\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2625 - val_loss: 0.8428\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4051 - val_loss: 0.6580\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4594 - val_loss: 0.9113\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5133 - val_loss: 0.3762\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5821 - val_loss: 0.2196\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3598 - val_loss: 0.1466\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2933 - val_loss: 0.3684\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4253 - val_loss: 0.6968\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3413 - val_loss: 0.3020\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6160 - val_loss: 3.3446\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7014 - val_loss: 0.3828\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4479 - val_loss: 0.3661\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2858 - val_loss: 0.1996\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4941 - val_loss: 0.5460\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5128 - val_loss: 0.3668\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3170 - val_loss: 0.3463\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3230 - val_loss: 0.4149\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5681 - val_loss: 0.5852\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3186 - val_loss: 0.2055\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3536 - val_loss: 0.4376\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3091 - val_loss: 0.5141\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4688 - val_loss: 0.9364\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3361 - val_loss: 0.5113\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4464 - val_loss: 0.2902\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2550 - val_loss: 0.2373\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4856 - val_loss: 1.3908\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4548 - val_loss: 0.4105\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2396 - val_loss: 0.3421\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5972 - val_loss: 0.2483\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2599 - val_loss: 0.5191\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6502 - val_loss: 0.1385\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3005 - val_loss: 0.1654\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3819 - val_loss: 0.3270\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4333 - val_loss: 0.4580\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2514 - val_loss: 0.4108\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3872 - val_loss: 0.2212\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2449 - val_loss: 0.3209\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3318 - val_loss: 0.3454\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3265 - val_loss: 0.5171\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2587 - val_loss: 0.2876\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2229 - val_loss: 0.4780\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3157 - val_loss: 0.4996\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4817 - val_loss: 0.3783\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3436 - val_loss: 0.1300\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2925 - val_loss: 0.3598\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 0.2675\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2515 - val_loss: 0.1439\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3230 - val_loss: 0.4839\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3057 - val_loss: 0.8324\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3092 - val_loss: 0.2488\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2964 - val_loss: 0.3978\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3512 - val_loss: 0.2092\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2018 - val_loss: 0.2408\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6118 - val_loss: 0.2339\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2413 - val_loss: 0.3985\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1501 - val_loss: 0.4342\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3040 - val_loss: 0.3743\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3491 - val_loss: 0.4862\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3030 - val_loss: 0.3904\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.5636 - val_loss: 1.0561\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1952 - val_loss: 0.2548\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1353 - val_loss: 0.1044\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1222 - val_loss: 0.4781\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2197 - val_loss: 0.3272\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1763 - val_loss: 0.1386\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3295 - val_loss: 0.5212\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2531 - val_loss: 0.3980\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1913 - val_loss: 0.2059\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3213 - val_loss: 0.2540\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2758 - val_loss: 0.3521\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1866 - val_loss: 0.5000\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2534 - val_loss: 0.3915\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2487 - val_loss: 0.2764\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4229 - val_loss: 0.4383\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2322 - val_loss: 0.1831\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1810 - val_loss: 0.2433\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5426 - val_loss: 1.0316\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1774 - val_loss: 0.1467\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1427 - val_loss: 0.2704\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2473 - val_loss: 0.5243\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3711 - val_loss: 2.0723\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2332 - val_loss: 0.2503\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1951 - val_loss: 0.2912\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2320 - val_loss: 0.7232\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2369 - val_loss: 1.3558\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2797 - val_loss: 0.2857\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2324 - val_loss: 0.1382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1896 - val_loss: 0.2019\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3686 - val_loss: 0.5844\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1958 - val_loss: 0.1751\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1760 - val_loss: 0.2208\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1503 - val_loss: 0.2090\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2180 - val_loss: 0.4445\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3539 - val_loss: 0.5966\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2263 - val_loss: 0.3431\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2563 - val_loss: 0.1614\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2079 - val_loss: 0.2094\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2844 - val_loss: 0.2837\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2875 - val_loss: 0.1460\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1985 - val_loss: 0.2970\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1919 - val_loss: 0.3543\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1990 - val_loss: 0.5328\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2802 - val_loss: 0.4435\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2338 - val_loss: 0.2549\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1979 - val_loss: 0.1271\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2557 - val_loss: 0.2654\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1912 - val_loss: 0.2725\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1938 - val_loss: 0.3145\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1513 - val_loss: 0.2035\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2385 - val_loss: 0.2594\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3129 - val_loss: 1.9093\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2045 - val_loss: 0.2080\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1558 - val_loss: 0.4045\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1901 - val_loss: 1.5120\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2290 - val_loss: 0.4575\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1694 - val_loss: 0.1195\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1866 - val_loss: 0.2436\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1849 - val_loss: 0.4509\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2301 - val_loss: 0.2888\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2701 - val_loss: 0.1196\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1305 - val_loss: 0.0877\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2239 - val_loss: 0.2350\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1566 - val_loss: 0.2856\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2091 - val_loss: 0.3041\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2959 - val_loss: 0.4113\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1520 - val_loss: 0.2572\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1711 - val_loss: 0.1419\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.14207498424913473\n",
      "Mean Absolute Error (MAE): 0.26579765654734716\n",
      "Root Mean Squared Error (RMSE): 0.37692835426528304\n",
      "Time taken: 665.0742435455322\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 10ms/step - loss: 1020.4621 - val_loss: 670.4594\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 506.0621 - val_loss: 395.7012\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 272.2363 - val_loss: 194.0356\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 124.8513 - val_loss: 64.8792\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 46.3612 - val_loss: 26.0797\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.7819 - val_loss: 15.5281\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.0296 - val_loss: 5.6269\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.5677 - val_loss: 4.2476\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.3023 - val_loss: 16.5223\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.3059 - val_loss: 2.2989\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.0900 - val_loss: 1.9500\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.1198 - val_loss: 2.7954\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5512 - val_loss: 2.1489\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3300 - val_loss: 1.4205\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4044 - val_loss: 1.1369\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.2771 - val_loss: 1.0763\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.6037 - val_loss: 1.2200\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.8354 - val_loss: 0.6285\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2966 - val_loss: 1.7893\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0015 - val_loss: 1.7887\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0319 - val_loss: 1.4541\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3444 - val_loss: 1.6882\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4075 - val_loss: 0.7457\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1334 - val_loss: 0.7868\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1908 - val_loss: 1.8425\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1285 - val_loss: 1.2771\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4285 - val_loss: 1.5034\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0582 - val_loss: 1.6783\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9973 - val_loss: 1.9390\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9302 - val_loss: 0.8603\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.0704 - val_loss: 2.5842\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.8911 - val_loss: 0.5483\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7074 - val_loss: 0.3207\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.5732 - val_loss: 2.7498\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7129 - val_loss: 1.5284\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5978 - val_loss: 0.6240\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.0413 - val_loss: 0.7234\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6336 - val_loss: 1.0167\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6930 - val_loss: 0.6340\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1.0853 - val_loss: 2.4790\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.9110 - val_loss: 0.5022\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7077 - val_loss: 2.0235\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7921 - val_loss: 0.5209\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6644 - val_loss: 2.3359\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5185 - val_loss: 0.5597\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6713 - val_loss: 0.8919\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7823 - val_loss: 1.2673\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6210 - val_loss: 0.6128\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6773 - val_loss: 1.2779\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6095 - val_loss: 0.2362\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4928 - val_loss: 0.2428\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 4.2055 - val_loss: 4.1759\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.9444 - val_loss: 0.5285\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4844 - val_loss: 1.0354\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5239 - val_loss: 0.5079\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4530 - val_loss: 0.4675\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.5798 - val_loss: 0.6813\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5641 - val_loss: 0.5052\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6339 - val_loss: 0.7852\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5218 - val_loss: 0.8907\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5380 - val_loss: 0.4393\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4995 - val_loss: 0.7679\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5550 - val_loss: 0.8005\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6636 - val_loss: 0.3831\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5170 - val_loss: 0.6057\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5911 - val_loss: 0.6848\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.7318 - val_loss: 0.3547\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5200 - val_loss: 0.4159\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4580 - val_loss: 2.5652\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5473 - val_loss: 0.5191\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4333 - val_loss: 0.5717\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5010 - val_loss: 0.3147\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4765 - val_loss: 0.2889\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3983 - val_loss: 1.0324\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5390 - val_loss: 0.4492\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5800 - val_loss: 0.4420\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3893 - val_loss: 0.3906\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3509 - val_loss: 0.2142\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5110 - val_loss: 0.3526\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5918 - val_loss: 0.5389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4166 - val_loss: 1.1789\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4635 - val_loss: 0.1847\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3098 - val_loss: 0.2582\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5325 - val_loss: 0.6995\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3749 - val_loss: 0.3286\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3021 - val_loss: 0.2184\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4533 - val_loss: 0.2620\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3592 - val_loss: 0.3144\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4263 - val_loss: 0.5154\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5641 - val_loss: 0.5860\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2905 - val_loss: 0.6714\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4341 - val_loss: 1.0703\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4451 - val_loss: 0.4023\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3629 - val_loss: 0.5803\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2785 - val_loss: 0.2549\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3645 - val_loss: 0.9127\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3273 - val_loss: 0.7012\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5602 - val_loss: 0.5473\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.9979 - val_loss: 0.5534\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2808 - val_loss: 0.1594\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2169 - val_loss: 0.5020\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2158 - val_loss: 0.2196\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2739 - val_loss: 0.9691\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4832 - val_loss: 0.2817\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4305 - val_loss: 0.4633\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2372 - val_loss: 0.3727\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2719 - val_loss: 0.5444\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.5366 - val_loss: 0.7176\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4457 - val_loss: 0.6333\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2214 - val_loss: 0.0894\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4178 - val_loss: 0.1915\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3018 - val_loss: 0.5238\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3808 - val_loss: 0.6391\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3197 - val_loss: 0.1895\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2107 - val_loss: 0.2801\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2198 - val_loss: 1.1899\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3869 - val_loss: 0.2796\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1962 - val_loss: 0.1688\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3679 - val_loss: 0.4481\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3346 - val_loss: 0.4275\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3311 - val_loss: 0.2539\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1841 - val_loss: 0.2196\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2404 - val_loss: 0.2669\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2652 - val_loss: 0.4372\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4164 - val_loss: 0.2422\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2816 - val_loss: 0.2620\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3796 - val_loss: 0.1114\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3189 - val_loss: 0.4604\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3384 - val_loss: 0.2141\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4198 - val_loss: 0.3435\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2452 - val_loss: 0.1783\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1692 - val_loss: 0.1206\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2415 - val_loss: 0.6014\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2934 - val_loss: 0.3449\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2856 - val_loss: 0.2118\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2391 - val_loss: 0.6572\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2429 - val_loss: 0.1636\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2344 - val_loss: 0.2587\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.6694 - val_loss: 0.7544\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2321 - val_loss: 0.1926\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1279 - val_loss: 0.6636\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1977 - val_loss: 0.2401\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3461 - val_loss: 0.6363\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2575 - val_loss: 0.4920\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3023 - val_loss: 4.3156\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3395 - val_loss: 0.3495\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3425 - val_loss: 0.1411\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1352 - val_loss: 0.0742\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1860 - val_loss: 2.1434\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2622 - val_loss: 0.2256\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2355 - val_loss: 0.3711\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2063 - val_loss: 0.2514\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1856 - val_loss: 0.1914\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3231 - val_loss: 0.3587\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1638 - val_loss: 0.4744\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.5567 - val_loss: 0.1900\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1741 - val_loss: 0.2692\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2337 - val_loss: 0.6824\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2764 - val_loss: 0.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1395 - val_loss: 0.3091\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2426 - val_loss: 0.1411\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2528 - val_loss: 0.5829\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2151 - val_loss: 0.4895\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.4486 - val_loss: 0.5128\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2087 - val_loss: 0.1845\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1912 - val_loss: 0.2306\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2085 - val_loss: 0.0989\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1719 - val_loss: 0.5029\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2934 - val_loss: 0.1616\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2102 - val_loss: 0.2000\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2307 - val_loss: 0.9968\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3588 - val_loss: 0.2685\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2320 - val_loss: 0.1757\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1659 - val_loss: 0.2713\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2093 - val_loss: 0.3767\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1790 - val_loss: 0.2200\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1502 - val_loss: 0.4442\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3277 - val_loss: 0.2464\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1724 - val_loss: 0.2676\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1574 - val_loss: 0.1818\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2112 - val_loss: 0.5194\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1891 - val_loss: 1.3820\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2401 - val_loss: 0.2822\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1708 - val_loss: 0.1779\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2248 - val_loss: 0.7648\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2799 - val_loss: 0.1466\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1394 - val_loss: 0.2452\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2428 - val_loss: 0.1814\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2816 - val_loss: 0.3545\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1514 - val_loss: 0.1738\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1275 - val_loss: 0.2461\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.2311 - val_loss: 0.1651\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.1530 - val_loss: 0.1089\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 0.3592 - val_loss: 0.2605\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.2765 - val_loss: 0.0798\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1662 - val_loss: 0.1758\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.1650 - val_loss: 0.0968\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.3302 - val_loss: 0.1302\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.0923 - val_loss: 0.3592\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 0.2002 - val_loss: 0.3301\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.3299111456708171\n",
      "Mean Absolute Error (MAE): 0.40852689287407057\n",
      "Root Mean Squared Error (RMSE): 0.5743789216804679\n",
      "Time taken: 1016.1681442260742\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 20ms/step - loss: 941.3399 - val_loss: 573.0815\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 441.0185 - val_loss: 329.1707\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 249.1354 - val_loss: 180.1443\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 124.9945 - val_loss: 72.3490\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.6795 - val_loss: 26.5326\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.6677 - val_loss: 14.5045\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 11.6141 - val_loss: 7.9782\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 7.2914 - val_loss: 3.7527\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 8.7777 - val_loss: 10.4931\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.2092 - val_loss: 3.0911\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 4.4825 - val_loss: 1.9047\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.9791 - val_loss: 3.4368\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1583 - val_loss: 2.1775\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 3.2394 - val_loss: 3.1590\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.3480 - val_loss: 0.7971\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.8309 - val_loss: 4.5435\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 2.3286 - val_loss: 2.7550\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.7710 - val_loss: 0.8162\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.5981 - val_loss: 1.2312\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1.4846 - val_loss: 1.3890\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.6987 - val_loss: 2.3627\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.4131 - val_loss: 1.3593\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.3207 - val_loss: 0.7043\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.4949 - val_loss: 0.4919\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.6280 - val_loss: 1.3399\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.0357 - val_loss: 0.8695\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9099 - val_loss: 0.6242\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2178 - val_loss: 1.4341\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.0157 - val_loss: 0.4707\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.7879 - val_loss: 0.9866\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2474 - val_loss: 0.8900\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8590 - val_loss: 1.6336\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1.2389 - val_loss: 0.6558\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6631 - val_loss: 0.7346\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.8923 - val_loss: 1.8181\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.9447 - val_loss: 0.6774\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.1596 - val_loss: 1.4886\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 1.2101 - val_loss: 0.7617\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5040 - val_loss: 0.3636\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.5708 - val_loss: 0.7033\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4580 - val_loss: 0.6840\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7156 - val_loss: 0.7094\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.2204 - val_loss: 12.7337\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 3.1406 - val_loss: 0.3756\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4149 - val_loss: 0.2389\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3672 - val_loss: 0.2288\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5379 - val_loss: 0.6903\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.9626 - val_loss: 0.7894\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4393 - val_loss: 0.3301\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6319 - val_loss: 0.8160\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7853 - val_loss: 0.2306\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5835 - val_loss: 1.7955\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6397 - val_loss: 0.3594\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6281 - val_loss: 0.2501\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7518 - val_loss: 2.3987\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5573 - val_loss: 0.5217\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6167 - val_loss: 0.5215\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4731 - val_loss: 0.3516\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3919 - val_loss: 1.1151\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.7892 - val_loss: 1.0870\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4415 - val_loss: 0.3296\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6841 - val_loss: 0.4775\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4122 - val_loss: 1.5441\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1.9397 - val_loss: 0.3716\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3089 - val_loss: 0.4155\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3897 - val_loss: 0.5137\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2802 - val_loss: 0.2214\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6429 - val_loss: 0.3731\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3326 - val_loss: 0.5437\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.5046 - val_loss: 0.5999\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.6473 - val_loss: 0.3458\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4624 - val_loss: 0.3396\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.9813 - val_loss: 2.0756\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3167 - val_loss: 0.3461\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2438 - val_loss: 0.2085\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4318 - val_loss: 1.2366\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6444 - val_loss: 0.2780\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3551 - val_loss: 0.3828\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6710 - val_loss: 0.4241\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4133 - val_loss: 0.7036\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6331 - val_loss: 1.8384\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4186 - val_loss: 0.3571\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3613 - val_loss: 0.8221\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.6108 - val_loss: 0.3260\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4621 - val_loss: 0.5019\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5146 - val_loss: 0.5906\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2784 - val_loss: 0.2897\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2801 - val_loss: 0.3223\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3553 - val_loss: 0.3535\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5122 - val_loss: 0.2413\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2919 - val_loss: 0.3922\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3115 - val_loss: 0.2934\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.7163 - val_loss: 0.5308\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2921 - val_loss: 0.5029\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2968 - val_loss: 1.3275\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.5543 - val_loss: 0.9227\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2137 - val_loss: 0.3467\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4129 - val_loss: 0.1781\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2086 - val_loss: 0.1961\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3699 - val_loss: 0.2725\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2729 - val_loss: 0.4946\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4617 - val_loss: 0.5277\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3422 - val_loss: 0.6204\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3419 - val_loss: 0.3071\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2119 - val_loss: 0.6080\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4163 - val_loss: 0.3289\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2833 - val_loss: 2.0088\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3914 - val_loss: 0.5898\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.3684 - val_loss: 0.2945\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2639 - val_loss: 0.4960\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2645 - val_loss: 0.3458\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2117 - val_loss: 0.1389\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3359 - val_loss: 0.4110\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4033 - val_loss: 0.3546\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2295 - val_loss: 0.2008\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2499 - val_loss: 0.5696\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2777 - val_loss: 0.6058\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3047 - val_loss: 0.5321\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.4706 - val_loss: 0.2950\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.4566 - val_loss: 0.5879\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3378 - val_loss: 0.1612\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2985 - val_loss: 0.4300\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2573 - val_loss: 0.1901\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2511 - val_loss: 0.8860\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.1671 - val_loss: 0.1717\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2844 - val_loss: 0.3684\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3169 - val_loss: 0.7969\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2915 - val_loss: 0.3805\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3071 - val_loss: 0.3521\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1891 - val_loss: 0.5496\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2419 - val_loss: 0.2349\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2672 - val_loss: 0.1783\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.2346 - val_loss: 0.5286\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1830 - val_loss: 0.3102\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3987 - val_loss: 0.3078\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2111 - val_loss: 0.1647\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1989 - val_loss: 0.3562\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3496 - val_loss: 0.2916\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2906 - val_loss: 0.2122\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2429 - val_loss: 0.2753\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2885 - val_loss: 0.3501\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3094 - val_loss: 0.1708\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1719 - val_loss: 1.2162\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 0.3003 - val_loss: 0.4436\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1947 - val_loss: 0.1515\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2171 - val_loss: 0.4386\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1686 - val_loss: 0.5116\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2317 - val_loss: 1.0024\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2312 - val_loss: 0.1747\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2236 - val_loss: 0.2856\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2171 - val_loss: 0.1526\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2039 - val_loss: 0.1091\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.2193 - val_loss: 0.4562\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2164 - val_loss: 0.2083\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3943 - val_loss: 0.4680\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1968 - val_loss: 0.3209\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3287 - val_loss: 0.1238\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1922 - val_loss: 0.2014\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1891 - val_loss: 0.2372\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 0.1620 - val_loss: 0.0979\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1742 - val_loss: 0.4306\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2951 - val_loss: 0.2058\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3376 - val_loss: 0.2280\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1281 - val_loss: 0.1512\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2571 - val_loss: 0.1938\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2410 - val_loss: 1.8779\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1755 - val_loss: 0.2712\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2036 - val_loss: 0.3797\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.1551 - val_loss: 0.2051\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1536 - val_loss: 0.1208\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1682 - val_loss: 0.1302\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2147 - val_loss: 0.1342\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2500 - val_loss: 1.2114\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3103 - val_loss: 0.2270\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1373 - val_loss: 0.2889\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1188 - val_loss: 0.1611\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3131 - val_loss: 0.6202\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1527 - val_loss: 0.7532\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3437 - val_loss: 0.3608\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1660 - val_loss: 0.2690\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2090 - val_loss: 0.1128\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1304 - val_loss: 0.4384\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1485 - val_loss: 0.2116\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.3055 - val_loss: 0.4009\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2090 - val_loss: 0.1479\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2930 - val_loss: 0.7021\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1391 - val_loss: 0.2770\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1520 - val_loss: 0.1805\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.0984 - val_loss: 0.3945\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2688 - val_loss: 0.2162\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2317 - val_loss: 0.5723\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1724 - val_loss: 0.5465\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1683 - val_loss: 0.1446\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.2253 - val_loss: 0.2934\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1489 - val_loss: 0.1486\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1881 - val_loss: 0.1452\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1474 - val_loss: 0.1712\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 0.1347 - val_loss: 0.1534\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.1711 - val_loss: 0.2815\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 0.3222 - val_loss: 0.1736\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.17278052606893604\n",
      "Mean Absolute Error (MAE): 0.3133569986511745\n",
      "Root Mean Squared Error (RMSE): 0.41566876965792854\n",
      "Time taken: 1293.831743478775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_15752\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.217094  0.327353  0.465933   660.967461\n",
      "1        2  1.816538  0.946402  1.347790   667.429696\n",
      "2        3  0.142075  0.265798  0.376928   665.074244\n",
      "3        4  0.329911  0.408527  0.574379  1016.168144\n",
      "4        5  0.172781  0.313357  0.415669  1293.831743\n",
      "5  Average  0.535680  0.452287  0.636140   860.694258\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
