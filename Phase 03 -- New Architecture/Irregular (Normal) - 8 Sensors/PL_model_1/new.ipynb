{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 11ms/step - loss: 1024.6210 - val_loss: 727.1505\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 530.0785 - val_loss: 392.7081\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 269.1170 - val_loss: 170.8312\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 112.9102 - val_loss: 65.0918\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 41.8897 - val_loss: 28.5645\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.2059 - val_loss: 11.0545\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 11.8119 - val_loss: 16.5850\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.9550 - val_loss: 4.5053\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 5.3518 - val_loss: 2.7741\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 3.8787 - val_loss: 1.5713\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.3432 - val_loss: 1.5508\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.6741 - val_loss: 1.5567\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1642 - val_loss: 2.2702\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 2.0380 - val_loss: 1.2777\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.0245 - val_loss: 1.6861\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.2945 - val_loss: 2.6091\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.8231 - val_loss: 0.6374\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3860 - val_loss: 0.6149\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4735 - val_loss: 1.2538\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6375 - val_loss: 1.3077\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1065 - val_loss: 1.2559\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2876 - val_loss: 0.6735\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6374 - val_loss: 1.1631\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6775 - val_loss: 1.3722\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1776 - val_loss: 1.0745\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3670 - val_loss: 1.3072\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0970 - val_loss: 0.6132\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0726 - val_loss: 3.7354\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6948 - val_loss: 1.7529\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0687 - val_loss: 1.2317\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8147 - val_loss: 0.5413\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8612 - val_loss: 1.3362\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1644 - val_loss: 8.2911\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3380 - val_loss: 0.6666\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5463 - val_loss: 0.7565\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5832 - val_loss: 0.5695\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8319 - val_loss: 0.2875\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8249 - val_loss: 0.7973\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9019 - val_loss: 0.7359\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5578 - val_loss: 0.9160\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6320 - val_loss: 1.1915\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0157 - val_loss: 0.7217\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7017 - val_loss: 0.8781\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6124 - val_loss: 0.5526\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9823 - val_loss: 1.7665\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7618 - val_loss: 0.2737\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.7789 - val_loss: 0.5249\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4620 - val_loss: 0.3868\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.9303 - val_loss: 0.9963\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5837 - val_loss: 0.7760\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1903 - val_loss: 0.5657\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4961 - val_loss: 0.4534\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6502 - val_loss: 0.8755\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4279 - val_loss: 0.6276\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9026 - val_loss: 1.0602\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5642 - val_loss: 0.4433\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4858 - val_loss: 0.2579\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4719 - val_loss: 1.1806\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7523 - val_loss: 0.9617\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5708 - val_loss: 0.3496\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6141 - val_loss: 0.6133\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5178 - val_loss: 0.7795\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4283 - val_loss: 0.4493\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3686 - val_loss: 0.3141\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4049 - val_loss: 0.3339\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5124 - val_loss: 0.6974\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8894 - val_loss: 0.8212\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5377 - val_loss: 0.3806\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4176 - val_loss: 0.7132\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3200 - val_loss: 0.2723\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3832 - val_loss: 0.6381\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6352 - val_loss: 0.7752\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4904 - val_loss: 0.3256\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5811 - val_loss: 0.3712\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3807 - val_loss: 0.2450\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4962 - val_loss: 1.1231\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3790 - val_loss: 0.3494\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3041 - val_loss: 0.3497\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6663 - val_loss: 0.5152\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6355 - val_loss: 1.4044\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4966 - val_loss: 0.3774\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2320 - val_loss: 0.3689\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3431 - val_loss: 0.2166\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5275 - val_loss: 0.4551\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4517 - val_loss: 0.3720\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3217 - val_loss: 0.4074\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3059 - val_loss: 0.3183\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4185 - val_loss: 0.3867\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3382 - val_loss: 0.4118\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4828 - val_loss: 0.6295\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3203 - val_loss: 0.7157\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3810 - val_loss: 0.4271\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4790 - val_loss: 0.8129\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4235 - val_loss: 0.6579\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3980 - val_loss: 0.5748\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3454 - val_loss: 0.3032\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3790 - val_loss: 0.3273\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4437 - val_loss: 1.0761\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3662 - val_loss: 0.1332\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4130 - val_loss: 0.2540\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2543 - val_loss: 0.0910\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4069 - val_loss: 0.7180\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3675 - val_loss: 0.3463\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4028 - val_loss: 0.2020\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2659 - val_loss: 0.4867\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2191 - val_loss: 0.1988\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2101 - val_loss: 0.4047\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4354 - val_loss: 0.7170\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2400 - val_loss: 0.3136\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4300 - val_loss: 0.5310\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1835 - val_loss: 0.1705\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5714 - val_loss: 1.1126\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2609 - val_loss: 0.4731\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3747 - val_loss: 0.2767\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2085 - val_loss: 0.2025\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3524 - val_loss: 0.2852\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3764 - val_loss: 1.2505\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3040 - val_loss: 0.2269\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3487 - val_loss: 1.8139\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2390 - val_loss: 0.1350\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7280 - val_loss: 0.8476\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1363 - val_loss: 0.2449\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1276 - val_loss: 0.1228\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1163 - val_loss: 0.1000\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2002 - val_loss: 0.2584\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1630 - val_loss: 0.2055\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2716 - val_loss: 0.2433\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2050 - val_loss: 0.1336\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2780 - val_loss: 0.5204\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2384 - val_loss: 0.2111\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1632 - val_loss: 0.0942\n",
      "16/16 [==============================] - 1s 17ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.09016068141213902\n",
      "Mean Absolute Error (MAE): 0.22421496683103337\n",
      "Root Mean Squared Error (RMSE): 0.30026768292998\n",
      "Time taken: 423.0479440689087\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 11ms/step - loss: 995.5091 - val_loss: 728.6693\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 554.6907 - val_loss: 404.7933\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 277.7456 - val_loss: 182.9422\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 117.4853 - val_loss: 62.0392\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 42.4653 - val_loss: 29.5243\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.4728 - val_loss: 11.0215\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 11.0248 - val_loss: 9.5024\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.3321 - val_loss: 9.2970\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.4248 - val_loss: 2.1977\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.4406 - val_loss: 2.3868\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.4925 - val_loss: 5.6151\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.3091 - val_loss: 1.7449\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.2921 - val_loss: 1.8995\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6972 - val_loss: 3.0947\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.6007 - val_loss: 2.9060\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1299 - val_loss: 1.9306\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4389 - val_loss: 1.6024\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.0485 - val_loss: 3.2197\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 2.0693 - val_loss: 0.7905\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.7777 - val_loss: 2.3037\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.8470 - val_loss: 1.3270\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4317 - val_loss: 0.8823\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.2792 - val_loss: 0.9510\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4258 - val_loss: 1.2015\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.5645 - val_loss: 1.4854\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4206 - val_loss: 0.8357\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1183 - val_loss: 1.2837\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.3401 - val_loss: 4.5147\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4069 - val_loss: 1.3469\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8156 - val_loss: 1.0887\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9039 - val_loss: 1.4054\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4386 - val_loss: 2.0357\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8233 - val_loss: 1.6374\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4087 - val_loss: 1.8906\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6808 - val_loss: 0.6826\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6919 - val_loss: 1.4750\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7638 - val_loss: 0.3567\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7215 - val_loss: 0.3402\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0211 - val_loss: 1.3525\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0320 - val_loss: 0.5971\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5773 - val_loss: 0.5789\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7929 - val_loss: 2.5105\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6894 - val_loss: 1.3121\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.7096 - val_loss: 0.6310\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6348 - val_loss: 0.8994\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5683 - val_loss: 0.3465\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4817 - val_loss: 0.6183\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4943 - val_loss: 0.8165\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5634 - val_loss: 0.4685\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7531 - val_loss: 0.5792\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5156 - val_loss: 1.1389\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7632 - val_loss: 0.6318\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5134 - val_loss: 1.2182\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7322 - val_loss: 0.5053\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6312 - val_loss: 0.3958\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4499 - val_loss: 0.4681\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6718 - val_loss: 1.4094\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.7716 - val_loss: 0.6485\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4301 - val_loss: 0.3834\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3709 - val_loss: 0.6166\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7979 - val_loss: 0.2537\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.7751 - val_loss: 0.6511\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4898 - val_loss: 0.5253\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3499 - val_loss: 0.6321\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9373 - val_loss: 1.3657\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6378 - val_loss: 0.4419\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4252 - val_loss: 0.2317\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3596 - val_loss: 0.4309\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6759 - val_loss: 0.2883\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4372 - val_loss: 0.6872\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3724 - val_loss: 0.6860\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8835 - val_loss: 0.2678\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6080 - val_loss: 0.3502\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3876 - val_loss: 0.2057\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3798 - val_loss: 0.6051\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4094 - val_loss: 1.9169\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4463 - val_loss: 0.8189\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6614 - val_loss: 1.4024\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8546 - val_loss: 0.2338\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3040 - val_loss: 0.2817\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5648 - val_loss: 0.2289\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2636 - val_loss: 0.1156\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3319 - val_loss: 0.2369\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3644 - val_loss: 0.1314\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2866 - val_loss: 0.4357\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3111 - val_loss: 0.4067\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4740 - val_loss: 0.5312\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4076 - val_loss: 0.8146\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4524 - val_loss: 0.7120\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3809 - val_loss: 0.4200\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2330 - val_loss: 1.5245\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4932 - val_loss: 0.5805\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2613 - val_loss: 0.2347\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5457 - val_loss: 2.1710\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5234 - val_loss: 0.7322\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3273 - val_loss: 0.7553\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2958 - val_loss: 0.8182\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4946 - val_loss: 1.1112\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1856 - val_loss: 0.1460\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1895 - val_loss: 0.4469\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2610 - val_loss: 0.2153\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2282 - val_loss: 0.3609\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2313 - val_loss: 0.2679\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1780 - val_loss: 0.2462\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2556 - val_loss: 0.1665\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4434 - val_loss: 0.8995\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3121 - val_loss: 0.4213\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4230 - val_loss: 1.1854\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4635 - val_loss: 0.1774\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2525 - val_loss: 0.2905\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3189 - val_loss: 0.1381\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2284 - val_loss: 0.1819\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.1158009229346419\n",
      "Mean Absolute Error (MAE): 0.25653447628650605\n",
      "Root Mean Squared Error (RMSE): 0.3402953466250191\n",
      "Time taken: 367.0021834373474\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 11ms/step - loss: 999.4774 - val_loss: 645.9631\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 499.3176 - val_loss: 410.3550\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 295.5807 - val_loss: 228.4767\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 146.9825 - val_loss: 90.6695\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 58.7502 - val_loss: 31.6225\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 24.6780 - val_loss: 13.8329\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 14.1281 - val_loss: 11.2074\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.8907 - val_loss: 4.6596\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.6913 - val_loss: 5.7109\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.1427 - val_loss: 3.0965\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.3916 - val_loss: 1.8484\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.1827 - val_loss: 3.5700\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.5747 - val_loss: 2.1545\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3678 - val_loss: 1.7760\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.8749 - val_loss: 2.2388\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6413 - val_loss: 1.2526\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7077 - val_loss: 0.9505\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.3521 - val_loss: 1.5042\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.3980 - val_loss: 1.8366\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1415 - val_loss: 0.5787\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1688 - val_loss: 1.4487\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.2268 - val_loss: 2.4424\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0090 - val_loss: 0.8946\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.2786 - val_loss: 5.5096\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.5424 - val_loss: 2.3947\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1249 - val_loss: 2.9862\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2683 - val_loss: 0.5103\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8171 - val_loss: 0.8023\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8369 - val_loss: 0.6308\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3423 - val_loss: 2.6635\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8143 - val_loss: 0.4498\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6921 - val_loss: 0.6838\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6127 - val_loss: 0.6131\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.9695 - val_loss: 2.1198\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6823 - val_loss: 0.4759\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7240 - val_loss: 0.7205\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4288 - val_loss: 1.4529\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7873 - val_loss: 0.9117\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9550 - val_loss: 0.5520\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0538 - val_loss: 0.7250\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7640 - val_loss: 0.6058\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7005 - val_loss: 0.7940\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5409 - val_loss: 0.6102\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7261 - val_loss: 0.4374\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6496 - val_loss: 0.5818\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5200 - val_loss: 0.2160\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5846 - val_loss: 0.4309\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5297 - val_loss: 1.4225\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5362 - val_loss: 0.4967\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6561 - val_loss: 0.3671\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8510 - val_loss: 0.3324\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6161 - val_loss: 1.4212\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5967 - val_loss: 0.5855\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5340 - val_loss: 0.9049\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4474 - val_loss: 1.2426\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6036 - val_loss: 0.3687\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8505 - val_loss: 0.9007\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5058 - val_loss: 0.3899\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4686 - val_loss: 0.3526\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3917 - val_loss: 0.4330\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.1210 - val_loss: 0.5860\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3126 - val_loss: 0.2359\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3716 - val_loss: 0.5298\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2908 - val_loss: 0.1907\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4061 - val_loss: 0.3727\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4330 - val_loss: 0.4803\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5192 - val_loss: 1.3529\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4265 - val_loss: 0.8047\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3685 - val_loss: 1.0272\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4282 - val_loss: 0.3582\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5052 - val_loss: 0.4494\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3798 - val_loss: 0.3009\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6816 - val_loss: 0.4803\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5475 - val_loss: 0.2577\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4532 - val_loss: 0.5996\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3528 - val_loss: 0.2219\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.5272 - val_loss: 0.2286\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2981 - val_loss: 0.3668\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2168 - val_loss: 0.1162\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2693 - val_loss: 0.1370\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2289 - val_loss: 0.5115\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4972 - val_loss: 0.3201\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8721 - val_loss: 0.3089\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2588 - val_loss: 0.3732\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2597 - val_loss: 0.3478\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2675 - val_loss: 0.3570\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2748 - val_loss: 0.3107\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5421 - val_loss: 0.3642\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3230 - val_loss: 0.4482\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2876 - val_loss: 0.2347\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4699 - val_loss: 0.1648\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3811 - val_loss: 0.2963\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3674 - val_loss: 0.3001\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5685 - val_loss: 0.1678\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3404 - val_loss: 0.1138\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2470 - val_loss: 0.3760\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3250 - val_loss: 0.5286\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3492 - val_loss: 0.3282\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5123 - val_loss: 0.3419\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2340 - val_loss: 0.6689\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2634 - val_loss: 0.1499\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6282 - val_loss: 0.4689\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4427 - val_loss: 0.3005\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2135 - val_loss: 0.1718\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2780 - val_loss: 0.8540\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6204 - val_loss: 0.2132\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3647 - val_loss: 0.5387\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2738 - val_loss: 0.1722\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2775 - val_loss: 0.6470\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3097 - val_loss: 0.4266\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2723 - val_loss: 0.3275\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2321 - val_loss: 0.1871\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3659 - val_loss: 0.2681\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4241 - val_loss: 0.1758\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1633 - val_loss: 0.1264\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2455 - val_loss: 5.8130\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0544 - val_loss: 0.1780\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1672 - val_loss: 0.1177\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1210 - val_loss: 0.2087\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1654 - val_loss: 0.2222\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3008 - val_loss: 0.1646\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1528 - val_loss: 0.3392\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1994 - val_loss: 0.1113\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3569 - val_loss: 5.6252\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4970 - val_loss: 0.1913\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2179 - val_loss: 0.1539\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1671 - val_loss: 0.2521\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1715 - val_loss: 0.3318\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1965 - val_loss: 0.3172\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2091 - val_loss: 0.2154\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3381 - val_loss: 0.4075\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2193 - val_loss: 0.6293\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2269 - val_loss: 0.4343\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2451 - val_loss: 0.6536\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2258 - val_loss: 0.3318\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2780 - val_loss: 0.1909\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2973 - val_loss: 0.2882\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4558 - val_loss: 0.3695\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1651 - val_loss: 0.0887\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3074 - val_loss: 0.1506\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2097 - val_loss: 0.5292\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2385 - val_loss: 0.1170\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1725 - val_loss: 1.1480\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1787 - val_loss: 1.6112\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2192 - val_loss: 0.2465\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3617 - val_loss: 0.1689\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1934 - val_loss: 0.1927\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2888 - val_loss: 0.1767\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2088 - val_loss: 0.1460\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1854 - val_loss: 0.3733\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1849 - val_loss: 0.4472\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3535 - val_loss: 0.2569\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1754 - val_loss: 0.2688\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1760 - val_loss: 0.1915\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4493 - val_loss: 0.9392\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1910 - val_loss: 0.2011\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2312 - val_loss: 0.5376\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1591 - val_loss: 0.1890\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2145 - val_loss: 0.1646\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2058 - val_loss: 0.3332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2430 - val_loss: 0.4625\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3194 - val_loss: 0.3123\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1776 - val_loss: 0.1720\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2106 - val_loss: 0.3541\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2171 - val_loss: 0.1747\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2220 - val_loss: 0.6045\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1573 - val_loss: 0.2013\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3470 - val_loss: 0.6882\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1294 - val_loss: 0.1212\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.08929270734272653\n",
      "Mean Absolute Error (MAE): 0.21027519589332921\n",
      "Root Mean Squared Error (RMSE): 0.2988188537270139\n",
      "Time taken: 541.8886458873749\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 11ms/step - loss: 974.1193 - val_loss: 638.4595\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 504.5652 - val_loss: 404.6097\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 272.5661 - val_loss: 183.0594\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 116.6876 - val_loss: 65.7149\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 44.5170 - val_loss: 20.4010\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 18.3711 - val_loss: 12.8232\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.3986 - val_loss: 9.8182\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 8.1688 - val_loss: 3.2012\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.1575 - val_loss: 3.9625\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.1990 - val_loss: 6.8108\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.9147 - val_loss: 2.7712\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.2748 - val_loss: 5.1927\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.8411 - val_loss: 1.6139\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.4329 - val_loss: 4.6706\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1777 - val_loss: 2.0054\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7839 - val_loss: 1.9568\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.6093 - val_loss: 1.1165\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.0023 - val_loss: 4.2901\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7120 - val_loss: 1.0075\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.2047 - val_loss: 0.8513\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2715 - val_loss: 1.4820\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4719 - val_loss: 2.2242\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2594 - val_loss: 0.9098\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3640 - val_loss: 1.2445\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3379 - val_loss: 0.9705\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2627 - val_loss: 1.3496\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1424 - val_loss: 3.0730\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9726 - val_loss: 1.5663\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3115 - val_loss: 1.6301\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 2.1345 - val_loss: 0.5831\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8060 - val_loss: 0.7729\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8203 - val_loss: 1.5679\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8316 - val_loss: 0.5312\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8920 - val_loss: 1.1968\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0084 - val_loss: 0.9266\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7816 - val_loss: 1.0469\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6798 - val_loss: 1.3579\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7153 - val_loss: 1.0701\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9352 - val_loss: 1.3642\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9727 - val_loss: 0.9283\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8729 - val_loss: 1.2356\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7685 - val_loss: 0.6038\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6733 - val_loss: 0.7153\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0466 - val_loss: 0.5884\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6551 - val_loss: 0.3979\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5333 - val_loss: 1.3066\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8611 - val_loss: 1.8443\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5711 - val_loss: 0.8169\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6210 - val_loss: 0.2995\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9427 - val_loss: 3.3236\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7387 - val_loss: 0.8022\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4049 - val_loss: 0.3524\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6524 - val_loss: 1.6999\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9083 - val_loss: 1.2811\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4759 - val_loss: 0.4390\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5131 - val_loss: 1.7256\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4252 - val_loss: 0.5143\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3727 - val_loss: 0.2507\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3558 - val_loss: 0.3131\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4007 - val_loss: 1.5007\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.4795 - val_loss: 0.2563\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5553 - val_loss: 0.6064\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4803 - val_loss: 0.3053\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5460 - val_loss: 1.2081\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6179 - val_loss: 0.5253\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4357 - val_loss: 0.3672\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4989 - val_loss: 0.5550\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3780 - val_loss: 0.3405\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0834 - val_loss: 0.9470\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5159 - val_loss: 0.6989\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4872 - val_loss: 0.2734\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3093 - val_loss: 0.6001\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3821 - val_loss: 1.2994\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6060 - val_loss: 0.7448\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5736 - val_loss: 0.3330\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3481 - val_loss: 0.4149\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.4531 - val_loss: 0.3271\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2580 - val_loss: 0.5889\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6287 - val_loss: 1.7629\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4313 - val_loss: 0.3612\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1968 - val_loss: 0.3267\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1535 - val_loss: 0.1258\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3270 - val_loss: 0.5193\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4750 - val_loss: 0.1891\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2803 - val_loss: 0.5752\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3986 - val_loss: 0.5926\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4534 - val_loss: 0.8232\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4953 - val_loss: 0.8589\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4588 - val_loss: 0.2529\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2637 - val_loss: 0.2162\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3699 - val_loss: 0.5082\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4985 - val_loss: 0.3028\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3639 - val_loss: 0.2108\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2134 - val_loss: 0.2993\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4766 - val_loss: 0.5013\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3081 - val_loss: 0.2957\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3319 - val_loss: 0.2417\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4930 - val_loss: 0.7258\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.4427 - val_loss: 0.2645\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5446 - val_loss: 2.3534\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3711 - val_loss: 0.1180\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2640 - val_loss: 0.5078\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2238 - val_loss: 0.1597\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4457 - val_loss: 0.4452\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3307 - val_loss: 0.2497\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5039 - val_loss: 0.4306\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3242 - val_loss: 0.3772\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2268 - val_loss: 0.4131\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2776 - val_loss: 0.7216\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3297 - val_loss: 0.3602\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2744 - val_loss: 2.0191\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4256 - val_loss: 0.2116\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2783 - val_loss: 0.6788\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2574 - val_loss: 0.7574\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3755 - val_loss: 0.3104\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2065 - val_loss: 0.1946\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5395 - val_loss: 0.5537\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2626 - val_loss: 0.5637\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3466 - val_loss: 0.1645\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2958 - val_loss: 0.2372\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1795 - val_loss: 0.2902\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3434 - val_loss: 0.2489\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2363 - val_loss: 0.6696\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3039 - val_loss: 0.6314\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3856 - val_loss: 0.2439\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1693 - val_loss: 0.5122\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2145 - val_loss: 0.3264\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3321 - val_loss: 0.6802\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2524 - val_loss: 0.2847\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3181 - val_loss: 0.4035\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1807 - val_loss: 0.1980\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.11802600945408766\n",
      "Mean Absolute Error (MAE): 0.24247054370272733\n",
      "Root Mean Squared Error (RMSE): 0.343549136884504\n",
      "Time taken: 374.3486852645874\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 10ms/step - loss: 969.8235 - val_loss: 646.7700\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 486.6486 - val_loss: 404.7547\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 262.7337 - val_loss: 197.0223\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 113.3302 - val_loss: 63.3050\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 43.7344 - val_loss: 32.9265\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 18.9976 - val_loss: 14.8255\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 11.6653 - val_loss: 8.0783\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 7.9084 - val_loss: 8.0649\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 5.4886 - val_loss: 2.8804\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.8293 - val_loss: 6.1929\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.6254 - val_loss: 4.0076\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.6989 - val_loss: 4.8496\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.7568 - val_loss: 3.3857\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3829 - val_loss: 1.5244\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3596 - val_loss: 2.4597\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.0627 - val_loss: 1.6489\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 3.5639 - val_loss: 1.9617\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.8384 - val_loss: 0.8876\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4026 - val_loss: 3.3840\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.2460 - val_loss: 1.0777\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.6345 - val_loss: 0.9867\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.9748 - val_loss: 2.8263\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1841 - val_loss: 0.8091\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4090 - val_loss: 2.2742\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4950 - val_loss: 1.0457\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8854 - val_loss: 0.6879\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0043 - val_loss: 1.6579\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9082 - val_loss: 0.7906\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4412 - val_loss: 4.2334\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.2600 - val_loss: 1.2545\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.1905 - val_loss: 0.5617\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7079 - val_loss: 1.0159\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8116 - val_loss: 0.5794\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5914 - val_loss: 0.4188\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0115 - val_loss: 1.1704\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6074 - val_loss: 1.6728\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6200 - val_loss: 0.7471\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7946 - val_loss: 1.2469\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8207 - val_loss: 1.5001\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0920 - val_loss: 0.4930\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6949 - val_loss: 0.8770\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6887 - val_loss: 0.4778\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2372 - val_loss: 1.6388\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6084 - val_loss: 1.0073\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6413 - val_loss: 0.9906\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6472 - val_loss: 0.6349\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4669 - val_loss: 0.4494\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6479 - val_loss: 0.3890\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7900 - val_loss: 0.6974\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5746 - val_loss: 0.3223\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7015 - val_loss: 0.4202\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8816 - val_loss: 1.1821\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6295 - val_loss: 0.3508\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4623 - val_loss: 0.4145\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7786 - val_loss: 0.3288\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5991 - val_loss: 4.9541\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6257 - val_loss: 0.7993\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5144 - val_loss: 0.8893\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4382 - val_loss: 0.4709\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5489 - val_loss: 1.9114\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6021 - val_loss: 1.2124\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5651 - val_loss: 1.5034\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7440 - val_loss: 0.5147\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4374 - val_loss: 0.7568\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5090 - val_loss: 0.3602\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5165 - val_loss: 0.2424\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4827 - val_loss: 0.8920\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4976 - val_loss: 0.3444\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5809 - val_loss: 0.5134\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3632 - val_loss: 1.5576\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5748 - val_loss: 1.1874\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4166 - val_loss: 0.9816\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5947 - val_loss: 0.6390\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.0932 - val_loss: 0.8931\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3270 - val_loss: 0.5615\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2273 - val_loss: 0.3027\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1731 - val_loss: 0.2627\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3252 - val_loss: 0.3388\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5076 - val_loss: 0.3129\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1889 - val_loss: 0.2939\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4418 - val_loss: 0.9752\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3981 - val_loss: 0.4287\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4125 - val_loss: 0.4147\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9842 - val_loss: 0.6731\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3468 - val_loss: 0.1444\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2690 - val_loss: 0.5976\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3153 - val_loss: 0.2840\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4311 - val_loss: 0.3983\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3620 - val_loss: 0.4088\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5065 - val_loss: 0.2983\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2703 - val_loss: 0.1549\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3853 - val_loss: 0.2460\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2881 - val_loss: 0.5426\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4210 - val_loss: 0.5224\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4101 - val_loss: 0.4755\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3281 - val_loss: 0.1822\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3828 - val_loss: 0.8352\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3403 - val_loss: 0.2150\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2679 - val_loss: 0.2917\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.4401 - val_loss: 0.3128\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3699 - val_loss: 0.7238\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1458 - val_loss: 0.4252\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2270 - val_loss: 0.2204\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1277 - val_loss: 0.1570\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1775 - val_loss: 0.1156\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2175 - val_loss: 0.2993\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2506 - val_loss: 0.1921\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1853 - val_loss: 0.2493\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3986 - val_loss: 0.2502\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3088 - val_loss: 0.4038\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6052 - val_loss: 0.5052\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4413 - val_loss: 0.4199\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2597 - val_loss: 0.3083\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1529 - val_loss: 0.2295\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3572 - val_loss: 0.2238\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2108 - val_loss: 0.4698\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2098 - val_loss: 0.2674\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4369 - val_loss: 0.3501\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2830 - val_loss: 0.6200\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3643 - val_loss: 0.2136\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2813 - val_loss: 0.1743\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1941 - val_loss: 0.2260\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2949 - val_loss: 0.4827\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3096 - val_loss: 0.2866\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4413 - val_loss: 0.8633\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2822 - val_loss: 0.1658\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1755 - val_loss: 0.3150\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2994 - val_loss: 0.3415\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2055 - val_loss: 0.6082\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4342 - val_loss: 0.5233\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1569 - val_loss: 0.3211\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1552 - val_loss: 0.3076\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3030 - val_loss: 0.6198\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.6568 - val_loss: 0.1561\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1584 - val_loss: 0.1134\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1308 - val_loss: 0.0914\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1576 - val_loss: 0.1961\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1983 - val_loss: 0.1855\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1459 - val_loss: 0.1193\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2021 - val_loss: 0.2661\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2829 - val_loss: 0.3062\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2117 - val_loss: 0.1154\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1433 - val_loss: 0.3408\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2884 - val_loss: 1.0223\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3004 - val_loss: 0.3174\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3109 - val_loss: 0.4040\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2840 - val_loss: 0.1818\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2616 - val_loss: 0.4182\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2204 - val_loss: 0.2041\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2340 - val_loss: 0.4481\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2910 - val_loss: 0.2537\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2710 - val_loss: 1.0806\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4125 - val_loss: 0.1538\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2115 - val_loss: 0.6300\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1862 - val_loss: 0.2703\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4015 - val_loss: 1.0078\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2443 - val_loss: 0.2054\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2240 - val_loss: 0.1242\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2633 - val_loss: 0.3869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1387 - val_loss: 0.0982\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1743 - val_loss: 0.2191\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2706 - val_loss: 0.2137\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2782 - val_loss: 0.7964\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2326 - val_loss: 0.2637\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3648 - val_loss: 0.1656\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.09129365217203395\n",
      "Mean Absolute Error (MAE): 0.22479286226917702\n",
      "Root Mean Squared Error (RMSE): 0.3021483942900143\n",
      "Time taken: 485.61414670944214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=5,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_2608\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.090161  0.224215  0.300268  423.047944\n",
      "1        2  0.115801  0.256534  0.340295  367.002183\n",
      "2        3  0.089293  0.210275  0.298819  541.888646\n",
      "3        4  0.118026  0.242471  0.343549  374.348685\n",
      "4        5  0.091294  0.224793  0.302148  485.614147\n",
      "5  Average  0.100915  0.231658  0.317016  438.380321\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ca437",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
