{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1066.8387 - val_loss: 808.8101\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 679.0692 - val_loss: 529.7864\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 394.9251 - val_loss: 275.1215\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 186.3278 - val_loss: 105.5277\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 74.0909 - val_loss: 48.7221\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 31.2872 - val_loss: 26.6599\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.4957 - val_loss: 14.1302\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.5865 - val_loss: 6.4524\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8806 - val_loss: 10.9245\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0934 - val_loss: 5.4904\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.1540 - val_loss: 3.7351\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.7999 - val_loss: 10.1395\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.9265 - val_loss: 7.7884\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7728 - val_loss: 3.1197\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2853 - val_loss: 4.2358\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6093 - val_loss: 1.5185\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4846 - val_loss: 1.6626\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9124 - val_loss: 2.4368\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5072 - val_loss: 2.5130\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5175 - val_loss: 1.2150\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.6276 - val_loss: 1.3851\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8378 - val_loss: 4.3751\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2239 - val_loss: 0.8956\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2091 - val_loss: 0.8375\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7091 - val_loss: 3.0964\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3220 - val_loss: 0.8644\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3735 - val_loss: 1.4097\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5805 - val_loss: 0.6548\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4791 - val_loss: 0.9728\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9167 - val_loss: 0.9296\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7713 - val_loss: 0.8120\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5003 - val_loss: 3.2056\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0529 - val_loss: 0.9277\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8655 - val_loss: 0.6201\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9097 - val_loss: 1.0282\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9787 - val_loss: 0.9545\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9531 - val_loss: 1.6178\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0773 - val_loss: 0.5915\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1164 - val_loss: 1.0909\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6261 - val_loss: 0.3603\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6166 - val_loss: 0.6362\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6491 - val_loss: 1.2047\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6478 - val_loss: 0.3543\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6047 - val_loss: 1.0120\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8268 - val_loss: 0.6186\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2857 - val_loss: 0.4288\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6636 - val_loss: 1.5400\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4990 - val_loss: 0.4648\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5123 - val_loss: 0.4926\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7086 - val_loss: 1.8289\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9567 - val_loss: 0.7445\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4083 - val_loss: 1.4462\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7724 - val_loss: 0.4826\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3704 - val_loss: 0.9406\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8522 - val_loss: 1.6996\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9284 - val_loss: 1.8423\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5651 - val_loss: 0.2397\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4151 - val_loss: 0.4174\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6593 - val_loss: 0.5491\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7082 - val_loss: 0.4660\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3922 - val_loss: 0.3820\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4667 - val_loss: 0.4143\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5853 - val_loss: 1.1202\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4692 - val_loss: 0.3637\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5047 - val_loss: 0.5430\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3071 - val_loss: 1.0390\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2830 - val_loss: 0.3400\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3358 - val_loss: 0.4566\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3048 - val_loss: 0.3187\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4489 - val_loss: 0.2980\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4056 - val_loss: 0.6248\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5567 - val_loss: 0.5897\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6660 - val_loss: 0.3991\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3358 - val_loss: 0.6681\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3289 - val_loss: 0.9316\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4147 - val_loss: 0.3746\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5139 - val_loss: 0.3808\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2708 - val_loss: 0.3505\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5812 - val_loss: 0.4680\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7159 - val_loss: 2.8586\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5767 - val_loss: 0.4058\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2454 - val_loss: 0.2849\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2350 - val_loss: 0.0956\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1592 - val_loss: 0.3077\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2981 - val_loss: 0.2127\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3487 - val_loss: 0.3569\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2390 - val_loss: 0.1886\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3585 - val_loss: 0.2249\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3853 - val_loss: 0.2784\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3887 - val_loss: 0.2140\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5427 - val_loss: 2.0645\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7603 - val_loss: 0.4888\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2809 - val_loss: 0.4169\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3215 - val_loss: 0.4058\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3609 - val_loss: 0.5554\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2618 - val_loss: 0.6239\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5630 - val_loss: 0.4238\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3010 - val_loss: 0.2389\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2891 - val_loss: 0.5552\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5144 - val_loss: 2.7266\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6764 - val_loss: 0.5235\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1684 - val_loss: 0.1728\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2510 - val_loss: 0.4014\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3893 - val_loss: 0.6740\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3130 - val_loss: 0.3118\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4013 - val_loss: 0.5631\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4010 - val_loss: 0.3226\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2849 - val_loss: 0.2121\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3169 - val_loss: 0.6186\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4395 - val_loss: 0.6868\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4847 - val_loss: 0.2053\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2168 - val_loss: 0.2834\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2945 - val_loss: 0.2822\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4179 - val_loss: 1.9658\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4618 - val_loss: 0.2776\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2040 - val_loss: 0.1075\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1845 - val_loss: 0.1305\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2805 - val_loss: 0.3203\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3014 - val_loss: 0.4218\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2747 - val_loss: 0.2813\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3291 - val_loss: 0.1705\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.1843 - val_loss: 0.1147\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1493 - val_loss: 0.1514\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1094 - val_loss: 0.1967\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1467 - val_loss: 0.1861\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2297 - val_loss: 0.2362\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4827 - val_loss: 0.2261\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1737 - val_loss: 0.1243\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1982 - val_loss: 0.5721\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2283 - val_loss: 0.2971\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3582 - val_loss: 0.4216\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2427 - val_loss: 0.2212\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2569 - val_loss: 0.4840\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2752 - val_loss: 0.2391\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3647 - val_loss: 0.5532\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3453 - val_loss: 0.2551\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2374 - val_loss: 0.4759\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2330 - val_loss: 0.1852\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2260 - val_loss: 0.7108\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2909 - val_loss: 0.3274\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2340 - val_loss: 0.7756\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4211 - val_loss: 0.3806\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2060 - val_loss: 0.4531\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2425 - val_loss: 0.1254\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2748 - val_loss: 2.3378\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2793 - val_loss: 0.3357\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3311 - val_loss: 0.4979\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2013 - val_loss: 0.2667\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1067 - val_loss: 0.1610\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0893 - val_loss: 0.0433\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1123 - val_loss: 0.2498\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1451 - val_loss: 0.1780\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2411 - val_loss: 0.2909\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1885 - val_loss: 0.1670\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2665 - val_loss: 0.1613\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1878 - val_loss: 0.5694\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2631 - val_loss: 0.1382\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1459 - val_loss: 0.2992\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2667 - val_loss: 0.3489\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2310 - val_loss: 0.1839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1874 - val_loss: 0.1057\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2203 - val_loss: 0.5021\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2078 - val_loss: 0.2990\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1986 - val_loss: 0.1970\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2568 - val_loss: 0.1574\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2856 - val_loss: 0.4068\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1659 - val_loss: 0.1780\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3497 - val_loss: 0.7561\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2005 - val_loss: 0.2366\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1761 - val_loss: 0.3788\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1569 - val_loss: 0.1232\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3999 - val_loss: 0.3217\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1716 - val_loss: 0.3729\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3123 - val_loss: 0.3712\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3267 - val_loss: 0.2279\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2053 - val_loss: 0.1911\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2780 - val_loss: 0.2436\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1481 - val_loss: 0.1543\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2415 - val_loss: 0.5481\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2885 - val_loss: 0.1130\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2191 - val_loss: 0.8069\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2332 - val_loss: 0.1952\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1207 - val_loss: 0.2874\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1691 - val_loss: 0.7570\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3780 - val_loss: 0.1205\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1913 - val_loss: 1.0020\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2981 - val_loss: 0.1752\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1799 - val_loss: 0.1529\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1166 - val_loss: 0.3484\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1968 - val_loss: 0.1628\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2034 - val_loss: 0.2579\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2643 - val_loss: 0.1099\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3604 - val_loss: 8.9029\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3218 - val_loss: 0.1968\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1360 - val_loss: 0.2260\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1705 - val_loss: 0.3218\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1313 - val_loss: 0.0644\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1527 - val_loss: 0.6653\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2655 - val_loss: 0.5547\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3829 - val_loss: 0.3136\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.313484094239275\n",
      "Mean Absolute Error (MAE): 0.40280778694634617\n",
      "Root Mean Squared Error (RMSE): 0.5598965031497116\n",
      "Time taken: 557.4390232563019\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1011.6379 - val_loss: 656.4866\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 512.8200 - val_loss: 437.9040\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 316.0509 - val_loss: 251.8734\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 166.3501 - val_loss: 104.0784\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 69.2426 - val_loss: 44.9848\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 30.3417 - val_loss: 20.2335\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.2727 - val_loss: 13.4961\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.9362 - val_loss: 7.8753\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.1104 - val_loss: 4.7180\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.9387 - val_loss: 9.1967\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.2075 - val_loss: 2.3043\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.4779 - val_loss: 2.7285\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5490 - val_loss: 12.9647\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7907 - val_loss: 7.5110\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.9596 - val_loss: 1.7727\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8043 - val_loss: 3.3838\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5406 - val_loss: 2.4141\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2873 - val_loss: 2.3859\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8553 - val_loss: 1.5503\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8175 - val_loss: 2.5292\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1509 - val_loss: 1.0754\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6618 - val_loss: 0.6718\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3540 - val_loss: 0.9538\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3993 - val_loss: 0.7286\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2478 - val_loss: 1.2720\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.0729 - val_loss: 0.8885\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2037 - val_loss: 1.4444\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3122 - val_loss: 1.6530\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3184 - val_loss: 0.8717\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0945 - val_loss: 1.8678\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0187 - val_loss: 2.8330\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2400 - val_loss: 1.0771\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0353 - val_loss: 7.8469\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7328 - val_loss: 0.6297\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7556 - val_loss: 0.6037\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6597 - val_loss: 0.3786\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7026 - val_loss: 0.7577\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1061 - val_loss: 2.5451\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8035 - val_loss: 0.6378\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1359 - val_loss: 0.6233\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7271 - val_loss: 0.5647\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6183 - val_loss: 0.8510\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2436 - val_loss: 1.4229\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6704 - val_loss: 0.6656\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7122 - val_loss: 0.5755\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5512 - val_loss: 0.4316\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4485 - val_loss: 0.4564\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6318 - val_loss: 0.6904\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7142 - val_loss: 1.1388\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0569 - val_loss: 0.5418\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6569 - val_loss: 0.4266\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6430 - val_loss: 0.9732\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5216 - val_loss: 0.6925\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1024 - val_loss: 0.4449\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7394 - val_loss: 0.9970\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5245 - val_loss: 1.9009\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8103 - val_loss: 1.6717\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5301 - val_loss: 0.3708\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4320 - val_loss: 0.3542\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6355 - val_loss: 0.3446\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5366 - val_loss: 0.7304\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4574 - val_loss: 0.2903\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3769 - val_loss: 0.4081\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8597 - val_loss: 1.5697\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4928 - val_loss: 0.2716\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3045 - val_loss: 1.9377\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8087 - val_loss: 0.3848\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3901 - val_loss: 0.2620\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2518 - val_loss: 0.1945\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3034 - val_loss: 1.1628\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4589 - val_loss: 0.3905\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3122 - val_loss: 0.1558\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3907 - val_loss: 0.2686\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6467 - val_loss: 1.1270\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5576 - val_loss: 0.6193\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3848 - val_loss: 0.2094\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3967 - val_loss: 0.5883\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6977 - val_loss: 0.5172\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4388 - val_loss: 0.5539\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.8924 - val_loss: 3.5084\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3317 - val_loss: 0.2006\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1754 - val_loss: 0.2643\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2018 - val_loss: 0.2521\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2451 - val_loss: 0.6131\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2261 - val_loss: 0.2390\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4289 - val_loss: 0.3293\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2974 - val_loss: 0.3648\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4129 - val_loss: 0.4309\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3429 - val_loss: 0.4590\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5206 - val_loss: 0.4090\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4182 - val_loss: 0.2642\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3157 - val_loss: 0.3692\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6572 - val_loss: 0.3051\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2239 - val_loss: 0.2588\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2597 - val_loss: 0.2563\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4838 - val_loss: 0.8426\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4314 - val_loss: 0.3710\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3051 - val_loss: 0.6241\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4357 - val_loss: 0.1667\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2754 - val_loss: 0.4965\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6812 - val_loss: 0.2262\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2767 - val_loss: 0.1705\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2925 - val_loss: 1.6711\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4866 - val_loss: 0.2644\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2631 - val_loss: 0.3837\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2360 - val_loss: 0.2996\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 0.3433\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3240 - val_loss: 0.3187\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5477 - val_loss: 0.2404\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1943 - val_loss: 0.3954\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3350 - val_loss: 0.8035\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4358 - val_loss: 0.3229\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2508 - val_loss: 0.2832\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5633 - val_loss: 3.0601\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2440 - val_loss: 0.1566\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1834 - val_loss: 0.6408\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3482 - val_loss: 0.0881\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1568 - val_loss: 0.3210\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2390 - val_loss: 0.3960\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2829 - val_loss: 0.3422\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4442 - val_loss: 0.1420\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3156 - val_loss: 0.6988\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2563 - val_loss: 0.1188\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1562 - val_loss: 0.2981\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3397 - val_loss: 0.1962\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2601 - val_loss: 0.1675\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2572 - val_loss: 0.2225\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3613 - val_loss: 0.7307\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2353 - val_loss: 0.1054\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2406 - val_loss: 0.2090\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4501 - val_loss: 0.2166\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2785 - val_loss: 0.2525\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2223 - val_loss: 0.2943\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1944 - val_loss: 0.5203\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4046 - val_loss: 0.3572\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3223 - val_loss: 0.7782\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2354 - val_loss: 0.1883\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3993 - val_loss: 0.2933\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1856 - val_loss: 0.1647\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1966 - val_loss: 0.7512\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6439 - val_loss: 0.2403\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1568 - val_loss: 0.4622\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2146 - val_loss: 0.1933\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2250 - val_loss: 0.1654\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2806 - val_loss: 0.2425\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3178 - val_loss: 0.4100\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2790 - val_loss: 0.7417\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1716 - val_loss: 0.1249\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3211 - val_loss: 0.7270\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4051 - val_loss: 0.5637\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1391 - val_loss: 0.1462\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2324 - val_loss: 0.3371\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2234 - val_loss: 0.2832\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2655 - val_loss: 0.2319\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2513 - val_loss: 0.7725\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 9.5783\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9818 - val_loss: 0.2596\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1024 - val_loss: 0.1426\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1231 - val_loss: 0.1359\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1425 - val_loss: 0.2200\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3538 - val_loss: 1.2303\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4656 - val_loss: 0.2145\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1231 - val_loss: 0.1667\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1297 - val_loss: 0.1869\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1123 - val_loss: 0.2704\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1241 - val_loss: 0.0835\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2565 - val_loss: 0.1035\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2236 - val_loss: 0.1335\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2739 - val_loss: 0.8340\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3547 - val_loss: 0.2633\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1608 - val_loss: 0.1995\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1642 - val_loss: 0.1211\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2585 - val_loss: 1.0686\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1940 - val_loss: 0.5147\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2237 - val_loss: 0.7351\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3223 - val_loss: 0.1763\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2257 - val_loss: 0.1655\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2787 - val_loss: 0.1761\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1895 - val_loss: 0.3323\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2501 - val_loss: 0.2231\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1673 - val_loss: 0.2440\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2129 - val_loss: 0.7186\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2074 - val_loss: 0.2291\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1901 - val_loss: 0.2398\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1894 - val_loss: 0.3408\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2078 - val_loss: 0.1634\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1538 - val_loss: 0.3137\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3733 - val_loss: 0.2081\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1211 - val_loss: 0.0667\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.0920 - val_loss: 0.1952\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0877 - val_loss: 0.1136\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1768 - val_loss: 0.0592\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1416 - val_loss: 0.2051\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1625 - val_loss: 0.3321\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2333 - val_loss: 0.2162\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1083 - val_loss: 0.1962\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3158 - val_loss: 0.1830\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2283 - val_loss: 0.1441\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1608 - val_loss: 0.3110\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2061 - val_loss: 0.3914\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.3908935708229653\n",
      "Mean Absolute Error (MAE): 0.4437081373294362\n",
      "Root Mean Squared Error (RMSE): 0.6252148197403555\n",
      "Time taken: 556.0827009677887\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1023.5143 - val_loss: 666.6819\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 489.7860 - val_loss: 351.0137\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 257.1212 - val_loss: 177.1547\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 118.0126 - val_loss: 78.9742\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 48.9325 - val_loss: 30.4767\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 22.5316 - val_loss: 14.2699\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.4688 - val_loss: 19.1063\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.0619 - val_loss: 4.7931\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.2691 - val_loss: 3.4153\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.5696 - val_loss: 6.3633\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.0187 - val_loss: 4.7515\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4447 - val_loss: 1.6075\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7249 - val_loss: 3.9657\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6636 - val_loss: 3.2010\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6404 - val_loss: 4.9454\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2850 - val_loss: 1.5727\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0544 - val_loss: 2.3886\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0716 - val_loss: 1.5688\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8090 - val_loss: 2.3741\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1125 - val_loss: 1.9064\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6981 - val_loss: 1.4098\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6717 - val_loss: 2.0885\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2932 - val_loss: 0.8172\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5743 - val_loss: 3.6272\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4057 - val_loss: 3.2787\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1596 - val_loss: 2.6439\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1302 - val_loss: 0.8360\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4921 - val_loss: 3.0711\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2024 - val_loss: 0.6485\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0058 - val_loss: 1.6213\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9303 - val_loss: 0.8481\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2155 - val_loss: 1.9176\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9662 - val_loss: 1.4254\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8977 - val_loss: 1.0182\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8133 - val_loss: 1.7078\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9761 - val_loss: 0.9773\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6386 - val_loss: 1.1249\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9628 - val_loss: 0.6650\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8668 - val_loss: 0.9927\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6407 - val_loss: 1.5668\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2131 - val_loss: 1.1500\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6687 - val_loss: 0.5948\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2398 - val_loss: 0.8349\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5387 - val_loss: 1.5852\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6949 - val_loss: 0.5784\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4998 - val_loss: 0.6136\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4192 - val_loss: 0.2864\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 0.3408\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5535 - val_loss: 2.0831\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9524 - val_loss: 1.0408\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4841 - val_loss: 0.7768\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0362 - val_loss: 0.4148\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3974 - val_loss: 0.3596\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4525 - val_loss: 0.3723\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7384 - val_loss: 0.5623\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7171 - val_loss: 0.6810\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4617 - val_loss: 0.3910\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4351 - val_loss: 0.3737\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3234 - val_loss: 0.2897\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4563 - val_loss: 0.8160\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6003 - val_loss: 0.3121\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4566 - val_loss: 0.2795\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7419 - val_loss: 0.5855\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5557 - val_loss: 0.6725\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5133 - val_loss: 0.3814\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5717 - val_loss: 0.5056\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3653 - val_loss: 0.2647\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5191 - val_loss: 0.6121\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4543 - val_loss: 0.4339\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4408 - val_loss: 0.1545\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7101 - val_loss: 0.6572\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2959 - val_loss: 0.2641\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4429 - val_loss: 1.2387\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.6938 - val_loss: 0.5063\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2981 - val_loss: 0.2260\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1890 - val_loss: 0.1472\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3025 - val_loss: 0.2120\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2758 - val_loss: 0.4978\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2779 - val_loss: 0.5111\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4079 - val_loss: 1.8384\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3047 - val_loss: 0.4152\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2554 - val_loss: 0.2335\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5209 - val_loss: 0.5344\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8544 - val_loss: 0.3661\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2640 - val_loss: 0.2813\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3438 - val_loss: 0.3733\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3707 - val_loss: 0.5045\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4325 - val_loss: 0.3827\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4579 - val_loss: 1.0625\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2498 - val_loss: 0.2775\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7209 - val_loss: 0.6596\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3488 - val_loss: 0.8296\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.5415\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4810 - val_loss: 0.2816\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4255 - val_loss: 0.4738\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2203 - val_loss: 0.6263\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4819 - val_loss: 0.6392\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9683 - val_loss: 1.1292\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2425 - val_loss: 0.3190\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1965 - val_loss: 0.2437\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2405 - val_loss: 0.1268\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1881 - val_loss: 0.1716\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2706 - val_loss: 0.5616\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2755 - val_loss: 0.2574\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2578 - val_loss: 0.2450\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2877 - val_loss: 0.2311\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3243 - val_loss: 1.2102\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7404 - val_loss: 0.9869\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3017 - val_loss: 0.1264\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1319 - val_loss: 0.2258\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3169 - val_loss: 1.6835\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3904 - val_loss: 0.3983\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3540 - val_loss: 0.2055\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2018 - val_loss: 0.1661\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2139 - val_loss: 0.5020\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2643 - val_loss: 0.2378\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1824 - val_loss: 0.1759\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2263 - val_loss: 0.7131\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3443 - val_loss: 0.4354\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1568 - val_loss: 0.2109\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1990 - val_loss: 0.2005\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2395 - val_loss: 1.0684\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5045 - val_loss: 0.2586\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2548 - val_loss: 0.2540\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2614 - val_loss: 0.2806\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2545 - val_loss: 1.2703\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3329 - val_loss: 0.3862\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3064 - val_loss: 0.3729\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3232 - val_loss: 0.2496\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1837 - val_loss: 0.2442\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7277 - val_loss: 1.0576\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1925 - val_loss: 0.2005\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2462 - val_loss: 0.3676\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2634 - val_loss: 0.1534\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2242 - val_loss: 0.6337\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3122 - val_loss: 0.3420\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1958 - val_loss: 0.2901\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2233 - val_loss: 0.7030\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3451 - val_loss: 0.4436\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1793 - val_loss: 0.1507\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1824 - val_loss: 0.4338\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2752 - val_loss: 0.9324\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3361 - val_loss: 0.4146\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2334 - val_loss: 0.5437\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2725 - val_loss: 0.4836\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2002 - val_loss: 0.2049\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2066 - val_loss: 1.3164\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2106 - val_loss: 0.1639\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3311 - val_loss: 1.4636\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3801 - val_loss: 0.4632\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2205 - val_loss: 0.2101\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1902 - val_loss: 0.5596\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2876 - val_loss: 0.1180\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1873 - val_loss: 0.2435\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2621 - val_loss: 0.3725\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3379 - val_loss: 1.8310\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2394 - val_loss: 0.1391\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2184 - val_loss: 0.3504\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3045 - val_loss: 0.4332\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4623 - val_loss: 2.5504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1592 - val_loss: 0.3326\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1755 - val_loss: 0.3022\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2181 - val_loss: 0.3116\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3329 - val_loss: 0.1462\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1208 - val_loss: 0.5705\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1585 - val_loss: 0.2752\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2191 - val_loss: 0.7830\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3999 - val_loss: 0.0751\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1544 - val_loss: 0.2758\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1764 - val_loss: 0.2151\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4485 - val_loss: 0.3103\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2294 - val_loss: 0.2924\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1802 - val_loss: 0.2794\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2144 - val_loss: 0.3598\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1521 - val_loss: 0.3082\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3017 - val_loss: 0.2598\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2026 - val_loss: 0.1427\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1208 - val_loss: 0.1347\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1514 - val_loss: 0.3447\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1494 - val_loss: 3.0719\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2840 - val_loss: 0.4564\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1067 - val_loss: 0.1001\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0961 - val_loss: 0.0859\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0653 - val_loss: 0.1608\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0827 - val_loss: 0.1022\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1570 - val_loss: 0.1707\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1255 - val_loss: 0.2010\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2146 - val_loss: 0.3094\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2814 - val_loss: 0.0960\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1205 - val_loss: 0.0799\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1132 - val_loss: 0.0994\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1841 - val_loss: 0.4142\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2193 - val_loss: 0.2974\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1386 - val_loss: 0.1679\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1813 - val_loss: 0.7394\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2610 - val_loss: 0.3376\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2342 - val_loss: 0.1537\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1050 - val_loss: 0.0940\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2015 - val_loss: 0.2736\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1970 - val_loss: 0.1966\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.19628866326484254\n",
      "Mean Absolute Error (MAE): 0.31003579939219894\n",
      "Root Mean Squared Error (RMSE): 0.4430447644029241\n",
      "Time taken: 560.0423307418823\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 981.0938 - val_loss: 651.1861\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 471.1363 - val_loss: 357.4383\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 244.5752 - val_loss: 181.8955\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 111.7961 - val_loss: 66.4123\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 44.5905 - val_loss: 46.2621\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.4094 - val_loss: 13.8999\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.4406 - val_loss: 8.4968\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.5671 - val_loss: 5.4438\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.5865 - val_loss: 5.0273\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6003 - val_loss: 3.6560\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0957 - val_loss: 6.0359\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.8617 - val_loss: 8.5778\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5181 - val_loss: 4.0617\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0598 - val_loss: 1.5631\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0545 - val_loss: 1.8840\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5154 - val_loss: 1.5956\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0745 - val_loss: 1.7353\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2136 - val_loss: 1.2823\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0729 - val_loss: 3.0438\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9753 - val_loss: 1.6890\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7231 - val_loss: 1.1339\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6629 - val_loss: 2.4082\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1977 - val_loss: 1.4099\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3605 - val_loss: 1.4589\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6356 - val_loss: 1.8784\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0080 - val_loss: 0.9490\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2086 - val_loss: 1.1352\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0927 - val_loss: 0.9799\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1270 - val_loss: 0.7627\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3674 - val_loss: 0.9352\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0844 - val_loss: 0.8583\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8792 - val_loss: 0.9253\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1183 - val_loss: 0.7795\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2925 - val_loss: 0.5916\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2577 - val_loss: 1.9197\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8465 - val_loss: 0.6353\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2749 - val_loss: 0.6320\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7021 - val_loss: 0.5303\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8406 - val_loss: 0.4687\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3166 - val_loss: 1.8921\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0625 - val_loss: 0.6626\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6392 - val_loss: 0.6725\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8210 - val_loss: 1.0365\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.0623 - val_loss: 1.5279\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6130 - val_loss: 0.4154\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4720 - val_loss: 0.3716\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3639 - val_loss: 0.2699\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5341 - val_loss: 0.2846\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5431 - val_loss: 1.0964\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5296 - val_loss: 1.1237\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1550 - val_loss: 1.8757\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5317 - val_loss: 0.3518\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5203 - val_loss: 1.0386\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6363 - val_loss: 1.1782\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4324 - val_loss: 0.4320\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2639 - val_loss: 0.9319\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0224 - val_loss: 0.2829\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6504 - val_loss: 0.4583\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3880 - val_loss: 0.5235\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3884 - val_loss: 0.5497\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4078 - val_loss: 0.7654\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5626 - val_loss: 0.8509\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6876 - val_loss: 0.5084\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4369 - val_loss: 0.6905\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.0822 - val_loss: 22.3341\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7729 - val_loss: 1.5231\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3875 - val_loss: 0.1981\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2647 - val_loss: 0.2831\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3749 - val_loss: 1.2157\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3252 - val_loss: 0.3666\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3356 - val_loss: 0.2729\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6453 - val_loss: 0.4884\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3902 - val_loss: 0.7965\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3538 - val_loss: 0.6970\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5910 - val_loss: 0.3185\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3836 - val_loss: 0.8649\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5213 - val_loss: 0.2960\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3529 - val_loss: 0.2196\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3821 - val_loss: 0.3183\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5513 - val_loss: 0.2729\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3384 - val_loss: 0.8004\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3369 - val_loss: 0.1355\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2459 - val_loss: 0.2783\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6423 - val_loss: 0.7717\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3492 - val_loss: 1.1153\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3643 - val_loss: 0.2747\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3801 - val_loss: 0.5107\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6032 - val_loss: 0.7813\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3760 - val_loss: 0.5328\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2395 - val_loss: 0.5002\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.6305 - val_loss: 5.7573\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4163 - val_loss: 0.3825\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2211 - val_loss: 0.3565\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3368 - val_loss: 0.4868\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3847 - val_loss: 0.1511\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4085 - val_loss: 0.2385\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3994 - val_loss: 0.2436\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2030 - val_loss: 0.2236\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3587 - val_loss: 0.2905\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3457 - val_loss: 0.2773\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3881 - val_loss: 0.3284\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3625 - val_loss: 0.3108\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3256 - val_loss: 0.9306\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2390 - val_loss: 0.2684\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3762 - val_loss: 0.1406\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3015 - val_loss: 0.4559\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2305 - val_loss: 0.2846\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4979 - val_loss: 0.5933\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4200 - val_loss: 0.2001\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2310 - val_loss: 0.1366\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3489 - val_loss: 0.4467\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2074 - val_loss: 0.1390\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3038 - val_loss: 0.1813\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2988 - val_loss: 0.5272\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2714 - val_loss: 0.4336\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2423 - val_loss: 0.6221\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3954 - val_loss: 0.8382\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4720 - val_loss: 0.1853\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2407 - val_loss: 0.1514\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2305 - val_loss: 0.2061\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2818 - val_loss: 0.3812\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3227 - val_loss: 0.3523\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9331 - val_loss: 0.1972\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2247 - val_loss: 0.3744\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1359 - val_loss: 0.2106\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1287 - val_loss: 0.1235\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1455 - val_loss: 0.0591\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1705 - val_loss: 0.0979\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1989 - val_loss: 0.3635\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4100 - val_loss: 0.2233\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2496 - val_loss: 0.0940\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2205 - val_loss: 0.3782\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3008 - val_loss: 0.8677\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2297 - val_loss: 0.2590\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1625 - val_loss: 0.2285\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1662 - val_loss: 0.1310\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2993 - val_loss: 0.3049\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2633 - val_loss: 0.5014\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3462 - val_loss: 0.2401\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2707 - val_loss: 0.2940\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3107 - val_loss: 0.1672\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2350 - val_loss: 0.1233\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2071 - val_loss: 0.2518\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2652 - val_loss: 0.3930\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2033 - val_loss: 0.1909\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5632 - val_loss: 9.1340\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5936 - val_loss: 0.2089\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1788 - val_loss: 0.1723\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1127 - val_loss: 0.0769\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1295 - val_loss: 0.3314\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1872 - val_loss: 0.1542\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1826 - val_loss: 0.1117\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2479 - val_loss: 0.1851\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1631 - val_loss: 0.3836\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2069 - val_loss: 0.1346\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1330 - val_loss: 0.1304\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2105 - val_loss: 0.1766\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3705 - val_loss: 2.5517\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4515 - val_loss: 0.0984\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2029 - val_loss: 0.2497\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1911 - val_loss: 0.1678\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1715 - val_loss: 0.1390\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2251 - val_loss: 0.1955\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3129 - val_loss: 0.1098\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2361 - val_loss: 0.2248\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2793 - val_loss: 0.1019\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1959 - val_loss: 0.6610\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2155 - val_loss: 0.1639\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2300 - val_loss: 0.1221\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2228 - val_loss: 0.1684\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2064 - val_loss: 0.1937\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4031 - val_loss: 1.6855\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2425 - val_loss: 0.2032\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2155 - val_loss: 0.2604\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3848 - val_loss: 0.3072\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2132 - val_loss: 0.2226\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1594 - val_loss: 0.1949\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1865 - val_loss: 0.5845\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1931 - val_loss: 0.1443\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2746 - val_loss: 0.1460\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2113 - val_loss: 0.1260\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1636 - val_loss: 0.2162\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1596 - val_loss: 0.2605\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3351 - val_loss: 0.1903\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1365 - val_loss: 0.0960\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1766 - val_loss: 0.5406\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3275 - val_loss: 0.1647\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2281 - val_loss: 0.1761\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1311 - val_loss: 0.1073\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1423 - val_loss: 0.5131\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2150 - val_loss: 0.1974\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2249 - val_loss: 0.4926\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2255 - val_loss: 0.1954\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1954 - val_loss: 0.1618\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1741 - val_loss: 0.1913\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1295 - val_loss: 0.1021\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3509 - val_loss: 0.3305\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4644 - val_loss: 0.1203\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1354 - val_loss: 0.2838\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1215 - val_loss: 0.0998\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.09968305993054981\n",
      "Mean Absolute Error (MAE): 0.21476594675543706\n",
      "Root Mean Squared Error (RMSE): 0.3157262420682668\n",
      "Time taken: 573.3603513240814\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 998.4528 - val_loss: 715.3311\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 538.2646 - val_loss: 420.8548\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 293.4265 - val_loss: 207.5830\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 128.5584 - val_loss: 73.7113\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 53.1588 - val_loss: 40.3748\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.0618 - val_loss: 16.4662\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.2376 - val_loss: 7.4757\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.1119 - val_loss: 5.0328\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.2264 - val_loss: 2.7333\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.0429 - val_loss: 8.6666\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.8274 - val_loss: 3.0569\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4731 - val_loss: 1.6157\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8114 - val_loss: 2.2238\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2031 - val_loss: 3.1513\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6365 - val_loss: 1.0016\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0002 - val_loss: 7.9373\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4128 - val_loss: 2.7707\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6622 - val_loss: 1.3850\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5797 - val_loss: 0.8588\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4467 - val_loss: 2.0658\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1248 - val_loss: 2.2065\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8718 - val_loss: 1.6589\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8984 - val_loss: 5.5960\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7801 - val_loss: 1.5283\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3201 - val_loss: 1.1684\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2777 - val_loss: 1.1647\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0952 - val_loss: 0.9149\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3783 - val_loss: 0.7013\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0673 - val_loss: 1.3000\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9556 - val_loss: 0.4377\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4977 - val_loss: 1.2911\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1233 - val_loss: 0.5908\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3243 - val_loss: 1.0627\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6908 - val_loss: 0.9229\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5411 - val_loss: 2.2408\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8120 - val_loss: 1.0905\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5486 - val_loss: 0.4199\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4666 - val_loss: 0.5191\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4486 - val_loss: 0.4018\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4317 - val_loss: 0.2600\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4590 - val_loss: 0.5418\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7353 - val_loss: 1.2321\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5734 - val_loss: 0.3920\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6660 - val_loss: 0.5361\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8255 - val_loss: 1.0884\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9505 - val_loss: 0.8140\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7575 - val_loss: 0.9838\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6730 - val_loss: 1.8211\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7461 - val_loss: 2.2555\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7319 - val_loss: 0.4920\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6552 - val_loss: 1.4534\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7631 - val_loss: 0.6757\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7186 - val_loss: 1.7099\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6096 - val_loss: 0.3918\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4733 - val_loss: 0.6745\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6483 - val_loss: 1.2466\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5953 - val_loss: 0.6263\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6180 - val_loss: 0.4558\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3799 - val_loss: 0.2797\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6400 - val_loss: 0.7971\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5514 - val_loss: 0.6450\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.8549 - val_loss: 0.7276\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3645 - val_loss: 0.4133\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3028 - val_loss: 0.2272\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2698 - val_loss: 0.2630\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3519 - val_loss: 0.2872\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3351 - val_loss: 0.2238\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3855 - val_loss: 1.2700\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4681 - val_loss: 0.4549\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5195 - val_loss: 0.4616\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3556 - val_loss: 0.4649\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8542 - val_loss: 0.8248\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6663 - val_loss: 0.9162\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2530 - val_loss: 0.3187\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5893 - val_loss: 0.2313\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3761 - val_loss: 0.5798\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4525 - val_loss: 0.4623\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3382 - val_loss: 1.4600\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 2.6683 - val_loss: 0.3231\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3249 - val_loss: 0.1734\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2928 - val_loss: 0.2850\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1713 - val_loss: 0.2022\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2801 - val_loss: 0.3678\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6559 - val_loss: 0.8754\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4900 - val_loss: 0.5055\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2746 - val_loss: 0.2526\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2858 - val_loss: 1.1130\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3141 - val_loss: 0.4587\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4024 - val_loss: 0.2378\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2511 - val_loss: 0.1821\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3637 - val_loss: 0.2343\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4730 - val_loss: 0.3484\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4096 - val_loss: 0.6219\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2972 - val_loss: 0.3692\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2704 - val_loss: 0.2510\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.4184 - val_loss: 0.4899\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3371 - val_loss: 0.6465\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.4406 - val_loss: 0.4219\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3187 - val_loss: 0.4371\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3470 - val_loss: 0.5085\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3544 - val_loss: 0.3496\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4288 - val_loss: 0.1438\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2474 - val_loss: 0.2334\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2854 - val_loss: 0.2922\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3448 - val_loss: 0.4587\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5029 - val_loss: 0.3202\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2438 - val_loss: 0.2710\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3109 - val_loss: 0.4287\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2889 - val_loss: 0.2956\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4808 - val_loss: 0.4285\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2075 - val_loss: 0.8490\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.3236 - val_loss: 0.3918\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5855 - val_loss: 0.4057\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4035 - val_loss: 0.6857\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3413 - val_loss: 0.1870\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3364 - val_loss: 0.3094\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1727 - val_loss: 0.2008\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2230 - val_loss: 0.2170\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3500 - val_loss: 0.8222\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4113 - val_loss: 0.3305\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2739 - val_loss: 0.5026\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2403 - val_loss: 0.1970\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2153 - val_loss: 0.2180\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2788 - val_loss: 0.3500\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2840 - val_loss: 0.2675\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3737 - val_loss: 0.1631\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.2760 - val_loss: 0.5820\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.5056 - val_loss: 0.2234\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2235 - val_loss: 0.5204\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2831 - val_loss: 0.3720\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2483 - val_loss: 0.3827\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2524 - val_loss: 0.3120\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2591 - val_loss: 0.1356\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2034 - val_loss: 0.2901\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1599 - val_loss: 0.3719\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2275 - val_loss: 0.1012\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3951 - val_loss: 0.3860\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3511 - val_loss: 0.4437\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3186 - val_loss: 0.1078\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1838 - val_loss: 0.1539\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2385 - val_loss: 0.4382\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2456 - val_loss: 0.3321\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3708 - val_loss: 0.2779\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2296 - val_loss: 0.2052\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1523 - val_loss: 0.1193\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1836 - val_loss: 0.1401\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3228 - val_loss: 0.2197\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1762 - val_loss: 0.3843\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2644 - val_loss: 0.2048\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1871 - val_loss: 0.2384\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3038 - val_loss: 0.6137\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2383 - val_loss: 0.3750\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2661 - val_loss: 0.2409\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1996 - val_loss: 0.1802\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3306 - val_loss: 0.1635\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1376 - val_loss: 0.0761\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1416 - val_loss: 0.3799\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5381 - val_loss: 0.1940\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2991 - val_loss: 0.1946\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1246 - val_loss: 0.2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2682 - val_loss: 0.5804\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1896 - val_loss: 0.4487\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1540 - val_loss: 0.1537\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2377 - val_loss: 0.5535\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3140 - val_loss: 0.2216\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2160 - val_loss: 0.1984\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2524 - val_loss: 0.9780\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2115 - val_loss: 0.5292\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1985 - val_loss: 0.2831\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1503 - val_loss: 0.1355\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2813 - val_loss: 0.4556\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1701 - val_loss: 1.2367\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2701 - val_loss: 0.4579\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2034 - val_loss: 0.2879\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2994 - val_loss: 0.1580\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1792 - val_loss: 0.0680\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1300 - val_loss: 0.2580\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2291 - val_loss: 0.8554\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2824 - val_loss: 0.1997\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.2522\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2302 - val_loss: 0.2409\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2231 - val_loss: 0.2677\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2610 - val_loss: 0.3250\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1596 - val_loss: 0.2297\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1007 - val_loss: 0.1105\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3932 - val_loss: 5.9226\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2546 - val_loss: 0.3783\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1119 - val_loss: 0.1343\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1080 - val_loss: 0.2826\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2946 - val_loss: 0.1429\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1189 - val_loss: 0.3852\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1674 - val_loss: 0.2285\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1468 - val_loss: 0.2016\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1338 - val_loss: 0.0931\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1789 - val_loss: 0.2175\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2222 - val_loss: 0.1847\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1800 - val_loss: 0.3108\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1812 - val_loss: 0.1101\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1603 - val_loss: 0.8592\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1723 - val_loss: 0.5018\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.4995911144442626\n",
      "Mean Absolute Error (MAE): 0.5133683106000049\n",
      "Root Mean Squared Error (RMSE): 0.7068175963035036\n",
      "Time taken: 522.6557886600494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=6, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_10636\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.313484  0.402808  0.559897  557.439023\n",
      "1        2  0.390894  0.443708  0.625215  556.082701\n",
      "2        3  0.196289  0.310036  0.443045  560.042331\n",
      "3        4  0.099683  0.214766  0.315726  573.360351\n",
      "4        5  0.499591  0.513368  0.706818  522.655789\n",
      "5  Average  0.299988  0.376937  0.530140  553.916039\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
