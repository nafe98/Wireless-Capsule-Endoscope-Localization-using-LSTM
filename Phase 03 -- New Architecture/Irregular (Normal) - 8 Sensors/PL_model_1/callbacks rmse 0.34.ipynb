{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 12ms/step - loss: 949.2359 - val_loss: 620.6617\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 472.2083 - val_loss: 360.9965\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 259.1798 - val_loss: 202.8199\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 110.8699 - val_loss: 61.4094\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 41.6198 - val_loss: 23.3671\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 19.3804 - val_loss: 10.8677\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8658 - val_loss: 8.8406\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 7.1724 - val_loss: 8.6828\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 5.8650 - val_loss: 4.9723\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 4.3366 - val_loss: 2.2172\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 4.8622 - val_loss: 2.7651\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.9236 - val_loss: 3.0594\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 3.2202 - val_loss: 1.9815\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.7892 - val_loss: 2.7369\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.0748 - val_loss: 1.5593\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.3807 - val_loss: 1.3235\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.4340 - val_loss: 2.3193\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.1400 - val_loss: 4.1145\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.1511 - val_loss: 0.7273\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.9458 - val_loss: 3.8047\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.0945 - val_loss: 1.5719\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.9346 - val_loss: 1.1534\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.7614 - val_loss: 1.4284\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.4354 - val_loss: 0.9768\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.3761 - val_loss: 1.2990\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.6410 - val_loss: 0.6017\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1490 - val_loss: 0.6271\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.9617 - val_loss: 1.7198\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 4.2308 - val_loss: 4.9664\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.4354 - val_loss: 0.9555\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7491 - val_loss: 0.5092\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5988 - val_loss: 0.2543\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6325 - val_loss: 0.7100\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.8647 - val_loss: 2.1065\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0469 - val_loss: 1.5134\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8828 - val_loss: 1.9020\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7767 - val_loss: 0.3074\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8964 - val_loss: 1.0813\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1319 - val_loss: 1.3063\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.6870 - val_loss: 1.4572\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7395 - val_loss: 0.6151\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9809 - val_loss: 1.7399\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1400 - val_loss: 2.3825\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6689 - val_loss: 0.5258\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6127 - val_loss: 0.8854\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6630 - val_loss: 1.5578\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7693 - val_loss: 0.7641\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.3674 - val_loss: 0.7980\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8173 - val_loss: 0.6583\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6011 - val_loss: 0.5546\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7408 - val_loss: 0.9871\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4019 - val_loss: 0.2815\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6453 - val_loss: 1.5390\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5670 - val_loss: 0.6708\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5224 - val_loss: 0.3442\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4453 - val_loss: 0.3999\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7645 - val_loss: 0.6990\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.7475 - val_loss: 1.6297\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.9252 - val_loss: 0.4684\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5135 - val_loss: 0.3793\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5222 - val_loss: 3.8167\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5886 - val_loss: 0.2442\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5650 - val_loss: 0.2606\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5948 - val_loss: 0.5677\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4362 - val_loss: 0.3871\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9696 - val_loss: 0.3537\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4210 - val_loss: 1.0248\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4165 - val_loss: 0.4373\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4196 - val_loss: 0.2555\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7184 - val_loss: 0.5245\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4309 - val_loss: 0.3913\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4076 - val_loss: 0.2961\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4960 - val_loss: 0.8352\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5266 - val_loss: 0.1991\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5335 - val_loss: 0.2749\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4700 - val_loss: 0.9684\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5068 - val_loss: 0.9254\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5431 - val_loss: 0.4074\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3538 - val_loss: 0.5271\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4079 - val_loss: 0.2679\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 9ms/step - loss: 0.7145 - val_loss: 4.5973\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7280 - val_loss: 0.2093\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3425 - val_loss: 0.1420\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3230 - val_loss: 0.3802\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3134 - val_loss: 0.2031\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4283 - val_loss: 0.3810\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.2994 - val_loss: 1.1823\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4121 - val_loss: 1.4804\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5578 - val_loss: 0.8994\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3180 - val_loss: 0.3809\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3036 - val_loss: 0.4763\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2886 - val_loss: 0.4294\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5105 - val_loss: 0.3264\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3904 - val_loss: 0.7510\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2426 - val_loss: 0.1947\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.1752 - val_loss: 0.3286\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3440 - val_loss: 0.6623\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2233 - val_loss: 0.1870\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4030 - val_loss: 0.4128\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3188 - val_loss: 0.1857\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2202 - val_loss: 0.2890\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1967 - val_loss: 0.3364\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5477 - val_loss: 0.2615\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2164 - val_loss: 0.3204\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3583 - val_loss: 0.3429\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3037 - val_loss: 0.4886\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2631 - val_loss: 1.1497\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2814 - val_loss: 0.2591\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1188 - val_loss: 0.0917\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1225 - val_loss: 0.0839\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1715 - val_loss: 0.5029\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2037 - val_loss: 0.1519\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2694 - val_loss: 0.4459\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2752 - val_loss: 0.6331\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4559 - val_loss: 0.9805\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2758 - val_loss: 0.1562\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1726 - val_loss: 0.9209\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3550 - val_loss: 0.1593\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2921 - val_loss: 0.2010\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2233 - val_loss: 0.3692\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7330 - val_loss: 0.3599\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2685 - val_loss: 0.4139\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1844 - val_loss: 0.2496\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3444 - val_loss: 0.1967\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2316 - val_loss: 0.1314\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2212 - val_loss: 0.2211\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3266 - val_loss: 0.4795\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2831 - val_loss: 0.4708\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6253 - val_loss: 0.5558\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3218 - val_loss: 0.2628\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2474 - val_loss: 0.7184\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2257 - val_loss: 0.4258\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2636 - val_loss: 0.4448\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3028 - val_loss: 0.3381\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.1748 - val_loss: 0.3087\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2350 - val_loss: 0.1578\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3043 - val_loss: 0.2316\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.1865 - val_loss: 0.3807\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3435 - val_loss: 0.6242\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3169 - val_loss: 0.8735\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.08514805560846493\n",
      "Mean Absolute Error (MAE): 0.20652020050474296\n",
      "Root Mean Squared Error (RMSE): 0.29180139754371454\n",
      "Time taken: 469.80776858329773\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 12ms/step - loss: 1005.8854 - val_loss: 732.9996\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 525.1572 - val_loss: 408.3966\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 250.7605 - val_loss: 154.1640\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 107.3630 - val_loss: 52.0659\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 42.1723 - val_loss: 30.2512\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.6313 - val_loss: 10.3971\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 10.8063 - val_loss: 8.7272\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 7.2232 - val_loss: 4.9665\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.1262 - val_loss: 5.1353\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.4199 - val_loss: 3.0399\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0075 - val_loss: 3.4250\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.2628 - val_loss: 6.7430\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8365 - val_loss: 1.9706\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4446 - val_loss: 1.8250\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7961 - val_loss: 1.5765\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7836 - val_loss: 2.2531\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.0938 - val_loss: 2.7212\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4266 - val_loss: 1.6299\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.1616 - val_loss: 1.5785\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2675 - val_loss: 1.2257\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1405 - val_loss: 0.7034\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1816 - val_loss: 2.4532\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3993 - val_loss: 4.9884\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3188 - val_loss: 0.9242\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1340 - val_loss: 0.9714\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0378 - val_loss: 1.0332\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9415 - val_loss: 0.5969\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3073 - val_loss: 1.3079\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.3588 - val_loss: 1.2454\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8420 - val_loss: 2.0374\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9970 - val_loss: 0.4673\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7613 - val_loss: 1.6942\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.7453 - val_loss: 1.3500\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1260 - val_loss: 1.0323\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.5817 - val_loss: 1.4574\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.7186 - val_loss: 0.4896\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.7361 - val_loss: 1.5146\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0418 - val_loss: 1.4989\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7498 - val_loss: 0.3190\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9381 - val_loss: 0.8340\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1247 - val_loss: 1.0226\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5958 - val_loss: 0.4410\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5918 - val_loss: 0.4361\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8357 - val_loss: 1.3294\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7020 - val_loss: 0.7988\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.0094 - val_loss: 5.9484\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7019 - val_loss: 0.4713\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3605 - val_loss: 0.3957\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6117 - val_loss: 1.1193\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4619 - val_loss: 0.6003\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9207 - val_loss: 1.0128\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4301 - val_loss: 0.2973\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.6705 - val_loss: 0.5499\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6783 - val_loss: 1.0422\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.5657 - val_loss: 0.7162\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.9274 - val_loss: 2.5683\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.7561 - val_loss: 0.2879\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.4600 - val_loss: 0.3420\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5667 - val_loss: 0.1710\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4989 - val_loss: 0.6167\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7347 - val_loss: 1.0965\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9919 - val_loss: 0.2697\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2579 - val_loss: 0.2522\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4572 - val_loss: 0.6244\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4966 - val_loss: 1.1077\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5587 - val_loss: 1.4372\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7748 - val_loss: 0.3131\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3889 - val_loss: 0.6975\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7536 - val_loss: 0.9131\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4046 - val_loss: 0.2852\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3249 - val_loss: 0.4055\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4133 - val_loss: 0.1621\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.4619 - val_loss: 0.2316\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3380 - val_loss: 0.3206\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6555 - val_loss: 1.9084\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6159 - val_loss: 0.5523\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3840 - val_loss: 0.3323\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5253 - val_loss: 0.5346\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2114 - val_loss: 0.1400\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3673 - val_loss: 0.3411\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6095 - val_loss: 0.5397\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2884 - val_loss: 0.2276\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3981 - val_loss: 1.4724\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7828 - val_loss: 0.5049\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1785 - val_loss: 0.3953\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2736 - val_loss: 0.7401\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2633 - val_loss: 0.4369\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2871 - val_loss: 0.3139\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2934 - val_loss: 0.3526\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3871 - val_loss: 0.2098\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4494 - val_loss: 0.8692\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2732 - val_loss: 0.2552\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4510 - val_loss: 0.3105\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3308 - val_loss: 1.3950\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3357 - val_loss: 0.3161\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4068 - val_loss: 0.7664\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3724 - val_loss: 0.3715\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3715 - val_loss: 0.6089\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5331 - val_loss: 0.5090\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4891 - val_loss: 0.2054\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2112 - val_loss: 0.5324\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7652 - val_loss: 1.6812\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4009 - val_loss: 0.3002\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2061 - val_loss: 0.1859\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2257 - val_loss: 0.3915\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2897 - val_loss: 0.3678\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2695 - val_loss: 0.5224\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3152 - val_loss: 0.1456\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4602 - val_loss: 0.4435\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.13725153856043978\n",
      "Mean Absolute Error (MAE): 0.2687357949881976\n",
      "Root Mean Squared Error (RMSE): 0.3704747475340793\n",
      "Time taken: 335.9568135738373\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 12ms/step - loss: 963.7971 - val_loss: 640.1886\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 480.7932 - val_loss: 350.7881\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 241.8521 - val_loss: 155.0418\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 100.0366 - val_loss: 51.4008\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 39.8402 - val_loss: 44.2751\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 18.0003 - val_loss: 16.7153\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 9.1374 - val_loss: 6.9113\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.8280 - val_loss: 8.0170\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 5.3981 - val_loss: 2.6903\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 4.6699 - val_loss: 1.6897\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 4.1279 - val_loss: 2.6501\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3694 - val_loss: 2.5534\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.9032 - val_loss: 1.5292\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7564 - val_loss: 1.3845\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.9394 - val_loss: 1.7417\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.0003 - val_loss: 1.6868\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.5571 - val_loss: 2.1635\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7333 - val_loss: 1.9516\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.9717 - val_loss: 0.8398\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3716 - val_loss: 1.7917\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.3890 - val_loss: 1.9451\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1425 - val_loss: 0.5387\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1930 - val_loss: 3.2256\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1789 - val_loss: 1.0197\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8781 - val_loss: 0.5853\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2941 - val_loss: 3.1881\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.1138 - val_loss: 0.7713\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9232 - val_loss: 1.7526\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9593 - val_loss: 0.7075\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9376 - val_loss: 0.6285\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9685 - val_loss: 0.7520\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7405 - val_loss: 1.0038\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0876 - val_loss: 1.8335\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9698 - val_loss: 1.0129\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6023 - val_loss: 2.1692\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9776 - val_loss: 1.4193\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6409 - val_loss: 0.2853\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4155 - val_loss: 0.3125\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6785 - val_loss: 0.9298\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.8404 - val_loss: 0.5521\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5037 - val_loss: 1.8392\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.1223 - val_loss: 0.8673\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.7525 - val_loss: 0.8231\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4988 - val_loss: 0.2499\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 1.2139 - val_loss: 0.8026\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.8649 - val_loss: 0.3958\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.4373 - val_loss: 1.7685\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 4s 12ms/step - loss: 0.8668 - val_loss: 0.7745\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 0.7732 - val_loss: 0.6269\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.5535 - val_loss: 1.1373\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 5s 12ms/step - loss: 2.3927 - val_loss: 0.7328\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.3924 - val_loss: 0.2268\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.4454 - val_loss: 0.4747\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4908 - val_loss: 0.4073\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5041 - val_loss: 0.4311\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5859 - val_loss: 0.5925\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3494 - val_loss: 0.9457\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3757 - val_loss: 0.5389\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5448 - val_loss: 0.7414\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2949 - val_loss: 0.5107\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5604 - val_loss: 0.3268\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7889 - val_loss: 0.2397\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3767 - val_loss: 0.2475\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6898 - val_loss: 0.3087\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3648 - val_loss: 0.5659\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3839 - val_loss: 0.2531\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7880 - val_loss: 0.4128\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3359 - val_loss: 0.4069\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3969 - val_loss: 0.4568\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7785 - val_loss: 0.9488\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5051 - val_loss: 0.5478\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4054 - val_loss: 0.1741\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3358 - val_loss: 0.6228\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2968 - val_loss: 0.4004\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.6422 - val_loss: 0.9793\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4142 - val_loss: 0.6017\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3581 - val_loss: 0.2215\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3913 - val_loss: 0.6674\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5399 - val_loss: 0.5529\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5910 - val_loss: 0.4406\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3122 - val_loss: 0.1788\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3085 - val_loss: 0.3013\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2567 - val_loss: 0.2286\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0821 - val_loss: 1.1478\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2528 - val_loss: 0.2913\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1879 - val_loss: 0.1814\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2725 - val_loss: 0.2281\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4157 - val_loss: 0.2371\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4586 - val_loss: 0.3864\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2747 - val_loss: 0.2631\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4547 - val_loss: 0.2749\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3234 - val_loss: 0.1116\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2708 - val_loss: 0.4466\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3700 - val_loss: 1.9821\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5556 - val_loss: 0.2465\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2343 - val_loss: 0.3005\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3004 - val_loss: 0.3639\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8103 - val_loss: 0.6092\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3029 - val_loss: 1.0926\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1971 - val_loss: 0.1201\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2303 - val_loss: 0.6022\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6201 - val_loss: 1.2709\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3262 - val_loss: 0.1217\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2747 - val_loss: 0.3992\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.8026 - val_loss: 3.3577\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3434 - val_loss: 0.1502\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1537 - val_loss: 0.2390\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1451 - val_loss: 0.2167\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2315 - val_loss: 0.6098\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1990 - val_loss: 0.1918\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2353 - val_loss: 0.7330\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3115 - val_loss: 0.3346\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2603 - val_loss: 0.3392\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5684 - val_loss: 0.5687\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2458 - val_loss: 1.1546\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2287 - val_loss: 0.1768\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3095 - val_loss: 0.4641\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2555 - val_loss: 0.1515\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3940 - val_loss: 0.3796\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2473 - val_loss: 0.7014\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2567 - val_loss: 0.3496\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3047 - val_loss: 0.8810\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.11137288751377596\n",
      "Mean Absolute Error (MAE): 0.234223481012083\n",
      "Root Mean Squared Error (RMSE): 0.3337257669311376\n",
      "Time taken: 399.34435272216797\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 10ms/step - loss: 972.2335 - val_loss: 656.0215\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 492.8543 - val_loss: 407.7162\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 262.7357 - val_loss: 198.2260\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 113.7611 - val_loss: 68.3242\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 44.4881 - val_loss: 23.4105\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 19.7717 - val_loss: 11.3525\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 11.5348 - val_loss: 7.1901\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 7.8581 - val_loss: 5.5057\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.1553 - val_loss: 3.1272\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 4.4553 - val_loss: 2.9734\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.7427 - val_loss: 2.4397\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0385 - val_loss: 3.0889\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.1024 - val_loss: 2.3064\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.6213 - val_loss: 4.5805\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 2.3399 - val_loss: 2.5918\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7769 - val_loss: 3.9477\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.7948 - val_loss: 3.4444\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.9333 - val_loss: 1.3539\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7409 - val_loss: 1.2125\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3473 - val_loss: 0.8404\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3623 - val_loss: 1.5812\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3283 - val_loss: 1.0803\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2811 - val_loss: 4.4459\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2670 - val_loss: 1.3024\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3842 - val_loss: 0.9926\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.4878 - val_loss: 11.4604\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 1.2057 - val_loss: 0.9097\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1470 - val_loss: 1.5862\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7730 - val_loss: 0.4278\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2338 - val_loss: 0.6370\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9420 - val_loss: 1.6933\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1851 - val_loss: 0.7342\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9316 - val_loss: 0.9059\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0301 - val_loss: 1.6102\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9185 - val_loss: 0.7725\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8267 - val_loss: 1.2499\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8497 - val_loss: 1.2450\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8541 - val_loss: 1.7310\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1429 - val_loss: 1.0451\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7385 - val_loss: 1.6980\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0543 - val_loss: 0.9832\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4537 - val_loss: 0.5827\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5781 - val_loss: 0.5169\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5646 - val_loss: 0.9208\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5822 - val_loss: 0.3779\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9230 - val_loss: 0.7714\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6775 - val_loss: 0.5652\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5333 - val_loss: 0.3260\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9036 - val_loss: 0.6441\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5869 - val_loss: 0.3009\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 1.9357 - val_loss: 7.3252\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6166 - val_loss: 0.4223\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4879 - val_loss: 0.7656\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4285 - val_loss: 0.2637\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4829 - val_loss: 0.4328\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6904 - val_loss: 0.2641\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4696 - val_loss: 0.5814\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6421 - val_loss: 0.5376\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6699 - val_loss: 0.4992\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3482 - val_loss: 0.5610\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2850 - val_loss: 0.2872\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3214 - val_loss: 0.3222\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4004 - val_loss: 0.6845\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4720 - val_loss: 0.4343\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4023 - val_loss: 0.8283\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4324 - val_loss: 0.3986\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6954 - val_loss: 0.3880\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4886 - val_loss: 0.3825\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5448 - val_loss: 0.5653\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4625 - val_loss: 0.8568\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5208 - val_loss: 0.4494\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5175 - val_loss: 0.3544\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5284 - val_loss: 0.3540\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4173 - val_loss: 0.2853\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6292 - val_loss: 0.2696\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.4287 - val_loss: 0.7675\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3080 - val_loss: 0.8889\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3882 - val_loss: 0.2212\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6157 - val_loss: 0.3331\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5021 - val_loss: 0.3190\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2953 - val_loss: 0.1827\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3682 - val_loss: 0.9307\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4322 - val_loss: 0.2466\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5273 - val_loss: 0.4181\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3255 - val_loss: 0.3266\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5355 - val_loss: 0.7478\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2623 - val_loss: 0.3287\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4339 - val_loss: 0.7202\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3355 - val_loss: 0.4762\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4637 - val_loss: 0.9917\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3289 - val_loss: 0.2907\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4965 - val_loss: 0.2608\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4747 - val_loss: 0.1506\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3798 - val_loss: 0.3230\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7845 - val_loss: 0.4146\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2257 - val_loss: 0.2618\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2497 - val_loss: 0.4015\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2464 - val_loss: 0.3693\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1758 - val_loss: 0.1542\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2972 - val_loss: 0.4461\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3440 - val_loss: 0.3134\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2277 - val_loss: 0.2687\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4794 - val_loss: 0.5049\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3441 - val_loss: 0.1976\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2253 - val_loss: 0.1260\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3572 - val_loss: 0.3527\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2766 - val_loss: 0.4498\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3034 - val_loss: 0.3018\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6797 - val_loss: 0.4202\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3105 - val_loss: 0.3102\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1706 - val_loss: 0.1210\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5041 - val_loss: 0.3036\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.1985 - val_loss: 0.4925\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2230 - val_loss: 0.4299\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6430 - val_loss: 0.1322\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2641 - val_loss: 0.3477\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2944 - val_loss: 0.2372\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2968 - val_loss: 0.3180\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3050 - val_loss: 0.1830\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3218 - val_loss: 0.3856\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2675 - val_loss: 0.4413\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4678 - val_loss: 0.4913\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2515 - val_loss: 0.1545\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2401 - val_loss: 0.1514\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3496 - val_loss: 0.1876\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3018 - val_loss: 0.5090\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2787 - val_loss: 0.1737\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2231 - val_loss: 0.2016\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5373 - val_loss: 0.8964\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3029 - val_loss: 0.4540\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2022 - val_loss: 0.1409\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2157 - val_loss: 0.1719\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3056 - val_loss: 0.4706\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2730 - val_loss: 0.2377\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1874 - val_loss: 0.1880\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3580 - val_loss: 0.2196\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3078 - val_loss: 0.2789\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2221 - val_loss: 1.3869\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3958 - val_loss: 0.3645\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1479 - val_loss: 0.2832\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2370 - val_loss: 0.5537\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.12110980853865172\n",
      "Mean Absolute Error (MAE): 0.22985041494534\n",
      "Root Mean Squared Error (RMSE): 0.348008345501443\n",
      "Time taken: 389.78987669944763\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 10ms/step - loss: 957.9547 - val_loss: 612.1695\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 458.7890 - val_loss: 339.5945\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 249.4061 - val_loss: 181.5063\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 113.0535 - val_loss: 73.0461\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 43.5532 - val_loss: 29.4372\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 19.5888 - val_loss: 9.7666\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 11.2955 - val_loss: 6.0875\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 9.3556 - val_loss: 7.1038\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.1240 - val_loss: 6.1713\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.9054 - val_loss: 2.3628\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.8099 - val_loss: 3.6401\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0006 - val_loss: 3.7443\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5341 - val_loss: 1.7954\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.0700 - val_loss: 8.1621\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.2917 - val_loss: 2.4100\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3704 - val_loss: 2.6447\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7494 - val_loss: 1.5790\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6641 - val_loss: 1.9220\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.7080 - val_loss: 1.4483\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.6897 - val_loss: 1.6189\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.3798 - val_loss: 2.4843\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.5246 - val_loss: 2.1714\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8823 - val_loss: 0.9255\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0152 - val_loss: 1.1172\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2445 - val_loss: 0.7640\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.1271 - val_loss: 1.5103\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9563 - val_loss: 2.4013\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.3882 - val_loss: 1.6167\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8053 - val_loss: 0.9125\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.7449 - val_loss: 0.9739\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.9367 - val_loss: 0.7021\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0861 - val_loss: 3.3863\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0927 - val_loss: 0.6390\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8730 - val_loss: 1.0035\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.8115 - val_loss: 0.9296\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1342 - val_loss: 1.9323\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1021 - val_loss: 0.7991\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5357 - val_loss: 0.5288\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9614 - val_loss: 1.7926\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.7460 - val_loss: 0.5805\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4322 - val_loss: 0.5116\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4361 - val_loss: 0.6814\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6269 - val_loss: 0.9834\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5343 - val_loss: 0.6400\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8373 - val_loss: 0.7051\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5697 - val_loss: 0.3533\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6900 - val_loss: 1.2276\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8430 - val_loss: 0.7818\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6385 - val_loss: 0.3055\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6488 - val_loss: 0.9339\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4405 - val_loss: 0.7616\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.2260 - val_loss: 2.4803\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6555 - val_loss: 0.5911\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4330 - val_loss: 0.9934\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4329 - val_loss: 0.3506\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5808 - val_loss: 0.8012\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3687 - val_loss: 1.1642\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.8695 - val_loss: 0.4062\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5100 - val_loss: 0.6051\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7197 - val_loss: 0.6722\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5066 - val_loss: 0.2642\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.5036 - val_loss: 0.7050\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5132 - val_loss: 0.5652\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3935 - val_loss: 0.7156\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7860 - val_loss: 0.8913\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5650 - val_loss: 1.2274\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3838 - val_loss: 0.5825\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4485 - val_loss: 0.3836\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.7397 - val_loss: 0.7608\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4384 - val_loss: 0.3187\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2752 - val_loss: 0.5238\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5030 - val_loss: 0.1638\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3079 - val_loss: 0.4135\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4359 - val_loss: 0.6702\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4118 - val_loss: 0.8047\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5184 - val_loss: 0.2845\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5107 - val_loss: 0.2901\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2988 - val_loss: 0.2968\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7076 - val_loss: 0.3798\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3828 - val_loss: 0.7946\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4134 - val_loss: 0.5043\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3452 - val_loss: 0.5515\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4186 - val_loss: 1.6340\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7591 - val_loss: 0.3539\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4600 - val_loss: 0.2898\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3907 - val_loss: 0.5970\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2702 - val_loss: 0.2991\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2768 - val_loss: 0.2316\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3474 - val_loss: 0.3361\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5218 - val_loss: 0.2646\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2670 - val_loss: 0.2850\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5673 - val_loss: 0.6376\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3119 - val_loss: 0.2612\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2935 - val_loss: 1.1220\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5391 - val_loss: 0.4226\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3448 - val_loss: 0.2018\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3272 - val_loss: 0.2163\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5298 - val_loss: 0.2032\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3248 - val_loss: 0.2852\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3438 - val_loss: 0.2575\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3602 - val_loss: 0.5220\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3204 - val_loss: 0.5605\n",
      "16/16 [==============================] - 1s 19ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.16352062407581233\n",
      "Mean Absolute Error (MAE): 0.2911336326292271\n",
      "Root Mean Squared Error (RMSE): 0.40437683424723075\n",
      "Time taken: 290.39126205444336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=5,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_6344\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.085148  0.206520  0.291801  469.807769\n",
      "1        2  0.137252  0.268736  0.370475  335.956814\n",
      "2        3  0.111373  0.234223  0.333726  399.344353\n",
      "3        4  0.121110  0.229850  0.348008  389.789877\n",
      "4        5  0.163521  0.291134  0.404377  290.391262\n",
      "5  Average  0.123681  0.246093  0.349677  377.058015\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ca437",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
