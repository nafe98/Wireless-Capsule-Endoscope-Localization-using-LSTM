{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     103.508252  125.193887  \n",
       "1     106.645699  137.372609  \n",
       "2     103.789337  135.667714  \n",
       "3     102.460744  129.928887  \n",
       "4     116.786233  139.061346  \n",
       "...          ...         ...  \n",
       "2438  123.942335  108.196626  \n",
       "2439  136.835759  113.267986  \n",
       "2440  129.875574  120.944104  \n",
       "2441  125.361425  123.071554  \n",
       "2442  127.958184  113.784393  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 15s 23ms/step - loss: 1249.2063 - val_loss: 931.8046\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 707.8464 - val_loss: 634.8139\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 484.9422 - val_loss: 396.7341\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 340.4836 - val_loss: 320.9299\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 253.6278 - val_loss: 229.2549\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 184.3634 - val_loss: 147.0852\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 128.5887 - val_loss: 107.5625\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 98.9498 - val_loss: 95.4605\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 77.2907 - val_loss: 62.6476\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 68.1487 - val_loss: 64.6569\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 60.7303 - val_loss: 44.7564\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 56.1142 - val_loss: 45.1559\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 52.3012 - val_loss: 47.4757\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 50.2041 - val_loss: 50.2443\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 50.6024 - val_loss: 37.9631\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 49.6964 - val_loss: 46.9324\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 48.5022 - val_loss: 60.2193\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 47.2928 - val_loss: 41.1556\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 46.2327 - val_loss: 40.0840\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 45.2597 - val_loss: 54.7893\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 45.3502 - val_loss: 49.3605\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 45.1580 - val_loss: 40.8314\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 42.0764 - val_loss: 34.4755\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 42.7828 - val_loss: 42.2324\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 40.2777 - val_loss: 32.0651\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 41.4025 - val_loss: 34.0277\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 40.8814 - val_loss: 41.5330\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 40.9882 - val_loss: 43.6960\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 40.9056 - val_loss: 33.4056\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 38.6498 - val_loss: 35.9784\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 39.8349 - val_loss: 39.4069\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 37.6441 - val_loss: 32.9416\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.3678 - val_loss: 32.8725\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.9710 - val_loss: 33.5151\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 38.3000 - val_loss: 37.8529\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.3658 - val_loss: 37.3439\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.5114 - val_loss: 35.9472\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 35.2535 - val_loss: 43.9853\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.2995 - val_loss: 40.5094\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 34.4107 - val_loss: 35.9310\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.8932 - val_loss: 33.8154\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 35.1505 - val_loss: 31.2311\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.1891 - val_loss: 39.5850\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.3923 - val_loss: 32.6487\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.3769 - val_loss: 34.7602\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.0629 - val_loss: 33.8189\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 32.9645 - val_loss: 31.0017\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 33.4450 - val_loss: 36.0708\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.1158 - val_loss: 32.1359\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 32.1233 - val_loss: 34.5982\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 32.0441 - val_loss: 33.5253\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.0443 - val_loss: 30.4062\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.5240 - val_loss: 31.7880\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.2635 - val_loss: 31.5160\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.1198 - val_loss: 30.5519\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 29.5787 - val_loss: 38.4848\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.0173 - val_loss: 33.1458\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 30.8393 - val_loss: 31.9765\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.5191 - val_loss: 32.1921\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 30.5211 - val_loss: 29.9975\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.4212 - val_loss: 32.0168\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.3188 - val_loss: 29.4422\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.8710 - val_loss: 32.0766\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.9518 - val_loss: 36.4428\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.0044 - val_loss: 29.9601\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.4681 - val_loss: 31.6691\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.5975 - val_loss: 31.6731\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.9709 - val_loss: 32.2959\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.4219 - val_loss: 34.7217\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.5830 - val_loss: 33.6592\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 28.1132 - val_loss: 29.3350\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.8921 - val_loss: 33.8736\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.1747 - val_loss: 28.9573\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.7006 - val_loss: 32.4772\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 28.5702 - val_loss: 32.1718\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 26.8091 - val_loss: 30.1510\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.2328 - val_loss: 34.9363\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.5215 - val_loss: 31.9346\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 20ms/step - loss: 27.0986 - val_loss: 30.9473\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.5596 - val_loss: 46.3998\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.0753 - val_loss: 37.7204\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.4976 - val_loss: 28.9595\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.9417 - val_loss: 32.5634\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.9544 - val_loss: 30.1910\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.6495 - val_loss: 33.2137\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.8345 - val_loss: 30.9305\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.7192 - val_loss: 40.8377\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.6920 - val_loss: 31.5147\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 25.8200 - val_loss: 28.6842\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.9629 - val_loss: 38.2815\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.8733 - val_loss: 32.6736\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.5834 - val_loss: 30.3463\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.1509 - val_loss: 40.3085\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.8662 - val_loss: 32.1821\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.7601 - val_loss: 30.6134\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.0938 - val_loss: 32.2436\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.5409 - val_loss: 30.2696\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.7860 - val_loss: 30.7451\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.3156 - val_loss: 35.8497\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.8235 - val_loss: 30.9096\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.6222 - val_loss: 40.5287\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.2330 - val_loss: 29.8411\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.1213 - val_loss: 31.5556\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.3632 - val_loss: 32.4450\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.4742 - val_loss: 31.5168\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.2345 - val_loss: 35.2338\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.2900 - val_loss: 30.7592\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.6647 - val_loss: 32.6011\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.6889 - val_loss: 32.6989\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.6074 - val_loss: 33.6205\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.8891 - val_loss: 30.0280\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.5720 - val_loss: 35.9566\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.5261 - val_loss: 30.6755\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.2594 - val_loss: 33.4999\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.4111 - val_loss: 29.6641\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.1360 - val_loss: 35.4360\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.0680 - val_loss: 31.5209\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.1129 - val_loss: 32.7732\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.1016 - val_loss: 33.9991\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.2380 - val_loss: 31.4882\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.7628 - val_loss: 32.5708\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.3718 - val_loss: 30.2781\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.6620 - val_loss: 31.6862\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.6986 - val_loss: 29.7201\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.7314 - val_loss: 31.9860\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.9067 - val_loss: 29.9859\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.5846 - val_loss: 36.0361\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.7122 - val_loss: 35.7989\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.0766 - val_loss: 34.9127\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.5994 - val_loss: 37.6872\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.4708 - val_loss: 34.9032\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 21.0007 - val_loss: 32.2436\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.5606 - val_loss: 33.2223\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.5916 - val_loss: 32.5591\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.5153 - val_loss: 38.2347\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.1543 - val_loss: 32.8770\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.9361 - val_loss: 31.3424\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.0257 - val_loss: 34.6702\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.3013 - val_loss: 32.7576\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.8541 - val_loss: 30.2837\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.1093 - val_loss: 39.7382\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.9559 - val_loss: 32.4909\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.2566 - val_loss: 41.4639\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.5523 - val_loss: 31.0312\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.7020 - val_loss: 32.4731\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.3840 - val_loss: 29.9007\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.7788 - val_loss: 29.9377\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.1885 - val_loss: 38.5754\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.9030 - val_loss: 31.9843\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.1278 - val_loss: 30.3722\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.8625 - val_loss: 31.7224\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.6800 - val_loss: 31.2794\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.6510 - val_loss: 33.8277\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 19.8255 - val_loss: 38.1358\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.2276 - val_loss: 34.6697\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 20ms/step - loss: 19.1740 - val_loss: 34.1131\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.2602 - val_loss: 30.9918\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.8525 - val_loss: 34.5363\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.3769 - val_loss: 34.4656\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.4031 - val_loss: 32.1966\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.1067 - val_loss: 39.4692\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.5101 - val_loss: 36.0368\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.2353 - val_loss: 32.0511\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.1775 - val_loss: 35.9040\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.5694 - val_loss: 34.2111\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.0126 - val_loss: 31.7163\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.2917 - val_loss: 33.0859\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.2101 - val_loss: 31.5641\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 17.8656 - val_loss: 31.5377\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.4091 - val_loss: 33.4257\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.8792 - val_loss: 36.1302\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.7016 - val_loss: 31.8661\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.6798 - val_loss: 33.5821\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.1484 - val_loss: 31.7923\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.5689 - val_loss: 32.3095\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6033 - val_loss: 32.9958\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.7118 - val_loss: 31.1019\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6968 - val_loss: 35.9668\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 17.1010 - val_loss: 31.6355\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6059 - val_loss: 33.9190\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.0286 - val_loss: 32.7879\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 18.0889 - val_loss: 33.6154\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.8925 - val_loss: 32.5354\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.3476 - val_loss: 31.4879\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.6481 - val_loss: 30.8789\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.3819 - val_loss: 31.0762\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.1351 - val_loss: 34.8956\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.8131 - val_loss: 33.4562\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.8003 - val_loss: 32.0992\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.5362 - val_loss: 40.6032\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.1464 - val_loss: 30.6329\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.3678 - val_loss: 33.1595\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.4757 - val_loss: 31.9334\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.3865 - val_loss: 32.4298\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.1946 - val_loss: 38.0973\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.1779 - val_loss: 32.6448\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 15.8227 - val_loss: 31.1300\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 15.6342 - val_loss: 31.5475\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.0372 - val_loss: 38.1402\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 15.7731 - val_loss: 32.9648\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 32.953570317131806\n",
      "Mean Absolute Error (MAE): 3.796272788697211\n",
      "Root Mean Squared Error (RMSE): 5.740520038910395\n",
      "Time taken: 1553.0079486370087\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 23ms/step - loss: 1173.7783 - val_loss: 818.7825\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 660.3428 - val_loss: 571.5885\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 440.3417 - val_loss: 375.6294\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 309.8425 - val_loss: 263.4876\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 229.4446 - val_loss: 184.0368\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 8s 22ms/step - loss: 158.0401 - val_loss: 120.7504\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 114.0000 - val_loss: 86.3446\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 88.4209 - val_loss: 74.3500\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 74.9382 - val_loss: 78.9301\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 64.0389 - val_loss: 47.0100\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 57.6790 - val_loss: 52.1809\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 54.8032 - val_loss: 53.8735\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 52.6625 - val_loss: 47.9988\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 48.4230 - val_loss: 48.2768\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 48.9916 - val_loss: 58.2991\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 46.3033 - val_loss: 40.6728\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 46.4383 - val_loss: 39.5829\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 44.0839 - val_loss: 56.2230\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 44.5386 - val_loss: 70.1338\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 43.7950 - val_loss: 37.8226\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 42.6608 - val_loss: 46.8490\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 41.0758 - val_loss: 36.7978\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 41.0402 - val_loss: 40.6094\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 39.6426 - val_loss: 37.4026\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 41.3392 - val_loss: 38.3657\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.6326 - val_loss: 37.0831\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 40.1196 - val_loss: 43.5523\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 38.5801 - val_loss: 42.1254\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.9663 - val_loss: 45.1982\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.3687 - val_loss: 67.6343\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 37.6184 - val_loss: 50.7951\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 38.1670 - val_loss: 46.1168\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.7535 - val_loss: 34.3962\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.4186 - val_loss: 36.9293\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.8979 - val_loss: 44.8574\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.7078 - val_loss: 37.6144\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 34.1896 - val_loss: 39.1188\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.9060 - val_loss: 44.1906\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 34.2318 - val_loss: 42.5953\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 35.2237 - val_loss: 33.5208\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.7589 - val_loss: 36.3683\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 34.3095 - val_loss: 35.1121\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.1461 - val_loss: 34.8900\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.1809 - val_loss: 37.7245\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.4395 - val_loss: 40.5882\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.4837 - val_loss: 32.4597\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.5078 - val_loss: 36.5794\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.3593 - val_loss: 33.7634\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.9266 - val_loss: 33.2201\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.3249 - val_loss: 43.3037\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.9838 - val_loss: 36.4834\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.8798 - val_loss: 33.3385\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.3204 - val_loss: 31.9725\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.1598 - val_loss: 37.9366\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.4717 - val_loss: 32.3839\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 29.7327 - val_loss: 36.6946\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.0627 - val_loss: 41.2595\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 30.4130 - val_loss: 31.8132\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.5468 - val_loss: 35.2425\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 29.2018 - val_loss: 30.3672\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.8502 - val_loss: 37.0999\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.7541 - val_loss: 37.7744\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.3838 - val_loss: 32.4704\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.5059 - val_loss: 30.9860\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.5090 - val_loss: 31.5952\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.1644 - val_loss: 34.0410\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.7858 - val_loss: 33.9957\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.2439 - val_loss: 39.4702\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.4093 - val_loss: 32.3927\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.4312 - val_loss: 32.3858\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.1680 - val_loss: 41.8289\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.9799 - val_loss: 34.3540\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.7214 - val_loss: 32.4170\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.9104 - val_loss: 32.5215\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.0801 - val_loss: 35.1710\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.1714 - val_loss: 52.8001\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.2652 - val_loss: 37.6256\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 26.3664 - val_loss: 34.6000\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 20ms/step - loss: 26.2862 - val_loss: 34.0843\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.5200 - val_loss: 36.4973\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.1996 - val_loss: 34.7944\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.5245 - val_loss: 35.8777\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.2120 - val_loss: 33.6657\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.8074 - val_loss: 39.4561\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.8373 - val_loss: 31.1875\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.0186 - val_loss: 32.0560\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.1678 - val_loss: 33.2595\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.6533 - val_loss: 34.9562\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 25.6999 - val_loss: 31.7692\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 25.3175 - val_loss: 34.5484\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.0371 - val_loss: 33.6014\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 25.8226 - val_loss: 39.9874\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.4985 - val_loss: 37.6697\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.9682 - val_loss: 30.2010\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.7538 - val_loss: 35.4631\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.5989 - val_loss: 32.4113\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.8400 - val_loss: 35.7324\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.1619 - val_loss: 31.3968\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.8611 - val_loss: 34.8402\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.4644 - val_loss: 33.8042\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.1613 - val_loss: 33.5515\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.9531 - val_loss: 32.5204\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 24.7646 - val_loss: 33.3164\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.3582 - val_loss: 35.7029\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.6001 - val_loss: 33.7878\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.4200 - val_loss: 34.3839\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.8903 - val_loss: 41.6284\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.8421 - val_loss: 31.5223\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 23.5752 - val_loss: 34.9009\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.8434 - val_loss: 34.5532\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.7077 - val_loss: 35.5858\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.8297 - val_loss: 46.3771\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.6347 - val_loss: 33.0234\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 24.7357 - val_loss: 33.1920\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.8979 - val_loss: 33.3770\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.6935 - val_loss: 35.2725\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.1380 - val_loss: 32.3115\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 23.2780 - val_loss: 35.7117\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.7931 - val_loss: 32.7061\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 21.4496 - val_loss: 33.4221\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.0757 - val_loss: 33.2452\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 21.5403 - val_loss: 33.2938\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 22.6437 - val_loss: 33.3967\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.2295 - val_loss: 35.1035\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.0867 - val_loss: 32.5212\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.0338 - val_loss: 33.9997\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.0988 - val_loss: 32.0644\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.5807 - val_loss: 35.8570\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.7707 - val_loss: 34.8429\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.7397 - val_loss: 32.3919\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.5226 - val_loss: 32.4592\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.4052 - val_loss: 41.3556\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.0739 - val_loss: 31.8098\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.9572 - val_loss: 37.8919\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.7926 - val_loss: 34.8752\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 21.5632 - val_loss: 32.7092\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.2112 - val_loss: 31.6740\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.5709 - val_loss: 31.5492\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.6414 - val_loss: 32.0204\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.7703 - val_loss: 34.2071\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.4272 - val_loss: 35.4408\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.9016 - val_loss: 32.9719\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 21.0325 - val_loss: 30.2887\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.2881 - val_loss: 32.2946\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.9810 - val_loss: 38.0906\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.1819 - val_loss: 37.0155\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.0955 - val_loss: 31.8052\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 20.5463 - val_loss: 34.2254\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.7862 - val_loss: 32.1716\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.9277 - val_loss: 37.8796\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.9576 - val_loss: 38.2820\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 20.0979 - val_loss: 32.3916\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.6499 - val_loss: 32.9317\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.4107 - val_loss: 37.8535\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.2157 - val_loss: 31.9827\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 19ms/step - loss: 19.0772 - val_loss: 36.0892\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.9298 - val_loss: 35.1108\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.5017 - val_loss: 34.6527\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.1160 - val_loss: 35.7351\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.8548 - val_loss: 37.0866\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.6898 - val_loss: 31.0861\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 19.0311 - val_loss: 32.9472\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.3805 - val_loss: 32.0319\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.0018 - val_loss: 40.8066\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.1595 - val_loss: 34.5231\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 19.2481 - val_loss: 36.0988\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.2908 - val_loss: 34.2364\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.0014 - val_loss: 33.2869\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.9953 - val_loss: 35.5105\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 19.2185 - val_loss: 38.2937\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.0911 - val_loss: 35.1521\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.4815 - val_loss: 35.9111\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.1473 - val_loss: 34.0514\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.3628 - val_loss: 37.0359\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.5725 - val_loss: 37.9011\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.8305 - val_loss: 34.2162\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.6542 - val_loss: 38.2971\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 18.2240 - val_loss: 42.7206\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6059 - val_loss: 38.0461\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.5149 - val_loss: 36.5309\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 18.0112 - val_loss: 36.7358\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6293 - val_loss: 33.6696\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.6533 - val_loss: 36.2480\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.4886 - val_loss: 33.4079\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.2075 - val_loss: 37.6490\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.2710 - val_loss: 34.3081\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.9384 - val_loss: 33.5219\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 17.2578 - val_loss: 33.8060\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.9422 - val_loss: 34.6024\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.6529 - val_loss: 34.0304\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.0907 - val_loss: 35.6688\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.6956 - val_loss: 36.4537\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.4033 - val_loss: 33.6117\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.5889 - val_loss: 34.0976\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.0722 - val_loss: 36.4938\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 17.1704 - val_loss: 35.5424\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 15.9924 - val_loss: 38.7806\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 15.9298 - val_loss: 36.9857\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 16.2778 - val_loss: 34.2616\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 16.4008 - val_loss: 35.3717\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 35.398478947180166\n",
      "Mean Absolute Error (MAE): 3.8772182986855257\n",
      "Root Mean Squared Error (RMSE): 5.949662086806289\n",
      "Time taken: 1537.4554288387299\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 23ms/step - loss: 1224.9568 - val_loss: 858.5860\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 715.0670 - val_loss: 579.5812\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 483.9080 - val_loss: 373.3586\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 332.6581 - val_loss: 262.0755\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 232.1660 - val_loss: 171.0033\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 160.5682 - val_loss: 108.0427\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 114.4574 - val_loss: 117.5160\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 89.2679 - val_loss: 77.4769\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 71.8528 - val_loss: 45.5532\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 63.5469 - val_loss: 52.6681\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 58.4639 - val_loss: 49.8226\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 54.8584 - val_loss: 40.7583\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 51.2106 - val_loss: 37.9778\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 50.5234 - val_loss: 35.3403\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 49.5117 - val_loss: 33.4079\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 49.2443 - val_loss: 48.9194\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 48.6312 - val_loss: 46.5766\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 45.2912 - val_loss: 33.8671\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 43.9650 - val_loss: 36.7110\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 43.2906 - val_loss: 35.9641\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 44.2377 - val_loss: 33.9262\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 44.5878 - val_loss: 31.1996\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 43.1051 - val_loss: 32.4496\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 42.1946 - val_loss: 34.4235\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 42.0094 - val_loss: 42.0293\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 40.0200 - val_loss: 36.2791\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.6091 - val_loss: 41.5169\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 38.8690 - val_loss: 32.9151\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.9204 - val_loss: 39.5556\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.9125 - val_loss: 30.0558\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.2402 - val_loss: 45.8860\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 36.5350 - val_loss: 31.5387\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 36.6999 - val_loss: 31.3373\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 38.5418 - val_loss: 34.4258\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.6677 - val_loss: 36.7116\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 36.2101 - val_loss: 37.3287\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.5619 - val_loss: 37.3249\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.1706 - val_loss: 28.9205\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 35.9667 - val_loss: 30.8529\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 34.3411 - val_loss: 30.2068\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 34.7452 - val_loss: 29.5865\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 33.3693 - val_loss: 38.4203\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.9796 - val_loss: 32.5888\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.4367 - val_loss: 29.4198\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 34.0483 - val_loss: 35.1900\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 34.4097 - val_loss: 33.3118\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.4221 - val_loss: 29.6844\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.0374 - val_loss: 39.4212\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.3574 - val_loss: 36.8530\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 32.0732 - val_loss: 27.4186\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.4698 - val_loss: 29.7209\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.4224 - val_loss: 29.8061\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.5242 - val_loss: 27.6406\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.2395 - val_loss: 30.1652\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 32.5279 - val_loss: 33.5581\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.2268 - val_loss: 31.6036\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 31.7224 - val_loss: 32.2817\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 33.1753 - val_loss: 28.3696\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 29.7090 - val_loss: 30.5633\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.6887 - val_loss: 27.4157\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 29.2707 - val_loss: 28.2870\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.8327 - val_loss: 29.4328\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.6263 - val_loss: 37.8018\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 31.6565 - val_loss: 32.7819\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 30.0940 - val_loss: 30.0599\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.1479 - val_loss: 32.5876\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.6229 - val_loss: 28.8107\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 29.0994 - val_loss: 27.7668\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.6249 - val_loss: 29.1342\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.3728 - val_loss: 30.6434\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.3791 - val_loss: 29.7545\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 28.1583 - val_loss: 29.2953\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.2610 - val_loss: 28.3287\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.7378 - val_loss: 27.1217\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.4991 - val_loss: 41.3951\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.8558 - val_loss: 31.5787\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 28.4722 - val_loss: 29.2133\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 8s 19ms/step - loss: 27.4662 - val_loss: 30.6208\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 8s 19ms/step - loss: 26.8484 - val_loss: 28.5704\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.1764 - val_loss: 28.9826\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.5509 - val_loss: 31.0505\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.8006 - val_loss: 29.8629\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 27.0274 - val_loss: 29.4593\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 27.1282 - val_loss: 33.1906\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.5665 - val_loss: 28.6180\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 26.6729 - val_loss: 32.9979\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 10s 24ms/step - loss: 25.3928 - val_loss: 29.2712\n",
      "Epoch 88/200\n",
      " 22/391 [>.............................] - ETA: 6s - loss: 21.3943"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))  \n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3)) \n",
    "\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
