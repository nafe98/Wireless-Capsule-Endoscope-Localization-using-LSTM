{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor31</th>\n",
       "      <th>sensor32</th>\n",
       "      <th>sensor33</th>\n",
       "      <th>sensor34</th>\n",
       "      <th>sensor35</th>\n",
       "      <th>sensor36</th>\n",
       "      <th>sensor37</th>\n",
       "      <th>sensor38</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>98.486310</td>\n",
       "      <td>114.350441</td>\n",
       "      <td>94.640859</td>\n",
       "      <td>106.953540</td>\n",
       "      <td>64.978215</td>\n",
       "      <td>97.715284</td>\n",
       "      <td>100.958065</td>\n",
       "      <td>79.916748</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>98.554454</td>\n",
       "      <td>114.300289</td>\n",
       "      <td>94.502647</td>\n",
       "      <td>106.903363</td>\n",
       "      <td>64.852153</td>\n",
       "      <td>97.629184</td>\n",
       "      <td>100.748741</td>\n",
       "      <td>79.916828</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>98.619946</td>\n",
       "      <td>114.252414</td>\n",
       "      <td>94.363750</td>\n",
       "      <td>106.854902</td>\n",
       "      <td>64.726622</td>\n",
       "      <td>97.538685</td>\n",
       "      <td>100.540280</td>\n",
       "      <td>79.917199</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>98.682352</td>\n",
       "      <td>114.206770</td>\n",
       "      <td>94.224487</td>\n",
       "      <td>106.808302</td>\n",
       "      <td>64.601630</td>\n",
       "      <td>97.443768</td>\n",
       "      <td>100.332822</td>\n",
       "      <td>79.917498</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>98.741457</td>\n",
       "      <td>114.163396</td>\n",
       "      <td>94.085302</td>\n",
       "      <td>106.763575</td>\n",
       "      <td>64.477338</td>\n",
       "      <td>97.344503</td>\n",
       "      <td>100.126469</td>\n",
       "      <td>79.917435</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>120.348427</td>\n",
       "      <td>110.226970</td>\n",
       "      <td>65.937064</td>\n",
       "      <td>99.118810</td>\n",
       "      <td>89.378921</td>\n",
       "      <td>92.752195</td>\n",
       "      <td>107.325962</td>\n",
       "      <td>66.677708</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>120.207879</td>\n",
       "      <td>110.100697</td>\n",
       "      <td>65.974152</td>\n",
       "      <td>98.940990</td>\n",
       "      <td>89.576649</td>\n",
       "      <td>92.955555</td>\n",
       "      <td>107.285487</td>\n",
       "      <td>66.838702</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>120.065890</td>\n",
       "      <td>109.972625</td>\n",
       "      <td>66.009462</td>\n",
       "      <td>98.765885</td>\n",
       "      <td>89.775816</td>\n",
       "      <td>93.164896</td>\n",
       "      <td>107.242743</td>\n",
       "      <td>66.997592</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>119.922320</td>\n",
       "      <td>109.842729</td>\n",
       "      <td>66.043401</td>\n",
       "      <td>98.593356</td>\n",
       "      <td>89.976451</td>\n",
       "      <td>93.380239</td>\n",
       "      <td>107.197804</td>\n",
       "      <td>67.154415</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>119.777157</td>\n",
       "      <td>109.711116</td>\n",
       "      <td>66.076308</td>\n",
       "      <td>98.423175</td>\n",
       "      <td>90.178535</td>\n",
       "      <td>93.601502</td>\n",
       "      <td>107.150847</td>\n",
       "      <td>67.309163</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor31  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...   98.486310   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...   98.554454   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...   98.619946   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...   98.682352   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...   98.741457   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  120.348427   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  120.207879   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  120.065890   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  119.922320   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  119.777157   \n",
       "\n",
       "        sensor32   sensor33    sensor34   sensor35   sensor36    sensor37  \\\n",
       "0     114.350441  94.640859  106.953540  64.978215  97.715284  100.958065   \n",
       "1     114.300289  94.502647  106.903363  64.852153  97.629184  100.748741   \n",
       "2     114.252414  94.363750  106.854902  64.726622  97.538685  100.540280   \n",
       "3     114.206770  94.224487  106.808302  64.601630  97.443768  100.332822   \n",
       "4     114.163396  94.085302  106.763575  64.477338  97.344503  100.126469   \n",
       "...          ...        ...         ...        ...        ...         ...   \n",
       "2438  110.226970  65.937064   99.118810  89.378921  92.752195  107.325962   \n",
       "2439  110.100697  65.974152   98.940990  89.576649  92.955555  107.285487   \n",
       "2440  109.972625  66.009462   98.765885  89.775816  93.164896  107.242743   \n",
       "2441  109.842729  66.043401   98.593356  89.976451  93.380239  107.197804   \n",
       "2442  109.711116  66.076308   98.423175  90.178535  93.601502  107.150847   \n",
       "\n",
       "       sensor38   sensor39    sensor40  \n",
       "0     79.916748  80.511088   86.266577  \n",
       "1     79.916828  80.667880   86.301348  \n",
       "2     79.917199  80.823085   86.337129  \n",
       "3     79.917498  80.976531   86.373826  \n",
       "4     79.917435  81.128195   86.411394  \n",
       "...         ...        ...         ...  \n",
       "2438  66.677708  88.095891  110.712051  \n",
       "2439  66.838702  88.034588  110.548388  \n",
       "2440  66.997592  87.974760  110.380633  \n",
       "2441  67.154415  87.916419  110.208561  \n",
       "2442  67.309163  87.859482  110.032132  \n",
       "\n",
       "[2443 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:40]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 15s 21ms/step - loss: 1167.0251 - val_loss: 952.3694\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 920.0893 - val_loss: 927.2826\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 819.5822 - val_loss: 746.9665\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 519.0195 - val_loss: 356.3820\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 213.7822 - val_loss: 127.9678\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 80.8968 - val_loss: 49.8182\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 33.9850 - val_loss: 34.4517\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 15.9470 - val_loss: 9.9997\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.9816 - val_loss: 7.8088\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 5.2634 - val_loss: 4.6557\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.4717 - val_loss: 3.6716\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.6936 - val_loss: 2.6308\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.5552 - val_loss: 5.2451\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.3834 - val_loss: 1.5371\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.5404 - val_loss: 2.8855\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.4232 - val_loss: 1.3588\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5828 - val_loss: 3.1499\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6935 - val_loss: 43.6706\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.5654 - val_loss: 1.4409\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9627 - val_loss: 2.2383\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7086 - val_loss: 1.7221\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1571 - val_loss: 1.5865\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8856 - val_loss: 1.1491\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0246 - val_loss: 0.9780\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7709 - val_loss: 1.3059\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0080 - val_loss: 1.5327\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3710 - val_loss: 1.9106\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8877 - val_loss: 0.7056\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3387 - val_loss: 3.9595\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3228 - val_loss: 9.0241\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0416 - val_loss: 0.3870\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5472 - val_loss: 0.7620\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5057 - val_loss: 0.9766\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7784 - val_loss: 0.7716\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5782 - val_loss: 0.4263\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9733 - val_loss: 1.5300\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7820 - val_loss: 0.4069\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6954 - val_loss: 0.9003\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4149 - val_loss: 0.7277\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 5.2330 - val_loss: 1.7307\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4931 - val_loss: 0.5713\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3404 - val_loss: 0.3629\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3105 - val_loss: 0.2918\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3461 - val_loss: 0.5565\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3993 - val_loss: 0.3573\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7865 - val_loss: 2.2079\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3669 - val_loss: 0.8475\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3623 - val_loss: 0.4423\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3585 - val_loss: 0.3867\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4052 - val_loss: 0.5561\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4880 - val_loss: 0.9389\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6863 - val_loss: 1.0895\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3932 - val_loss: 0.5017\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6451 - val_loss: 0.8380\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5435 - val_loss: 0.8353\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2654 - val_loss: 0.5081\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3261 - val_loss: 1.3962\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8322 - val_loss: 0.3010\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3814 - val_loss: 0.7201\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2981 - val_loss: 0.7771\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3794 - val_loss: 0.9845\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3974 - val_loss: 0.2347\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2774 - val_loss: 0.4749\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3195 - val_loss: 0.6487\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4823 - val_loss: 0.3126\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3033 - val_loss: 1.1855\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3960 - val_loss: 0.5825\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3250 - val_loss: 0.2998\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1997 - val_loss: 0.4693\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2477 - val_loss: 1.0559\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3117 - val_loss: 18.6865\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3572 - val_loss: 0.2620\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1613 - val_loss: 0.1691\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2208 - val_loss: 0.4239\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1502 - val_loss: 0.4055\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3173 - val_loss: 0.6498\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3179 - val_loss: 0.7654\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4052 - val_loss: 0.8865\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2639 - val_loss: 0.1638\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1319 - val_loss: 0.2270\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2551 - val_loss: 0.4914\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4160 - val_loss: 0.5075\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2088 - val_loss: 0.3403\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2152 - val_loss: 0.2067\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3469 - val_loss: 0.3308\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2500 - val_loss: 0.3681\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1874 - val_loss: 0.2522\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2473 - val_loss: 0.2691\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1683 - val_loss: 0.4587\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2990 - val_loss: 0.1956\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3878 - val_loss: 0.4204\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1838 - val_loss: 0.2836\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1379 - val_loss: 0.3564\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2460 - val_loss: 0.8122\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3060 - val_loss: 1.1747\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2631 - val_loss: 0.1714\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2378 - val_loss: 0.2492\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1242 - val_loss: 0.1731\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3040 - val_loss: 0.6230\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.3371 - val_loss: 0.2733\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1813 - val_loss: 0.2069\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1257 - val_loss: 0.2069\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1005 - val_loss: 0.1296\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1032 - val_loss: 0.1023\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1296 - val_loss: 0.1547\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1328 - val_loss: 0.1151\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1400 - val_loss: 0.5241\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1769 - val_loss: 0.2812\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1778 - val_loss: 0.8324\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4024 - val_loss: 0.1724\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1299 - val_loss: 0.1916\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1358 - val_loss: 0.2034\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1681 - val_loss: 1.4496\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5050 - val_loss: 0.1042\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1160 - val_loss: 0.2081\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1062 - val_loss: 0.2352\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1615 - val_loss: 0.3851\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1509 - val_loss: 0.2169\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1610 - val_loss: 0.1799\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2640 - val_loss: 0.4706\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1676 - val_loss: 0.0814\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1839 - val_loss: 0.4602\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3492 - val_loss: 0.7253\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1718 - val_loss: 0.2179\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1727 - val_loss: 0.0884\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1555 - val_loss: 0.7883\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1870 - val_loss: 0.1622\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1339 - val_loss: 0.2181\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1112 - val_loss: 0.3904\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1875 - val_loss: 0.1275\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2281 - val_loss: 0.2051\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1225 - val_loss: 1.2139\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2322 - val_loss: 0.2065\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1042 - val_loss: 0.1208\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1194 - val_loss: 0.2692\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1282 - val_loss: 0.2745\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1782 - val_loss: 0.8326\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1180 - val_loss: 0.1076\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1305 - val_loss: 0.4744\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2775 - val_loss: 0.2951\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1635 - val_loss: 0.1360\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1383 - val_loss: 0.3571\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0962 - val_loss: 0.0873\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0987 - val_loss: 0.3002\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1312 - val_loss: 0.4482\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1940 - val_loss: 0.3300\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1888 - val_loss: 0.2062\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1100 - val_loss: 0.0797\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0964 - val_loss: 0.5035\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1214 - val_loss: 0.1773\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1677 - val_loss: 0.0849\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1748 - val_loss: 0.4911\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0905 - val_loss: 0.1541\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1691 - val_loss: 1.3763\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1068 - val_loss: 0.1136\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2435 - val_loss: 0.7583\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1332 - val_loss: 0.0745\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0592 - val_loss: 0.0926\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1268 - val_loss: 0.2483\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0579 - val_loss: 0.1051\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1704 - val_loss: 0.3475\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0814 - val_loss: 0.1010\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0823 - val_loss: 0.1315\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2573 - val_loss: 0.2150\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0816 - val_loss: 0.0646\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1463 - val_loss: 0.1630\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0794 - val_loss: 0.1351\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0749 - val_loss: 0.6167\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1461 - val_loss: 0.1947\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1012 - val_loss: 0.0988\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0973 - val_loss: 0.1997\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0871 - val_loss: 0.1008\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1099 - val_loss: 0.2610\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1432 - val_loss: 0.1988\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1106 - val_loss: 0.2785\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1015 - val_loss: 0.2388\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0890 - val_loss: 0.1519\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1095 - val_loss: 0.2156\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1304 - val_loss: 0.1647\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1392 - val_loss: 0.1026\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0925 - val_loss: 0.1996\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0598 - val_loss: 0.0885\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0817 - val_loss: 0.2640\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1512 - val_loss: 0.1061\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1140 - val_loss: 0.1074\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0801 - val_loss: 0.2353\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0904 - val_loss: 0.1500\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0953 - val_loss: 0.2584\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1056 - val_loss: 0.1569\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0906 - val_loss: 0.0928\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1098 - val_loss: 0.1757\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0686 - val_loss: 0.0787\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0657 - val_loss: 0.0964\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1163 - val_loss: 0.2826\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0941 - val_loss: 0.2133\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.06405507430990497\n",
      "Mean Absolute Error (MAE): 0.18425576855572592\n",
      "Root Mean Squared Error (RMSE): 0.25309103956858087\n",
      "Time taken: 955.3534197807312\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 17ms/step - loss: 1127.9698 - val_loss: 920.4161\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 876.6985 - val_loss: 794.1638\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 603.1807 - val_loss: 467.1067\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 358.7088 - val_loss: 306.5473\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 219.7950 - val_loss: 172.4825\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 104.3453 - val_loss: 76.4017\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 48.8055 - val_loss: 39.0880\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 24.7247 - val_loss: 18.6972\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 11.4905 - val_loss: 10.9111\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 7.1051 - val_loss: 5.9111\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 4.8859 - val_loss: 5.5246\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 4.0951 - val_loss: 3.9739\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.7501 - val_loss: 3.2112\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.4393 - val_loss: 1.4943\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7638 - val_loss: 2.7966\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5928 - val_loss: 1.2279\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7607 - val_loss: 2.7434\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5837 - val_loss: 1.7334\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.2836 - val_loss: 3.8974\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.4761 - val_loss: 0.7000\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9487 - val_loss: 0.8184\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7314 - val_loss: 2.2158\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7869 - val_loss: 0.9070\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 5.6894 - val_loss: 10.3486\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.9576 - val_loss: 1.2933\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7281 - val_loss: 0.5925\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7165 - val_loss: 0.8409\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4903 - val_loss: 0.4152\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4964 - val_loss: 0.7227\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.0322 - val_loss: 3.4988\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6321 - val_loss: 0.2734\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6600 - val_loss: 0.5129\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9237 - val_loss: 4.0497\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7419 - val_loss: 0.7745\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6086 - val_loss: 0.9727\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6671 - val_loss: 1.5271\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9311 - val_loss: 1.5381\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7012 - val_loss: 0.6735\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5499 - val_loss: 1.5384\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6586 - val_loss: 1.0560\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9693 - val_loss: 2.5402\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6057 - val_loss: 1.3181\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.8277 - val_loss: 11.5417\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5154 - val_loss: 0.6643\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2904 - val_loss: 0.5539\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3606 - val_loss: 0.4360\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3247 - val_loss: 0.7445\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6779 - val_loss: 0.6881\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4082 - val_loss: 0.5172\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5293 - val_loss: 1.4091\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5321 - val_loss: 1.2697\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5266 - val_loss: 0.4184\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6733 - val_loss: 0.4145\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2534 - val_loss: 0.2786\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2676 - val_loss: 0.5110\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6511 - val_loss: 0.7010\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3507 - val_loss: 2.6736\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2256 - val_loss: 0.4804\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4259 - val_loss: 0.3537\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.2313 - val_loss: 2.5125\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3347 - val_loss: 0.2611\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1814 - val_loss: 0.2443\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1614 - val_loss: 0.2094\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1808 - val_loss: 0.1949\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1879 - val_loss: 0.1988\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2692 - val_loss: 0.1636\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2869 - val_loss: 0.6184\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5563 - val_loss: 0.7844\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2659 - val_loss: 0.0985\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2085 - val_loss: 0.1669\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2457 - val_loss: 0.2698\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5105 - val_loss: 1.1271\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3679 - val_loss: 0.2408\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2209 - val_loss: 0.2145\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4596 - val_loss: 0.6417\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2803 - val_loss: 0.1637\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1816 - val_loss: 0.6555\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2887 - val_loss: 0.3509\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2453 - val_loss: 0.1626\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1655 - val_loss: 0.2044\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2771 - val_loss: 0.4324\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2318 - val_loss: 0.7520\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4232 - val_loss: 1.0474\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2513 - val_loss: 1.6213\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1679 - val_loss: 0.1270\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1194 - val_loss: 0.4790\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2238 - val_loss: 0.2982\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2163 - val_loss: 0.4031\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2030 - val_loss: 1.1343\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4947 - val_loss: 0.3471\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1630 - val_loss: 0.2928\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2737 - val_loss: 0.4392\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1737 - val_loss: 0.4163\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1704 - val_loss: 0.1923\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1617 - val_loss: 0.5346\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5568 - val_loss: 0.4945\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1280 - val_loss: 0.0978\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1306 - val_loss: 0.6102\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1696 - val_loss: 0.3269\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1856 - val_loss: 0.5686\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1941 - val_loss: 0.1141\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3016 - val_loss: 0.3900\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1760 - val_loss: 0.1469\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2123 - val_loss: 0.2226\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1466 - val_loss: 0.1038\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1397 - val_loss: 0.3219\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1693 - val_loss: 0.5473\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3183 - val_loss: 0.3191\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2541 - val_loss: 0.2197\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0971 - val_loss: 0.2598\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1127 - val_loss: 0.4373\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1956 - val_loss: 0.1836\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1842 - val_loss: 0.2389\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1208 - val_loss: 0.1940\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1645 - val_loss: 0.2326\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2238 - val_loss: 0.2359\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1410 - val_loss: 0.7011\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1249 - val_loss: 0.0877\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0641 - val_loss: 0.5478\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2805 - val_loss: 0.4682\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1614 - val_loss: 0.2305\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0748 - val_loss: 0.5104\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1128 - val_loss: 0.2192\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2370 - val_loss: 0.2669\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2081 - val_loss: 0.1831\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0967 - val_loss: 0.2439\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0687 - val_loss: 0.0878\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0607 - val_loss: 0.0389\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0556 - val_loss: 0.1557\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0688 - val_loss: 0.1801\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0797 - val_loss: 0.1628\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0747 - val_loss: 0.1557\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0886 - val_loss: 0.2283\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1357 - val_loss: 0.1062\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1019 - val_loss: 0.1173\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1443 - val_loss: 0.4277\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1717 - val_loss: 0.2821\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0798 - val_loss: 0.9987\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1034 - val_loss: 0.2263\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0938 - val_loss: 0.5040\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1337 - val_loss: 0.0565\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0944 - val_loss: 0.1680\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0938 - val_loss: 0.1100\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1377 - val_loss: 0.4503\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1530 - val_loss: 0.4652\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1639 - val_loss: 0.1077\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0788 - val_loss: 0.5438\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1554 - val_loss: 0.0987\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0963 - val_loss: 0.4168\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1093 - val_loss: 0.1367\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1838 - val_loss: 0.8550\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0767 - val_loss: 0.0587\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1137 - val_loss: 0.1443\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1416 - val_loss: 0.2338\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1567 - val_loss: 0.0977\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1079 - val_loss: 0.1524\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0759 - val_loss: 0.3726\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1868 - val_loss: 0.1371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.03895673190171267\n",
      "Mean Absolute Error (MAE): 0.1445228774905103\n",
      "Root Mean Squared Error (RMSE): 0.19737459791399872\n",
      "Time taken: 761.283910036087\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 18ms/step - loss: 1111.8303 - val_loss: 915.1115\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 922.5151 - val_loss: 905.2746\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 852.0217 - val_loss: 765.8063\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 520.4440 - val_loss: 356.1929\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 210.5810 - val_loss: 138.6481\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 75.8473 - val_loss: 66.5785\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 29.1310 - val_loss: 25.6047\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 14.6140 - val_loss: 12.8646\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.6924 - val_loss: 6.5992\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.4580 - val_loss: 7.5305\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.8476 - val_loss: 2.4578\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9148 - val_loss: 8.4876\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.1152 - val_loss: 5.9442\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.8417 - val_loss: 1.3466\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.6092 - val_loss: 2.6602\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5190 - val_loss: 3.9270\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.6816 - val_loss: 2.4396\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2217 - val_loss: 1.4727\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.4629 - val_loss: 7.0751\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5015 - val_loss: 2.7669\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0878 - val_loss: 1.2051\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9132 - val_loss: 1.3142\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 7.4304 - val_loss: 2.7873\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0678 - val_loss: 1.9235\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7091 - val_loss: 0.5742\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6814 - val_loss: 0.8825\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6467 - val_loss: 3.0950\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7835 - val_loss: 2.7535\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7284 - val_loss: 0.5172\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6948 - val_loss: 1.1459\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9023 - val_loss: 4.5207\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7905 - val_loss: 2.5000\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9419 - val_loss: 3.6561\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7718 - val_loss: 1.1592\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4268 - val_loss: 0.8271\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5691 - val_loss: 0.5392\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4136 - val_loss: 0.3896\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4929 - val_loss: 0.6952\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5783 - val_loss: 1.5592\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5880 - val_loss: 0.7894\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3071 - val_loss: 0.7102\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6662 - val_loss: 0.5228\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7137 - val_loss: 1.4409\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2934 - val_loss: 0.4560\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4478 - val_loss: 0.6644\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3533 - val_loss: 0.4675\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5144 - val_loss: 0.6100\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4950 - val_loss: 0.2973\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6366 - val_loss: 1.2101\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4021 - val_loss: 0.5673\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4982 - val_loss: 0.9394\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4107 - val_loss: 0.8907\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4540 - val_loss: 0.7224\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2433 - val_loss: 0.2144\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2660 - val_loss: 0.7794\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8406 - val_loss: 4.4172\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7070 - val_loss: 0.7907\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1846 - val_loss: 0.3150\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2981 - val_loss: 0.5882\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2172 - val_loss: 0.3943\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2501 - val_loss: 0.5539\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4773 - val_loss: 3.4307\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4105 - val_loss: 0.2792\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1793 - val_loss: 0.5900\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3742 - val_loss: 0.4287\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1806 - val_loss: 0.5655\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2536 - val_loss: 0.2484\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2369 - val_loss: 0.5733\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4032 - val_loss: 1.0343\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2933 - val_loss: 1.1596\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5219 - val_loss: 0.1799\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1146 - val_loss: 0.2087\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1494 - val_loss: 0.4409\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6675 - val_loss: 0.1577\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1627 - val_loss: 0.1221\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1674 - val_loss: 0.4419\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4033 - val_loss: 0.2883\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2139 - val_loss: 0.1504\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1590 - val_loss: 0.2310\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2029 - val_loss: 0.8465\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2282 - val_loss: 0.9676\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5142 - val_loss: 0.3079\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1405 - val_loss: 0.6467\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1253 - val_loss: 0.2904\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1585 - val_loss: 0.2815\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2918 - val_loss: 0.6714\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2122 - val_loss: 0.5364\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2164 - val_loss: 0.1895\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1382 - val_loss: 0.1820\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1846 - val_loss: 0.2391\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1544 - val_loss: 0.1886\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2161 - val_loss: 3.9523\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7097 - val_loss: 0.1989\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1610 - val_loss: 0.3880\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0787 - val_loss: 0.1575\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1391 - val_loss: 0.2082\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1363 - val_loss: 0.5648\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1854 - val_loss: 0.1786\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2119 - val_loss: 0.6441\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1184 - val_loss: 0.4391\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1919 - val_loss: 0.2169\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2075 - val_loss: 0.4846\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2502 - val_loss: 0.0959\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1669 - val_loss: 0.1203\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0822 - val_loss: 0.2456\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2397 - val_loss: 0.2548\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2054 - val_loss: 0.2830\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1204 - val_loss: 0.0619\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1909 - val_loss: 0.6825\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1876 - val_loss: 0.3625\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2953 - val_loss: 0.0836\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0726 - val_loss: 0.1246\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1609 - val_loss: 0.2459\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0782 - val_loss: 0.0953\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1798 - val_loss: 0.3736\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1597 - val_loss: 0.4867\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1989 - val_loss: 0.1848\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0937 - val_loss: 0.1086\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1587 - val_loss: 0.2576\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1309 - val_loss: 0.3315\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1443 - val_loss: 0.1079\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1155 - val_loss: 0.1612\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0921 - val_loss: 0.1382\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1503 - val_loss: 0.2027\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2076 - val_loss: 0.2087\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1388 - val_loss: 0.0772\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1035 - val_loss: 0.1637\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2147 - val_loss: 1.4994\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1140 - val_loss: 0.3700\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0794 - val_loss: 0.1082\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1673 - val_loss: 0.2188\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1221 - val_loss: 0.3766\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1346 - val_loss: 0.0782\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0822 - val_loss: 0.2740\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1400 - val_loss: 0.1661\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0830 - val_loss: 0.5941\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0949 - val_loss: 0.1862\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1563 - val_loss: 0.2856\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.06184424277974102\n",
      "Mean Absolute Error (MAE): 0.1859800184711737\n",
      "Root Mean Squared Error (RMSE): 0.24868502725283045\n",
      "Time taken: 662.9181416034698\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 19ms/step - loss: 1141.1398 - val_loss: 957.4061\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 877.6487 - val_loss: 821.4532\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 651.5029 - val_loss: 538.5959\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 421.0484 - val_loss: 325.5244\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 285.0051 - val_loss: 203.5856\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 147.5752 - val_loss: 90.1650\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 64.1941 - val_loss: 37.8595\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 27.1636 - val_loss: 13.5588\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 13.9248 - val_loss: 12.6288\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 9.0524 - val_loss: 7.8028\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.4870 - val_loss: 4.6551\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.4131 - val_loss: 2.7202\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.8520 - val_loss: 3.8394\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.9017 - val_loss: 0.9640\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.5721 - val_loss: 1.5528\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2924 - val_loss: 2.8063\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.6980 - val_loss: 6.2027\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7545 - val_loss: 1.2365\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2254 - val_loss: 2.5863\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.7061 - val_loss: 1.1359\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2479 - val_loss: 1.5399\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0230 - val_loss: 1.2543\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.1269 - val_loss: 1.2859\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.2597 - val_loss: 3.6502\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2189 - val_loss: 0.8603\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7562 - val_loss: 1.1579\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7926 - val_loss: 3.6499\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7623 - val_loss: 0.4016\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5580 - val_loss: 1.1183\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4840 - val_loss: 2.6273\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7888 - val_loss: 2.1403\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6300 - val_loss: 0.6283\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5051 - val_loss: 0.6495\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7270 - val_loss: 0.4978\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6078 - val_loss: 0.4650\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 6.1089 - val_loss: 3.0133\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0845 - val_loss: 0.7148\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5027 - val_loss: 0.9303\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4480 - val_loss: 0.3376\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4086 - val_loss: 0.4668\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6051 - val_loss: 0.3849\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3732 - val_loss: 0.4097\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5089 - val_loss: 0.3543\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8427 - val_loss: 3.1054\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3703 - val_loss: 0.2776\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2926 - val_loss: 0.2849\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7680 - val_loss: 1.4442\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4706 - val_loss: 0.2872\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6103 - val_loss: 1.8732\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4518 - val_loss: 0.4664\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2738 - val_loss: 0.2992\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6245 - val_loss: 1.1582\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5859 - val_loss: 0.8993\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7400 - val_loss: 0.2687\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2101 - val_loss: 0.7625\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4488 - val_loss: 0.4946\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4514 - val_loss: 2.1047\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4259 - val_loss: 0.4213\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4105 - val_loss: 0.4883\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3404 - val_loss: 1.5197\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5355 - val_loss: 1.4589\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5248 - val_loss: 0.5016\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2665 - val_loss: 0.2857\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2535 - val_loss: 0.5748\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3135 - val_loss: 0.3742\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4664 - val_loss: 1.4066\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4846 - val_loss: 0.6277\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3466 - val_loss: 1.7939\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4453 - val_loss: 0.3791\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2408 - val_loss: 0.3547\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2906 - val_loss: 0.9870\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6576 - val_loss: 0.5037\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1741 - val_loss: 0.1486\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1379 - val_loss: 0.4267\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2001 - val_loss: 0.2599\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5082 - val_loss: 0.9350\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2277 - val_loss: 0.5287\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1610 - val_loss: 0.2405\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3088 - val_loss: 1.1639\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6789 - val_loss: 0.5028\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2035 - val_loss: 0.6954\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2098 - val_loss: 0.2834\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1186 - val_loss: 0.4513\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3808 - val_loss: 0.4370\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2238 - val_loss: 0.3882\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1825 - val_loss: 0.2351\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1587 - val_loss: 0.3122\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2255 - val_loss: 0.7320\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4448 - val_loss: 0.6266\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1887 - val_loss: 0.5479\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2022 - val_loss: 0.5341\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1751 - val_loss: 0.4072\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1815 - val_loss: 0.2575\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2683 - val_loss: 0.5148\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7847 - val_loss: 12.1469\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5513 - val_loss: 0.5161\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1195 - val_loss: 0.1293\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1173 - val_loss: 0.4196\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1014 - val_loss: 0.1138\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1282 - val_loss: 0.2623\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1553 - val_loss: 0.1493\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1466 - val_loss: 0.4147\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4389 - val_loss: 0.2874\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1612 - val_loss: 0.1583\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1603 - val_loss: 0.1878\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1723 - val_loss: 0.0867\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1760 - val_loss: 0.3300\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2949 - val_loss: 1.7333\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1488 - val_loss: 0.3233\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1332 - val_loss: 0.6316\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1731 - val_loss: 0.6530\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2342 - val_loss: 0.1857\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1375 - val_loss: 0.1536\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1602 - val_loss: 0.3306\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0893 - val_loss: 0.3497\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2046 - val_loss: 0.5637\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2031 - val_loss: 0.2737\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2558 - val_loss: 0.2922\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1726 - val_loss: 0.1017\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1483 - val_loss: 0.1466\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0973 - val_loss: 0.1812\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2508 - val_loss: 0.1279\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1493 - val_loss: 0.3105\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1396 - val_loss: 0.5209\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1232 - val_loss: 0.5363\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1766 - val_loss: 0.1711\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1159 - val_loss: 0.7637\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2175 - val_loss: 0.2246\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1028 - val_loss: 0.2525\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2225 - val_loss: 1.0446\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0929 - val_loss: 0.1634\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1134 - val_loss: 0.1687\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1350 - val_loss: 0.3809\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1622 - val_loss: 0.1152\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1886 - val_loss: 0.4183\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1416 - val_loss: 0.1381\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.08648949166787882\n",
      "Mean Absolute Error (MAE): 0.21827337914493694\n",
      "Root Mean Squared Error (RMSE): 0.29409095815389974\n",
      "Time taken: 660.3125941753387\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 18ms/step - loss: 1152.7900 - val_loss: 903.9522\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 919.4861 - val_loss: 896.5602\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 708.4350 - val_loss: 490.3222\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 384.8368 - val_loss: 232.9641\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 172.4704 - val_loss: 105.5980\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 68.8211 - val_loss: 47.7140\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 32.1153 - val_loss: 23.7387\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 15.7692 - val_loss: 9.8471\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 9.1671 - val_loss: 20.2608\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.7583 - val_loss: 3.0627\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.6697 - val_loss: 2.8621\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0982 - val_loss: 3.0573\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.4093 - val_loss: 1.7835\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.9828 - val_loss: 1.6660\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.0003 - val_loss: 4.0254\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.7053 - val_loss: 4.4214\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3337 - val_loss: 1.6678\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3650 - val_loss: 1.6501\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2218 - val_loss: 0.6044\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1640 - val_loss: 1.1428\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1589 - val_loss: 4.8269\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.4315 - val_loss: 2.2499\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2764 - val_loss: 0.8593\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8304 - val_loss: 0.8910\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8501 - val_loss: 1.6026\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3927 - val_loss: 2.3014\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.1918 - val_loss: 0.6816\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6460 - val_loss: 1.0674\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5901 - val_loss: 0.8404\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4835 - val_loss: 0.4296\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6086 - val_loss: 1.3142\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4861 - val_loss: 0.3791\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0435 - val_loss: 0.8908\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.1830 - val_loss: 1.5984\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0683 - val_loss: 0.5873\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5272 - val_loss: 1.1448\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5081 - val_loss: 0.5578\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6057 - val_loss: 0.5026\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6385 - val_loss: 2.0975\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9117 - val_loss: 0.6788\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5580 - val_loss: 0.6722\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8391 - val_loss: 1.3074\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4076 - val_loss: 0.8078\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6237 - val_loss: 2.1039\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8862 - val_loss: 0.6312\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4323 - val_loss: 1.0249\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4289 - val_loss: 1.4235\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5219 - val_loss: 0.4231\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 4.6156 - val_loss: 0.8266\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4247 - val_loss: 0.4501\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2790 - val_loss: 0.4313\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2045 - val_loss: 0.2550\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2522 - val_loss: 0.1611\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3789 - val_loss: 0.2952\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8006 - val_loss: 0.9005\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3681 - val_loss: 0.7022\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2832 - val_loss: 0.5501\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4073 - val_loss: 0.2572\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3091 - val_loss: 0.8913\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3098 - val_loss: 0.4178\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5473 - val_loss: 0.6559\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7361 - val_loss: 0.6283\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4141 - val_loss: 0.8382\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2945 - val_loss: 0.3925\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2188 - val_loss: 0.2605\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3223 - val_loss: 0.5874\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5651 - val_loss: 1.7507\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4670 - val_loss: 0.4289\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3441 - val_loss: 0.5459\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2179 - val_loss: 0.6236\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3472 - val_loss: 1.7607\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4407 - val_loss: 0.1617\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3017 - val_loss: 0.3063\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3039 - val_loss: 0.2776\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2933 - val_loss: 0.4507\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5057 - val_loss: 0.4821\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3106 - val_loss: 0.1656\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2502 - val_loss: 0.3826\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1718 - val_loss: 0.6613\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6325 - val_loss: 0.2097\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2106 - val_loss: 0.1328\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1389 - val_loss: 0.2872\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3617 - val_loss: 0.3396\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3194 - val_loss: 0.6839\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3341 - val_loss: 0.1733\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1411 - val_loss: 0.3900\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2762 - val_loss: 0.4890\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2208 - val_loss: 0.1410\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4070 - val_loss: 1.3813\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3669 - val_loss: 0.5322\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2010 - val_loss: 0.1625\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3374 - val_loss: 0.1608\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1430 - val_loss: 0.1612\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2438 - val_loss: 0.2136\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2450 - val_loss: 0.3747\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2078 - val_loss: 0.2492\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2663 - val_loss: 0.2273\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3688 - val_loss: 0.7215\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1180 - val_loss: 0.2074\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1683 - val_loss: 0.1534\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2360 - val_loss: 0.6813\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2086 - val_loss: 0.1769\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2017 - val_loss: 0.3202\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1740 - val_loss: 0.1652\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2160 - val_loss: 0.1723\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1293 - val_loss: 0.4446\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4245 - val_loss: 0.3122\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1692 - val_loss: 0.0849\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0960 - val_loss: 0.1640\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1889 - val_loss: 0.2156\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2130 - val_loss: 0.3917\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2163 - val_loss: 0.6575\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2093 - val_loss: 0.6802\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2612 - val_loss: 0.1477\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1274 - val_loss: 0.1633\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1639 - val_loss: 0.2958\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2385 - val_loss: 0.1684\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1142 - val_loss: 0.4041\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2538 - val_loss: 0.2892\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2149 - val_loss: 0.1541\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1198 - val_loss: 0.5921\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2722 - val_loss: 0.2352\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0979 - val_loss: 0.0862\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1158 - val_loss: 0.1323\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1759 - val_loss: 0.2873\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1604 - val_loss: 0.1989\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2721 - val_loss: 0.3570\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1780 - val_loss: 0.1235\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0715 - val_loss: 0.1063\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1192 - val_loss: 0.3307\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1811 - val_loss: 0.2646\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1719 - val_loss: 0.1317\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1460 - val_loss: 0.4065\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2041 - val_loss: 0.2042\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1140 - val_loss: 0.2322\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1627 - val_loss: 0.1251\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1937 - val_loss: 0.8573\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2267 - val_loss: 0.0689\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0912 - val_loss: 0.0626\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0940 - val_loss: 0.0939\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1077 - val_loss: 0.1198\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1236 - val_loss: 0.2307\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1356 - val_loss: 0.4971\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1843 - val_loss: 0.3491\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2115 - val_loss: 0.1837\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0828 - val_loss: 0.0688\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0837 - val_loss: 0.2561\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1832 - val_loss: 0.1634\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0884 - val_loss: 0.2516\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0936 - val_loss: 0.3767\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1311 - val_loss: 0.1835\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1784 - val_loss: 0.2330\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0807 - val_loss: 0.2031\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1259 - val_loss: 0.1710\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1317 - val_loss: 0.2413\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2541 - val_loss: 0.1486\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0749 - val_loss: 0.1719\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0946 - val_loss: 0.0852\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0940 - val_loss: 0.1730\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1620 - val_loss: 0.3278\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1509 - val_loss: 0.1803\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0997 - val_loss: 0.1770\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1033 - val_loss: 0.1125\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1476 - val_loss: 0.0850\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1117 - val_loss: 0.3269\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1695 - val_loss: 0.1923\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1393 - val_loss: 0.1751\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0773 - val_loss: 0.0944\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0962 - val_loss: 0.0964\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.06234935906120595\n",
      "Mean Absolute Error (MAE): 0.1795918643999966\n",
      "Root Mean Squared Error (RMSE): 0.24969853636176154\n",
      "Time taken: 820.5734989643097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 40, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 40, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 40, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 40, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_4576\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.064055  0.184256  0.253091  955.353420\n",
      "1        2  0.038957  0.144523  0.197375  761.283910\n",
      "2        3  0.061844  0.185980  0.248685  662.918142\n",
      "3        4  0.086489  0.218273  0.294091  660.312594\n",
      "4        5  0.062349  0.179592  0.249699  820.573499\n",
      "5  Average  0.062739  0.182525  0.248588  772.088313\n",
      "Results saved to 'Sensors 40_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 40_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 40_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
