{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "      <th>sensor17</th>\n",
       "      <th>sensor18</th>\n",
       "      <th>sensor19</th>\n",
       "      <th>sensor20</th>\n",
       "      <th>sensor21</th>\n",
       "      <th>sensor22</th>\n",
       "      <th>sensor23</th>\n",
       "      <th>sensor24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>105.712779</td>\n",
       "      <td>121.945497</td>\n",
       "      <td>99.432166</td>\n",
       "      <td>128.899324</td>\n",
       "      <td>58.501333</td>\n",
       "      <td>110.130325</td>\n",
       "      <td>114.460375</td>\n",
       "      <td>137.944819</td>\n",
       "      <td>86.571673</td>\n",
       "      <td>122.829726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>105.436000</td>\n",
       "      <td>121.824863</td>\n",
       "      <td>99.434655</td>\n",
       "      <td>128.945618</td>\n",
       "      <td>58.667602</td>\n",
       "      <td>110.053984</td>\n",
       "      <td>114.213203</td>\n",
       "      <td>137.995399</td>\n",
       "      <td>86.320894</td>\n",
       "      <td>122.586872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>105.163506</td>\n",
       "      <td>121.700246</td>\n",
       "      <td>99.436847</td>\n",
       "      <td>128.991865</td>\n",
       "      <td>58.829436</td>\n",
       "      <td>109.977993</td>\n",
       "      <td>113.965633</td>\n",
       "      <td>138.043376</td>\n",
       "      <td>86.068453</td>\n",
       "      <td>122.344712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>104.895214</td>\n",
       "      <td>121.571460</td>\n",
       "      <td>99.438800</td>\n",
       "      <td>129.038635</td>\n",
       "      <td>58.986556</td>\n",
       "      <td>109.902395</td>\n",
       "      <td>113.717897</td>\n",
       "      <td>138.088554</td>\n",
       "      <td>85.814484</td>\n",
       "      <td>122.103192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>104.631075</td>\n",
       "      <td>121.438527</td>\n",
       "      <td>99.440355</td>\n",
       "      <td>129.086514</td>\n",
       "      <td>59.138762</td>\n",
       "      <td>109.827363</td>\n",
       "      <td>113.470118</td>\n",
       "      <td>138.130735</td>\n",
       "      <td>85.559264</td>\n",
       "      <td>121.862268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>119.305234</td>\n",
       "      <td>110.972701</td>\n",
       "      <td>132.321305</td>\n",
       "      <td>111.864702</td>\n",
       "      <td>94.890228</td>\n",
       "      <td>67.225165</td>\n",
       "      <td>139.161757</td>\n",
       "      <td>124.658162</td>\n",
       "      <td>116.104149</td>\n",
       "      <td>93.524410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>119.403156</td>\n",
       "      <td>111.115335</td>\n",
       "      <td>132.401446</td>\n",
       "      <td>111.784564</td>\n",
       "      <td>94.752109</td>\n",
       "      <td>67.131476</td>\n",
       "      <td>139.103427</td>\n",
       "      <td>124.568659</td>\n",
       "      <td>116.135667</td>\n",
       "      <td>93.472193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>119.503552</td>\n",
       "      <td>111.260795</td>\n",
       "      <td>132.483216</td>\n",
       "      <td>111.704794</td>\n",
       "      <td>94.613702</td>\n",
       "      <td>67.034952</td>\n",
       "      <td>139.045788</td>\n",
       "      <td>124.478127</td>\n",
       "      <td>116.168216</td>\n",
       "      <td>93.418615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>119.606420</td>\n",
       "      <td>111.408884</td>\n",
       "      <td>132.566582</td>\n",
       "      <td>111.625492</td>\n",
       "      <td>94.475247</td>\n",
       "      <td>66.935847</td>\n",
       "      <td>138.989106</td>\n",
       "      <td>124.386573</td>\n",
       "      <td>116.202245</td>\n",
       "      <td>93.363889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>119.711606</td>\n",
       "      <td>111.559314</td>\n",
       "      <td>132.651472</td>\n",
       "      <td>111.546756</td>\n",
       "      <td>94.337191</td>\n",
       "      <td>66.834354</td>\n",
       "      <td>138.933604</td>\n",
       "      <td>124.293977</td>\n",
       "      <td>116.238161</td>\n",
       "      <td>93.308119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor15  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  105.712779   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  105.436000   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  105.163506   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  104.895214   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  104.631075   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  119.305234   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  119.403156   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  119.503552   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  119.606420   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  119.711606   \n",
       "\n",
       "        sensor16    sensor17    sensor18   sensor19    sensor20    sensor21  \\\n",
       "0     121.945497   99.432166  128.899324  58.501333  110.130325  114.460375   \n",
       "1     121.824863   99.434655  128.945618  58.667602  110.053984  114.213203   \n",
       "2     121.700246   99.436847  128.991865  58.829436  109.977993  113.965633   \n",
       "3     121.571460   99.438800  129.038635  58.986556  109.902395  113.717897   \n",
       "4     121.438527   99.440355  129.086514  59.138762  109.827363  113.470118   \n",
       "...          ...         ...         ...        ...         ...         ...   \n",
       "2438  110.972701  132.321305  111.864702  94.890228   67.225165  139.161757   \n",
       "2439  111.115335  132.401446  111.784564  94.752109   67.131476  139.103427   \n",
       "2440  111.260795  132.483216  111.704794  94.613702   67.034952  139.045788   \n",
       "2441  111.408884  132.566582  111.625492  94.475247   66.935847  138.989106   \n",
       "2442  111.559314  132.651472  111.546756  94.337191   66.834354  138.933604   \n",
       "\n",
       "        sensor22    sensor23    sensor24  \n",
       "0     137.944819   86.571673  122.829726  \n",
       "1     137.995399   86.320894  122.586872  \n",
       "2     138.043376   86.068453  122.344712  \n",
       "3     138.088554   85.814484  122.103192  \n",
       "4     138.130735   85.559264  121.862268  \n",
       "...          ...         ...         ...  \n",
       "2438  124.658162  116.104149   93.524410  \n",
       "2439  124.568659  116.135667   93.472193  \n",
       "2440  124.478127  116.168216   93.418615  \n",
       "2441  124.386573  116.202245   93.363889  \n",
       "2442  124.293977  116.238161   93.308119  \n",
       "\n",
       "[2443 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:24]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 12s 18ms/step - loss: 1082.9247 - val_loss: 932.8248\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 835.3924 - val_loss: 703.1025\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 545.4095 - val_loss: 425.8346\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 325.1036 - val_loss: 262.2670\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 152.7034 - val_loss: 102.3773\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 62.5787 - val_loss: 35.7968\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 31.1155 - val_loss: 20.3086\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 15.6844 - val_loss: 36.1962\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 12.1253 - val_loss: 8.5366\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.3133 - val_loss: 3.5780\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2416 - val_loss: 5.2514\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5287 - val_loss: 3.1257\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.5906 - val_loss: 4.5654\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.7803 - val_loss: 3.0766\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3669 - val_loss: 1.9309\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.4944 - val_loss: 1.0607\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.8688 - val_loss: 2.7471\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.0885 - val_loss: 1.5931\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5316 - val_loss: 1.2376\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3705 - val_loss: 1.0847\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5036 - val_loss: 1.4388\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4564 - val_loss: 0.7993\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9758 - val_loss: 1.3528\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2205 - val_loss: 0.6527\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.8029 - val_loss: 8.0245\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5717 - val_loss: 0.7817\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8337 - val_loss: 1.5868\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8419 - val_loss: 0.9748\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8305 - val_loss: 1.0455\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7592 - val_loss: 0.6556\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8895 - val_loss: 0.9792\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8768 - val_loss: 0.6853\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9244 - val_loss: 0.9624\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6805 - val_loss: 0.7964\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7102 - val_loss: 2.4366\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6798 - val_loss: 1.1646\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4567 - val_loss: 0.4905\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5322 - val_loss: 0.5206\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5124 - val_loss: 1.1652\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7356 - val_loss: 0.4233\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6293 - val_loss: 1.2204\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5240 - val_loss: 1.5759\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7188 - val_loss: 0.9427\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5365 - val_loss: 2.2120\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4484 - val_loss: 0.2474\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3024 - val_loss: 0.2341\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3396 - val_loss: 0.5950\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5395 - val_loss: 0.2946\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5336 - val_loss: 0.4965\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4982 - val_loss: 0.2528\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4055 - val_loss: 0.5673\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5028 - val_loss: 0.4884\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4575 - val_loss: 0.5197\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4611 - val_loss: 0.3696\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4600 - val_loss: 0.5658\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5463 - val_loss: 0.8862\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6921 - val_loss: 0.1968\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2794 - val_loss: 0.3300\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8503 - val_loss: 0.4547\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2236 - val_loss: 0.1582\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2012 - val_loss: 0.3063\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3684 - val_loss: 0.4235\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4060 - val_loss: 0.4210\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3189 - val_loss: 0.3380\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4561 - val_loss: 0.8319\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5542 - val_loss: 0.4518\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3084 - val_loss: 0.3898\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2450 - val_loss: 0.3561\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2825 - val_loss: 0.3320\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4656 - val_loss: 4.3924\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4854 - val_loss: 0.5690\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2870 - val_loss: 1.0756\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3255 - val_loss: 0.2175\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2636 - val_loss: 0.4663\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2691 - val_loss: 0.4444\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2871 - val_loss: 0.5423\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4986 - val_loss: 0.1993\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2350 - val_loss: 0.6544\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2936 - val_loss: 0.3197\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2245 - val_loss: 0.5447\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4097 - val_loss: 0.2814\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1896 - val_loss: 0.2284\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2747 - val_loss: 1.0980\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3417 - val_loss: 0.3588\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1823 - val_loss: 0.1316\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1692 - val_loss: 0.4182\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1904 - val_loss: 1.3390\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4221 - val_loss: 1.6744\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3330 - val_loss: 1.2624\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.2811 - val_loss: 1.1356\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1954 - val_loss: 0.2403\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1323 - val_loss: 0.1981\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1182 - val_loss: 0.0998\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1013 - val_loss: 0.1781\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1365 - val_loss: 0.3443\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2184 - val_loss: 0.2689\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1367 - val_loss: 0.2268\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2133 - val_loss: 0.1978\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2436 - val_loss: 0.1691\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2100 - val_loss: 0.4147\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2268 - val_loss: 0.3477\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4580 - val_loss: 0.1880\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1248 - val_loss: 0.1833\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1512 - val_loss: 0.1949\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2261 - val_loss: 0.4776\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2783 - val_loss: 0.8773\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4304 - val_loss: 0.1403\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1584 - val_loss: 0.1401\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1556 - val_loss: 0.1596\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2061 - val_loss: 0.8461\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2658 - val_loss: 0.4481\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1407 - val_loss: 0.2558\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1818 - val_loss: 0.5228\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2308 - val_loss: 0.2929\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1215 - val_loss: 0.4335\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2852 - val_loss: 0.3903\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1919 - val_loss: 0.2365\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1537 - val_loss: 0.3002\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2025 - val_loss: 0.1395\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2236 - val_loss: 0.3635\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1953 - val_loss: 0.4607\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3626 - val_loss: 0.2131\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1019 - val_loss: 0.0864\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2257 - val_loss: 0.7652\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1351 - val_loss: 0.4080\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2005 - val_loss: 0.7291\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1458 - val_loss: 0.1119\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1213 - val_loss: 0.1520\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1629 - val_loss: 0.2192\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1440 - val_loss: 0.4838\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1249 - val_loss: 0.3226\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4066 - val_loss: 1.3596\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1517 - val_loss: 0.2664\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0603 - val_loss: 0.0532\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0493 - val_loss: 0.0868\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0504 - val_loss: 0.0585\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0823 - val_loss: 0.2147\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0770 - val_loss: 0.0633\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0964 - val_loss: 0.1495\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2385 - val_loss: 3.1930\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1781 - val_loss: 0.4152\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1582 - val_loss: 0.1257\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1487 - val_loss: 0.1092\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1241 - val_loss: 0.0747\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0977 - val_loss: 0.1626\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1378 - val_loss: 0.5154\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1790 - val_loss: 0.1908\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2040 - val_loss: 0.4354\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1008 - val_loss: 0.1399\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1121 - val_loss: 0.5002\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2920 - val_loss: 0.3516\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1282 - val_loss: 0.5114\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1157 - val_loss: 0.0824\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1003 - val_loss: 0.1128\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2339 - val_loss: 0.3832\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1189 - val_loss: 0.3844\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0815 - val_loss: 0.0912\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1287 - val_loss: 0.3650\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2520 - val_loss: 0.2048\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1503 - val_loss: 0.2799\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0602 - val_loss: 0.0604\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0634 - val_loss: 0.0854\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1434 - val_loss: 0.4439\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1888 - val_loss: 0.7855\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.05334042407613088\n",
      "Mean Absolute Error (MAE): 0.16820775139365743\n",
      "Root Mean Squared Error (RMSE): 0.2309554590741056\n",
      "Time taken: 667.6853847503662\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1080.0636 - val_loss: 847.2486\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 651.0952 - val_loss: 543.9972\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 405.4471 - val_loss: 326.9235\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 244.6632 - val_loss: 200.6711\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 129.3768 - val_loss: 112.1549\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 63.8806 - val_loss: 41.3347\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 29.2935 - val_loss: 16.5948\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.9162 - val_loss: 12.0292\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.9899 - val_loss: 11.1102\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2579 - val_loss: 4.7172\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0879 - val_loss: 4.5388\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.2867 - val_loss: 3.0921\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.5115 - val_loss: 1.6642\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7838 - val_loss: 3.2056\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3060 - val_loss: 1.2936\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3751 - val_loss: 1.3480\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.8522 - val_loss: 2.2254\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.3063 - val_loss: 1.1415\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2530 - val_loss: 0.7215\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7786 - val_loss: 1.2238\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7676 - val_loss: 3.1303\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4414 - val_loss: 0.9153\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9884 - val_loss: 1.9923\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4904 - val_loss: 3.9601\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1532 - val_loss: 0.9676\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9410 - val_loss: 0.7081\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7785 - val_loss: 1.1367\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7356 - val_loss: 1.0182\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6005 - val_loss: 1.2172\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0026 - val_loss: 6.1684\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9166 - val_loss: 0.4854\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7496 - val_loss: 0.5334\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7581 - val_loss: 0.7336\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0338 - val_loss: 0.7839\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8341 - val_loss: 0.3578\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4724 - val_loss: 0.2244\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5667 - val_loss: 0.7753\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9405 - val_loss: 0.6354\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8305 - val_loss: 1.1545\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5686 - val_loss: 0.5611\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5168 - val_loss: 0.8072\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5003 - val_loss: 0.5324\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6415 - val_loss: 1.0429\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8560 - val_loss: 1.5485\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6621 - val_loss: 0.4579\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5275 - val_loss: 0.4747\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5143 - val_loss: 0.3761\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6254 - val_loss: 1.3467\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6485 - val_loss: 0.8341\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3840 - val_loss: 0.3341\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4661 - val_loss: 1.0475\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3886 - val_loss: 0.6893\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1748 - val_loss: 4.4953\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7138 - val_loss: 1.0439\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2456 - val_loss: 0.2643\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2336 - val_loss: 0.4764\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4185 - val_loss: 0.5751\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2278 - val_loss: 0.3486\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2669 - val_loss: 0.3790\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7073 - val_loss: 0.4200\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3356 - val_loss: 1.1603\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2628 - val_loss: 0.2919\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3701 - val_loss: 0.5253\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3414 - val_loss: 0.4110\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3142 - val_loss: 0.3964\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4865 - val_loss: 0.5731\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.22432684174584624\n",
      "Mean Absolute Error (MAE): 0.3590565856462516\n",
      "Root Mean Squared Error (RMSE): 0.47363154640062377\n",
      "Time taken: 265.31568026542664\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1093.0781 - val_loss: 760.7825\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 630.9028 - val_loss: 471.8560\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 391.8487 - val_loss: 295.7898\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 224.3087 - val_loss: 129.9254\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 106.3002 - val_loss: 57.8093\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 48.8896 - val_loss: 39.7641\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 24.4073 - val_loss: 15.9278\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.7968 - val_loss: 8.1620\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 9.6681 - val_loss: 11.2841\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.1013 - val_loss: 2.4178\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.9144 - val_loss: 6.4799\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.0710 - val_loss: 2.7886\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.9833 - val_loss: 5.7186\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.8289 - val_loss: 3.3023\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.2561 - val_loss: 5.1249\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3553 - val_loss: 1.5075\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7556 - val_loss: 2.0651\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2086 - val_loss: 2.5500\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.4938 - val_loss: 2.1528\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6651 - val_loss: 0.8977\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3833 - val_loss: 0.9645\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3196 - val_loss: 2.2945\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.0840 - val_loss: 4.8379\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7193 - val_loss: 0.6941\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3731 - val_loss: 0.9070\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9702 - val_loss: 1.0034\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2856 - val_loss: 1.5869\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3906 - val_loss: 1.4309\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3956 - val_loss: 1.4793\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8693 - val_loss: 1.6025\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2907 - val_loss: 1.1987\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8926 - val_loss: 0.6043\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8654 - val_loss: 1.2182\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7234 - val_loss: 0.8862\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6811 - val_loss: 0.7127\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.2484 - val_loss: 2.7784\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6456 - val_loss: 1.0034\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1137 - val_loss: 2.9021\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6716 - val_loss: 0.6146\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4753 - val_loss: 0.5309\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4541 - val_loss: 0.4397\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6286 - val_loss: 0.9483\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.1875 - val_loss: 0.9839\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9683 - val_loss: 0.9953\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6772 - val_loss: 0.8475\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4777 - val_loss: 0.4192\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4011 - val_loss: 0.4420\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5032 - val_loss: 1.2050\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8051 - val_loss: 0.3585\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8164 - val_loss: 0.3312\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6693 - val_loss: 0.8203\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4567 - val_loss: 0.4474\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2721 - val_loss: 0.3149\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4308 - val_loss: 0.5395\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3152 - val_loss: 0.5954\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4546 - val_loss: 0.4329\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2806 - val_loss: 0.4432\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3034 - val_loss: 0.2488\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3101 - val_loss: 1.0061\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5974 - val_loss: 2.5438\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4055 - val_loss: 0.6426\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2608 - val_loss: 0.3604\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5718 - val_loss: 0.5040\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3667 - val_loss: 0.5988\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3079 - val_loss: 0.2546\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4933 - val_loss: 0.4218\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4295 - val_loss: 1.9044\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3339 - val_loss: 0.2323\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3180 - val_loss: 0.3642\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7341 - val_loss: 1.1725\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3660 - val_loss: 0.7695\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2239 - val_loss: 1.9655\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3293 - val_loss: 0.3621\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3990 - val_loss: 0.3192\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3034 - val_loss: 0.2599\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4119 - val_loss: 0.2895\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2824 - val_loss: 0.4743\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5108 - val_loss: 0.5403\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2575 - val_loss: 0.5746\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3054 - val_loss: 0.2780\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3040 - val_loss: 0.2884\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4804 - val_loss: 0.5751\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3698 - val_loss: 0.2949\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2858 - val_loss: 1.0043\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2151 - val_loss: 0.8170\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2892 - val_loss: 0.2202\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3200 - val_loss: 0.2516\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2001 - val_loss: 0.1255\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4018 - val_loss: 0.9951\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3754 - val_loss: 1.2807\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1821 - val_loss: 0.2873\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3013 - val_loss: 0.1959\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2953 - val_loss: 0.2890\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2747 - val_loss: 0.3889\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2711 - val_loss: 0.4908\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2338 - val_loss: 0.4154\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3628 - val_loss: 0.4312\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1789 - val_loss: 0.3710\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2350 - val_loss: 0.5739\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2417 - val_loss: 0.2248\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4265 - val_loss: 0.7132\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1381 - val_loss: 0.1184\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2382 - val_loss: 0.1070\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0901 - val_loss: 0.2227\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2899 - val_loss: 0.3086\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2348 - val_loss: 0.2662\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1725 - val_loss: 0.3133\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1886 - val_loss: 0.3253\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3809 - val_loss: 0.4288\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2137 - val_loss: 0.2042\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1715 - val_loss: 0.6794\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1895 - val_loss: 0.4427\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2156 - val_loss: 0.1842\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2204 - val_loss: 0.2058\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1393 - val_loss: 0.1777\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0953 - val_loss: 0.3112\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1854 - val_loss: 0.6378\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2695 - val_loss: 0.6102\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2833 - val_loss: 0.3773\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2558 - val_loss: 0.2251\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0969 - val_loss: 0.0891\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1854 - val_loss: 0.3315\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1875 - val_loss: 0.5752\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1236 - val_loss: 0.0733\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1658 - val_loss: 0.2421\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4587 - val_loss: 3.2339\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6981 - val_loss: 0.5417\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1524 - val_loss: 0.0905\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0869 - val_loss: 0.0694\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0791 - val_loss: 0.0773\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0728 - val_loss: 0.0654\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0855 - val_loss: 0.0763\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0840 - val_loss: 0.0634\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0914 - val_loss: 0.2297\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0989 - val_loss: 0.1384\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1879 - val_loss: 0.5078\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1889 - val_loss: 0.1254\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0907 - val_loss: 0.1267\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1220 - val_loss: 0.1332\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1376 - val_loss: 0.3516\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1567 - val_loss: 0.1653\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2624 - val_loss: 0.2733\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1548 - val_loss: 0.1021\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0898 - val_loss: 0.1110\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1575 - val_loss: 1.2760\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1918 - val_loss: 0.5859\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1442 - val_loss: 0.1067\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1271 - val_loss: 0.1519\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1707 - val_loss: 1.2483\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1862 - val_loss: 0.3308\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1111 - val_loss: 0.1325\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1117 - val_loss: 0.2110\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1156 - val_loss: 0.3902\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1374 - val_loss: 0.2808\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1747 - val_loss: 0.2702\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1503 - val_loss: 0.1088\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0950 - val_loss: 0.1182\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2009 - val_loss: 0.1380\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1171 - val_loss: 0.1846\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1717 - val_loss: 0.2087\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2969 - val_loss: 0.4548\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0801 - val_loss: 0.2169\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0836 - val_loss: 0.1061\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.06317265268412174\n",
      "Mean Absolute Error (MAE): 0.18288044909616466\n",
      "Root Mean Squared Error (RMSE): 0.2513417050235033\n",
      "Time taken: 623.1068739891052\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1082.5127 - val_loss: 863.0476\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 660.1457 - val_loss: 519.3764\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 381.5539 - val_loss: 268.2835\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 190.1216 - val_loss: 117.8596\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 86.9259 - val_loss: 57.4170\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 39.0596 - val_loss: 39.8697\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 19.8234 - val_loss: 21.4998\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.6712 - val_loss: 6.1376\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.6850 - val_loss: 22.7102\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.7394 - val_loss: 6.3678\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.1338 - val_loss: 5.7422\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3140 - val_loss: 3.0863\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.5489 - val_loss: 1.7715\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.2567 - val_loss: 1.5196\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.1519 - val_loss: 1.5396\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.8011 - val_loss: 0.8775\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6435 - val_loss: 1.4957\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2372 - val_loss: 0.8222\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6076 - val_loss: 1.0832\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1971 - val_loss: 0.9552\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2181 - val_loss: 1.2346\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2020 - val_loss: 0.9523\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3487 - val_loss: 1.3051\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0562 - val_loss: 1.2402\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8932 - val_loss: 0.9436\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7174 - val_loss: 1.6026\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9559 - val_loss: 0.7271\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6311 - val_loss: 1.3001\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0947 - val_loss: 10.6078\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.0209 - val_loss: 0.7554\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6070 - val_loss: 0.3846\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5364 - val_loss: 0.5255\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3997 - val_loss: 0.4468\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4956 - val_loss: 0.6337\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5815 - val_loss: 0.8056\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3828 - val_loss: 1.7786\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7347 - val_loss: 0.3901\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4280 - val_loss: 0.2398\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4071 - val_loss: 2.0023\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5736 - val_loss: 1.0887\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0735 - val_loss: 3.5529\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3042 - val_loss: 0.4335\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3524 - val_loss: 0.5280\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4438 - val_loss: 0.4656\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7805 - val_loss: 4.0841\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5360 - val_loss: 0.3233\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2330 - val_loss: 0.1975\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7639 - val_loss: 1.6870\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5437 - val_loss: 0.3338\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3367 - val_loss: 0.7277\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4785 - val_loss: 0.4506\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3704 - val_loss: 0.3551\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3774 - val_loss: 0.2996\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2764 - val_loss: 0.3436\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3709 - val_loss: 0.1994\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1980 - val_loss: 0.2784\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3189 - val_loss: 0.5948\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5567 - val_loss: 0.2994\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3717 - val_loss: 0.2089\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4695 - val_loss: 0.4220\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4502 - val_loss: 1.0935\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3456 - val_loss: 0.5410\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2471 - val_loss: 0.1807\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2804 - val_loss: 0.3117\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3506 - val_loss: 0.4575\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3572 - val_loss: 0.5814\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2752 - val_loss: 0.5836\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7333 - val_loss: 0.9132\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6096 - val_loss: 2.0038\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3106 - val_loss: 0.4909\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2005 - val_loss: 0.0924\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1984 - val_loss: 0.3109\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2966 - val_loss: 0.4072\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2040 - val_loss: 0.4031\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4275 - val_loss: 1.5336\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5160 - val_loss: 0.1642\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1635 - val_loss: 0.1023\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2081 - val_loss: 0.4668\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4798 - val_loss: 0.8441\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2964 - val_loss: 0.7641\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2576 - val_loss: 0.1854\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1620 - val_loss: 0.4842\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2944 - val_loss: 0.2977\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2749 - val_loss: 0.2203\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1662 - val_loss: 0.2076\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2662 - val_loss: 0.2299\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1647 - val_loss: 0.1515\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2302 - val_loss: 0.6256\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4192 - val_loss: 0.3519\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2549 - val_loss: 0.2285\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1311 - val_loss: 0.2302\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1475 - val_loss: 0.1303\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4252 - val_loss: 0.3227\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1732 - val_loss: 0.3835\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1611 - val_loss: 0.5303\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3491 - val_loss: 0.1269\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1295 - val_loss: 0.1450\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1572 - val_loss: 0.1355\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4767 - val_loss: 8.0266\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3912 - val_loss: 0.2182\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0867 - val_loss: 0.0571\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0781 - val_loss: 0.1484\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1161 - val_loss: 0.0848\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0729 - val_loss: 0.1451\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1065 - val_loss: 0.1227\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2268 - val_loss: 0.2184\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1677 - val_loss: 0.1314\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1114 - val_loss: 0.2596\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2142 - val_loss: 0.8046\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3289 - val_loss: 0.1018\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1133 - val_loss: 0.2438\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2072 - val_loss: 0.1958\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3745 - val_loss: 0.2361\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1095 - val_loss: 0.0996\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1185 - val_loss: 0.1510\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1336 - val_loss: 0.1462\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7989 - val_loss: 2.0689\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2960 - val_loss: 0.1304\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0882 - val_loss: 0.0799\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0724 - val_loss: 0.0648\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1007 - val_loss: 0.0875\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0955 - val_loss: 0.3513\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1097 - val_loss: 0.1127\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1012 - val_loss: 0.3957\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1115 - val_loss: 0.2042\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1628 - val_loss: 0.2019\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1802 - val_loss: 0.2062\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1450 - val_loss: 0.1919\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2232 - val_loss: 0.4321\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1855 - val_loss: 0.1185\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1064 - val_loss: 0.0823\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.056829985027041334\n",
      "Mean Absolute Error (MAE): 0.17666042293398299\n",
      "Root Mean Squared Error (RMSE): 0.23839040464549183\n",
      "Time taken: 504.7113296985626\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1088.7609 - val_loss: 855.3704\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 669.8726 - val_loss: 569.8568\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 444.0039 - val_loss: 413.6497\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 285.0557 - val_loss: 249.0600\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 147.7392 - val_loss: 99.3899\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 65.2900 - val_loss: 46.8894\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 30.1716 - val_loss: 17.7872\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 15.5583 - val_loss: 11.6511\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.7277 - val_loss: 6.5249\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.1319 - val_loss: 5.3307\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.6874 - val_loss: 3.6655\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.6985 - val_loss: 5.3322\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.9196 - val_loss: 2.5878\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4358 - val_loss: 1.5821\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.5381 - val_loss: 3.2578\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.9657 - val_loss: 2.9073\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.0108 - val_loss: 1.2557\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4597 - val_loss: 1.1916\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9722 - val_loss: 1.6573\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4424 - val_loss: 0.7732\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3358 - val_loss: 1.5805\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1714 - val_loss: 2.6213\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3997 - val_loss: 1.9959\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3862 - val_loss: 3.4739\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1676 - val_loss: 2.5606\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9015 - val_loss: 2.7615\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4831 - val_loss: 1.5713\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8006 - val_loss: 0.4638\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6126 - val_loss: 1.1871\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7079 - val_loss: 0.8174\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7238 - val_loss: 0.5491\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9197 - val_loss: 1.2776\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8282 - val_loss: 0.6392\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9655 - val_loss: 1.2870\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9097 - val_loss: 0.4569\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6426 - val_loss: 0.3964\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6674 - val_loss: 0.7315\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7114 - val_loss: 0.3550\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9567 - val_loss: 1.1776\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4844 - val_loss: 1.8627\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7114 - val_loss: 0.5031\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6219 - val_loss: 1.3076\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6294 - val_loss: 0.5066\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5464 - val_loss: 1.6516\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4318 - val_loss: 2.4397\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9947 - val_loss: 3.1855\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6519 - val_loss: 0.6684\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2751 - val_loss: 0.5616\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5814 - val_loss: 0.6793\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4560 - val_loss: 0.4424\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3403 - val_loss: 0.8566\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6199 - val_loss: 0.9243\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7102 - val_loss: 0.2389\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6177 - val_loss: 0.9994\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5376 - val_loss: 0.2516\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3487 - val_loss: 0.3152\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5415 - val_loss: 0.8502\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3160 - val_loss: 0.8370\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5237 - val_loss: 0.4127\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3250 - val_loss: 0.2330\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2468 - val_loss: 0.4789\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6587 - val_loss: 0.4296\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3602 - val_loss: 1.1677\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3708 - val_loss: 0.2076\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2548 - val_loss: 0.1635\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3383 - val_loss: 0.3203\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4936 - val_loss: 0.5951\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4089 - val_loss: 1.6966\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2712 - val_loss: 0.8869\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2914 - val_loss: 0.2652\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3148 - val_loss: 0.6217\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2812 - val_loss: 0.6078\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3385 - val_loss: 0.1778\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4419 - val_loss: 0.5457\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3379 - val_loss: 0.7498\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2481 - val_loss: 0.3853\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2244 - val_loss: 0.3561\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3336 - val_loss: 0.1438\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3197 - val_loss: 0.2833\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2555 - val_loss: 0.9379\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2193 - val_loss: 0.1572\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2274 - val_loss: 0.5907\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3511 - val_loss: 0.4796\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3088 - val_loss: 0.6896\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5652 - val_loss: 0.3774\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1202 - val_loss: 0.1825\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1517 - val_loss: 0.1531\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1562 - val_loss: 0.3481\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3704 - val_loss: 1.4146\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1997 - val_loss: 0.0945\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1868 - val_loss: 0.0881\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3828 - val_loss: 4.1470\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3425 - val_loss: 0.1444\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1402 - val_loss: 0.1977\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1236 - val_loss: 0.2040\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4788 - val_loss: 1.0457\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2843 - val_loss: 0.1334\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1085 - val_loss: 0.1173\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2462 - val_loss: 0.6055\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1735 - val_loss: 0.1098\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1437 - val_loss: 0.4605\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2188 - val_loss: 14.8612\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5569 - val_loss: 0.1725\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0988 - val_loss: 0.1494\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0734 - val_loss: 0.3351\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0972 - val_loss: 0.0748\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0858 - val_loss: 0.1389\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1427 - val_loss: 0.7716\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1598 - val_loss: 0.5227\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2008 - val_loss: 0.8766\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1691 - val_loss: 0.1899\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0813 - val_loss: 0.2080\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1149 - val_loss: 0.2458\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5622 - val_loss: 0.5662\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1090 - val_loss: 0.1519\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1935 - val_loss: 0.1357\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0849 - val_loss: 0.1217\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1169 - val_loss: 0.4627\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1755 - val_loss: 0.4938\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1834 - val_loss: 0.6063\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2055 - val_loss: 0.1254\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1630 - val_loss: 0.1520\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1349 - val_loss: 0.5147\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1625 - val_loss: 0.2290\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6520 - val_loss: 0.9136\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0669 - val_loss: 0.0608\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0789 - val_loss: 0.4952\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0944 - val_loss: 0.0710\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1488 - val_loss: 0.1976\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1420 - val_loss: 0.3203\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1253 - val_loss: 0.1648\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1226 - val_loss: 0.3040\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2575 - val_loss: 0.3334\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1250 - val_loss: 0.2323\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1492 - val_loss: 0.2804\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1900 - val_loss: 0.4129\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2109 - val_loss: 0.1381\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1459 - val_loss: 0.2019\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1278 - val_loss: 0.1545\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0745 - val_loss: 0.3336\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2825 - val_loss: 0.1432\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1101 - val_loss: 0.1217\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1000 - val_loss: 0.2524\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1353 - val_loss: 0.1232\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1476 - val_loss: 0.1287\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1191 - val_loss: 0.4396\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1546 - val_loss: 0.3146\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1496 - val_loss: 0.2472\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0997 - val_loss: 0.1210\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1133 - val_loss: 0.2478\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1173 - val_loss: 0.1218\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1538 - val_loss: 0.1653\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1170 - val_loss: 0.1397\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2386 - val_loss: 0.4949\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1366 - val_loss: 0.2674\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1213 - val_loss: 0.0684\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.060661697794217505\n",
      "Mean Absolute Error (MAE): 0.1834388427871314\n",
      "Root Mean Squared Error (RMSE): 0.24629595570008353\n",
      "Time taken: 597.6990730762482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 24, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 24, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 24, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_19856\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.053340  0.168208  0.230955  667.685385\n",
      "1        2  0.224327  0.359057  0.473632  265.315680\n",
      "2        3  0.063173  0.182880  0.251342  623.106874\n",
      "3        4  0.056830  0.176660  0.238390  504.711330\n",
      "4        5  0.060662  0.183439  0.246296  597.699073\n",
      "5  Average  0.091666  0.214049  0.288123  531.703668\n",
      "Results saved to 'Sensors 24_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 24_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 24_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
