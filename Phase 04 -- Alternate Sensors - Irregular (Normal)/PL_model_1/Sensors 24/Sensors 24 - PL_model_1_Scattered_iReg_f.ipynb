{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "      <th>sensor17</th>\n",
       "      <th>sensor18</th>\n",
       "      <th>sensor19</th>\n",
       "      <th>sensor20</th>\n",
       "      <th>sensor21</th>\n",
       "      <th>sensor22</th>\n",
       "      <th>sensor23</th>\n",
       "      <th>sensor24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>107.051532</td>\n",
       "      <td>124.324976</td>\n",
       "      <td>91.735403</td>\n",
       "      <td>122.737112</td>\n",
       "      <td>59.428276</td>\n",
       "      <td>112.945303</td>\n",
       "      <td>107.048113</td>\n",
       "      <td>137.193866</td>\n",
       "      <td>82.867633</td>\n",
       "      <td>121.244903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>113.850953</td>\n",
       "      <td>117.801200</td>\n",
       "      <td>104.299209</td>\n",
       "      <td>125.049258</td>\n",
       "      <td>56.772454</td>\n",
       "      <td>108.156125</td>\n",
       "      <td>116.168202</td>\n",
       "      <td>138.846561</td>\n",
       "      <td>87.815198</td>\n",
       "      <td>127.885537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>101.090832</td>\n",
       "      <td>113.383786</td>\n",
       "      <td>108.974984</td>\n",
       "      <td>128.985264</td>\n",
       "      <td>58.017412</td>\n",
       "      <td>106.333965</td>\n",
       "      <td>125.125617</td>\n",
       "      <td>139.026194</td>\n",
       "      <td>79.962744</td>\n",
       "      <td>120.686542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>109.590169</td>\n",
       "      <td>126.267858</td>\n",
       "      <td>97.167399</td>\n",
       "      <td>134.473699</td>\n",
       "      <td>53.601933</td>\n",
       "      <td>113.049890</td>\n",
       "      <td>104.513667</td>\n",
       "      <td>130.516633</td>\n",
       "      <td>89.381611</td>\n",
       "      <td>122.638217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>101.223119</td>\n",
       "      <td>118.873622</td>\n",
       "      <td>97.616879</td>\n",
       "      <td>140.035649</td>\n",
       "      <td>55.501190</td>\n",
       "      <td>114.036270</td>\n",
       "      <td>114.494675</td>\n",
       "      <td>139.247307</td>\n",
       "      <td>85.307319</td>\n",
       "      <td>120.171501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>119.940259</td>\n",
       "      <td>103.272859</td>\n",
       "      <td>136.990878</td>\n",
       "      <td>109.946398</td>\n",
       "      <td>102.971514</td>\n",
       "      <td>65.770481</td>\n",
       "      <td>138.989434</td>\n",
       "      <td>135.979666</td>\n",
       "      <td>111.415908</td>\n",
       "      <td>83.023104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>117.365194</td>\n",
       "      <td>114.416527</td>\n",
       "      <td>136.976776</td>\n",
       "      <td>110.236827</td>\n",
       "      <td>90.418915</td>\n",
       "      <td>62.016384</td>\n",
       "      <td>139.955896</td>\n",
       "      <td>121.036566</td>\n",
       "      <td>120.217169</td>\n",
       "      <td>90.140067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>123.582865</td>\n",
       "      <td>113.049668</td>\n",
       "      <td>133.633092</td>\n",
       "      <td>112.781527</td>\n",
       "      <td>87.321252</td>\n",
       "      <td>68.554362</td>\n",
       "      <td>139.143021</td>\n",
       "      <td>125.317923</td>\n",
       "      <td>114.306234</td>\n",
       "      <td>95.384183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>124.604990</td>\n",
       "      <td>113.155807</td>\n",
       "      <td>127.248332</td>\n",
       "      <td>108.509153</td>\n",
       "      <td>96.605607</td>\n",
       "      <td>71.343902</td>\n",
       "      <td>140.910940</td>\n",
       "      <td>126.953390</td>\n",
       "      <td>123.237372</td>\n",
       "      <td>101.313107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>116.554557</td>\n",
       "      <td>109.144032</td>\n",
       "      <td>136.355528</td>\n",
       "      <td>117.016562</td>\n",
       "      <td>101.283816</td>\n",
       "      <td>63.681543</td>\n",
       "      <td>141.898885</td>\n",
       "      <td>119.674883</td>\n",
       "      <td>119.761631</td>\n",
       "      <td>88.441370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor15  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  107.051532   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  113.850953   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  101.090832   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  109.590169   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  101.223119   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  119.940259   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  117.365194   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  123.582865   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  124.604990   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  116.554557   \n",
       "\n",
       "        sensor16    sensor17    sensor18    sensor19    sensor20    sensor21  \\\n",
       "0     124.324976   91.735403  122.737112   59.428276  112.945303  107.048113   \n",
       "1     117.801200  104.299209  125.049258   56.772454  108.156125  116.168202   \n",
       "2     113.383786  108.974984  128.985264   58.017412  106.333965  125.125617   \n",
       "3     126.267858   97.167399  134.473699   53.601933  113.049890  104.513667   \n",
       "4     118.873622   97.616879  140.035649   55.501190  114.036270  114.494675   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  103.272859  136.990878  109.946398  102.971514   65.770481  138.989434   \n",
       "2439  114.416527  136.976776  110.236827   90.418915   62.016384  139.955896   \n",
       "2440  113.049668  133.633092  112.781527   87.321252   68.554362  139.143021   \n",
       "2441  113.155807  127.248332  108.509153   96.605607   71.343902  140.910940   \n",
       "2442  109.144032  136.355528  117.016562  101.283816   63.681543  141.898885   \n",
       "\n",
       "        sensor22    sensor23    sensor24  \n",
       "0     137.193866   82.867633  121.244903  \n",
       "1     138.846561   87.815198  127.885537  \n",
       "2     139.026194   79.962744  120.686542  \n",
       "3     130.516633   89.381611  122.638217  \n",
       "4     139.247307   85.307319  120.171501  \n",
       "...          ...         ...         ...  \n",
       "2438  135.979666  111.415908   83.023104  \n",
       "2439  121.036566  120.217169   90.140067  \n",
       "2440  125.317923  114.306234   95.384183  \n",
       "2441  126.953390  123.237372  101.313107  \n",
       "2442  119.674883  119.761631   88.441370  \n",
       "\n",
       "[2443 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:24]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 14s 18ms/step - loss: 1121.7469 - val_loss: 909.7353\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 806.5511 - val_loss: 794.3934\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 585.2119 - val_loss: 489.2908\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 408.8341 - val_loss: 374.7616\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 302.7029 - val_loss: 270.4706\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 208.0051 - val_loss: 166.2535\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 119.4183 - val_loss: 99.4708\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 69.7997 - val_loss: 51.9605\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 44.9703 - val_loss: 41.9119\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 32.0922 - val_loss: 37.3429\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 24.3670 - val_loss: 20.5829\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 21.2787 - val_loss: 17.1966\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 18.4186 - val_loss: 19.3430\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 17.0867 - val_loss: 16.2775\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 16.9503 - val_loss: 19.4016\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 15.6935 - val_loss: 21.3727\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 14.6236 - val_loss: 14.7178\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 14.5684 - val_loss: 15.3871\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.0454 - val_loss: 12.1611\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.7557 - val_loss: 16.0884\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.6327 - val_loss: 11.5316\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.3530 - val_loss: 13.8256\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.7784 - val_loss: 11.8385\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.1766 - val_loss: 15.2214\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 11.0673 - val_loss: 13.7120\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.6840 - val_loss: 13.7625\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.9105 - val_loss: 16.6431\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 10.0501 - val_loss: 11.1143\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8509 - val_loss: 11.3942\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8463 - val_loss: 12.8975\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.5434 - val_loss: 12.8319\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.7503 - val_loss: 16.8004\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.7875 - val_loss: 10.3292\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.8811 - val_loss: 11.3624\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.7166 - val_loss: 11.3170\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 9.2416 - val_loss: 10.3105\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.2724 - val_loss: 11.9214\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 10.1289 - val_loss: 15.3573\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.3030 - val_loss: 11.9389\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.2280 - val_loss: 10.1304\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.6843 - val_loss: 10.8175\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 8.5117 - val_loss: 9.3150\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.0866 - val_loss: 9.2664\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 7.5210 - val_loss: 9.3319\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.4225 - val_loss: 12.5751\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 7.4706 - val_loss: 9.4354\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.6097 - val_loss: 9.8189\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.8841 - val_loss: 8.8728\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.6309 - val_loss: 11.8957\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.6174 - val_loss: 11.6367\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.6229 - val_loss: 11.0191\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 7.2622 - val_loss: 9.5963\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 6.6108 - val_loss: 9.6203\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.6023 - val_loss: 9.3228\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 8.0332 - val_loss: 13.9961\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.5556 - val_loss: 10.8627\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 6.6123 - val_loss: 9.6290\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 6.4272 - val_loss: 13.6617\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.0548 - val_loss: 10.1280\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7527 - val_loss: 10.4052\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9868 - val_loss: 15.9429\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.0883 - val_loss: 17.3055\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.3071 - val_loss: 9.3109\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8512 - val_loss: 9.3398\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.1634 - val_loss: 9.6259\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.7195 - val_loss: 10.1171\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.7757 - val_loss: 8.5343\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.2892 - val_loss: 8.4786\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.8245 - val_loss: 9.5814\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.7231 - val_loss: 9.2629\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 6.3707 - val_loss: 8.7245\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.9288 - val_loss: 9.4038\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.8282 - val_loss: 11.1764\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.5802 - val_loss: 12.1089\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.3644 - val_loss: 8.8295\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2253 - val_loss: 9.1194\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.7756 - val_loss: 9.2959\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.0040 - val_loss: 10.7857\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.2207 - val_loss: 8.5950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.0724 - val_loss: 10.1061\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.6840 - val_loss: 8.3356\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5840 - val_loss: 8.9496\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.1666 - val_loss: 12.3469\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.9158 - val_loss: 8.5783\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.7164 - val_loss: 9.3005\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.6141 - val_loss: 9.8866\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.9454 - val_loss: 8.5626\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7376 - val_loss: 8.9620\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4317 - val_loss: 18.9310\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.4555 - val_loss: 8.5205\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2962 - val_loss: 8.2628\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5774 - val_loss: 10.3429\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.5053 - val_loss: 11.5984\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5196 - val_loss: 10.0981\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3153 - val_loss: 10.5642\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5673 - val_loss: 10.2937\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3337 - val_loss: 8.1432\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.7592 - val_loss: 9.2340\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.1951 - val_loss: 10.4505\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.0244 - val_loss: 9.3019\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.7159 - val_loss: 8.6044\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.8839 - val_loss: 8.7302\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.9677 - val_loss: 10.0827\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.1261 - val_loss: 21.3589\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0633 - val_loss: 11.6845\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4659 - val_loss: 8.8121\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5337 - val_loss: 8.9828\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.7113 - val_loss: 12.4875\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.6030 - val_loss: 11.4684\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0789 - val_loss: 10.2217\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4373 - val_loss: 9.0298\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.5674 - val_loss: 8.9732\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.7371 - val_loss: 8.0798\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3255 - val_loss: 9.3369\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2512 - val_loss: 10.9855\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5340 - val_loss: 10.9276\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.2270 - val_loss: 9.9789\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2289 - val_loss: 11.4125\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5995 - val_loss: 8.9021\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0509 - val_loss: 9.2214\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0747 - val_loss: 31.0267\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2065 - val_loss: 8.6615\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8983 - val_loss: 9.5552\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8665 - val_loss: 9.6332\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.9031 - val_loss: 8.6485\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.7824 - val_loss: 9.0155\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.8649 - val_loss: 9.6793\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8512 - val_loss: 9.1680\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7222 - val_loss: 8.5965\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.6807 - val_loss: 9.0511\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.6423 - val_loss: 10.2884\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2024 - val_loss: 9.6590\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.1217 - val_loss: 10.8582\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.4482 - val_loss: 9.7498\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.5867 - val_loss: 9.1806\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4590 - val_loss: 8.9422\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.3692 - val_loss: 10.1852\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3391 - val_loss: 9.3077\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6914 - val_loss: 9.7422\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8007 - val_loss: 10.2753\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.4487 - val_loss: 11.0421\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0713 - val_loss: 32.1917\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.6468 - val_loss: 10.7432\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 8.077629131224727\n",
      "Mean Absolute Error (MAE): 1.897281826244125\n",
      "Root Mean Squared Error (RMSE): 2.8421170157515907\n",
      "Time taken: 589.5265500545502\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1143.9220 - val_loss: 935.7991\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 808.8737 - val_loss: 730.0625\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 544.7193 - val_loss: 478.6716\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 360.0418 - val_loss: 313.6246\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 225.6165 - val_loss: 171.6691\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 113.9700 - val_loss: 101.8402\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 61.8526 - val_loss: 46.6915\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 38.7405 - val_loss: 37.0172\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 30.5457 - val_loss: 29.1343\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 23.8528 - val_loss: 19.2065\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 20.8468 - val_loss: 17.2088\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 18.1223 - val_loss: 15.8771\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 16.9670 - val_loss: 23.3998\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 16.7524 - val_loss: 17.5110\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 15.8590 - val_loss: 15.4870\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.5131 - val_loss: 11.7106\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.4289 - val_loss: 14.3984\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.2113 - val_loss: 12.3914\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.2937 - val_loss: 11.9957\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.2829 - val_loss: 12.7629\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.6366 - val_loss: 11.0033\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.6828 - val_loss: 18.9724\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.4224 - val_loss: 11.5065\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.6277 - val_loss: 20.4183\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.6229 - val_loss: 11.4356\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.9865 - val_loss: 19.9003\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.5710 - val_loss: 15.7943\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.0711 - val_loss: 12.2507\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.6946 - val_loss: 13.0316\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.0918 - val_loss: 9.6809\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.5019 - val_loss: 10.8985\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.5262 - val_loss: 12.5072\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.6473 - val_loss: 14.4757\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.0245 - val_loss: 9.4127\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.0210 - val_loss: 10.8340\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.4914 - val_loss: 10.2930\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.5282 - val_loss: 9.4303\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.3421 - val_loss: 14.8042\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.8708 - val_loss: 9.6439\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.3486 - val_loss: 13.8796\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.6088 - val_loss: 11.4940\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.7780 - val_loss: 15.1463\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.5563 - val_loss: 11.2755\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.7638 - val_loss: 10.4647\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.5692 - val_loss: 15.6615\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.5938 - val_loss: 11.2880\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.6532 - val_loss: 14.7395\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.6472 - val_loss: 11.6400\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.2847 - val_loss: 9.6430\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.1721 - val_loss: 9.0528\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2652 - val_loss: 12.6072\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.3188 - val_loss: 8.6117\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.8361 - val_loss: 8.7214\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2303 - val_loss: 8.9477\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2799 - val_loss: 12.1606\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9653 - val_loss: 9.3511\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7676 - val_loss: 10.6777\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2567 - val_loss: 15.2416\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.3563 - val_loss: 9.2729\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.5634 - val_loss: 10.9074\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2188 - val_loss: 10.0049\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.8617 - val_loss: 11.4450\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.0545 - val_loss: 9.9703\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.8657 - val_loss: 8.2261\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.1105 - val_loss: 9.8078\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.4799 - val_loss: 8.1292\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9634 - val_loss: 10.7884\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6576 - val_loss: 11.2908\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.6821 - val_loss: 11.4022\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.6118 - val_loss: 7.8213\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4002 - val_loss: 10.0274\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4560 - val_loss: 10.5385\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.6731 - val_loss: 11.5975\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6581 - val_loss: 9.5758\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1338 - val_loss: 9.1375\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.7660 - val_loss: 7.7815\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.5022 - val_loss: 15.3967\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2547 - val_loss: 9.1685\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1244 - val_loss: 8.8065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.7489 - val_loss: 8.3198\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.3131 - val_loss: 12.1198\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.9232 - val_loss: 8.8235\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.7888 - val_loss: 7.7583\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2271 - val_loss: 16.4643\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6701 - val_loss: 8.2699\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2469 - val_loss: 9.9030\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2949 - val_loss: 20.4193\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8804 - val_loss: 9.1861\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5482 - val_loss: 7.9812\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5115 - val_loss: 13.7305\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.2291 - val_loss: 9.3632\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3764 - val_loss: 9.9607\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8229 - val_loss: 10.8935\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7865 - val_loss: 8.2258\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.6303 - val_loss: 11.3644\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8857 - val_loss: 7.9881\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.9940 - val_loss: 9.5593\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.7098 - val_loss: 11.5030\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.0595 - val_loss: 8.0063\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.5999 - val_loss: 8.9759\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.8718 - val_loss: 11.1383\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5707 - val_loss: 10.1121\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5509 - val_loss: 10.8490\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.4232 - val_loss: 10.7148\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5170 - val_loss: 13.1278\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8304 - val_loss: 8.3832\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1856 - val_loss: 8.3803\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.6193 - val_loss: 9.5933\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4102 - val_loss: 9.1854\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.1983 - val_loss: 8.7481\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0176 - val_loss: 8.4858\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.2557 - val_loss: 11.0852\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0130 - val_loss: 8.2195\n",
      "16/16 [==============================] - 1s 23ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 7.76103788731691\n",
      "Mean Absolute Error (MAE): 1.8846269747259414\n",
      "Root Mean Squared Error (RMSE): 2.785863939124973\n",
      "Time taken: 434.33558893203735\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1109.5988 - val_loss: 933.0444\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 734.4603 - val_loss: 629.5956\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 423.8242 - val_loss: 339.2280\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 226.5626 - val_loss: 165.4010\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 114.2049 - val_loss: 121.3710\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 62.8339 - val_loss: 79.2271\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 40.1914 - val_loss: 36.8025\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 31.4097 - val_loss: 42.1403\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 24.6100 - val_loss: 38.1464\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 22.3377 - val_loss: 22.8408\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 21.0526 - val_loss: 28.7057\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 16.8831 - val_loss: 18.6142\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 17.1741 - val_loss: 28.5714\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 16.8427 - val_loss: 34.3293\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.5634 - val_loss: 19.2383\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.9466 - val_loss: 14.2030\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.6165 - val_loss: 13.4040\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.9606 - val_loss: 19.3053\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.9037 - val_loss: 16.3008\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.8262 - val_loss: 18.5282\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.6862 - val_loss: 16.8518\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.1889 - val_loss: 13.3725\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.0017 - val_loss: 12.9130\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.1095 - val_loss: 10.8480\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.5347 - val_loss: 11.2503\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 10.3152 - val_loss: 13.2176\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 10.6908 - val_loss: 11.0977\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.3056 - val_loss: 15.5638\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8768 - val_loss: 11.7630\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8070 - val_loss: 12.5924\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.3859 - val_loss: 16.2596\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.1639 - val_loss: 16.7541\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 9.6906 - val_loss: 12.8912\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.1344 - val_loss: 20.2413\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.9859 - val_loss: 16.2957\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.7072 - val_loss: 12.7895\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.7804 - val_loss: 18.5881\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.1225 - val_loss: 10.4355\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.0049 - val_loss: 15.2288\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.1817 - val_loss: 17.6563\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.6807 - val_loss: 17.6088\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.7231 - val_loss: 10.3153\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.2233 - val_loss: 10.9671\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.7981 - val_loss: 16.1349\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.1487 - val_loss: 20.6003\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.1625 - val_loss: 10.0434\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.0785 - val_loss: 15.4196\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.3447 - val_loss: 15.4218\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.8286 - val_loss: 9.3778\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.4997 - val_loss: 10.0008\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.9804 - val_loss: 10.1003\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.8130 - val_loss: 11.5494\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.2799 - val_loss: 9.5697\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9781 - val_loss: 11.2346\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.3242 - val_loss: 13.5909\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2611 - val_loss: 10.2767\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.2597 - val_loss: 8.9146\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.6233 - val_loss: 13.5466\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.1435 - val_loss: 11.7253\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7999 - val_loss: 14.2195\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.8048 - val_loss: 9.4837\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6853 - val_loss: 9.6383\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7874 - val_loss: 24.9186\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.3679 - val_loss: 10.5574\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.5740 - val_loss: 10.8910\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.6687 - val_loss: 15.8617\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.3010 - val_loss: 10.1334\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6779 - val_loss: 9.2660\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.4528 - val_loss: 11.6054\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6913 - val_loss: 11.1013\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.1297 - val_loss: 10.3000\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4417 - val_loss: 9.3193\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.1790 - val_loss: 10.1080\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.2539 - val_loss: 9.9188\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.8230 - val_loss: 12.0574\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.3837 - val_loss: 10.1790\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.5065 - val_loss: 8.9788\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.3365 - val_loss: 9.5712\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 5.0597 - val_loss: 18.1993\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.6159 - val_loss: 10.8275\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.8665 - val_loss: 10.4476\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.9683 - val_loss: 8.9681\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.3292 - val_loss: 11.2372\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6218 - val_loss: 10.1034\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7762 - val_loss: 9.8844\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8849 - val_loss: 11.7024\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.0496 - val_loss: 8.9789\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 8.915412618042415\n",
      "Mean Absolute Error (MAE): 1.9985096587955746\n",
      "Root Mean Squared Error (RMSE): 2.985868821305185\n",
      "Time taken: 337.57936429977417\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 14ms/step - loss: 1099.3124 - val_loss: 898.0800\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 812.1255 - val_loss: 653.4825\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 524.8542 - val_loss: 396.6976\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 322.3248 - val_loss: 300.3512\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 195.6393 - val_loss: 137.2091\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 107.1524 - val_loss: 67.8882\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 61.0532 - val_loss: 53.5368\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 43.3422 - val_loss: 47.6159\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 31.8313 - val_loss: 41.1284\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 24.8384 - val_loss: 24.0035\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 21.4065 - val_loss: 65.8040\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 20.1738 - val_loss: 22.3081\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 18.0827 - val_loss: 25.8325\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 17.6687 - val_loss: 22.2364\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 16.7521 - val_loss: 37.3391\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.6628 - val_loss: 28.0763\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.0219 - val_loss: 14.7534\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.2518 - val_loss: 37.8233\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.9419 - val_loss: 26.4313\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.0625 - val_loss: 16.9964\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.7691 - val_loss: 13.4265\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.8847 - val_loss: 15.8177\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.8502 - val_loss: 22.9184\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.5363 - val_loss: 28.1073\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.8510 - val_loss: 18.6680\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.2095 - val_loss: 13.0460\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.9946 - val_loss: 14.8267\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.2884 - val_loss: 11.8587\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8960 - val_loss: 14.0383\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.5734 - val_loss: 11.9278\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.9116 - val_loss: 11.7323\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.4840 - val_loss: 11.9322\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.3038 - val_loss: 9.6073\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.8060 - val_loss: 10.7374\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.7218 - val_loss: 9.7391\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.1214 - val_loss: 12.2312\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.7188 - val_loss: 12.1673\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.5849 - val_loss: 13.0476\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.1131 - val_loss: 9.0887\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.1586 - val_loss: 11.0776\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.7237 - val_loss: 10.3377\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.5248 - val_loss: 12.0411\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.2470 - val_loss: 10.8518\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.1906 - val_loss: 9.7550\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.4807 - val_loss: 11.9924\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.6587 - val_loss: 13.2484\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.4404 - val_loss: 9.9779\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.9029 - val_loss: 10.2456\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.5989 - val_loss: 12.8822\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2673 - val_loss: 11.1115\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.0970 - val_loss: 10.5069\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.3518 - val_loss: 10.7136\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7730 - val_loss: 10.3157\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.8621 - val_loss: 8.8807\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9707 - val_loss: 16.4854\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.4122 - val_loss: 14.4970\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2589 - val_loss: 12.2057\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.8202 - val_loss: 12.0854\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.5970 - val_loss: 8.9659\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.6830 - val_loss: 10.1762\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.9547 - val_loss: 9.3284\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.2824 - val_loss: 8.9659\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.4060 - val_loss: 9.2463\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0838 - val_loss: 9.9973\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0156 - val_loss: 11.2058\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.7278 - val_loss: 10.7175\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8122 - val_loss: 9.6640\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8685 - val_loss: 13.1747\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.9380 - val_loss: 10.9798\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8185 - val_loss: 10.8046\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8151 - val_loss: 9.0783\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0286 - val_loss: 13.2312\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.7670 - val_loss: 11.1931\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1240 - val_loss: 8.7922\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.3869 - val_loss: 9.5300\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.9965 - val_loss: 11.3455\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8704 - val_loss: 10.0540\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2388 - val_loss: 30.7753\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 6.0876 - val_loss: 11.4959\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.1535 - val_loss: 9.0362\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.9766 - val_loss: 10.4830\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1600 - val_loss: 8.7338\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.7444 - val_loss: 9.3166\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3939 - val_loss: 10.4100\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7658 - val_loss: 8.6185\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.9040 - val_loss: 8.2686\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4440 - val_loss: 8.5225\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6945 - val_loss: 11.5685\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.1219 - val_loss: 8.4397\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1549 - val_loss: 10.3440\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.1036 - val_loss: 10.0800\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2169 - val_loss: 16.6860\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4849 - val_loss: 11.6147\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0686 - val_loss: 8.8143\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.1643 - val_loss: 13.1910\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.8386 - val_loss: 9.7953\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7761 - val_loss: 11.3589\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.0200 - val_loss: 10.2649\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0493 - val_loss: 8.4451\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4000 - val_loss: 8.7951\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8767 - val_loss: 8.6422\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0092 - val_loss: 9.0691\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2882 - val_loss: 12.6556\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3714 - val_loss: 9.5617\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.4904 - val_loss: 9.6493\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.2146 - val_loss: 8.8749\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2804 - val_loss: 10.7168\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4184 - val_loss: 12.5586\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.0748 - val_loss: 9.5409\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.1590 - val_loss: 10.0143\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2840 - val_loss: 11.3274\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.2181 - val_loss: 10.5127\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1324 - val_loss: 9.0925\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0821 - val_loss: 9.6951\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8082 - val_loss: 8.9440\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.8381 - val_loss: 9.5264\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 8.26635206818795\n",
      "Mean Absolute Error (MAE): 1.9391081945957052\n",
      "Root Mean Squared Error (RMSE): 2.8751264438608524\n",
      "Time taken: 448.6088888645172\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1145.6620 - val_loss: 890.7652\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 724.0152 - val_loss: 558.7035\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 471.6643 - val_loss: 379.7100\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 337.8745 - val_loss: 312.7241\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 232.1785 - val_loss: 169.3084\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 129.3126 - val_loss: 89.1906\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 71.1874 - val_loss: 47.0191\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 43.7054 - val_loss: 31.3772\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 29.7388 - val_loss: 29.9691\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 23.9342 - val_loss: 24.1281\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 20.9037 - val_loss: 18.2689\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 17.7734 - val_loss: 19.1500\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 18.2922 - val_loss: 15.2315\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 15.3596 - val_loss: 31.0802\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 15.3451 - val_loss: 19.1886\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 13.3080 - val_loss: 16.3145\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 15.7840 - val_loss: 21.4910\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.1755 - val_loss: 16.1123\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.4868 - val_loss: 12.0461\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.3860 - val_loss: 12.4127\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.8028 - val_loss: 14.7189\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.0701 - val_loss: 13.5051\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.8161 - val_loss: 12.8747\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.8355 - val_loss: 11.7448\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.7881 - val_loss: 17.0023\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 10.0617 - val_loss: 18.0274\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.9615 - val_loss: 11.4576\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.2462 - val_loss: 17.8821\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.1313 - val_loss: 11.7513\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.6266 - val_loss: 13.3841\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.6650 - val_loss: 11.3640\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.6082 - val_loss: 11.2458\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.0826 - val_loss: 10.8005\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.6357 - val_loss: 12.2182\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.4760 - val_loss: 13.3436\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.4323 - val_loss: 10.8950\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.7427 - val_loss: 10.6961\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.6906 - val_loss: 11.8962\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.2381 - val_loss: 12.6745\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.9533 - val_loss: 13.3395\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.3491 - val_loss: 9.2367\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.6562 - val_loss: 11.4118\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.8697 - val_loss: 10.4422\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.1051 - val_loss: 11.5611\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.0196 - val_loss: 15.1295\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.6315 - val_loss: 14.3344\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.4170 - val_loss: 12.4601\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.0229 - val_loss: 14.6167\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.5895 - val_loss: 10.1534\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.9749 - val_loss: 9.7947\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.3224 - val_loss: 10.6075\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.4363 - val_loss: 11.1862\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.3664 - val_loss: 10.5951\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.7206 - val_loss: 10.6799\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8394 - val_loss: 9.3958\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.1185 - val_loss: 8.8880\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.5240 - val_loss: 9.3782\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7584 - val_loss: 11.3042\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6518 - val_loss: 11.0783\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4905 - val_loss: 10.3553\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.0796 - val_loss: 21.3548\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.5807 - val_loss: 10.9893\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7192 - val_loss: 14.8525\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.2916 - val_loss: 10.0523\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.3108 - val_loss: 14.0265\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.3511 - val_loss: 12.1608\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0502 - val_loss: 9.6835\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.6201 - val_loss: 9.9389\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.1455 - val_loss: 9.9478\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.0255 - val_loss: 10.6593\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.4194 - val_loss: 11.5176\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.0330 - val_loss: 13.3691\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7283 - val_loss: 12.3614\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.7724 - val_loss: 16.6088\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.9105 - val_loss: 10.3572\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6351 - val_loss: 9.9339\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6770 - val_loss: 9.3415\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5667 - val_loss: 10.2705\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 4.2679 - val_loss: 9.3680\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8298 - val_loss: 15.8927\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.6886 - val_loss: 10.6074\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.3525 - val_loss: 10.5925\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.2818 - val_loss: 9.9810\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3128 - val_loss: 12.9063\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6013 - val_loss: 10.7548\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.3401 - val_loss: 12.0298\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 8.888541085145398\n",
      "Mean Absolute Error (MAE): 2.073097232602994\n",
      "Root Mean Squared Error (RMSE): 2.981365640968145\n",
      "Time taken: 333.636029958725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 24, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 24, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 24, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_7320\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  8.077629  1.897282  2.842117  589.526550\n",
      "1        2  7.761038  1.884627  2.785864  434.335589\n",
      "2        3  8.915413  1.998510  2.985869  337.579364\n",
      "3        4  8.266352  1.939108  2.875126  448.608889\n",
      "4        5  8.888541  2.073097  2.981366  333.636030\n",
      "5  Average  8.381795  1.958525  2.894068  428.737284\n",
      "Results saved to 'Sensors 24_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 24_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 24_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
