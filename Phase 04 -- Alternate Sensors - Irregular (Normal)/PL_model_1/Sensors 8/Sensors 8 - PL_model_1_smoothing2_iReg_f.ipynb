{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 11ms/step - loss: 1000.5517 - val_loss: 664.8084\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 514.9014 - val_loss: 445.1117\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 316.4106 - val_loss: 241.5768\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 167.1332 - val_loss: 121.7234\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 76.4078 - val_loss: 65.7842\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 35.9573 - val_loss: 35.9196\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 17.9905 - val_loss: 11.9935\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 9.9650 - val_loss: 6.6830\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2148 - val_loss: 4.7290\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.6657 - val_loss: 6.2042\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9419 - val_loss: 3.5310\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5282 - val_loss: 3.0249\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8402 - val_loss: 2.8036\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2639 - val_loss: 1.5427\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0296 - val_loss: 3.7702\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4046 - val_loss: 3.1766\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.9036 - val_loss: 5.0878\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7221 - val_loss: 1.5899\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.1178 - val_loss: 3.1798\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7566 - val_loss: 1.6868\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6017 - val_loss: 0.6903\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0279 - val_loss: 2.7408\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5160 - val_loss: 1.1432\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9833 - val_loss: 1.7009\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8806 - val_loss: 1.0419\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1496 - val_loss: 2.0103\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3665 - val_loss: 1.5309\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3372 - val_loss: 0.9582\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0540 - val_loss: 3.2199\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1697 - val_loss: 0.9442\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9393 - val_loss: 0.5551\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0256 - val_loss: 2.4296\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3654 - val_loss: 3.2302\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1968 - val_loss: 2.2725\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8611 - val_loss: 0.7189\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7527 - val_loss: 0.4348\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5771 - val_loss: 0.3297\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8278 - val_loss: 1.3580\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8757 - val_loss: 1.1063\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6681 - val_loss: 2.4721\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6286 - val_loss: 1.4637\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7142 - val_loss: 0.9495\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8422 - val_loss: 2.3284\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0028 - val_loss: 0.5694\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8651 - val_loss: 0.4884\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5441 - val_loss: 0.9720\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8774 - val_loss: 0.9254\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5220 - val_loss: 0.8326\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5483 - val_loss: 1.0139\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6752 - val_loss: 0.5851\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6875 - val_loss: 0.5632\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6131 - val_loss: 0.9140\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5451 - val_loss: 0.5906\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4219 - val_loss: 1.0628\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7780 - val_loss: 1.3388\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6725 - val_loss: 0.4003\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5136 - val_loss: 1.2249\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4697 - val_loss: 0.9155\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1412 - val_loss: 6.3070\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8570 - val_loss: 0.4574\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2591 - val_loss: 0.4457\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4247 - val_loss: 0.4994\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5197 - val_loss: 0.2642\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4207 - val_loss: 0.4015\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5482 - val_loss: 0.2231\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5154 - val_loss: 0.5214\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2218 - val_loss: 5.3798\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3221 - val_loss: 0.2765\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2949 - val_loss: 0.4059\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5254 - val_loss: 0.2721\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3921 - val_loss: 0.2784\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3128 - val_loss: 0.2632\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3268 - val_loss: 0.1951\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3409 - val_loss: 0.1730\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4427 - val_loss: 0.2683\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3987 - val_loss: 0.6894\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3659 - val_loss: 0.3624\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4126 - val_loss: 0.6687\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5837 - val_loss: 0.9202\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3446 - val_loss: 0.2173\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5684 - val_loss: 0.8333\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7587 - val_loss: 0.2454\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2167 - val_loss: 0.3543\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2389 - val_loss: 0.3600\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3237 - val_loss: 0.4850\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3210 - val_loss: 0.2188\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4792 - val_loss: 0.9817\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3500 - val_loss: 0.2491\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3465 - val_loss: 0.2133\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5905 - val_loss: 0.4627\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3112 - val_loss: 0.3515\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2440 - val_loss: 0.3620\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2806 - val_loss: 0.2458\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2385 - val_loss: 0.2783\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3140 - val_loss: 0.1524\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6012 - val_loss: 1.3682\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3098 - val_loss: 0.2681\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2131 - val_loss: 0.3860\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2669 - val_loss: 0.1197\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2633 - val_loss: 0.2521\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2287 - val_loss: 0.2768\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4258 - val_loss: 0.5659\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4739 - val_loss: 0.2227\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3925 - val_loss: 0.5841\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3254 - val_loss: 0.5866\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3693 - val_loss: 3.8608\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4064 - val_loss: 0.5204\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3263 - val_loss: 0.6800\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3399 - val_loss: 0.2016\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2697 - val_loss: 0.5346\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4270 - val_loss: 0.6553\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2152 - val_loss: 0.2808\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3666 - val_loss: 0.4515\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2542 - val_loss: 0.1630\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2410 - val_loss: 0.5147\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2705 - val_loss: 0.2349\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3338 - val_loss: 0.1365\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1818 - val_loss: 0.2946\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4085 - val_loss: 0.8140\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2506 - val_loss: 0.1054\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2663 - val_loss: 0.3935\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3726 - val_loss: 0.1881\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3008 - val_loss: 0.2248\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2861 - val_loss: 0.5214\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3746 - val_loss: 0.5883\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2916 - val_loss: 0.5547\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4344 - val_loss: 0.1306\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2598 - val_loss: 0.2221\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1486 - val_loss: 0.4902\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2551 - val_loss: 0.3250\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1725 - val_loss: 0.1986\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1089 - val_loss: 0.1637\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2010 - val_loss: 0.1787\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1155 - val_loss: 0.2680\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2426 - val_loss: 0.1387\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2059 - val_loss: 0.1285\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2690 - val_loss: 0.2524\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2651 - val_loss: 0.1754\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2960 - val_loss: 0.2613\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2570 - val_loss: 0.3326\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2676 - val_loss: 0.2139\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1768 - val_loss: 0.1773\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3366 - val_loss: 0.1904\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3574 - val_loss: 0.2166\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3114 - val_loss: 0.1374\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1624 - val_loss: 0.2031\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1902 - val_loss: 0.1556\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3147 - val_loss: 0.3206\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1888 - val_loss: 0.2411\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3255 - val_loss: 0.2464\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.10578790537836669\n",
      "Mean Absolute Error (MAE): 0.23461567726304003\n",
      "Root Mean Squared Error (RMSE): 0.3252505270992911\n",
      "Time taken: 411.88996744155884\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1024.7728 - val_loss: 707.7667\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 587.0543 - val_loss: 455.6955\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 348.5040 - val_loss: 298.2466\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 178.2601 - val_loss: 109.6105\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.7574 - val_loss: 69.2087\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 31.1896 - val_loss: 21.2935\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.3064 - val_loss: 8.8356\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.6808 - val_loss: 8.2957\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.7687 - val_loss: 3.4183\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.7002 - val_loss: 3.3699\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.6612 - val_loss: 3.6076\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2392 - val_loss: 2.8535\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2122 - val_loss: 2.8148\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.9923 - val_loss: 7.1330\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2291 - val_loss: 2.0784\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6605 - val_loss: 6.0954\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8324 - val_loss: 1.6559\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9040 - val_loss: 1.5275\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0773 - val_loss: 1.7438\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.7542 - val_loss: 0.8740\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4888 - val_loss: 1.7405\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8825 - val_loss: 1.2573\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3523 - val_loss: 1.1815\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3390 - val_loss: 1.5846\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5693 - val_loss: 0.9650\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.2157 - val_loss: 1.2037\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1544 - val_loss: 6.3383\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3333 - val_loss: 2.1358\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9566 - val_loss: 1.0535\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1080 - val_loss: 1.0931\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2376 - val_loss: 1.3760\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0430 - val_loss: 0.7772\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8358 - val_loss: 0.5041\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9138 - val_loss: 1.9301\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6780 - val_loss: 0.5445\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7609 - val_loss: 0.6006\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9696 - val_loss: 2.2465\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9207 - val_loss: 0.9779\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5072 - val_loss: 0.8214\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6260 - val_loss: 0.3835\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5087 - val_loss: 0.2955\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6253 - val_loss: 3.0820\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7091 - val_loss: 1.3446\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6535 - val_loss: 0.3662\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5727 - val_loss: 0.9331\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5664 - val_loss: 0.6568\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4335 - val_loss: 0.7263\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8666 - val_loss: 1.2124\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8663 - val_loss: 0.7831\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7259 - val_loss: 0.9264\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5152 - val_loss: 1.0972\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5508 - val_loss: 0.4571\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4507 - val_loss: 0.8286\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6979 - val_loss: 0.2746\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3948 - val_loss: 0.8970\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2142 - val_loss: 1.1044\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6533 - val_loss: 0.4547\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3471 - val_loss: 0.5753\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3242 - val_loss: 0.6271\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4371 - val_loss: 0.1679\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3469 - val_loss: 0.3227\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5169 - val_loss: 0.3032\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4321 - val_loss: 0.3574\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6149 - val_loss: 0.2329\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6975 - val_loss: 1.0996\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4010 - val_loss: 0.7989\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7906 - val_loss: 0.4395\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3879 - val_loss: 1.0515\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3727 - val_loss: 0.7173\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6438 - val_loss: 0.2833\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3199 - val_loss: 0.7657\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5027 - val_loss: 0.2825\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6146 - val_loss: 0.7259\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4988 - val_loss: 0.2282\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3725 - val_loss: 0.2722\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4006 - val_loss: 0.6013\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3459 - val_loss: 0.3807\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3933 - val_loss: 0.5006\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3866 - val_loss: 0.6011\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6767 - val_loss: 1.2650\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4098 - val_loss: 0.1578\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5004 - val_loss: 0.2682\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3156 - val_loss: 0.8686\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3872 - val_loss: 0.3759\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3015 - val_loss: 0.3035\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8039 - val_loss: 0.4001\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3130 - val_loss: 0.8417\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3924 - val_loss: 0.1777\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3600 - val_loss: 0.3041\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3544 - val_loss: 0.2250\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3815 - val_loss: 0.3200\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4071 - val_loss: 0.4536\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2857 - val_loss: 0.6212\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3265 - val_loss: 0.4651\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5448 - val_loss: 0.6100\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3505 - val_loss: 0.4778\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3182 - val_loss: 0.3269\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2936 - val_loss: 0.3482\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3491 - val_loss: 0.1888\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2358 - val_loss: 0.4087\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4163 - val_loss: 0.8548\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3810 - val_loss: 0.2774\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5252 - val_loss: 0.4639\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2379 - val_loss: 0.8402\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4427 - val_loss: 0.3118\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3828 - val_loss: 0.1997\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2380 - val_loss: 0.1776\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3504 - val_loss: 0.4356\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3097 - val_loss: 0.4090\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5378 - val_loss: 0.3172\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2607 - val_loss: 0.2040\n",
      "16/16 [==============================] - 1s 20ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.15834401143205257\n",
      "Mean Absolute Error (MAE): 0.29261590465273346\n",
      "Root Mean Squared Error (RMSE): 0.39792463034103903\n",
      "Time taken: 305.41185784339905\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 960.4810 - val_loss: 669.1489\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 525.9167 - val_loss: 427.2061\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 286.9141 - val_loss: 199.6698\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 133.1850 - val_loss: 100.1952\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 54.7791 - val_loss: 31.0537\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 23.6069 - val_loss: 13.1900\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.7788 - val_loss: 11.3044\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.6546 - val_loss: 4.8931\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.6554 - val_loss: 3.5304\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.4675 - val_loss: 4.2821\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.7246 - val_loss: 7.0116\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5391 - val_loss: 2.3669\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2513 - val_loss: 2.7363\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.8037 - val_loss: 10.1545\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1313 - val_loss: 2.6435\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9809 - val_loss: 3.7180\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3298 - val_loss: 2.6635\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3513 - val_loss: 1.9274\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1159 - val_loss: 0.7763\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9996 - val_loss: 0.7272\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9245 - val_loss: 1.2068\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7182 - val_loss: 1.8516\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6380 - val_loss: 1.4889\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3878 - val_loss: 0.6631\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1977 - val_loss: 1.4932\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6795 - val_loss: 1.4151\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2510 - val_loss: 0.9150\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5496 - val_loss: 0.7923\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2606 - val_loss: 1.1842\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9579 - val_loss: 0.8957\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6963 - val_loss: 2.1214\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0826 - val_loss: 1.3540\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3920 - val_loss: 1.8504\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0013 - val_loss: 0.4639\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6267 - val_loss: 0.3652\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7154 - val_loss: 0.8243\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5935 - val_loss: 0.4645\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9056 - val_loss: 0.8412\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6798 - val_loss: 0.6975\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7278 - val_loss: 1.1893\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.0463 - val_loss: 1.7327\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0979 - val_loss: 0.8226\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0385 - val_loss: 0.7469\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6916 - val_loss: 0.9311\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8404 - val_loss: 3.8748\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9105 - val_loss: 0.7825\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7147 - val_loss: 1.6338\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5896 - val_loss: 0.3288\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8357 - val_loss: 1.7685\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9421 - val_loss: 1.0263\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4916 - val_loss: 0.4268\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7680 - val_loss: 1.6288\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7007 - val_loss: 0.4420\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6981 - val_loss: 0.8748\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5903 - val_loss: 0.5884\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9414 - val_loss: 0.5576\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3668 - val_loss: 0.3440\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4950 - val_loss: 34.4635\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3108 - val_loss: 0.3545\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3905 - val_loss: 0.2209\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1953 - val_loss: 0.3577\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3021 - val_loss: 0.1427\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2874 - val_loss: 0.3235\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3813 - val_loss: 0.6610\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5852 - val_loss: 2.2723\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6414 - val_loss: 0.7091\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4133 - val_loss: 1.1298\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7481 - val_loss: 1.2583\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4784 - val_loss: 0.3970\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3730 - val_loss: 1.2931\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4025 - val_loss: 0.3822\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8606 - val_loss: 0.7916\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4108 - val_loss: 0.2400\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6685 - val_loss: 0.4305\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5133 - val_loss: 0.4483\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4373 - val_loss: 0.2919\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4267 - val_loss: 0.3752\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7663 - val_loss: 0.3758\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4850 - val_loss: 0.7739\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5159 - val_loss: 0.2613\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3056 - val_loss: 0.1989\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3716 - val_loss: 0.4690\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3700 - val_loss: 0.9063\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4816 - val_loss: 0.5326\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3322 - val_loss: 0.5639\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4619 - val_loss: 0.1520\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5802 - val_loss: 0.2798\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3136 - val_loss: 0.3658\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5000 - val_loss: 0.7735\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5493 - val_loss: 0.2430\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3608 - val_loss: 0.2413\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3320 - val_loss: 0.4256\n",
      "16/16 [==============================] - 1s 20ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.14157108793934742\n",
      "Mean Absolute Error (MAE): 0.2804544322366126\n",
      "Root Mean Squared Error (RMSE): 0.3762593360161943\n",
      "Time taken: 260.7419774532318\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1001.1172 - val_loss: 728.1339\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 554.0066 - val_loss: 472.5953\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 330.1993 - val_loss: 239.7973\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 160.0246 - val_loss: 99.8058\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 62.5535 - val_loss: 37.5905\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 28.5131 - val_loss: 16.3804\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 15.2570 - val_loss: 7.5819\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 8.8885 - val_loss: 6.8027\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.9303 - val_loss: 4.4896\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.5604 - val_loss: 8.4442\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.5123 - val_loss: 2.7256\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0099 - val_loss: 3.4718\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2759 - val_loss: 7.5044\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6982 - val_loss: 1.7751\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3727 - val_loss: 1.6297\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4128 - val_loss: 2.5408\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5056 - val_loss: 1.4096\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1928 - val_loss: 1.8861\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6355 - val_loss: 1.0271\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3537 - val_loss: 2.5031\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5105 - val_loss: 0.8347\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5710 - val_loss: 1.0060\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0833 - val_loss: 1.1727\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2154 - val_loss: 1.2204\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9576 - val_loss: 1.1712\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9802 - val_loss: 3.6803\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2790 - val_loss: 1.1621\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0328 - val_loss: 1.3471\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2795 - val_loss: 0.5830\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9904 - val_loss: 0.9958\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0758 - val_loss: 1.6481\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9333 - val_loss: 0.7685\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9984 - val_loss: 0.6855\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.8259 - val_loss: 0.5003\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6925 - val_loss: 0.5749\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8867 - val_loss: 4.0873\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9275 - val_loss: 1.0509\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5525 - val_loss: 0.5552\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6393 - val_loss: 0.2972\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7015 - val_loss: 0.3575\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5705 - val_loss: 1.3283\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3141 - val_loss: 0.7082\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6957 - val_loss: 0.5918\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5670 - val_loss: 1.1528\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7869 - val_loss: 0.3139\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3947 - val_loss: 0.2288\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5738 - val_loss: 0.7749\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1965 - val_loss: 1.4467\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6612 - val_loss: 0.2883\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5059 - val_loss: 0.3620\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6152 - val_loss: 0.5757\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6007 - val_loss: 0.2998\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5823 - val_loss: 0.2779\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3786 - val_loss: 0.4519\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8004 - val_loss: 1.4887\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6114 - val_loss: 0.4543\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0081 - val_loss: 1.0850\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5814 - val_loss: 0.3181\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3071 - val_loss: 0.2905\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4321 - val_loss: 0.3202\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4879 - val_loss: 0.2568\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5744 - val_loss: 1.6852\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7156 - val_loss: 1.4777\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4350 - val_loss: 0.5861\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5532 - val_loss: 0.4711\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4702 - val_loss: 0.6240\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4224 - val_loss: 0.4299\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4065 - val_loss: 0.2826\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3713 - val_loss: 0.8974\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0633 - val_loss: 0.3768\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2806 - val_loss: 0.5384\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3900 - val_loss: 0.4854\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2573 - val_loss: 0.1846\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2658 - val_loss: 0.6886\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4144 - val_loss: 0.1953\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5727 - val_loss: 0.3488\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5379 - val_loss: 0.6701\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4502 - val_loss: 0.3347\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3990 - val_loss: 0.5125\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3156 - val_loss: 0.6833\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6578 - val_loss: 3.1147\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5338 - val_loss: 0.4338\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3360 - val_loss: 0.1495\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3010 - val_loss: 0.3125\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3018 - val_loss: 0.2509\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3055 - val_loss: 0.3783\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4723 - val_loss: 0.2420\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4147 - val_loss: 0.9066\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3987 - val_loss: 0.1957\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4389 - val_loss: 0.6820\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3716 - val_loss: 0.2275\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3629 - val_loss: 0.5861\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5009 - val_loss: 0.1751\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2188 - val_loss: 0.3799\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3289 - val_loss: 0.3534\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4171 - val_loss: 0.2486\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3715 - val_loss: 0.5524\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4014 - val_loss: 0.3950\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2776 - val_loss: 0.2045\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3509 - val_loss: 0.3694\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3252 - val_loss: 0.2193\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1666 - val_loss: 0.1774\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4430 - val_loss: 0.9652\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3799 - val_loss: 0.3911\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3469 - val_loss: 0.2627\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1728 - val_loss: 0.2005\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3703 - val_loss: 0.5402\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3073 - val_loss: 0.3799\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2565 - val_loss: 0.1669\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2717 - val_loss: 0.1697\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5824 - val_loss: 0.2797\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1638 - val_loss: 0.1630\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1589 - val_loss: 0.1135\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2162 - val_loss: 0.1689\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1676 - val_loss: 0.2750\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2665 - val_loss: 0.6041\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2642 - val_loss: 0.4917\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3541 - val_loss: 0.6605\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3728 - val_loss: 0.1424\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1709 - val_loss: 0.2864\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1911 - val_loss: 0.2534\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2548 - val_loss: 0.5023\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2555 - val_loss: 0.2975\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4229 - val_loss: 0.3007\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2627 - val_loss: 0.1220\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2180 - val_loss: 1.2683\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3158 - val_loss: 0.3065\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3743 - val_loss: 0.5461\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1745 - val_loss: 0.1472\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1609 - val_loss: 0.2513\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3482 - val_loss: 0.5935\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3775 - val_loss: 0.4256\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2754 - val_loss: 0.2369\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3781 - val_loss: 0.5173\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2038 - val_loss: 0.5071\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3159 - val_loss: 0.2797\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2495 - val_loss: 0.2398\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2953 - val_loss: 0.3272\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2392 - val_loss: 0.2831\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1815 - val_loss: 0.1560\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1994 - val_loss: 0.0877\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2898 - val_loss: 0.2979\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1965 - val_loss: 0.5237\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2363 - val_loss: 0.2609\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1821 - val_loss: 0.3533\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2228 - val_loss: 0.1122\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1717 - val_loss: 0.3369\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2677 - val_loss: 0.2953\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3028 - val_loss: 0.3072\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1955 - val_loss: 0.2027\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1833 - val_loss: 0.1437\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2223 - val_loss: 0.5522\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3277 - val_loss: 0.1262\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1574 - val_loss: 0.4521\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1922 - val_loss: 0.3679\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2383 - val_loss: 0.7612\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3467 - val_loss: 0.1197\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4131 - val_loss: 0.4235\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2766 - val_loss: 0.1535\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2178 - val_loss: 0.5704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2937 - val_loss: 0.3560\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2729 - val_loss: 0.1766\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1473 - val_loss: 0.4238\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2928 - val_loss: 0.3821\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1260 - val_loss: 0.1304\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1922 - val_loss: 0.4092\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1439 - val_loss: 0.1535\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3172 - val_loss: 0.3988\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3415 - val_loss: 0.2228\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1267 - val_loss: 0.0772\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1255 - val_loss: 0.1890\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1783 - val_loss: 0.3490\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1441 - val_loss: 0.1359\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0822 - val_loss: 0.0661\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0728 - val_loss: 0.2374\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1454 - val_loss: 0.0959\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1175 - val_loss: 0.2780\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1466 - val_loss: 0.1267\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1088 - val_loss: 0.4257\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2208 - val_loss: 0.4559\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2228 - val_loss: 0.1109\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1729 - val_loss: 0.1577\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2039 - val_loss: 0.1654\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2043 - val_loss: 0.2033\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2452 - val_loss: 1.4228\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2407 - val_loss: 0.2311\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1216 - val_loss: 0.1221\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1813 - val_loss: 0.0756\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1910 - val_loss: 0.2064\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1139 - val_loss: 0.1711\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2413 - val_loss: 0.4495\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2107 - val_loss: 0.2386\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1943 - val_loss: 0.2340\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3516 - val_loss: 0.1336\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1193 - val_loss: 0.2310\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1476 - val_loss: 0.2112\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1664 - val_loss: 0.1622\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2078 - val_loss: 0.1798\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1804 - val_loss: 0.3027\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1873 - val_loss: 0.2003\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.19897805947080846\n",
      "Mean Absolute Error (MAE): 0.2983976032376259\n",
      "Root Mean Squared Error (RMSE): 0.44606956797209163\n",
      "Time taken: 546.0292768478394\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1013.5299 - val_loss: 683.7412\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 539.6304 - val_loss: 497.2739\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 324.9418 - val_loss: 250.2143\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 167.5269 - val_loss: 101.6478\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 72.4741 - val_loss: 37.8666\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 29.6749 - val_loss: 30.9418\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.9027 - val_loss: 9.3201\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.1964 - val_loss: 5.1804\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 7.1809 - val_loss: 6.8972\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5067 - val_loss: 3.6690\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4746 - val_loss: 2.2377\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2225 - val_loss: 1.3892\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1557 - val_loss: 2.1202\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6630 - val_loss: 1.8008\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4937 - val_loss: 1.9013\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1675 - val_loss: 1.2888\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1167 - val_loss: 3.3765\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7280 - val_loss: 1.6732\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8054 - val_loss: 1.2287\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3018 - val_loss: 1.2850\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5734 - val_loss: 0.6752\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1752 - val_loss: 2.1767\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.3578 - val_loss: 1.3321\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3232 - val_loss: 5.1399\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.4633 - val_loss: 0.6022\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6870 - val_loss: 0.6729\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0766 - val_loss: 0.8833\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8369 - val_loss: 0.9027\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6645 - val_loss: 0.8491\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7593 - val_loss: 1.8116\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9887 - val_loss: 1.6438\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9447 - val_loss: 1.0539\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8328 - val_loss: 2.0111\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9797 - val_loss: 0.4094\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8253 - val_loss: 1.1847\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0365 - val_loss: 0.9912\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7697 - val_loss: 1.3985\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3038 - val_loss: 2.1942\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8756 - val_loss: 1.6755\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6717 - val_loss: 1.3200\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8629 - val_loss: 0.4116\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6262 - val_loss: 0.7714\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7381 - val_loss: 0.4360\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6117 - val_loss: 1.4540\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4418 - val_loss: 0.5435\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7631 - val_loss: 1.1983\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1662 - val_loss: 1.5220\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8175 - val_loss: 1.0379\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6599 - val_loss: 0.4621\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5647 - val_loss: 0.3064\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4685 - val_loss: 2.9096\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8445 - val_loss: 0.3042\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4055 - val_loss: 0.4128\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3495 - val_loss: 0.2532\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5089 - val_loss: 0.2739\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8189 - val_loss: 0.2730\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3481 - val_loss: 0.4816\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5291 - val_loss: 0.8265\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6473 - val_loss: 1.6055\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6992 - val_loss: 0.5946\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5136 - val_loss: 0.2872\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5091 - val_loss: 0.9480\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0124 - val_loss: 0.6757\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2872 - val_loss: 0.4889\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2244 - val_loss: 0.4066\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2621 - val_loss: 0.3078\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3692 - val_loss: 0.3433\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2379 - val_loss: 0.1826\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4362 - val_loss: 0.2535\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3372 - val_loss: 0.4317\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4755 - val_loss: 0.5082\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4546 - val_loss: 0.4688\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6802 - val_loss: 0.9274\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3825 - val_loss: 0.6375\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3546 - val_loss: 0.5717\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3922 - val_loss: 0.4101\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4514 - val_loss: 0.6295\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4050 - val_loss: 0.6522\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5543 - val_loss: 0.4858\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5210 - val_loss: 0.6557\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5673 - val_loss: 0.5653\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3864 - val_loss: 0.3763\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3827 - val_loss: 0.9283\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4429 - val_loss: 0.6872\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2655 - val_loss: 0.2134\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3667 - val_loss: 0.5848\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6596 - val_loss: 0.2520\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2523 - val_loss: 0.3636\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4429 - val_loss: 0.3518\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2743 - val_loss: 0.6625\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3601 - val_loss: 0.8483\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3702 - val_loss: 0.3986\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2338 - val_loss: 0.2004\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4698 - val_loss: 0.5231\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3873 - val_loss: 0.2044\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2850 - val_loss: 0.7346\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4920 - val_loss: 0.3312\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1810 - val_loss: 0.1544\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2967 - val_loss: 0.3403\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5753 - val_loss: 0.1840\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4179 - val_loss: 0.1777\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2400 - val_loss: 0.2635\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4161 - val_loss: 0.6125\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2863 - val_loss: 0.5258\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4367 - val_loss: 16.9663\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8880 - val_loss: 0.5503\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1477 - val_loss: 0.1670\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1254 - val_loss: 0.4192\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2198 - val_loss: 0.2356\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1943 - val_loss: 0.6279\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4540 - val_loss: 0.7072\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2184 - val_loss: 0.1814\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2376 - val_loss: 0.1369\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2541 - val_loss: 0.6901\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3765 - val_loss: 0.1095\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2665 - val_loss: 1.0593\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5245 - val_loss: 0.0897\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3399 - val_loss: 0.3210\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1661 - val_loss: 0.2994\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1898 - val_loss: 0.0984\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4083 - val_loss: 0.6779\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3347 - val_loss: 0.4028\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2070 - val_loss: 0.3425\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1942 - val_loss: 0.2034\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2262 - val_loss: 0.6064\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4694 - val_loss: 0.4934\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3782 - val_loss: 0.8174\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2995 - val_loss: 0.3433\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2117 - val_loss: 0.2054\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2658 - val_loss: 0.2710\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2377 - val_loss: 0.2471\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3191 - val_loss: 0.7455\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2659 - val_loss: 0.1622\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5504 - val_loss: 0.2150\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2618 - val_loss: 0.1706\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1391 - val_loss: 0.1381\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2327 - val_loss: 0.2917\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1396 - val_loss: 0.0677\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1213 - val_loss: 0.2932\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1574 - val_loss: 0.1682\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1172 - val_loss: 0.2288\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1576 - val_loss: 0.1064\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2058 - val_loss: 1.1298\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2831 - val_loss: 0.3269\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1703 - val_loss: 0.1904\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2476 - val_loss: 0.3267\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2925 - val_loss: 0.2090\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1914 - val_loss: 0.3216\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4413 - val_loss: 0.1853\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1361 - val_loss: 0.2944\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1266 - val_loss: 0.0576\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3415 - val_loss: 0.1397\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3023 - val_loss: 0.2000\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1431 - val_loss: 0.1404\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2507 - val_loss: 0.2405\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2712 - val_loss: 0.7575\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2643 - val_loss: 0.2243\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2031 - val_loss: 0.1274\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2314 - val_loss: 0.1041\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3615 - val_loss: 0.3503\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1501 - val_loss: 0.1292\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1989 - val_loss: 0.2109\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1575 - val_loss: 0.2013\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3363 - val_loss: 0.4869\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4072 - val_loss: 0.2887\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1909 - val_loss: 0.5859\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2932 - val_loss: 0.1692\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1330 - val_loss: 0.0923\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2129 - val_loss: 0.1115\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2381 - val_loss: 0.2149\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1833 - val_loss: 0.1374\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1238 - val_loss: 0.1109\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2957 - val_loss: 0.4289\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2359 - val_loss: 0.1819\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3454 - val_loss: 0.1748\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1595 - val_loss: 0.1774\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1254 - val_loss: 0.1873\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2507 - val_loss: 0.4730\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2292 - val_loss: 0.1713\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1117 - val_loss: 0.2101\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1500 - val_loss: 0.3498\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.056943547947807566\n",
      "Mean Absolute Error (MAE): 0.17744280937740378\n",
      "Root Mean Squared Error (RMSE): 0.23862847262597892\n",
      "Time taken: 499.19064712524414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_20372\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.105788  0.234616  0.325251  411.889967\n",
      "1        2  0.158344  0.292616  0.397925  305.411858\n",
      "2        3  0.141571  0.280454  0.376259  260.741977\n",
      "3        4  0.198978  0.298398  0.446070  546.029277\n",
      "4        5  0.056944  0.177443  0.238628  499.190647\n",
      "5  Average  0.132325  0.256705  0.356827  404.652745\n",
      "Results saved to 'Sensors 8_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 8_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 8_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
