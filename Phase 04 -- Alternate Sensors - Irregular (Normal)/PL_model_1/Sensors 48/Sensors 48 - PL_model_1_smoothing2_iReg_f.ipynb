{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 21ms/step - loss: 1127.6996 - val_loss: 924.2287\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 913.7741 - val_loss: 983.1298\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 841.8087 - val_loss: 765.3754\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 645.2273 - val_loss: 486.1789\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 295.2566 - val_loss: 166.8576\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 98.8837 - val_loss: 72.2317\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 34.6091 - val_loss: 29.5312\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 14.6093 - val_loss: 11.8171\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.3031 - val_loss: 11.9977\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.1633 - val_loss: 6.9639\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.5450 - val_loss: 3.5797\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8666 - val_loss: 4.1530\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.6631 - val_loss: 2.6788\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.9920 - val_loss: 3.4520\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.6429 - val_loss: 4.3646\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.2935 - val_loss: 1.9163\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5430 - val_loss: 19.3390\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8297 - val_loss: 4.1675\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.8524 - val_loss: 0.9071\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.2096 - val_loss: 1.7349\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1973 - val_loss: 1.9332\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3830 - val_loss: 2.3342\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8292 - val_loss: 2.7831\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8195 - val_loss: 0.7916\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0943 - val_loss: 1.0452\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3139 - val_loss: 2.9748\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6376 - val_loss: 3.4019\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1704 - val_loss: 3.7512\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0720 - val_loss: 0.5236\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7148 - val_loss: 1.4600\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6947 - val_loss: 0.9479\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7778 - val_loss: 0.7965\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.6767 - val_loss: 0.6818\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9823 - val_loss: 0.7555\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8479 - val_loss: 5.6023\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5964 - val_loss: 0.6932\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7204 - val_loss: 0.3392\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5699 - val_loss: 0.4350\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7397 - val_loss: 1.6936\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.4610 - val_loss: 0.9770\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3447 - val_loss: 0.3580\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3272 - val_loss: 1.0795\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4475 - val_loss: 0.5939\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8619 - val_loss: 0.5875\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6375 - val_loss: 0.2802\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3482 - val_loss: 0.3713\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5221 - val_loss: 0.5575\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4484 - val_loss: 0.6730\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5679 - val_loss: 7.0316\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5274 - val_loss: 0.6704\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2556 - val_loss: 0.2854\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2646 - val_loss: 0.1914\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3769 - val_loss: 0.4781\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3409 - val_loss: 1.4467\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3941 - val_loss: 0.5030\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5235 - val_loss: 0.6651\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3288 - val_loss: 0.4131\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3107 - val_loss: 0.3424\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7353 - val_loss: 1.2601\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8326 - val_loss: 0.7459\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2672 - val_loss: 0.2386\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3367 - val_loss: 1.3345\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3328 - val_loss: 0.3959\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3347 - val_loss: 0.4237\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5319 - val_loss: 1.1207\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4455 - val_loss: 0.3645\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2700 - val_loss: 0.3502\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2209 - val_loss: 0.7329\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5762 - val_loss: 0.2300\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1968 - val_loss: 1.2053\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4089 - val_loss: 0.7368\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2988 - val_loss: 0.5061\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8821 - val_loss: 0.7581\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2095 - val_loss: 0.4904\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3956 - val_loss: 0.3830\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2570 - val_loss: 0.3610\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2441 - val_loss: 0.8550\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2371 - val_loss: 0.4625\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2411 - val_loss: 0.3904\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3391 - val_loss: 0.3653\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4382 - val_loss: 0.4927\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2374 - val_loss: 0.3319\n",
      "16/16 [==============================] - 1s 17ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.19139901043793292\n",
      "Mean Absolute Error (MAE): 0.3193337303003077\n",
      "Root Mean Squared Error (RMSE): 0.43749172613654413\n",
      "Time taken: 452.9226999282837\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 19ms/step - loss: 1129.4607 - val_loss: 906.5050\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 901.3082 - val_loss: 829.1199\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 740.8425 - val_loss: 573.9016\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 404.8269 - val_loss: 256.5296\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 167.1217 - val_loss: 98.4643\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 62.3996 - val_loss: 47.5393\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 26.0926 - val_loss: 16.5206\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.9987 - val_loss: 5.6930\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.1858 - val_loss: 11.3288\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.0331 - val_loss: 8.4119\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.5402 - val_loss: 3.0179\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.7808 - val_loss: 3.7730\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.2812 - val_loss: 1.3107\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6054 - val_loss: 1.3315\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8047 - val_loss: 2.5512\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3225 - val_loss: 2.8777\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1105 - val_loss: 1.6102\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3003 - val_loss: 7.6404\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.2430 - val_loss: 1.1570\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1590 - val_loss: 0.7897\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.2242 - val_loss: 3.5949\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.8302 - val_loss: 0.8369\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.9714 - val_loss: 2.1918\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3254 - val_loss: 4.5647\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1265 - val_loss: 0.6042\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7280 - val_loss: 0.7987\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3748 - val_loss: 10.3653\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3761 - val_loss: 0.7637\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7151 - val_loss: 0.9770\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7348 - val_loss: 0.6634\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9997 - val_loss: 4.7039\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8750 - val_loss: 0.2712\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1289 - val_loss: 3.3110\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4913 - val_loss: 0.6396\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7449 - val_loss: 1.0424\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8087 - val_loss: 0.4780\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6337 - val_loss: 0.5734\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4770 - val_loss: 2.4771\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1586 - val_loss: 18.2177\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9534 - val_loss: 0.2565\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3495 - val_loss: 0.3108\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3863 - val_loss: 0.4210\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4878 - val_loss: 1.1867\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5468 - val_loss: 1.1491\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4765 - val_loss: 1.5143\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0298 - val_loss: 0.5349\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2507 - val_loss: 0.4495\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3785 - val_loss: 0.3257\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5733 - val_loss: 1.6018\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5986 - val_loss: 0.7887\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3254 - val_loss: 0.4908\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4312 - val_loss: 0.6122\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4086 - val_loss: 2.3931\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8024 - val_loss: 3.1759\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5199 - val_loss: 0.5248\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5125 - val_loss: 0.5748\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2826 - val_loss: 0.3825\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8289 - val_loss: 0.6758\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2011 - val_loss: 0.1390\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3312 - val_loss: 1.4229\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5972 - val_loss: 3.0137\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4410 - val_loss: 0.6419\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5546 - val_loss: 0.3994\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2370 - val_loss: 0.3527\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2159 - val_loss: 0.7523\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3257 - val_loss: 0.2899\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4326 - val_loss: 0.4418\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1984 - val_loss: 0.3789\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3218 - val_loss: 1.6606\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2914 - val_loss: 0.4384\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5060 - val_loss: 1.1042\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3302 - val_loss: 0.5883\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2861 - val_loss: 0.5390\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2542 - val_loss: 0.2596\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3179 - val_loss: 3.3409\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0912 - val_loss: 0.2134\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1869 - val_loss: 0.3266\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1127 - val_loss: 0.2382\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2035 - val_loss: 0.3118\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1835 - val_loss: 0.3258\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1851 - val_loss: 0.2564\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2892 - val_loss: 3.2593\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4085 - val_loss: 0.7580\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3442 - val_loss: 0.3523\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3073 - val_loss: 0.5027\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2109 - val_loss: 0.5378\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2258 - val_loss: 0.0949\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1924 - val_loss: 0.1227\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2647 - val_loss: 0.3647\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3727 - val_loss: 0.5436\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3070 - val_loss: 0.4327\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1988 - val_loss: 0.2789\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2541 - val_loss: 0.4594\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4034 - val_loss: 0.1671\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1496 - val_loss: 0.4860\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2187 - val_loss: 0.1164\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2518 - val_loss: 0.3390\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1666 - val_loss: 0.7948\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2972 - val_loss: 0.2779\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1808 - val_loss: 0.1841\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3614 - val_loss: 0.1965\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1963 - val_loss: 0.2159\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1949 - val_loss: 0.2989\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1412 - val_loss: 0.1989\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1297 - val_loss: 0.4465\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2661 - val_loss: 0.2546\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1497 - val_loss: 0.8206\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3568 - val_loss: 0.1781\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1547 - val_loss: 0.4047\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0965 - val_loss: 0.0615\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2220 - val_loss: 0.4520\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2384 - val_loss: 0.1053\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2153 - val_loss: 0.5006\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1511 - val_loss: 0.2098\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2818 - val_loss: 0.2243\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1669 - val_loss: 0.0941\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1476 - val_loss: 0.2205\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1325 - val_loss: 0.1407\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1545 - val_loss: 0.2222\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1984 - val_loss: 0.1411\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1537 - val_loss: 0.7776\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1282 - val_loss: 0.1321\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2443 - val_loss: 0.6427\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1377 - val_loss: 0.1244\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1005 - val_loss: 0.3804\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1308 - val_loss: 0.2619\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2408 - val_loss: 0.3847\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1585 - val_loss: 0.1539\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1269 - val_loss: 0.3049\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4049 - val_loss: 0.1628\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0943 - val_loss: 0.0543\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0952 - val_loss: 0.1633\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1242 - val_loss: 0.1290\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1612 - val_loss: 0.2562\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2976 - val_loss: 0.1484\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1287 - val_loss: 0.0982\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0828 - val_loss: 0.1914\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1451 - val_loss: 0.2947\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2115 - val_loss: 0.0905\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0951 - val_loss: 0.2057\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1297 - val_loss: 0.1884\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1798 - val_loss: 0.1520\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1046 - val_loss: 0.1932\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1835 - val_loss: 0.1519\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1252 - val_loss: 0.5114\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1611 - val_loss: 0.1393\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1002 - val_loss: 0.2653\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0849 - val_loss: 0.0995\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2041 - val_loss: 0.2837\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1597 - val_loss: 0.1592\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0923 - val_loss: 0.1220\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1283 - val_loss: 0.1596\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1491 - val_loss: 0.2784\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1094 - val_loss: 0.2083\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.0742 - val_loss: 0.1525\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1555 - val_loss: 0.2608\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1037 - val_loss: 0.1964\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2945 - val_loss: 0.1726\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 0.0895 - val_loss: 0.1684\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.0819 - val_loss: 0.1797\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1578 - val_loss: 0.8649\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.05430975504562605\n",
      "Mean Absolute Error (MAE): 0.17745639572441552\n",
      "Root Mean Squared Error (RMSE): 0.23304453446847032\n",
      "Time taken: 875.0503187179565\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 18ms/step - loss: 1158.2167 - val_loss: 938.0027\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 904.8818 - val_loss: 951.6509\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 761.7576 - val_loss: 626.3298\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 459.7619 - val_loss: 323.6957\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 194.0562 - val_loss: 115.3214\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 79.4213 - val_loss: 84.2393\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 33.3216 - val_loss: 47.9383\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 16.2944 - val_loss: 12.4485\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.5841 - val_loss: 9.2018\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.5392 - val_loss: 15.5241\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.8225 - val_loss: 2.6498\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1301 - val_loss: 3.6092\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.6799 - val_loss: 1.8045\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.7235 - val_loss: 3.1430\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.8089 - val_loss: 4.4884\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.7569 - val_loss: 1.9050\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.2180 - val_loss: 6.0674\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.6969 - val_loss: 1.4739\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1744 - val_loss: 1.9142\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0235 - val_loss: 1.0811\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3636 - val_loss: 2.1300\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0502 - val_loss: 0.6044\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8926 - val_loss: 0.7553\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1062 - val_loss: 0.9582\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6679 - val_loss: 1.2801\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3066 - val_loss: 2.6081\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1550 - val_loss: 1.6165\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0638 - val_loss: 0.8143\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9868 - val_loss: 1.1678\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0003 - val_loss: 2.2123\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5850 - val_loss: 0.3643\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5058 - val_loss: 0.6312\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8977 - val_loss: 1.5302\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8341 - val_loss: 0.8359\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.8428 - val_loss: 0.4670\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4982 - val_loss: 1.0971\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0915 - val_loss: 1.8945\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7908 - val_loss: 4.4271\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0234 - val_loss: 0.8340\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4148 - val_loss: 0.5078\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5694 - val_loss: 1.5045\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0337 - val_loss: 0.4802\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4972 - val_loss: 0.4936\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6748 - val_loss: 0.7046\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4041 - val_loss: 0.5270\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3212 - val_loss: 1.0385\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.2041 - val_loss: 1.0563\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4811 - val_loss: 0.6405\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2813 - val_loss: 0.2250\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5101 - val_loss: 0.6856\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4135 - val_loss: 0.6092\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6056 - val_loss: 0.4065\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3204 - val_loss: 0.2964\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2991 - val_loss: 0.3155\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5599 - val_loss: 1.2377\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7146 - val_loss: 0.5240\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4029 - val_loss: 0.6502\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3057 - val_loss: 0.6712\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5524 - val_loss: 0.4753\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3674 - val_loss: 0.9238\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4716 - val_loss: 0.9633\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3501 - val_loss: 0.3371\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4878 - val_loss: 0.7630\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3198 - val_loss: 0.5785\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2261 - val_loss: 0.5142\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4273 - val_loss: 2.3829\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3798 - val_loss: 0.4020\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3332 - val_loss: 0.9152\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6472 - val_loss: 1.1639\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5091 - val_loss: 0.2011\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1913 - val_loss: 0.2202\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2570 - val_loss: 0.3003\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2332 - val_loss: 1.3565\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3571 - val_loss: 0.4216\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5045 - val_loss: 0.3021\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4418 - val_loss: 0.8588\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2591 - val_loss: 0.2630\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3122 - val_loss: 1.0515\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3022 - val_loss: 0.5770\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1859 - val_loss: 0.1674\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3127 - val_loss: 2.8014\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4347 - val_loss: 0.1469\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2007 - val_loss: 0.3683\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2089 - val_loss: 0.9067\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2109 - val_loss: 0.3544\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2390 - val_loss: 0.6225\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2669 - val_loss: 0.8524\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3831 - val_loss: 0.4210\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2203 - val_loss: 0.7050\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3429 - val_loss: 0.3103\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1597 - val_loss: 0.2001\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1587 - val_loss: 0.4617\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3930 - val_loss: 0.2001\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1066 - val_loss: 0.1036\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1574 - val_loss: 0.2083\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3175 - val_loss: 0.2611\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2252 - val_loss: 0.1467\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2162 - val_loss: 0.4108\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1354 - val_loss: 0.8032\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4784 - val_loss: 1.7711\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2387 - val_loss: 0.1766\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1051 - val_loss: 0.1692\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1619 - val_loss: 0.2924\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4114 - val_loss: 0.9054\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1151 - val_loss: 0.0580\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4284 - val_loss: 0.5247\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1015 - val_loss: 0.1142\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0728 - val_loss: 0.3456\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2131 - val_loss: 0.8867\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1853 - val_loss: 0.2196\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1768 - val_loss: 0.4764\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1945 - val_loss: 0.5649\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2403 - val_loss: 0.2437\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1801 - val_loss: 0.3218\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3228 - val_loss: 0.2907\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1411 - val_loss: 0.0927\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.0827 - val_loss: 0.1324\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2450 - val_loss: 0.2047\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1210 - val_loss: 0.0687\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1573 - val_loss: 0.1033\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1395 - val_loss: 0.3515\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3243 - val_loss: 0.1530\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0846 - val_loss: 0.0850\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1589 - val_loss: 0.2361\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2602 - val_loss: 0.5575\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1661 - val_loss: 0.1539\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1214 - val_loss: 0.1280\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1788 - val_loss: 0.8153\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1767 - val_loss: 0.1482\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1625 - val_loss: 0.1713\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1210 - val_loss: 0.5074\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1564 - val_loss: 0.2011\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1794 - val_loss: 0.5503\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2639 - val_loss: 0.4875\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0850 - val_loss: 0.0821\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.05799731095554234\n",
      "Mean Absolute Error (MAE): 0.18068044095960065\n",
      "Root Mean Squared Error (RMSE): 0.24082630868645216\n",
      "Time taken: 736.8801321983337\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1112.3330 - val_loss: 949.7031\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 895.9875 - val_loss: 920.1911\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 748.8783 - val_loss: 600.5498\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 437.1395 - val_loss: 323.0398\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 246.0999 - val_loss: 184.4026\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 125.4845 - val_loss: 93.8277\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 59.3836 - val_loss: 45.4113\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 25.2829 - val_loss: 19.4048\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 14.5009 - val_loss: 25.4020\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.7400 - val_loss: 6.8106\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.7031 - val_loss: 7.1215\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.4533 - val_loss: 3.5766\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.5987 - val_loss: 2.7681\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.0824 - val_loss: 2.4411\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.5781 - val_loss: 5.0168\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.0337 - val_loss: 1.8205\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6980 - val_loss: 3.3752\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.8614 - val_loss: 2.6173\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.8950 - val_loss: 39.3324\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 6.3207 - val_loss: 2.1491\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.4916 - val_loss: 0.8208\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9551 - val_loss: 1.4252\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7447 - val_loss: 1.4096\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3296 - val_loss: 1.3792\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.4650 - val_loss: 0.6162\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7397 - val_loss: 0.6247\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8017 - val_loss: 0.7187\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3492 - val_loss: 1.4663\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9754 - val_loss: 1.1042\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0169 - val_loss: 1.2838\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3232 - val_loss: 2.9872\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8213 - val_loss: 0.6120\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7535 - val_loss: 1.6025\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.8170 - val_loss: 1.2369\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7090 - val_loss: 1.7060\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.2553 - val_loss: 0.7506\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5102 - val_loss: 0.4543\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.9931 - val_loss: 1.4002\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7970 - val_loss: 0.9238\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5444 - val_loss: 0.6298\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5262 - val_loss: 0.5588\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5870 - val_loss: 0.3089\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5442 - val_loss: 0.6666\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4440 - val_loss: 0.3824\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3876 - val_loss: 2.2706\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3091 - val_loss: 0.7686\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4562 - val_loss: 0.7056\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5076 - val_loss: 0.5162\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4474 - val_loss: 0.9156\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5260 - val_loss: 0.7663\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3727 - val_loss: 0.2381\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6256 - val_loss: 1.5952\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7231 - val_loss: 1.0555\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4931 - val_loss: 0.4001\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2761 - val_loss: 0.7471\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6007 - val_loss: 0.5722\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4980 - val_loss: 0.3095\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2888 - val_loss: 0.5171\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4520 - val_loss: 1.1667\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.8135 - val_loss: 1.4789\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2703 - val_loss: 0.4209\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2584 - val_loss: 0.4019\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2312 - val_loss: 0.2039\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2028 - val_loss: 0.1619\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5161 - val_loss: 1.2177\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3439 - val_loss: 0.4728\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2079 - val_loss: 0.6174\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4004 - val_loss: 0.8542\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.6690 - val_loss: 1.0096\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2426 - val_loss: 0.4494\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4330 - val_loss: 0.5774\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2607 - val_loss: 0.3414\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3973 - val_loss: 0.6518\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3547 - val_loss: 0.4281\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2636 - val_loss: 0.4546\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1786 - val_loss: 0.8565\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3760 - val_loss: 0.6109\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3071 - val_loss: 0.9292\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5683 - val_loss: 0.9419\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2980 - val_loss: 1.5562\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3662 - val_loss: 0.2413\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1254 - val_loss: 0.1654\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1819 - val_loss: 0.4471\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2704 - val_loss: 2.8368\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3341 - val_loss: 0.2634\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2234 - val_loss: 0.5521\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2960 - val_loss: 0.2295\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3594 - val_loss: 0.6084\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1821 - val_loss: 0.1846\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1948 - val_loss: 0.5604\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2939 - val_loss: 0.8403\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2042 - val_loss: 0.6508\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2776 - val_loss: 0.2414\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1892 - val_loss: 0.4228\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.16185915931025216\n",
      "Mean Absolute Error (MAE): 0.31311330463544973\n",
      "Root Mean Squared Error (RMSE): 0.4023172371527874\n",
      "Time taken: 515.6702301502228\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 21ms/step - loss: 1168.6095 - val_loss: 928.0469\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 922.3950 - val_loss: 925.5712\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 828.6758 - val_loss: 775.0764\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 594.3215 - val_loss: 428.4574\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 284.9984 - val_loss: 191.0044\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 110.3131 - val_loss: 67.3480\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 47.4401 - val_loss: 35.2006\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 19.3848 - val_loss: 16.4142\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 10.8697 - val_loss: 8.2767\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.9331 - val_loss: 5.5655\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.3345 - val_loss: 5.1738\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.7726 - val_loss: 5.0540\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2052 - val_loss: 2.1510\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.8938 - val_loss: 1.5524\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0313 - val_loss: 4.2998\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.2633 - val_loss: 4.8301\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6452 - val_loss: 0.7140\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0644 - val_loss: 1.0707\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.2632 - val_loss: 6.9515\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4983 - val_loss: 0.8547\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3007 - val_loss: 2.4004\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7919 - val_loss: 1.0561\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1668 - val_loss: 2.7852\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6979 - val_loss: 0.4681\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.6354 - val_loss: 0.9426\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.8223 - val_loss: 0.6784\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0525 - val_loss: 3.3651\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1961 - val_loss: 0.8116\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5222 - val_loss: 0.3849\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9756 - val_loss: 3.1424\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8923 - val_loss: 0.5588\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4328 - val_loss: 1.3643\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8408 - val_loss: 1.3824\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8513 - val_loss: 0.9467\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6117 - val_loss: 1.7018\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1944 - val_loss: 0.3156\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3504 - val_loss: 0.2672\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4754 - val_loss: 0.7533\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5605 - val_loss: 1.0310\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.0116 - val_loss: 0.8133\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4017 - val_loss: 0.4871\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2484 - val_loss: 0.1340\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2192 - val_loss: 0.1288\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6116 - val_loss: 0.3533\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4348 - val_loss: 1.4282\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4166 - val_loss: 0.3912\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5641 - val_loss: 2.5958\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5267 - val_loss: 0.9482\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2912 - val_loss: 1.8254\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4301 - val_loss: 0.2640\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.7399 - val_loss: 1.1821\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5400 - val_loss: 0.3425\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3877 - val_loss: 0.1090\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3291 - val_loss: 0.8008\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2300 - val_loss: 0.3197\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3930 - val_loss: 0.6029\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6093 - val_loss: 0.4587\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4872 - val_loss: 2.8147\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4639 - val_loss: 0.5141\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1783 - val_loss: 0.3186\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2956 - val_loss: 0.8436\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0570 - val_loss: 1.1635\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2068 - val_loss: 0.1363\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1528 - val_loss: 0.4980\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2561 - val_loss: 0.7846\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3914 - val_loss: 0.7050\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2962 - val_loss: 0.2478\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3364 - val_loss: 0.6253\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4880 - val_loss: 0.3065\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1852 - val_loss: 0.4045\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3060 - val_loss: 0.3830\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2081 - val_loss: 0.5334\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4077 - val_loss: 0.3717\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2939 - val_loss: 0.3140\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3399 - val_loss: 0.8838\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4035 - val_loss: 1.9270\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8078 - val_loss: 0.1588\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1417 - val_loss: 0.1227\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2139 - val_loss: 0.2260\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1507 - val_loss: 0.2239\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1293 - val_loss: 0.2569\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2270 - val_loss: 0.1323\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3557 - val_loss: 0.5997\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.10898039179716619\n",
      "Mean Absolute Error (MAE): 0.24387891207969012\n",
      "Root Mean Squared Error (RMSE): 0.33012178328181585\n",
      "Time taken: 455.72912788391113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 48, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 48, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_3060\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.191399  0.319334  0.437492  452.922700\n",
      "1        2  0.054310  0.177456  0.233045  875.050319\n",
      "2        3  0.057997  0.180680  0.240826  736.880132\n",
      "3        4  0.161859  0.313113  0.402317  515.670230\n",
      "4        5  0.108980  0.243879  0.330122  455.729128\n",
      "5  Average  0.114909  0.246893  0.328760  607.250502\n",
      "Results saved to 'Sensors 48_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 48_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 48_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
