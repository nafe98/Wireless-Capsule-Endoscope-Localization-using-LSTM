{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 22ms/step - loss: 1161.5126 - val_loss: 888.1036\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 924.4514 - val_loss: 844.4290\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 848.8161 - val_loss: 751.6401\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 723.0663 - val_loss: 641.5462\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 511.3136 - val_loss: 355.3709\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 223.8171 - val_loss: 135.4229\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 96.9988 - val_loss: 68.0748\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 49.9265 - val_loss: 44.3834\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 32.6591 - val_loss: 28.7489\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 24.7939 - val_loss: 26.6937\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 21.9403 - val_loss: 23.7000\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 18.3096 - val_loss: 26.4078\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.1452 - val_loss: 14.6053\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.8675 - val_loss: 32.2630\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 13.5353 - val_loss: 16.3024\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.2138 - val_loss: 16.6516\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 9.6279 - val_loss: 27.3463\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 11.1965 - val_loss: 19.6657\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 11.2277 - val_loss: 10.6428\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.8308 - val_loss: 10.7353\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.7864 - val_loss: 29.8399\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.8662 - val_loss: 11.9762\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 9.1710 - val_loss: 35.5837\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.8617 - val_loss: 8.7306\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.3526 - val_loss: 9.8973\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.4858 - val_loss: 11.4400\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.4169 - val_loss: 12.4195\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.8189 - val_loss: 10.2944\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.8929 - val_loss: 8.2242\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.0818 - val_loss: 7.8414\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.2419 - val_loss: 11.1956\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.7046 - val_loss: 6.4014\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.0234 - val_loss: 9.7507\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.3505 - val_loss: 10.1402\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.7739 - val_loss: 12.3846\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.9387 - val_loss: 7.1759\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.6123 - val_loss: 15.5333\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.0970 - val_loss: 6.7337\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.7260 - val_loss: 9.2551\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.8525 - val_loss: 7.1407\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.3809 - val_loss: 11.4333\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.9265 - val_loss: 8.5907\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.8682 - val_loss: 9.6871\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.6063 - val_loss: 14.6645\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.5000 - val_loss: 6.4320\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.1735 - val_loss: 7.5599\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.2785 - val_loss: 6.1451\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.3218 - val_loss: 6.4367\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.2502 - val_loss: 12.4776\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.9982 - val_loss: 10.2008\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9408 - val_loss: 6.3338\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.3122 - val_loss: 9.4311\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.1664 - val_loss: 6.7923\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5063 - val_loss: 5.7148\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3283 - val_loss: 9.4995\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.2953 - val_loss: 5.7802\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.6276 - val_loss: 7.0782\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.7263 - val_loss: 7.9032\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.4662 - val_loss: 7.8200\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 3.4952 - val_loss: 7.5348\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.7018 - val_loss: 5.5299\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.0564 - val_loss: 6.2028\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 3.1180 - val_loss: 7.1143\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.4478 - val_loss: 9.6199\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 3.2839 - val_loss: 5.8471\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.3676 - val_loss: 5.9379\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.5037 - val_loss: 6.3014\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.9955 - val_loss: 8.4549\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1478 - val_loss: 6.2326\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.1702 - val_loss: 5.9145\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8229 - val_loss: 6.0088\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5136 - val_loss: 6.4521\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.0222 - val_loss: 6.8711\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9296 - val_loss: 6.7487\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9496 - val_loss: 5.4250\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.6942 - val_loss: 5.4229\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4692 - val_loss: 6.2479\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.4698 - val_loss: 8.0538\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5084 - val_loss: 6.1081\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 17ms/step - loss: 2.2508 - val_loss: 5.2111\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5402 - val_loss: 6.5404\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.6313 - val_loss: 11.1214\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6257 - val_loss: 5.1203\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2162 - val_loss: 6.6430\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1730 - val_loss: 8.6095\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3651 - val_loss: 8.1425\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.1572 - val_loss: 5.5625\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.0203 - val_loss: 6.3670\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3513 - val_loss: 7.5980\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1798 - val_loss: 5.2271\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.0722 - val_loss: 5.6437\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1066 - val_loss: 7.8762\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.9616 - val_loss: 5.6487\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1956 - val_loss: 6.3374\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.7422 - val_loss: 5.8822\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.9396 - val_loss: 5.6208\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.7962 - val_loss: 4.7269\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.8163 - val_loss: 5.9116\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.7442 - val_loss: 5.8070\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5544 - val_loss: 6.3643\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.8435 - val_loss: 7.9395\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2625 - val_loss: 5.6993\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.7302 - val_loss: 5.5972\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5019 - val_loss: 5.6533\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5033 - val_loss: 6.4991\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5760 - val_loss: 5.5442\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5393 - val_loss: 5.5468\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.5757 - val_loss: 6.6242\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6744 - val_loss: 7.5466\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1470 - val_loss: 5.5790\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5225 - val_loss: 6.6818\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5217 - val_loss: 6.7973\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5964 - val_loss: 5.2767\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.2478 - val_loss: 6.5458\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5334 - val_loss: 6.7816\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4026 - val_loss: 5.0613\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3992 - val_loss: 6.1072\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.6758 - val_loss: 7.2151\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9153 - val_loss: 6.5687\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.2967 - val_loss: 6.0289\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.9001 - val_loss: 10.5943\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2889 - val_loss: 5.2794\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.4859 - val_loss: 5.1426\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0916 - val_loss: 6.8436\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1480 - val_loss: 5.9144\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1056 - val_loss: 5.9462\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1828 - val_loss: 6.6645\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 4.7270104835058975\n",
      "Mean Absolute Error (MAE): 1.4319957209352339\n",
      "Root Mean Squared Error (RMSE): 2.174168917886993\n",
      "Time taken: 727.1414668560028\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1127.5212 - val_loss: 958.7015\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 900.9128 - val_loss: 900.4346\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 814.2238 - val_loss: 783.1533\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 631.1222 - val_loss: 501.8171\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 331.8041 - val_loss: 220.7795\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 156.9207 - val_loss: 124.3174\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 83.7694 - val_loss: 67.0472\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 49.5239 - val_loss: 54.7956\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 40.9200 - val_loss: 53.2841\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 29.1848 - val_loss: 32.2831\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 23.6549 - val_loss: 33.2737\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 20.6156 - val_loss: 24.9493\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 21.7471 - val_loss: 20.1941\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 16.4424 - val_loss: 19.3419\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 13.5560 - val_loss: 27.4675\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 14.0452 - val_loss: 14.3710\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 11.3796 - val_loss: 12.3880\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 11.9847 - val_loss: 9.7400\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 10.9078 - val_loss: 12.6179\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 10.4460 - val_loss: 17.3099\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 10.5457 - val_loss: 9.2060\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.5019 - val_loss: 23.8095\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 10.0221 - val_loss: 10.3208\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 8.4267 - val_loss: 14.1083\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 8.2639 - val_loss: 9.1210\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.4015 - val_loss: 8.3180\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 8.2899 - val_loss: 7.8123\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.8604 - val_loss: 14.2350\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.2862 - val_loss: 12.3517\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.4102 - val_loss: 7.9899\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.5749 - val_loss: 14.4470\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.2985 - val_loss: 8.8694\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 6.7346 - val_loss: 7.6091\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.2182 - val_loss: 11.3554\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.7289 - val_loss: 7.1144\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 6.4198 - val_loss: 6.6944\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.4611 - val_loss: 14.3996\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.6491 - val_loss: 9.1839\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.9737 - val_loss: 18.6107\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.3712 - val_loss: 8.7424\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.1013 - val_loss: 7.3777\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.5743 - val_loss: 6.2011\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.9016 - val_loss: 19.9476\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.0597 - val_loss: 6.3711\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.3050 - val_loss: 6.0421\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.4227 - val_loss: 7.2119\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.9091 - val_loss: 5.9029\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.0908 - val_loss: 5.8682\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.1270 - val_loss: 6.2187\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.5256 - val_loss: 7.9799\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.1364 - val_loss: 5.2435\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8856 - val_loss: 8.5694\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.8488 - val_loss: 5.8657\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9998 - val_loss: 8.1439\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.8354 - val_loss: 6.6075\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0767 - val_loss: 12.4450\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.7887 - val_loss: 8.8441\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.0618 - val_loss: 5.3877\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.4171 - val_loss: 6.2528\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6170 - val_loss: 7.4279\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.1576 - val_loss: 5.4196\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.2328 - val_loss: 5.1520\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.4612 - val_loss: 6.4048\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.7462 - val_loss: 5.7885\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2000 - val_loss: 6.4602\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.2988 - val_loss: 6.6145\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.5433 - val_loss: 5.4937\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.7700 - val_loss: 5.5175\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0464 - val_loss: 6.3579\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5400 - val_loss: 5.7093\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6775 - val_loss: 5.1514\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7280 - val_loss: 5.6536\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.2979 - val_loss: 5.2791\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8395 - val_loss: 6.1232\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6389 - val_loss: 5.7248\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3232 - val_loss: 6.1011\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5785 - val_loss: 5.5803\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5212 - val_loss: 5.2039\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.4452 - val_loss: 5.3728\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4309 - val_loss: 6.0272\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4570 - val_loss: 5.7878\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2428 - val_loss: 7.8761\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.0292 - val_loss: 5.3731\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.1248 - val_loss: 6.3260\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.3953 - val_loss: 8.4009\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1382 - val_loss: 6.1333\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.1284 - val_loss: 5.2621\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1041 - val_loss: 7.6425\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0538 - val_loss: 7.4784\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0770 - val_loss: 7.6528\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1834 - val_loss: 5.5415\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5287 - val_loss: 5.6325\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.4844 - val_loss: 5.7501\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.1103 - val_loss: 7.2664\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9355 - val_loss: 5.6930\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0110 - val_loss: 5.7652\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.4434 - val_loss: 7.4024\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.8765 - val_loss: 5.2298\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.6842 - val_loss: 5.2384\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.6146 - val_loss: 5.2381\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.4456 - val_loss: 6.4135\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 5.15130463689099\n",
      "Mean Absolute Error (MAE): 1.5664569824610963\n",
      "Root Mean Squared Error (RMSE): 2.2696485712310155\n",
      "Time taken: 567.7171483039856\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1150.0265 - val_loss: 949.5994\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 919.2794 - val_loss: 931.0820\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 862.7637 - val_loss: 853.3908\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 732.1353 - val_loss: 673.5757\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 489.2691 - val_loss: 348.0309\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 228.1382 - val_loss: 142.0094\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 104.3484 - val_loss: 85.5544\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 53.3898 - val_loss: 43.0388\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 34.8506 - val_loss: 30.7881\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 25.8978 - val_loss: 23.6685\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 21.3567 - val_loss: 15.7465\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 19.1577 - val_loss: 28.7427\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 15.9121 - val_loss: 15.4016\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 13.8651 - val_loss: 13.9267\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.7017 - val_loss: 20.1516\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.6270 - val_loss: 11.8381\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.8198 - val_loss: 13.7637\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 10.4616 - val_loss: 11.9354\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 11.6191 - val_loss: 10.1686\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 10.2549 - val_loss: 16.3629\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 9.9563 - val_loss: 10.1943\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 9.1897 - val_loss: 13.8088\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.5119 - val_loss: 15.3971\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 8.3968 - val_loss: 9.0323\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.6125 - val_loss: 14.5218\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.4848 - val_loss: 8.9038\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.3429 - val_loss: 9.9154\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 7.1792 - val_loss: 13.6179\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.6427 - val_loss: 12.9465\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.3373 - val_loss: 6.6366\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.3992 - val_loss: 7.3888\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.7543 - val_loss: 15.3644\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.6318 - val_loss: 6.7210\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.9538 - val_loss: 6.7554\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.7626 - val_loss: 6.1217\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.7301 - val_loss: 8.0457\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.4044 - val_loss: 7.7493\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.9101 - val_loss: 5.2399\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.2938 - val_loss: 6.0539\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.2110 - val_loss: 7.0817\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.8733 - val_loss: 5.9112\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.3622 - val_loss: 7.5844\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.6665 - val_loss: 11.9050\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.1892 - val_loss: 5.6980\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.6281 - val_loss: 6.9482\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.4146 - val_loss: 5.2663\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.1405 - val_loss: 8.9941\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.0842 - val_loss: 6.0465\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.9404 - val_loss: 15.3479\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.7758 - val_loss: 8.2593\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.1188 - val_loss: 5.3493\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6190 - val_loss: 4.7332\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7184 - val_loss: 7.9165\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.1448 - val_loss: 6.1810\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.1325 - val_loss: 11.1234\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.0899 - val_loss: 6.1501\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5030 - val_loss: 4.8515\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.7390 - val_loss: 5.9404\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.4973 - val_loss: 5.4204\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.5129 - val_loss: 7.5724\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.6560 - val_loss: 11.9009\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.2912 - val_loss: 5.8287\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.2839 - val_loss: 5.7576\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9599 - val_loss: 4.3488\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.4613 - val_loss: 4.9908\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.4845 - val_loss: 6.4409\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9022 - val_loss: 7.1837\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9486 - val_loss: 5.4805\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9251 - val_loss: 4.4762\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.1096 - val_loss: 8.7110\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2851 - val_loss: 12.5096\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.7900 - val_loss: 4.7847\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.2463 - val_loss: 7.8702\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1041 - val_loss: 6.3478\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9723 - val_loss: 7.7488\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.6481 - val_loss: 7.5432\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5960 - val_loss: 5.0309\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.4004 - val_loss: 5.0531\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4840 - val_loss: 4.8697\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4611 - val_loss: 5.7306\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3692 - val_loss: 6.4532\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4858 - val_loss: 4.5435\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3517 - val_loss: 6.3103\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2711 - val_loss: 4.9750\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.3010 - val_loss: 4.6665\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.6487 - val_loss: 9.4558\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8574 - val_loss: 5.4161\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.3905 - val_loss: 5.2291\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.3044 - val_loss: 6.4235\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0217 - val_loss: 4.3051\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0168 - val_loss: 5.0053\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.0444 - val_loss: 4.6733\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0850 - val_loss: 10.0004\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.0798 - val_loss: 4.4317\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7621 - val_loss: 4.3477\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.8226 - val_loss: 4.5452\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8352 - val_loss: 4.7514\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.8989 - val_loss: 4.8529\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.7822 - val_loss: 5.1499\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7886 - val_loss: 4.6054\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7095 - val_loss: 4.1347\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7221 - val_loss: 4.6724\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6567 - val_loss: 4.8684\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6029 - val_loss: 5.0834\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9372 - val_loss: 5.1880\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.8411 - val_loss: 4.2720\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8763 - val_loss: 5.5704\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3909 - val_loss: 4.5676\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.3590 - val_loss: 4.7732\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5697 - val_loss: 5.2104\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3783 - val_loss: 4.9484\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4166 - val_loss: 4.3451\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4097 - val_loss: 4.9666\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.5222 - val_loss: 5.0206\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4846 - val_loss: 4.9780\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4392 - val_loss: 4.8076\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1117 - val_loss: 9.5039\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.7561 - val_loss: 10.0474\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5655 - val_loss: 5.1198\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3921 - val_loss: 5.9224\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3290 - val_loss: 4.6578\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1236 - val_loss: 4.8096\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0389 - val_loss: 5.4708\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3564 - val_loss: 4.5768\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.2248 - val_loss: 4.4192\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1200 - val_loss: 4.9200\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3667 - val_loss: 4.6235\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.1494 - val_loss: 4.6625\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1758 - val_loss: 4.9653\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.9631 - val_loss: 4.3085\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0277 - val_loss: 6.3742\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 4.134460437385269\n",
      "Mean Absolute Error (MAE): 1.389254433609709\n",
      "Root Mean Squared Error (RMSE): 2.0333372660198967\n",
      "Time taken: 733.3122341632843\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1112.1278 - val_loss: 904.0601\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 898.9786 - val_loss: 888.0316\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 751.7925 - val_loss: 697.8099\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 427.0052 - val_loss: 291.6666\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 199.9899 - val_loss: 139.8024\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 98.7257 - val_loss: 112.2024\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 55.1467 - val_loss: 50.1649\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 38.3734 - val_loss: 33.6992\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 27.4546 - val_loss: 46.2123\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 23.7137 - val_loss: 35.4027\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 19.3675 - val_loss: 16.7808\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 16.8319 - val_loss: 33.6217\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.2987 - val_loss: 16.9155\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 13.6684 - val_loss: 17.5050\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 13.4903 - val_loss: 11.5805\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.0712 - val_loss: 19.0524\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 11.9869 - val_loss: 12.9541\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.8573 - val_loss: 16.4799\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 10.4895 - val_loss: 14.0988\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.1749 - val_loss: 13.8300\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.7937 - val_loss: 11.2648\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.4857 - val_loss: 15.8985\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 7.8230 - val_loss: 13.2859\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 8.1583 - val_loss: 10.4183\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 8.2706 - val_loss: 13.5319\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.4833 - val_loss: 8.0829\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.5344 - val_loss: 10.8138\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.4969 - val_loss: 18.8443\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.3568 - val_loss: 9.9533\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.6281 - val_loss: 8.1782\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.6315 - val_loss: 7.0841\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.9832 - val_loss: 8.0815\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.0193 - val_loss: 9.5019\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.5267 - val_loss: 9.0695\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.0437 - val_loss: 6.8272\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.1609 - val_loss: 7.6038\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.4769 - val_loss: 6.4866\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.6303 - val_loss: 7.4573\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.9371 - val_loss: 8.4999\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.4397 - val_loss: 6.9453\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.7504 - val_loss: 10.5082\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.6365 - val_loss: 5.0654\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.6239 - val_loss: 7.8030\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.5349 - val_loss: 7.7492\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.4688 - val_loss: 9.1967\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.9149 - val_loss: 8.9022\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.3808 - val_loss: 32.2009\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.0544 - val_loss: 21.1549\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.2361 - val_loss: 33.7675\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.6167 - val_loss: 7.5394\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9074 - val_loss: 8.7068\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7139 - val_loss: 5.9780\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.7860 - val_loss: 8.2829\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5699 - val_loss: 7.1960\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.8487 - val_loss: 4.8538\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.3694 - val_loss: 6.5304\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7049 - val_loss: 6.9835\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 5.7394 - val_loss: 11.8560\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.9133 - val_loss: 6.3590\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.3391 - val_loss: 6.6907\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.0314 - val_loss: 6.4328\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8869 - val_loss: 8.3738\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.4336 - val_loss: 10.7427\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.3774 - val_loss: 9.7786\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.3593 - val_loss: 29.2248\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.2560 - val_loss: 5.3671\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9933 - val_loss: 5.7417\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9969 - val_loss: 5.1685\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.7219 - val_loss: 5.9421\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8939 - val_loss: 6.0246\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.0300 - val_loss: 5.3345\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8817 - val_loss: 5.9141\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8137 - val_loss: 4.9617\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7047 - val_loss: 8.5073\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6401 - val_loss: 5.6547\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6250 - val_loss: 4.8027\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.8762 - val_loss: 6.7248\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7113 - val_loss: 6.6362\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5020 - val_loss: 5.1356\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 17ms/step - loss: 2.4128 - val_loss: 7.6681\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7899 - val_loss: 5.3333\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2922 - val_loss: 5.0998\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2936 - val_loss: 7.4818\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.2692 - val_loss: 5.2767\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.2999 - val_loss: 5.7362\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.6452 - val_loss: 37.8962\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.4802 - val_loss: 5.2610\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.1941 - val_loss: 5.2697\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.1123 - val_loss: 5.1134\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.0538 - val_loss: 5.6357\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9734 - val_loss: 5.3650\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0473 - val_loss: 8.2786\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0977 - val_loss: 5.0004\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2878 - val_loss: 7.3021\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9373 - val_loss: 4.9711\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9684 - val_loss: 8.9933\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9567 - val_loss: 5.8284\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0852 - val_loss: 6.1906\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0807 - val_loss: 6.1853\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7467 - val_loss: 5.6129\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7045 - val_loss: 8.0730\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.9605 - val_loss: 9.3730\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8685 - val_loss: 7.6229\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0084 - val_loss: 5.5561\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.7271 - val_loss: 5.2691\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4544 - val_loss: 4.9349\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 4.802724775497862\n",
      "Mean Absolute Error (MAE): 1.502755098728519\n",
      "Root Mean Squared Error (RMSE): 2.1915119838818726\n",
      "Time taken: 595.7863447666168\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1172.3127 - val_loss: 962.4813\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 920.0944 - val_loss: 938.3702\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 876.7711 - val_loss: 879.0511\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 719.7819 - val_loss: 606.1260\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 395.0284 - val_loss: 260.5004\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 184.4657 - val_loss: 125.9704\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 90.6876 - val_loss: 65.5904\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 51.2071 - val_loss: 36.3804\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 33.9867 - val_loss: 42.5640\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 27.0007 - val_loss: 32.0301\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 20.0497 - val_loss: 16.9802\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 17.7681 - val_loss: 23.4061\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.4399 - val_loss: 16.6312\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 14.7723 - val_loss: 13.9054\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 12.8543 - val_loss: 18.4670\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.7517 - val_loss: 15.0868\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.3137 - val_loss: 14.8638\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 10.1763 - val_loss: 12.4862\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 9.4444 - val_loss: 13.4712\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.5269 - val_loss: 16.5181\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 9.1130 - val_loss: 22.1462\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 8.7930 - val_loss: 12.0727\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 8.1713 - val_loss: 10.4405\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 7.9238 - val_loss: 10.4013\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 8.5000 - val_loss: 10.5086\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 8.6910 - val_loss: 7.9083\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.0594 - val_loss: 7.3815\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.9398 - val_loss: 13.8143\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 6.5875 - val_loss: 8.0495\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 7.6564 - val_loss: 13.3943\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.6417 - val_loss: 7.9491\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.9811 - val_loss: 9.2844\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.9695 - val_loss: 23.2739\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 6.4923 - val_loss: 10.0327\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.4036 - val_loss: 6.3812\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.4235 - val_loss: 9.0066\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.5042 - val_loss: 9.0257\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.6034 - val_loss: 8.0995\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.8342 - val_loss: 9.8986\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.2430 - val_loss: 7.3638\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.6626 - val_loss: 7.1593\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.5916 - val_loss: 6.5779\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.4597 - val_loss: 8.9506\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.0780 - val_loss: 7.1741\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.3715 - val_loss: 9.2057\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 5.1836 - val_loss: 11.9699\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.7366 - val_loss: 6.0888\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.4538 - val_loss: 6.9049\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0536 - val_loss: 8.8935\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7133 - val_loss: 8.1528\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.5910 - val_loss: 19.0041\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.1593 - val_loss: 7.5818\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8237 - val_loss: 6.9143\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.3886 - val_loss: 12.0224\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.5587 - val_loss: 5.6542\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5298 - val_loss: 9.8564\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6692 - val_loss: 5.7546\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.3540 - val_loss: 5.8281\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3025 - val_loss: 6.0526\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5257 - val_loss: 6.4418\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3293 - val_loss: 8.1361\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.4023 - val_loss: 6.5162\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3512 - val_loss: 6.2577\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.9107 - val_loss: 11.2675\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 3.1106 - val_loss: 5.1369\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5627 - val_loss: 6.2226\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8112 - val_loss: 5.5006\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8716 - val_loss: 7.4919\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9040 - val_loss: 5.8229\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.2336 - val_loss: 10.2503\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7967 - val_loss: 5.3368\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8897 - val_loss: 6.2512\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5965 - val_loss: 6.0065\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.5421 - val_loss: 7.8095\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.7581 - val_loss: 5.4285\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2112 - val_loss: 7.7485\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5068 - val_loss: 7.0324\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6064 - val_loss: 6.2716\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.4386 - val_loss: 5.6036\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 20ms/step - loss: 2.4465 - val_loss: 7.4210\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.4214 - val_loss: 5.7676\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.1068 - val_loss: 9.9291\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5357 - val_loss: 6.7587\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.4609 - val_loss: 6.0621\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.2401 - val_loss: 5.0279\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.0125 - val_loss: 6.9824\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.0071 - val_loss: 6.3135\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.0920 - val_loss: 5.9172\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1866 - val_loss: 5.6657\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.9320 - val_loss: 6.5066\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 2.3402 - val_loss: 10.4634\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 3.7430 - val_loss: 8.4523\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 2.0481 - val_loss: 5.5561\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.9157 - val_loss: 5.0397\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.8094 - val_loss: 5.2340\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.6513 - val_loss: 5.0251\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.6411 - val_loss: 5.8736\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9222 - val_loss: 5.3301\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.3149 - val_loss: 14.3438\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4795 - val_loss: 5.6386\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7564 - val_loss: 5.5725\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5880 - val_loss: 5.1624\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.6432 - val_loss: 5.6940\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5572 - val_loss: 7.1775\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.0033 - val_loss: 13.4002\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.8069 - val_loss: 6.7311\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7374 - val_loss: 5.5496\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.8182 - val_loss: 8.0509\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.2711 - val_loss: 5.8736\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.5651 - val_loss: 5.4695\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.5478 - val_loss: 6.2475\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.5268 - val_loss: 6.3128\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.3128 - val_loss: 5.6788\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.4529 - val_loss: 8.0369\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.5752 - val_loss: 5.8914\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4324 - val_loss: 5.8520\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3290 - val_loss: 5.5235\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3829 - val_loss: 6.1947\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2677 - val_loss: 7.0277\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4455 - val_loss: 5.6085\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5511 - val_loss: 22.5220\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4466 - val_loss: 5.0781\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1791 - val_loss: 5.8690\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1511 - val_loss: 6.4482\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1263 - val_loss: 5.5173\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.1386 - val_loss: 5.3955\n",
      "16/16 [==============================] - 1s 26ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 5.025122264074752\n",
      "Mean Absolute Error (MAE): 1.5522581926076118\n",
      "Root Mean Squared Error (RMSE): 2.2416784479658878\n",
      "Time taken: 739.9442267417908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 48, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 48, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_10572\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  4.727010  1.431996  2.174169  727.141467\n",
      "1        2  5.151305  1.566457  2.269649  567.717148\n",
      "2        3  4.134460  1.389254  2.033337  733.312234\n",
      "3        4  4.802725  1.502755  2.191512  595.786345\n",
      "4        5  5.025122  1.552258  2.241678  739.944227\n",
      "5  Average  4.768125  1.488544  2.182069  672.780284\n",
      "Results saved to 'Sensors 48_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 48_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 48_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
