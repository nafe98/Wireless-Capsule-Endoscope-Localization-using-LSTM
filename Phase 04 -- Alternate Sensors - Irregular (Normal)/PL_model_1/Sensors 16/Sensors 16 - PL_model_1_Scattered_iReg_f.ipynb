{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>67.947886</td>\n",
       "      <td>99.572673</td>\n",
       "      <td>110.860036</td>\n",
       "      <td>127.054607</td>\n",
       "      <td>107.051532</td>\n",
       "      <td>124.324976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>67.812086</td>\n",
       "      <td>102.363262</td>\n",
       "      <td>119.562646</td>\n",
       "      <td>126.677060</td>\n",
       "      <td>113.850953</td>\n",
       "      <td>117.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>69.814048</td>\n",
       "      <td>92.404807</td>\n",
       "      <td>108.156692</td>\n",
       "      <td>128.072802</td>\n",
       "      <td>101.090832</td>\n",
       "      <td>113.383786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>68.907057</td>\n",
       "      <td>96.459290</td>\n",
       "      <td>110.743724</td>\n",
       "      <td>125.010229</td>\n",
       "      <td>109.590169</td>\n",
       "      <td>126.267858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>73.751383</td>\n",
       "      <td>94.954049</td>\n",
       "      <td>119.152370</td>\n",
       "      <td>129.047302</td>\n",
       "      <td>101.223119</td>\n",
       "      <td>118.873622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>82.457246</td>\n",
       "      <td>54.138747</td>\n",
       "      <td>129.982822</td>\n",
       "      <td>126.579534</td>\n",
       "      <td>119.940259</td>\n",
       "      <td>103.272859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>78.918123</td>\n",
       "      <td>57.107506</td>\n",
       "      <td>139.217056</td>\n",
       "      <td>127.217398</td>\n",
       "      <td>117.365194</td>\n",
       "      <td>114.416527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>83.447222</td>\n",
       "      <td>75.444186</td>\n",
       "      <td>139.410454</td>\n",
       "      <td>129.015596</td>\n",
       "      <td>123.582865</td>\n",
       "      <td>113.049668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>81.220414</td>\n",
       "      <td>70.247052</td>\n",
       "      <td>132.356387</td>\n",
       "      <td>137.478002</td>\n",
       "      <td>124.604990</td>\n",
       "      <td>113.155807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>85.647172</td>\n",
       "      <td>57.589165</td>\n",
       "      <td>129.954877</td>\n",
       "      <td>127.169556</td>\n",
       "      <td>116.554557</td>\n",
       "      <td>109.144032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10   sensor11    sensor12  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  67.947886   99.572673   \n",
       "1     106.645699  137.372609   92.314999  112.314087  67.812086  102.363262   \n",
       "2     103.789337  135.667714   99.182335  106.232463  69.814048   92.404807   \n",
       "3     102.460744  129.928887   86.763744  106.168512  68.907057   96.459290   \n",
       "4     116.786233  139.061346   83.559242  103.091764  73.751383   94.954049   \n",
       "...          ...         ...         ...         ...        ...         ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  82.457246   54.138747   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  78.918123   57.107506   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  83.447222   75.444186   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  81.220414   70.247052   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  85.647172   57.589165   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     110.860036  127.054607  107.051532  124.324976  \n",
       "1     119.562646  126.677060  113.850953  117.801200  \n",
       "2     108.156692  128.072802  101.090832  113.383786  \n",
       "3     110.743724  125.010229  109.590169  126.267858  \n",
       "4     119.152370  129.047302  101.223119  118.873622  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  129.982822  126.579534  119.940259  103.272859  \n",
       "2439  139.217056  127.217398  117.365194  114.416527  \n",
       "2440  139.410454  129.015596  123.582865  113.049668  \n",
       "2441  132.356387  137.478002  124.604990  113.155807  \n",
       "2442  129.954877  127.169556  116.554557  109.144032  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1077.2994 - val_loss: 781.5928\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 620.7233 - val_loss: 488.6906\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 408.6073 - val_loss: 344.2351\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 309.0531 - val_loss: 266.5250\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 237.7203 - val_loss: 222.6339\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 141.9128 - val_loss: 94.8930\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 75.9642 - val_loss: 72.4700\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 48.6977 - val_loss: 57.6915\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 38.5655 - val_loss: 30.4934\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 30.6222 - val_loss: 29.4607\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 27.7455 - val_loss: 26.8239\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 26.1058 - val_loss: 24.0220\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.7644 - val_loss: 23.2622\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 21.3246 - val_loss: 28.8109\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 21.3297 - val_loss: 22.2798\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 22.0127 - val_loss: 25.6058\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.1738 - val_loss: 18.5844\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 19.4102 - val_loss: 24.2668\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.4612 - val_loss: 37.1979\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.1802 - val_loss: 26.2889\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.9294 - val_loss: 17.5140\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.7005 - val_loss: 15.1412\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.6823 - val_loss: 18.9838\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.6608 - val_loss: 17.6498\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.8681 - val_loss: 17.4209\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.6352 - val_loss: 17.2168\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1975 - val_loss: 21.1966\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.8277 - val_loss: 20.6153\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.0714 - val_loss: 24.3926\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.0130 - val_loss: 17.6836\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.5288 - val_loss: 19.6878\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.8398 - val_loss: 21.2986\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.2899 - val_loss: 19.1866\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 14.7991 - val_loss: 17.9566\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.5614 - val_loss: 18.8821\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.0216 - val_loss: 21.0453\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.2274 - val_loss: 23.5333\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.1545 - val_loss: 15.5766\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.4594 - val_loss: 18.7623\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.7968 - val_loss: 19.9500\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.0688 - val_loss: 18.2346\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.4253 - val_loss: 15.7080\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1843 - val_loss: 20.6376\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.3984 - val_loss: 27.2652\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.2834 - val_loss: 17.5802\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.1044 - val_loss: 18.4622\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.6817 - val_loss: 14.7534\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.4191 - val_loss: 17.5100\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.8383 - val_loss: 16.4314\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.8995 - val_loss: 20.2404\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.5440 - val_loss: 21.7722\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.2268 - val_loss: 14.9905\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.7826 - val_loss: 15.3574\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.9916 - val_loss: 17.1272\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.2549 - val_loss: 15.9807\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.9810 - val_loss: 14.5189\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.0176 - val_loss: 18.2183\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.5905 - val_loss: 14.9460\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4321 - val_loss: 24.0168\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.4612 - val_loss: 13.6452\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.0928 - val_loss: 28.0673\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5995 - val_loss: 15.9635\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.0540 - val_loss: 13.4814\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.1032 - val_loss: 17.5946\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.6517 - val_loss: 14.9868\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2031 - val_loss: 13.5935\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.0891 - val_loss: 16.7698\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.5788 - val_loss: 16.3826\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.1895 - val_loss: 16.6194\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5775 - val_loss: 16.7029\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.5763 - val_loss: 18.7535\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.4782 - val_loss: 15.8666\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.0937 - val_loss: 15.1729\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5678 - val_loss: 17.7340\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.7541 - val_loss: 14.5846\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.0769 - val_loss: 14.5818\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8240 - val_loss: 15.4579\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.9766 - val_loss: 15.3468\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 11ms/step - loss: 7.0152 - val_loss: 14.4410\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.1817 - val_loss: 15.9550\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.3549 - val_loss: 14.1121\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.1309 - val_loss: 14.9698\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.2143 - val_loss: 15.6032\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.3262 - val_loss: 17.3401\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9999 - val_loss: 25.0446\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4808 - val_loss: 13.7286\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.5756 - val_loss: 17.1778\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4443 - val_loss: 14.3105\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.3994 - val_loss: 15.8590\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7650 - val_loss: 14.7027\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.5441 - val_loss: 14.7913\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0153 - val_loss: 14.3900\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.0286 - val_loss: 15.4446\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 13.481473933587248\n",
      "Mean Absolute Error (MAE): 2.4772619593476617\n",
      "Root Mean Squared Error (RMSE): 3.671712670347075\n",
      "Time taken: 307.4469738006592\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 14ms/step - loss: 1022.3492 - val_loss: 766.9147\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 562.3210 - val_loss: 491.0758\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 379.0779 - val_loss: 339.1711\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 254.8053 - val_loss: 217.3997\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 142.6148 - val_loss: 101.0390\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 77.3008 - val_loss: 57.6199\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 49.9579 - val_loss: 50.3106\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 38.4574 - val_loss: 37.5065\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 32.4547 - val_loss: 39.0111\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 28.0998 - val_loss: 29.6184\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 24.8890 - val_loss: 31.1688\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 24.0572 - val_loss: 28.3232\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 22.2125 - val_loss: 32.9314\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 20.8552 - val_loss: 20.9280\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 19.8572 - val_loss: 20.6516\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 19.4366 - val_loss: 24.6385\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 18.5835 - val_loss: 24.4756\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.8528 - val_loss: 22.8716\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.4465 - val_loss: 20.4295\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.1698 - val_loss: 18.3100\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.6387 - val_loss: 21.8937\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.5091 - val_loss: 19.1851\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.4643 - val_loss: 26.8200\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.2149 - val_loss: 21.3586\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.5382 - val_loss: 21.6313\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1324 - val_loss: 29.3285\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.8976 - val_loss: 25.3837\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.6107 - val_loss: 20.2354\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.6935 - val_loss: 20.8718\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.7277 - val_loss: 18.8092\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.7358 - val_loss: 21.2507\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 16.1080 - val_loss: 18.7061\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.1469 - val_loss: 19.2432\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.6905 - val_loss: 22.1841\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.8399 - val_loss: 18.2640\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.0980 - val_loss: 20.0924\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.9828 - val_loss: 32.3234\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.8986 - val_loss: 21.5485\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.4921 - val_loss: 24.2426\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.0420 - val_loss: 18.0876\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.7569 - val_loss: 16.9767\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.3681 - val_loss: 22.2150\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.0756 - val_loss: 18.1856\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.9989 - val_loss: 17.6690\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.3213 - val_loss: 18.4815\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.9377 - val_loss: 19.3145\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.2300 - val_loss: 21.3107\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.4012 - val_loss: 15.9517\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.5257 - val_loss: 18.8321\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.0969 - val_loss: 28.5004\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.5324 - val_loss: 23.9492\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.9983 - val_loss: 17.8038\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.5464 - val_loss: 18.5598\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.6807 - val_loss: 16.5140\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1725 - val_loss: 19.0512\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.1596 - val_loss: 18.7842\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.3937 - val_loss: 18.7346\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.7137 - val_loss: 16.9897\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.1106 - val_loss: 19.5824\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.1955 - val_loss: 18.9441\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.9232 - val_loss: 23.3306\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.1998 - val_loss: 17.7127\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.4319 - val_loss: 21.6302\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.3440 - val_loss: 18.6409\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4887 - val_loss: 18.5747\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0141 - val_loss: 27.6578\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.0409 - val_loss: 18.6638\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.1171 - val_loss: 20.4772\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4549 - val_loss: 18.6390\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.2654 - val_loss: 18.3741\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8290 - val_loss: 18.9405\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2695 - val_loss: 19.0550\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.6256 - val_loss: 17.7308\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8039 - val_loss: 18.6238\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4892 - val_loss: 18.1640\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2897 - val_loss: 30.1462\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9401 - val_loss: 19.3237\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4275 - val_loss: 16.7756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 15.951883267454837\n",
      "Mean Absolute Error (MAE): 2.64713542558941\n",
      "Root Mean Squared Error (RMSE): 3.993980879705715\n",
      "Time taken: 259.558066368103\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1065.6005 - val_loss: 774.8115\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 599.8634 - val_loss: 487.7348\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 391.0527 - val_loss: 340.4172\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 243.0899 - val_loss: 175.3415\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 131.6417 - val_loss: 109.4238\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 73.2954 - val_loss: 66.4047\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 48.9425 - val_loss: 102.5399\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 38.4989 - val_loss: 37.9104\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 30.2056 - val_loss: 29.3902\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 25.7728 - val_loss: 29.9677\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 25.0553 - val_loss: 24.7560\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 23.3917 - val_loss: 37.3682\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.8917 - val_loss: 27.1860\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 21.3723 - val_loss: 35.9336\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.9208 - val_loss: 30.4352\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 20.1393 - val_loss: 26.9682\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 19.4688 - val_loss: 25.0122\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.8282 - val_loss: 24.6403\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.2259 - val_loss: 21.5845\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.6592 - val_loss: 19.9925\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.6436 - val_loss: 19.1331\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.4775 - val_loss: 30.7030\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.3441 - val_loss: 22.9849\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.8666 - val_loss: 21.0840\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.8904 - val_loss: 25.0824\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1190 - val_loss: 29.4288\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.8513 - val_loss: 19.4650\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.7922 - val_loss: 26.1041\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.1823 - val_loss: 17.9789\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.1847 - val_loss: 40.7372\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.1663 - val_loss: 19.8458\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.3030 - val_loss: 18.6657\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.2231 - val_loss: 17.8109\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.5937 - val_loss: 19.7298\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.9162 - val_loss: 21.1112\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.7267 - val_loss: 22.3951\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.0991 - val_loss: 16.6543\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.6064 - val_loss: 17.5145\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.9811 - val_loss: 18.6902\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.8757 - val_loss: 15.7277\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1513 - val_loss: 18.1961\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.8592 - val_loss: 22.1494\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.0630 - val_loss: 15.7736\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.1750 - val_loss: 19.1482\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.7218 - val_loss: 19.4161\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.9546 - val_loss: 19.3435\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.1794 - val_loss: 17.9849\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.1223 - val_loss: 16.5650\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.8044 - val_loss: 18.1733\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.7848 - val_loss: 17.1477\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.7703 - val_loss: 14.9337\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4188 - val_loss: 19.3488\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.3516 - val_loss: 17.7605\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.3233 - val_loss: 18.0059\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.0965 - val_loss: 18.7032\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.2076 - val_loss: 17.5093\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.2753 - val_loss: 17.1629\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.3307 - val_loss: 16.7875\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.2357 - val_loss: 16.5869\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.9971 - val_loss: 15.2880\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.9559 - val_loss: 15.3106\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4523 - val_loss: 16.3880\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2704 - val_loss: 16.4808\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.4554 - val_loss: 15.1231\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5915 - val_loss: 17.8494\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.0327 - val_loss: 14.6601\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.7238 - val_loss: 16.4730\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.9112 - val_loss: 28.5877\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.9819 - val_loss: 16.0416\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.0659 - val_loss: 15.0806\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.2785 - val_loss: 15.0391\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.0029 - val_loss: 16.1226\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.8254 - val_loss: 18.7331\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.2472 - val_loss: 20.1026\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.1728 - val_loss: 20.7366\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.9418 - val_loss: 17.7836\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 8.5743 - val_loss: 14.6672\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.6513 - val_loss: 16.6748\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 7.3930 - val_loss: 25.0050\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 6.8901 - val_loss: 16.1482\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.7571 - val_loss: 21.7379\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.5150 - val_loss: 14.1319\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.8692 - val_loss: 17.5290\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.8937 - val_loss: 19.2024\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7195 - val_loss: 19.6266\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.3728 - val_loss: 14.7894\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.0491 - val_loss: 15.3786\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.9199 - val_loss: 20.0798\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.7567 - val_loss: 15.2545\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.2157 - val_loss: 16.5321\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.3494 - val_loss: 15.3500\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.8570 - val_loss: 14.5518\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8306 - val_loss: 20.8627\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.2476 - val_loss: 16.5898\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.5885 - val_loss: 15.6996\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7263 - val_loss: 23.8423\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.1393 - val_loss: 21.0779\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.5340 - val_loss: 15.1005\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.3570 - val_loss: 20.1132\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.4728 - val_loss: 16.0192\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6479 - val_loss: 18.8810\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7942 - val_loss: 19.2883\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4011 - val_loss: 15.1472\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.9181 - val_loss: 15.1771\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1602 - val_loss: 20.1982\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.6862 - val_loss: 18.4583\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7596 - val_loss: 16.2441\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3354 - val_loss: 20.5541\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.9380 - val_loss: 15.9270\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.2869 - val_loss: 19.9623\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.6096 - val_loss: 19.1721\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3768 - val_loss: 16.5468\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 14.131677894373935\n",
      "Mean Absolute Error (MAE): 2.5072992621424395\n",
      "Root Mean Squared Error (RMSE): 3.759212403466175\n",
      "Time taken: 370.1051824092865\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1059.1829 - val_loss: 796.7979\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 602.4442 - val_loss: 517.5543\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 398.1804 - val_loss: 310.9720\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 253.6416 - val_loss: 195.4176\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 154.2337 - val_loss: 136.9166\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 88.7899 - val_loss: 60.1710\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 58.0568 - val_loss: 42.3882\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 42.7067 - val_loss: 40.2924\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 34.7811 - val_loss: 32.7162\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 29.7607 - val_loss: 33.8724\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 26.3824 - val_loss: 27.5654\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 25.3824 - val_loss: 26.0361\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 24.6076 - val_loss: 21.0172\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 23.4197 - val_loss: 31.9679\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 20.9702 - val_loss: 37.8855\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 21.0350 - val_loss: 36.2766\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 20.5283 - val_loss: 22.6147\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.2962 - val_loss: 26.2608\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.5940 - val_loss: 23.9605\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.2575 - val_loss: 18.1505\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.0282 - val_loss: 21.9277\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.6194 - val_loss: 20.2506\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.2224 - val_loss: 16.9736\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.3404 - val_loss: 21.8557\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.8608 - val_loss: 17.5745\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.2974 - val_loss: 18.0136\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.5909 - val_loss: 19.3182\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.9021 - val_loss: 18.7894\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.3507 - val_loss: 17.7507\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.7377 - val_loss: 18.1393\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.0186 - val_loss: 16.4027\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.0026 - val_loss: 24.7177\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.2902 - val_loss: 36.8896\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.5977 - val_loss: 18.5938\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.9408 - val_loss: 23.0648\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.7359 - val_loss: 18.0189\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 14.0319 - val_loss: 16.1282\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.1025 - val_loss: 23.5479\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.7383 - val_loss: 19.5033\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.5893 - val_loss: 14.7881\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.1985 - val_loss: 14.3721\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.4692 - val_loss: 16.2858\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.0524 - val_loss: 21.3704\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.5955 - val_loss: 19.3696\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1644 - val_loss: 20.4147\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.2927 - val_loss: 20.7447\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.2413 - val_loss: 16.3174\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.3383 - val_loss: 14.7424\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.3425 - val_loss: 16.4486\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.2705 - val_loss: 27.9103\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9853 - val_loss: 15.9600\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.9547 - val_loss: 18.1220\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.0339 - val_loss: 14.8846\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.6765 - val_loss: 18.0039\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.8472 - val_loss: 14.5312\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.2250 - val_loss: 15.8292\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.8824 - val_loss: 19.1747\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.6120 - val_loss: 14.0008\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.8764 - val_loss: 16.0872\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.1918 - val_loss: 15.6000\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.1231 - val_loss: 18.2263\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.8649 - val_loss: 14.2472\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.4912 - val_loss: 16.7743\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.5858 - val_loss: 16.7396\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.0348 - val_loss: 18.9049\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.9883 - val_loss: 15.9027\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.8068 - val_loss: 15.6623\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8907 - val_loss: 15.9910\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.5462 - val_loss: 17.3422\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5310 - val_loss: 17.5742\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.8379 - val_loss: 13.8026\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.2512 - val_loss: 16.0071\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8086 - val_loss: 16.5845\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.6166 - val_loss: 17.2500\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2662 - val_loss: 17.5798\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8781 - val_loss: 14.7778\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.4928 - val_loss: 29.2641\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0422 - val_loss: 15.8251\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 7.6718 - val_loss: 15.0057\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.8092 - val_loss: 14.1666\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.7363 - val_loss: 14.3843\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.5658 - val_loss: 18.1757\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.4148 - val_loss: 15.4106\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.3962 - val_loss: 19.8293\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.4998 - val_loss: 14.9629\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.0237 - val_loss: 15.7325\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.1458 - val_loss: 14.9908\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9180 - val_loss: 16.6603\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.0152 - val_loss: 13.7447\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.3762 - val_loss: 15.0890\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.5778 - val_loss: 15.2250\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.1351 - val_loss: 23.6170\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.3352 - val_loss: 17.9781\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.0272 - val_loss: 15.7893\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7385 - val_loss: 19.4876\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.3340 - val_loss: 15.3982\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7983 - val_loss: 16.7669\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.2357 - val_loss: 17.3387\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9801 - val_loss: 17.7837\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7314 - val_loss: 15.5383\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.6503 - val_loss: 15.6024\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.2159 - val_loss: 16.5319\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7693 - val_loss: 17.3819\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7977 - val_loss: 18.2632\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7101 - val_loss: 16.9826\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.9194 - val_loss: 15.8427\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.5345 - val_loss: 17.3617\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0656 - val_loss: 16.8453\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.0838 - val_loss: 21.4075\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.1814 - val_loss: 15.5237\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2152 - val_loss: 14.4197\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.2115 - val_loss: 16.0051\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4459 - val_loss: 18.8355\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8592 - val_loss: 18.6991\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.7958 - val_loss: 16.6916\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 5.6338 - val_loss: 17.7461\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8896 - val_loss: 14.8395\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3828 - val_loss: 17.7364\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.4694 - val_loss: 17.8219\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 13.74463914681212\n",
      "Mean Absolute Error (MAE): 2.4253920111815304\n",
      "Root Mean Squared Error (RMSE): 3.7073763157807598\n",
      "Time taken: 394.19659662246704\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1023.0352 - val_loss: 733.1990\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 586.0674 - val_loss: 491.2258\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 406.0730 - val_loss: 346.4091\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 285.1686 - val_loss: 234.9768\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 175.3257 - val_loss: 197.2083\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 97.2045 - val_loss: 81.0170\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 60.3142 - val_loss: 44.4623\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 43.8516 - val_loss: 42.9237\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 35.2426 - val_loss: 35.2468\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 30.4591 - val_loss: 33.2631\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 27.1347 - val_loss: 22.5242\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 24.7006 - val_loss: 27.7922\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 23.3993 - val_loss: 28.0582\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 22.9079 - val_loss: 21.6111\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 21.3231 - val_loss: 20.9771\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.8018 - val_loss: 20.5547\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.6893 - val_loss: 22.3480\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.8828 - val_loss: 23.6999\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.4054 - val_loss: 16.6365\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.2061 - val_loss: 23.4012\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.2212 - val_loss: 20.7284\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.8363 - val_loss: 17.9868\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.3976 - val_loss: 16.3150\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.3331 - val_loss: 17.0180\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 16.4103 - val_loss: 21.5482\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.1563 - val_loss: 19.9385\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.0644 - val_loss: 19.6220\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.1004 - val_loss: 16.1428\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.1918 - val_loss: 14.3198\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.4197 - val_loss: 15.4690\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 13.6230 - val_loss: 23.3184\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 14.4090 - val_loss: 14.8891\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.1239 - val_loss: 17.0103\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.3070 - val_loss: 15.8417\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.8959 - val_loss: 17.5362\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.3941 - val_loss: 16.2551\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.8793 - val_loss: 15.1318\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.2844 - val_loss: 15.6035\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.0731 - val_loss: 15.4360\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.9214 - val_loss: 13.5814\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.9654 - val_loss: 13.8561\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.7111 - val_loss: 16.9734\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.0138 - val_loss: 17.3648\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.2261 - val_loss: 20.6059\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.7794 - val_loss: 17.7807\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1812 - val_loss: 15.0427\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.4032 - val_loss: 16.5031\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.4205 - val_loss: 17.8561\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.3076 - val_loss: 14.5167\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.5645 - val_loss: 15.2563\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9033 - val_loss: 16.9847\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.0780 - val_loss: 18.9662\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4556 - val_loss: 23.0623\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.5047 - val_loss: 15.4945\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.1252 - val_loss: 17.0763\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5187 - val_loss: 23.3459\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.8460 - val_loss: 21.1242\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.4425 - val_loss: 20.6066\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4507 - val_loss: 14.3581\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.1875 - val_loss: 14.9110\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8782 - val_loss: 13.3708\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.3534 - val_loss: 19.6821\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.4988 - val_loss: 24.8509\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.9726 - val_loss: 20.2922\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5594 - val_loss: 15.1012\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.9061 - val_loss: 16.4935\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.0993 - val_loss: 19.1076\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8790 - val_loss: 21.7995\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.8243 - val_loss: 13.6158\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8851 - val_loss: 14.3388\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.9733 - val_loss: 15.1065\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.2264 - val_loss: 14.7282\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4300 - val_loss: 21.0536\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.1016 - val_loss: 18.8562\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.9595 - val_loss: 14.0330\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8157 - val_loss: 13.1371\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.6171 - val_loss: 13.5747\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4847 - val_loss: 13.3047\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9054 - val_loss: 21.5896\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.1126 - val_loss: 17.7640\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4565 - val_loss: 13.9407\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.0409 - val_loss: 13.2095\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.8321 - val_loss: 14.4430\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.9174 - val_loss: 16.6946\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.1422 - val_loss: 14.4497\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.3197 - val_loss: 16.0940\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.1387 - val_loss: 13.8607\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9259 - val_loss: 13.9609\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9830 - val_loss: 14.1157\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7575 - val_loss: 17.2118\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.6502 - val_loss: 14.3065\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.0296 - val_loss: 14.1951\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.6013 - val_loss: 12.8340\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.9948 - val_loss: 15.9397\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.8882 - val_loss: 13.6740\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.1759 - val_loss: 21.3934\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.3996 - val_loss: 14.8969\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.8060 - val_loss: 15.6474\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.5310 - val_loss: 13.8532\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8055 - val_loss: 15.9132\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.1753 - val_loss: 15.8315\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.2892 - val_loss: 14.0728\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.4305 - val_loss: 14.1417\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6765 - val_loss: 15.9545\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.0405 - val_loss: 14.4038\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.1144 - val_loss: 16.6101\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.6215 - val_loss: 21.1847\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.2393 - val_loss: 13.8771\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.5216 - val_loss: 14.0766\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.9180 - val_loss: 13.0661\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8489 - val_loss: 13.7470\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.0410 - val_loss: 21.0162\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 5.2681 - val_loss: 17.2002\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7897 - val_loss: 16.7968\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.7611 - val_loss: 16.9859\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3834 - val_loss: 18.7136\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.1315 - val_loss: 14.0414\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.4308 - val_loss: 15.6187\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.5153 - val_loss: 15.3944\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.9092 - val_loss: 16.7598\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.9768 - val_loss: 17.4370\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2933 - val_loss: 14.7531\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.9413 - val_loss: 26.5030\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 12.83392263125755\n",
      "Mean Absolute Error (MAE): 2.3243294533334304\n",
      "Root Mean Squared Error (RMSE): 3.582446458951976\n",
      "Time taken: 406.468811750412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_10184\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold        MSE       MAE      RMSE  Time taken\n",
      "0        1  13.481474  2.477262  3.671713  307.446974\n",
      "1        2  15.951883  2.647135  3.993981  259.558066\n",
      "2        3  14.131678  2.507299  3.759212  370.105182\n",
      "3        4  13.744639  2.425392  3.707376  394.196597\n",
      "4        5  12.833923  2.324329  3.582446  406.468812\n",
      "5  Average  14.028719  2.476284  3.742946  347.555126\n",
      "Results saved to 'Sensors 16_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 16_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 16_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
