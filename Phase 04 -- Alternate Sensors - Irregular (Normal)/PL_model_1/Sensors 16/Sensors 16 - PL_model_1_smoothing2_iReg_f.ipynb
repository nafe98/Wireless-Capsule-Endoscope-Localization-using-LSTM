{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>69.255084</td>\n",
       "      <td>97.482707</td>\n",
       "      <td>114.225516</td>\n",
       "      <td>129.327314</td>\n",
       "      <td>105.712779</td>\n",
       "      <td>121.945497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>69.394588</td>\n",
       "      <td>97.512139</td>\n",
       "      <td>114.149329</td>\n",
       "      <td>129.211959</td>\n",
       "      <td>105.436000</td>\n",
       "      <td>121.824863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>69.534232</td>\n",
       "      <td>97.543166</td>\n",
       "      <td>114.073031</td>\n",
       "      <td>129.094360</td>\n",
       "      <td>105.163506</td>\n",
       "      <td>121.700246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>69.674230</td>\n",
       "      <td>97.575632</td>\n",
       "      <td>113.996663</td>\n",
       "      <td>128.974681</td>\n",
       "      <td>104.895214</td>\n",
       "      <td>121.571460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>69.814823</td>\n",
       "      <td>97.609507</td>\n",
       "      <td>113.920433</td>\n",
       "      <td>128.853196</td>\n",
       "      <td>104.631075</td>\n",
       "      <td>121.438527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>82.209100</td>\n",
       "      <td>59.038592</td>\n",
       "      <td>133.164754</td>\n",
       "      <td>127.797332</td>\n",
       "      <td>119.305234</td>\n",
       "      <td>110.972701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>82.233775</td>\n",
       "      <td>59.061562</td>\n",
       "      <td>133.030608</td>\n",
       "      <td>127.851915</td>\n",
       "      <td>119.403156</td>\n",
       "      <td>111.115335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>82.256408</td>\n",
       "      <td>59.084139</td>\n",
       "      <td>132.896657</td>\n",
       "      <td>127.910104</td>\n",
       "      <td>119.503552</td>\n",
       "      <td>111.260795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>82.277304</td>\n",
       "      <td>59.106410</td>\n",
       "      <td>132.763045</td>\n",
       "      <td>127.971895</td>\n",
       "      <td>119.606420</td>\n",
       "      <td>111.408884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>82.296749</td>\n",
       "      <td>59.128432</td>\n",
       "      <td>132.629682</td>\n",
       "      <td>128.037204</td>\n",
       "      <td>119.711606</td>\n",
       "      <td>111.559314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10   sensor11   sensor12  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  69.255084  97.482707   \n",
       "1     105.788181  134.546280   90.431246  107.970290  69.394588  97.512139   \n",
       "2     105.613823  134.358052   90.503448  108.058270  69.534232  97.543166   \n",
       "3     105.437718  134.170555   90.574876  108.144722  69.674230  97.575632   \n",
       "4     105.260017  133.984101   90.645256  108.229792  69.814823  97.609507   \n",
       "...          ...         ...         ...         ...        ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  82.209100  59.038592   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  82.233775  59.061562   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  82.256408  59.084139   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  82.277304  59.106410   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  82.296749  59.128432   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     114.225516  129.327314  105.712779  121.945497  \n",
       "1     114.149329  129.211959  105.436000  121.824863  \n",
       "2     114.073031  129.094360  105.163506  121.700246  \n",
       "3     113.996663  128.974681  104.895214  121.571460  \n",
       "4     113.920433  128.853196  104.631075  121.438527  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  133.164754  127.797332  119.305234  110.972701  \n",
       "2439  133.030608  127.851915  119.403156  111.115335  \n",
       "2440  132.896657  127.910104  119.503552  111.260795  \n",
       "2441  132.763045  127.971895  119.606420  111.408884  \n",
       "2442  132.629682  128.037204  119.711606  111.559314  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1047.7314 - val_loss: 872.5685\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 573.6152 - val_loss: 459.0326\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 366.6440 - val_loss: 281.7728\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 218.7781 - val_loss: 174.1274\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 101.7977 - val_loss: 73.2067\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 45.7876 - val_loss: 25.1764\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.5882 - val_loss: 17.3962\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.2308 - val_loss: 16.5870\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.7008 - val_loss: 4.7202\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 4.8928 - val_loss: 5.6545\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2921 - val_loss: 6.5394\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 4.1976 - val_loss: 22.8197\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4108 - val_loss: 1.7496\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1413 - val_loss: 1.6183\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7205 - val_loss: 1.2499\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4002 - val_loss: 1.0949\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8602 - val_loss: 0.8871\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8481 - val_loss: 2.4342\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1885 - val_loss: 1.0690\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1976 - val_loss: 2.2679\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1536 - val_loss: 0.4703\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9224 - val_loss: 2.4691\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2093 - val_loss: 1.7766\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0841 - val_loss: 1.3142\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9824 - val_loss: 4.1170\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8742 - val_loss: 3.0374\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7796 - val_loss: 0.7999\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6665 - val_loss: 0.3852\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5070 - val_loss: 0.8655\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4815 - val_loss: 0.5740\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6445 - val_loss: 0.6869\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5792 - val_loss: 4.5905\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7510 - val_loss: 0.7668\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5909 - val_loss: 0.4408\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4522 - val_loss: 0.5108\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7940 - val_loss: 0.8380\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1036 - val_loss: 0.4640\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5256 - val_loss: 1.1343\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8411 - val_loss: 0.8360\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8763 - val_loss: 0.4004\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3937 - val_loss: 0.4718\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6013 - val_loss: 0.9677\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6184 - val_loss: 2.1648\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9843 - val_loss: 2.1276\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4245 - val_loss: 0.6114\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2483 - val_loss: 0.2477\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2169 - val_loss: 0.2093\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2376 - val_loss: 0.3767\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3413 - val_loss: 0.6437\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5409 - val_loss: 1.6967\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5604 - val_loss: 1.1359\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4691 - val_loss: 0.3622\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4273 - val_loss: 1.0377\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3757 - val_loss: 0.3442\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2904 - val_loss: 0.3972\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3840 - val_loss: 0.9444\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5852 - val_loss: 1.1425\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9259 - val_loss: 0.4493\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3282 - val_loss: 0.4800\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4309 - val_loss: 0.3536\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2405 - val_loss: 0.2033\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2148 - val_loss: 0.3728\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2940 - val_loss: 0.8127\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5608 - val_loss: 0.4247\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2561 - val_loss: 1.0156\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2828 - val_loss: 0.3507\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2992 - val_loss: 0.5035\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2854 - val_loss: 4.3386\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4004 - val_loss: 0.2418\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1791 - val_loss: 0.2103\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2418 - val_loss: 0.3141\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2391 - val_loss: 0.3042\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2052 - val_loss: 0.1857\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1938 - val_loss: 0.1842\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5384 - val_loss: 3.8364\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3252 - val_loss: 1.0781\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1724 - val_loss: 0.1164\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1356 - val_loss: 0.4636\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3483 - val_loss: 0.3253\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2185 - val_loss: 0.6005\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3750 - val_loss: 0.3424\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3949 - val_loss: 0.0933\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1611 - val_loss: 0.3586\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2397 - val_loss: 0.5007\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1740 - val_loss: 0.3368\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7391 - val_loss: 8.3752\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5699 - val_loss: 0.0725\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1058 - val_loss: 0.0857\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1079 - val_loss: 0.1429\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1125 - val_loss: 0.2153\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1547 - val_loss: 0.4906\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1606 - val_loss: 0.1364\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1348 - val_loss: 0.1329\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1803 - val_loss: 1.0062\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3249 - val_loss: 0.2082\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1483 - val_loss: 0.1650\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1720 - val_loss: 0.1030\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2325 - val_loss: 1.4501\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1868 - val_loss: 0.1897\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1986 - val_loss: 0.3033\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1955 - val_loss: 0.3063\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2249 - val_loss: 0.2252\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1625 - val_loss: 0.2583\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3049 - val_loss: 0.2256\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4298 - val_loss: 13.4721\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8201 - val_loss: 0.1853\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1082 - val_loss: 0.0547\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0815 - val_loss: 0.1434\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0899 - val_loss: 0.1585\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1257 - val_loss: 0.1153\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1115 - val_loss: 0.1734\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1868 - val_loss: 0.1393\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2304 - val_loss: 0.1968\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1219 - val_loss: 0.1273\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2640 - val_loss: 0.3279\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1811 - val_loss: 0.2137\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1117 - val_loss: 0.1418\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1881 - val_loss: 0.1848\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1587 - val_loss: 1.5431\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1352 - val_loss: 0.1034\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1477 - val_loss: 0.3603\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2081 - val_loss: 1.0540\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6703 - val_loss: 3.9184\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2243 - val_loss: 0.0662\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0591 - val_loss: 0.0675\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0757 - val_loss: 0.1580\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0715 - val_loss: 0.1736\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1632 - val_loss: 0.4136\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0978 - val_loss: 0.2969\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1464 - val_loss: 0.2537\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1599 - val_loss: 0.2981\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1275 - val_loss: 0.1315\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1305 - val_loss: 0.3405\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3277 - val_loss: 0.6232\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1213 - val_loss: 0.2645\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0821 - val_loss: 0.1139\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0550 - val_loss: 0.0631\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.05467620877084042\n",
      "Mean Absolute Error (MAE): 0.17827820885343626\n",
      "Root Mean Squared Error (RMSE): 0.23382944376369805\n",
      "Time taken: 420.6686415672302\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 1032.8589 - val_loss: 783.7045\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 561.0359 - val_loss: 460.1905\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 341.2632 - val_loss: 281.6808\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 199.8204 - val_loss: 160.8464\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 90.1223 - val_loss: 58.9899\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 41.8156 - val_loss: 40.5138\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.8394 - val_loss: 24.5415\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.2257 - val_loss: 10.0577\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4103 - val_loss: 7.7560\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.7708 - val_loss: 3.6065\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3774 - val_loss: 4.0777\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7592 - val_loss: 2.2543\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0226 - val_loss: 3.5406\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3891 - val_loss: 3.2885\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4434 - val_loss: 2.4568\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9162 - val_loss: 2.2618\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2601 - val_loss: 2.1143\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5230 - val_loss: 1.7199\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2163 - val_loss: 1.3119\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5155 - val_loss: 1.8480\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4394 - val_loss: 1.4003\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5658 - val_loss: 1.1441\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2065 - val_loss: 1.9795\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1397 - val_loss: 1.0391\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9635 - val_loss: 0.7375\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6644 - val_loss: 1.3169\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1764 - val_loss: 1.4762\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7098 - val_loss: 2.6368\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8559 - val_loss: 1.5997\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6991 - val_loss: 1.0496\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7468 - val_loss: 1.2062\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3516 - val_loss: 1.3371\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5289 - val_loss: 0.5830\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6092 - val_loss: 1.0167\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9069 - val_loss: 2.5281\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5406 - val_loss: 0.3320\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3229 - val_loss: 0.4170\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4746 - val_loss: 0.6024\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3505 - val_loss: 0.6072\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6924 - val_loss: 1.0851\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8576 - val_loss: 1.0525\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4881 - val_loss: 1.7987\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5955 - val_loss: 0.7192\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4271 - val_loss: 0.4132\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3857 - val_loss: 1.0958\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4621 - val_loss: 0.8462\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4219 - val_loss: 0.4963\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6079 - val_loss: 0.7199\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4380 - val_loss: 0.6196\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6193 - val_loss: 0.6327\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7973 - val_loss: 0.6589\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3453 - val_loss: 0.3351\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3140 - val_loss: 0.5429\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5487 - val_loss: 1.8117\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3967 - val_loss: 0.4326\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2541 - val_loss: 0.1972\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2552 - val_loss: 0.4534\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3931 - val_loss: 0.7936\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5856 - val_loss: 0.6890\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2748 - val_loss: 0.8065\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5105 - val_loss: 0.7106\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3505 - val_loss: 0.8817\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4069 - val_loss: 1.6892\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2737 - val_loss: 0.3491\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3676 - val_loss: 0.4140\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3237 - val_loss: 2.7622\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5435 - val_loss: 0.6872\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3954 - val_loss: 0.4300\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3005 - val_loss: 1.3487\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3849 - val_loss: 0.8062\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2642 - val_loss: 0.2516\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1965 - val_loss: 0.4590\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.1284 - val_loss: 1.7185\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3621 - val_loss: 0.5007\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1509 - val_loss: 0.1512\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1444 - val_loss: 0.1176\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1246 - val_loss: 0.0840\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1341 - val_loss: 0.1361\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1249 - val_loss: 0.4238\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2013 - val_loss: 0.2074\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1871 - val_loss: 0.5095\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2546 - val_loss: 0.3400\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1898 - val_loss: 0.3629\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6337 - val_loss: 0.1386\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1489 - val_loss: 0.2593\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1572 - val_loss: 0.5538\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3800 - val_loss: 0.3282\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2858 - val_loss: 0.6215\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2517 - val_loss: 0.3821\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1583 - val_loss: 0.2559\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1515 - val_loss: 0.3639\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1781 - val_loss: 0.3644\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2160 - val_loss: 1.4462\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9843 - val_loss: 0.3314\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1299 - val_loss: 0.0916\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0852 - val_loss: 0.1161\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1146 - val_loss: 0.3869\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1390 - val_loss: 0.3317\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2440 - val_loss: 0.2634\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0078 - val_loss: 0.3716\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1446 - val_loss: 0.0842\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0720 - val_loss: 0.0689\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1002 - val_loss: 0.0942\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1256 - val_loss: 0.1863\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1257 - val_loss: 0.2827\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1741 - val_loss: 0.2079\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1243 - val_loss: 0.1890\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2017 - val_loss: 0.2330\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2018 - val_loss: 0.2720\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2638 - val_loss: 0.2064\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1174 - val_loss: 0.2472\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1837 - val_loss: 0.2102\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1508 - val_loss: 0.1803\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1271 - val_loss: 0.2405\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3241 - val_loss: 0.4548\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1975 - val_loss: 0.4249\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2272 - val_loss: 0.6247\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1336 - val_loss: 0.1724\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1649 - val_loss: 0.2997\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1863 - val_loss: 0.2261\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2405 - val_loss: 0.3762\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1182 - val_loss: 0.1954\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2868 - val_loss: 0.4987\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3110 - val_loss: 0.0622\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1490 - val_loss: 0.3419\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1165 - val_loss: 0.2568\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1298 - val_loss: 0.1592\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1475 - val_loss: 0.1788\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1175 - val_loss: 0.3024\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2153 - val_loss: 0.3566\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1150 - val_loss: 0.1606\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2056 - val_loss: 0.9569\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1257 - val_loss: 0.1002\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1338 - val_loss: 0.1998\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1133 - val_loss: 0.2216\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1638 - val_loss: 0.1614\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9226 - val_loss: 0.2006\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0550 - val_loss: 0.0335\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0594 - val_loss: 0.0883\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0522 - val_loss: 0.1226\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0710 - val_loss: 0.3920\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0827 - val_loss: 0.1799\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0934 - val_loss: 0.3942\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1093 - val_loss: 0.1746\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2805 - val_loss: 0.3837\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0934 - val_loss: 0.3366\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0662 - val_loss: 0.3752\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1378 - val_loss: 0.3027\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2142 - val_loss: 0.3838\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1426 - val_loss: 0.4615\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1139 - val_loss: 0.1313\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0989 - val_loss: 0.1987\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1570 - val_loss: 0.4365\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1208 - val_loss: 0.2762\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1006 - val_loss: 0.1334\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1808 - val_loss: 0.4003\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2954 - val_loss: 0.4165\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1185 - val_loss: 0.0907\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0746 - val_loss: 0.2397\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1324 - val_loss: 0.1263\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0864 - val_loss: 0.3018\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0843 - val_loss: 0.2018\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2418 - val_loss: 0.1929\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1063 - val_loss: 0.1032\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0706 - val_loss: 0.2572\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0860 - val_loss: 0.2480\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1929 - val_loss: 0.8082\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1106 - val_loss: 0.3429\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.03347124438950987\n",
      "Mean Absolute Error (MAE): 0.13626024814715385\n",
      "Root Mean Squared Error (RMSE): 0.1829514809710757\n",
      "Time taken: 514.6424505710602\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 1080.0801 - val_loss: 764.4579\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 623.8598 - val_loss: 500.0196\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 421.8900 - val_loss: 321.6008\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 244.2151 - val_loss: 149.9245\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 112.1738 - val_loss: 71.7988\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 48.0592 - val_loss: 29.6208\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 21.6143 - val_loss: 12.7932\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.2546 - val_loss: 9.6435\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.2378 - val_loss: 8.5446\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.3742 - val_loss: 2.6041\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5225 - val_loss: 5.3463\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8863 - val_loss: 3.5625\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6483 - val_loss: 2.1664\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.3832 - val_loss: 1.0249\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1027 - val_loss: 2.6526\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5363 - val_loss: 1.2365\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8800 - val_loss: 1.3261\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1189 - val_loss: 0.6642\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2399 - val_loss: 1.7413\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3242 - val_loss: 1.1964\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2029 - val_loss: 0.4799\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2314 - val_loss: 0.6295\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9983 - val_loss: 0.9273\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0055 - val_loss: 1.1249\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7864 - val_loss: 0.8570\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8785 - val_loss: 0.3898\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8580 - val_loss: 0.6185\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7957 - val_loss: 46.5164\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7429 - val_loss: 0.8000\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6676 - val_loss: 0.3246\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5120 - val_loss: 0.4063\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3514 - val_loss: 0.4021\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7714 - val_loss: 0.7377\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5588 - val_loss: 0.6237\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6802 - val_loss: 0.9892\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5814 - val_loss: 0.6105\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4361 - val_loss: 0.5466\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9907 - val_loss: 1.7405\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4949 - val_loss: 0.6996\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5446 - val_loss: 1.7130\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6690 - val_loss: 0.2907\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6348 - val_loss: 3.4860\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5901 - val_loss: 0.7304\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4458 - val_loss: 0.2779\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3942 - val_loss: 1.1079\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6860 - val_loss: 0.3042\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7250 - val_loss: 1.2635\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4839 - val_loss: 0.6676\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3928 - val_loss: 0.8474\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7984 - val_loss: 0.3075\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7453 - val_loss: 0.8162\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2944 - val_loss: 0.9673\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4436 - val_loss: 0.3190\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2618 - val_loss: 0.8209\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4013 - val_loss: 1.1485\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6769 - val_loss: 0.2769\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2853 - val_loss: 1.4139\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6002 - val_loss: 30.5261\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7344 - val_loss: 0.3211\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2307 - val_loss: 0.3261\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2577 - val_loss: 0.4227\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2401 - val_loss: 0.3441\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3060 - val_loss: 0.5550\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2925 - val_loss: 0.3695\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3666 - val_loss: 0.4068\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2469 - val_loss: 0.5271\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6018 - val_loss: 1.5385\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2605 - val_loss: 0.4557\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3945 - val_loss: 0.3754\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2563 - val_loss: 0.1528\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3172 - val_loss: 0.6911\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4391 - val_loss: 0.1807\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2668 - val_loss: 0.8388\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3006 - val_loss: 0.1914\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3075 - val_loss: 0.5165\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2471 - val_loss: 0.8806\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3577 - val_loss: 0.8731\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2842 - val_loss: 1.4142\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4732 - val_loss: 0.3161\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2071 - val_loss: 0.2888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2369 - val_loss: 0.6212\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2525 - val_loss: 0.2154\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3736 - val_loss: 0.5101\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1804 - val_loss: 0.2039\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3452 - val_loss: 0.5495\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1498 - val_loss: 0.1957\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3507 - val_loss: 0.3629\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1945 - val_loss: 0.3160\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3608 - val_loss: 0.2153\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3435 - val_loss: 0.5654\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1899 - val_loss: 0.5811\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2736 - val_loss: 0.1186\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1777 - val_loss: 0.7470\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2113 - val_loss: 0.1709\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1706 - val_loss: 0.2039\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1904 - val_loss: 0.2609\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2894 - val_loss: 0.6102\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2230 - val_loss: 0.2665\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1830 - val_loss: 0.2585\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2822 - val_loss: 0.5202\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1981 - val_loss: 0.4168\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1184 - val_loss: 0.1714\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2702 - val_loss: 0.4950\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2804 - val_loss: 0.1324\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1708 - val_loss: 0.2458\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1664 - val_loss: 0.3605\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1577 - val_loss: 0.3033\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6513 - val_loss: 1.9290\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2040 - val_loss: 0.2464\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1124 - val_loss: 0.1497\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1356 - val_loss: 0.2026\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1131 - val_loss: 0.1225\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1872 - val_loss: 0.1470\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1288 - val_loss: 0.2172\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1534 - val_loss: 0.1148\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2163 - val_loss: 0.6687\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1369 - val_loss: 0.4102\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2544 - val_loss: 0.4110\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2066 - val_loss: 0.3322\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1124 - val_loss: 0.0762\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1428 - val_loss: 0.5077\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1685 - val_loss: 0.3499\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1178 - val_loss: 0.2198\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1604 - val_loss: 0.1693\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1478 - val_loss: 0.1985\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1794 - val_loss: 0.0939\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1726 - val_loss: 0.1717\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1580 - val_loss: 0.3378\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1935 - val_loss: 0.3101\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1302 - val_loss: 0.2643\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1261 - val_loss: 0.2583\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1823 - val_loss: 0.4130\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1195 - val_loss: 0.1621\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1302 - val_loss: 1.0954\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1894 - val_loss: 0.3351\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1234 - val_loss: 0.3273\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1527 - val_loss: 0.4365\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1046 - val_loss: 0.3956\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1524 - val_loss: 0.1135\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1851 - val_loss: 0.3702\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1391 - val_loss: 0.3298\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2194 - val_loss: 0.0737\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0801 - val_loss: 0.1183\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1318 - val_loss: 0.4519\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1259 - val_loss: 0.6631\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1871 - val_loss: 0.2154\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1501 - val_loss: 0.1233\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0970 - val_loss: 0.0688\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1345 - val_loss: 0.1383\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1807 - val_loss: 0.3628\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1029 - val_loss: 0.1356\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0934 - val_loss: 0.0743\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1020 - val_loss: 0.3929\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1453 - val_loss: 0.1488\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1241 - val_loss: 0.2903\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1237 - val_loss: 0.3289\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1007 - val_loss: 0.1691\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1271 - val_loss: 0.1035\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0672 - val_loss: 0.0789\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1535 - val_loss: 0.3854\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1042 - val_loss: 0.0827\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0755 - val_loss: 0.1640\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1539 - val_loss: 0.5498\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1151 - val_loss: 0.0736\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0724 - val_loss: 0.1813\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1767 - val_loss: 0.3601\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0714 - val_loss: 0.1227\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0965 - val_loss: 0.6019\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1383 - val_loss: 0.2255\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1706 - val_loss: 0.2552\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0723 - val_loss: 0.1918\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0672 - val_loss: 0.3721\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1515 - val_loss: 0.2077\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1031 - val_loss: 0.2081\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0955 - val_loss: 0.1975\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0840 - val_loss: 0.1498\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1214 - val_loss: 0.8951\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0887 - val_loss: 0.0956\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.06875282251272279\n",
      "Mean Absolute Error (MAE): 0.19456819705839515\n",
      "Root Mean Squared Error (RMSE): 0.26220759430787427\n",
      "Time taken: 543.3027882575989\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1054.3660 - val_loss: 779.0324\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 635.3925 - val_loss: 527.9147\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 408.8644 - val_loss: 352.1974\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 230.9455 - val_loss: 168.8945\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 95.1360 - val_loss: 65.5313\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 37.7353 - val_loss: 28.7835\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.9965 - val_loss: 12.7551\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.4476 - val_loss: 11.0929\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4655 - val_loss: 6.3857\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2700 - val_loss: 2.9689\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0920 - val_loss: 4.3819\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4938 - val_loss: 2.6019\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9689 - val_loss: 3.7406\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7719 - val_loss: 3.6264\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2501 - val_loss: 1.7162\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3720 - val_loss: 0.9605\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0806 - val_loss: 1.1577\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3016 - val_loss: 1.6782\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7524 - val_loss: 1.1533\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9867 - val_loss: 1.3144\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4115 - val_loss: 4.4422\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7474 - val_loss: 0.7091\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9487 - val_loss: 2.9186\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9578 - val_loss: 1.2183\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7492 - val_loss: 0.6671\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6367 - val_loss: 1.6541\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9883 - val_loss: 3.3008\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8222 - val_loss: 1.7335\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4504 - val_loss: 0.4561\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8001 - val_loss: 1.4418\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4575 - val_loss: 0.3380\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5510 - val_loss: 0.9916\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8059 - val_loss: 2.9729\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8625 - val_loss: 0.6730\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6201 - val_loss: 0.9503\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0083 - val_loss: 0.5069\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4704 - val_loss: 0.3880\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3137 - val_loss: 0.2940\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3138 - val_loss: 0.4545\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4242 - val_loss: 1.4081\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5602 - val_loss: 0.9660\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5487 - val_loss: 2.0552\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5083 - val_loss: 1.6090\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3818 - val_loss: 3.4076\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5838 - val_loss: 0.7133\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7227 - val_loss: 1.2986\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3908 - val_loss: 0.7823\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4494 - val_loss: 3.8536\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9264 - val_loss: 0.2210\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2804 - val_loss: 0.9615\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3314 - val_loss: 0.7884\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4654 - val_loss: 1.0477\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4017 - val_loss: 0.5237\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4172 - val_loss: 0.2880\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3910 - val_loss: 0.7450\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3884 - val_loss: 0.2005\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3568 - val_loss: 0.4278\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4237 - val_loss: 0.8127\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3401 - val_loss: 0.2092\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4192 - val_loss: 0.6519\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4257 - val_loss: 0.6638\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3953 - val_loss: 1.4071\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7016 - val_loss: 0.3741\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2167 - val_loss: 1.5201\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2439 - val_loss: 0.3161\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2160 - val_loss: 0.4982\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3424 - val_loss: 0.5065\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3887 - val_loss: 0.8635\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2933 - val_loss: 0.7218\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3349 - val_loss: 0.2951\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2873 - val_loss: 0.1794\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2466 - val_loss: 0.3594\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2095 - val_loss: 1.7218\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3951 - val_loss: 1.9444\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2578 - val_loss: 1.3267\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2211 - val_loss: 0.2134\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1668 - val_loss: 0.2268\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2384 - val_loss: 6.5196\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1948 - val_loss: 0.1462\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1113 - val_loss: 0.1391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1793 - val_loss: 0.1602\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1746 - val_loss: 0.2369\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2316 - val_loss: 0.4719\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2518 - val_loss: 0.1647\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2110 - val_loss: 0.2070\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1854 - val_loss: 0.3423\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3804 - val_loss: 0.3203\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4121 - val_loss: 0.8607\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1371 - val_loss: 0.1517\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2374 - val_loss: 0.3806\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7757 - val_loss: 0.1648\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1160 - val_loss: 0.1161\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1090 - val_loss: 0.1014\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0739 - val_loss: 0.0893\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1179 - val_loss: 0.3220\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1735 - val_loss: 0.5690\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2198 - val_loss: 0.5616\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1892 - val_loss: 0.5197\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1884 - val_loss: 0.1481\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1404 - val_loss: 0.2461\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1711 - val_loss: 0.2939\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3047 - val_loss: 0.6229\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1852 - val_loss: 0.2317\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2253 - val_loss: 2.1304\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1531 - val_loss: 0.4372\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2013 - val_loss: 0.2488\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2096 - val_loss: 0.3508\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1468 - val_loss: 0.5324\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1813 - val_loss: 0.3518\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2955 - val_loss: 0.3595\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1774 - val_loss: 0.3691\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1558 - val_loss: 0.2831\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1665 - val_loss: 0.2788\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2217 - val_loss: 1.1250\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3800 - val_loss: 0.4037\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1149 - val_loss: 0.3970\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1459 - val_loss: 0.2031\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1129 - val_loss: 0.4541\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2334 - val_loss: 0.1602\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1191 - val_loss: 0.1592\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1356 - val_loss: 0.3315\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5063 - val_loss: 0.2167\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0982 - val_loss: 0.2630\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0749 - val_loss: 0.1338\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.08928466318720003\n",
      "Mean Absolute Error (MAE): 0.21025146710270004\n",
      "Root Mean Squared Error (RMSE): 0.2988053935042004\n",
      "Time taken: 389.35287737846375\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1051.3405 - val_loss: 765.6260\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 573.7104 - val_loss: 487.0877\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 341.0216 - val_loss: 253.9095\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 197.3703 - val_loss: 124.8310\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 97.2924 - val_loss: 61.3863\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 45.3076 - val_loss: 21.7319\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 20.6305 - val_loss: 19.0184\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.7410 - val_loss: 8.3674\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.1797 - val_loss: 6.3654\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.6121 - val_loss: 5.7634\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8172 - val_loss: 3.4493\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9412 - val_loss: 2.5602\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7480 - val_loss: 2.4595\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2080 - val_loss: 3.5941\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4470 - val_loss: 2.8108\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3055 - val_loss: 1.3066\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5926 - val_loss: 1.3107\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8885 - val_loss: 1.2015\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4474 - val_loss: 1.7761\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5071 - val_loss: 1.2159\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1383 - val_loss: 1.5008\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.8253 - val_loss: 0.9761\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1600 - val_loss: 2.6524\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1489 - val_loss: 0.4585\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5765 - val_loss: 1.5592\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2635 - val_loss: 1.7590\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7631 - val_loss: 1.6376\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6469 - val_loss: 0.4425\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5673 - val_loss: 0.7128\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1131 - val_loss: 0.6564\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8958 - val_loss: 1.0387\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8911 - val_loss: 1.6482\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1948 - val_loss: 0.8153\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8333 - val_loss: 2.1867\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6294 - val_loss: 0.6859\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5985 - val_loss: 0.6232\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0199 - val_loss: 0.5370\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7821 - val_loss: 1.7982\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1253 - val_loss: 0.5343\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4270 - val_loss: 0.6299\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3926 - val_loss: 0.4447\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3078 - val_loss: 0.2136\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5551 - val_loss: 0.4320\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5306 - val_loss: 0.3440\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5681 - val_loss: 0.8771\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4705 - val_loss: 0.6546\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9375 - val_loss: 0.7558\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3540 - val_loss: 0.5453\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3805 - val_loss: 0.8479\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4753 - val_loss: 2.1236\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5808 - val_loss: 0.6397\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7224 - val_loss: 0.5320\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4103 - val_loss: 0.2698\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2621 - val_loss: 0.2921\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3949 - val_loss: 0.4630\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2650 - val_loss: 0.4852\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3325 - val_loss: 0.5032\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3827 - val_loss: 0.4144\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3593 - val_loss: 0.3689\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3763 - val_loss: 0.5220\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6142 - val_loss: 0.6509\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4021 - val_loss: 0.2451\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4368 - val_loss: 1.3594\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5416 - val_loss: 1.0929\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3668 - val_loss: 0.7429\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2533 - val_loss: 0.1325\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3071 - val_loss: 0.2909\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3828 - val_loss: 0.7032\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4721 - val_loss: 1.9583\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3782 - val_loss: 0.2938\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2620 - val_loss: 0.7034\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3356 - val_loss: 0.5791\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2115 - val_loss: 0.3365\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2938 - val_loss: 0.8345\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8628 - val_loss: 0.3874\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2394 - val_loss: 0.1938\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1472 - val_loss: 0.6683\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2750 - val_loss: 0.5237\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3291 - val_loss: 0.3580\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2277 - val_loss: 0.2503\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4029 - val_loss: 0.2187\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2543 - val_loss: 0.1330\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2069 - val_loss: 0.3224\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2748 - val_loss: 0.5569\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3230 - val_loss: 0.3758\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3148 - val_loss: 0.1576\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2581 - val_loss: 0.2162\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1849 - val_loss: 0.2004\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2386 - val_loss: 0.3647\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1940 - val_loss: 0.3186\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6862 - val_loss: 0.5988\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1729 - val_loss: 0.1289\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1899 - val_loss: 0.6783\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2263 - val_loss: 0.2374\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1828 - val_loss: 0.2623\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2997 - val_loss: 0.4807\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2133 - val_loss: 0.6310\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4247 - val_loss: 0.2656\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1810 - val_loss: 0.3624\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2418 - val_loss: 0.1521\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1634 - val_loss: 0.1694\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1046 - val_loss: 0.2690\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4104 - val_loss: 0.9067\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1687 - val_loss: 0.1842\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1551 - val_loss: 0.0897\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1907 - val_loss: 0.1862\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1403 - val_loss: 0.1371\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2600 - val_loss: 1.4458\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2594 - val_loss: 0.7442\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1633 - val_loss: 0.3846\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2295 - val_loss: 0.3651\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1639 - val_loss: 0.3057\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1343 - val_loss: 0.0996\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1978 - val_loss: 0.5467\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2634 - val_loss: 0.2558\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1866 - val_loss: 0.1775\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1834 - val_loss: 0.4224\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1973 - val_loss: 0.3286\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2033 - val_loss: 0.9518\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.4156 - val_loss: 0.2190\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1382 - val_loss: 0.1527\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0968 - val_loss: 0.0821\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0842 - val_loss: 0.1263\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0752 - val_loss: 0.0738\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0635 - val_loss: 0.0533\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1059 - val_loss: 0.3404\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1492 - val_loss: 0.1833\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1009 - val_loss: 0.0753\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1184 - val_loss: 0.0876\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1848 - val_loss: 0.4903\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1723 - val_loss: 0.5633\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1849 - val_loss: 0.8469\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1440 - val_loss: 0.0909\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1236 - val_loss: 0.2449\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1970 - val_loss: 0.3708\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1355 - val_loss: 0.2637\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2241 - val_loss: 0.1787\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1553 - val_loss: 0.0871\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1994 - val_loss: 0.2330\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1038 - val_loss: 0.1181\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0976 - val_loss: 0.1432\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2184 - val_loss: 0.2656\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1776 - val_loss: 0.1289\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1836 - val_loss: 0.1361\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1302 - val_loss: 0.1446\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1488 - val_loss: 0.1477\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1326 - val_loss: 0.3645\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1383 - val_loss: 0.1300\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1469 - val_loss: 0.3093\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1707 - val_loss: 0.2256\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1143 - val_loss: 0.0845\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1261 - val_loss: 0.2870\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1074 - val_loss: 0.2332\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1342 - val_loss: 0.2355\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1807 - val_loss: 0.2891\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.05332054992699522\n",
      "Mean Absolute Error (MAE): 0.1685696097168071\n",
      "Root Mean Squared Error (RMSE): 0.23091242913060184\n",
      "Time taken: 470.8845229148865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_17872\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.054676  0.178278  0.233829  420.668642\n",
      "1        2  0.033471  0.136260  0.182951  514.642451\n",
      "2        3  0.068753  0.194568  0.262208  543.302788\n",
      "3        4  0.089285  0.210251  0.298805  389.352877\n",
      "4        5  0.053321  0.168570  0.230912  470.884523\n",
      "5  Average  0.059901  0.177586  0.241741  467.770256\n",
      "Results saved to 'Sensors 16_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 16_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 16_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
