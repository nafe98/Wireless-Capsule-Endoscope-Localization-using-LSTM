{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor23</th>\n",
       "      <th>sensor24</th>\n",
       "      <th>sensor25</th>\n",
       "      <th>sensor26</th>\n",
       "      <th>sensor27</th>\n",
       "      <th>sensor28</th>\n",
       "      <th>sensor29</th>\n",
       "      <th>sensor30</th>\n",
       "      <th>sensor31</th>\n",
       "      <th>sensor32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>86.571673</td>\n",
       "      <td>122.829726</td>\n",
       "      <td>58.707989</td>\n",
       "      <td>90.409519</td>\n",
       "      <td>85.072693</td>\n",
       "      <td>107.080571</td>\n",
       "      <td>81.030659</td>\n",
       "      <td>104.832583</td>\n",
       "      <td>98.486310</td>\n",
       "      <td>114.350441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>86.320894</td>\n",
       "      <td>122.586872</td>\n",
       "      <td>58.617841</td>\n",
       "      <td>90.384621</td>\n",
       "      <td>84.864751</td>\n",
       "      <td>106.781760</td>\n",
       "      <td>81.133544</td>\n",
       "      <td>104.816294</td>\n",
       "      <td>98.554454</td>\n",
       "      <td>114.300289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>86.068453</td>\n",
       "      <td>122.344712</td>\n",
       "      <td>58.532474</td>\n",
       "      <td>90.359670</td>\n",
       "      <td>84.653879</td>\n",
       "      <td>106.483520</td>\n",
       "      <td>81.236269</td>\n",
       "      <td>104.800820</td>\n",
       "      <td>98.619946</td>\n",
       "      <td>114.252414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>85.814484</td>\n",
       "      <td>122.103192</td>\n",
       "      <td>58.451909</td>\n",
       "      <td>90.334759</td>\n",
       "      <td>84.440183</td>\n",
       "      <td>106.186037</td>\n",
       "      <td>81.338915</td>\n",
       "      <td>104.786250</td>\n",
       "      <td>98.682352</td>\n",
       "      <td>114.206770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>85.559264</td>\n",
       "      <td>121.862268</td>\n",
       "      <td>58.375979</td>\n",
       "      <td>90.309994</td>\n",
       "      <td>84.223756</td>\n",
       "      <td>105.889654</td>\n",
       "      <td>81.441453</td>\n",
       "      <td>104.772779</td>\n",
       "      <td>98.741457</td>\n",
       "      <td>114.163396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>116.104149</td>\n",
       "      <td>93.524410</td>\n",
       "      <td>78.941929</td>\n",
       "      <td>49.400951</td>\n",
       "      <td>100.959254</td>\n",
       "      <td>87.581895</td>\n",
       "      <td>105.254566</td>\n",
       "      <td>93.993383</td>\n",
       "      <td>120.348427</td>\n",
       "      <td>110.226970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>116.135667</td>\n",
       "      <td>93.472193</td>\n",
       "      <td>79.132533</td>\n",
       "      <td>49.457799</td>\n",
       "      <td>100.963601</td>\n",
       "      <td>87.554310</td>\n",
       "      <td>105.141257</td>\n",
       "      <td>93.960316</td>\n",
       "      <td>120.207879</td>\n",
       "      <td>110.100697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>116.168216</td>\n",
       "      <td>93.418615</td>\n",
       "      <td>79.327325</td>\n",
       "      <td>49.513880</td>\n",
       "      <td>100.965453</td>\n",
       "      <td>87.525868</td>\n",
       "      <td>105.026491</td>\n",
       "      <td>93.929907</td>\n",
       "      <td>120.065890</td>\n",
       "      <td>109.972625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>116.202245</td>\n",
       "      <td>93.363889</td>\n",
       "      <td>79.526316</td>\n",
       "      <td>49.569455</td>\n",
       "      <td>100.964752</td>\n",
       "      <td>87.496464</td>\n",
       "      <td>104.910210</td>\n",
       "      <td>93.902218</td>\n",
       "      <td>119.922320</td>\n",
       "      <td>109.842729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>116.238161</td>\n",
       "      <td>93.308119</td>\n",
       "      <td>79.729482</td>\n",
       "      <td>49.624792</td>\n",
       "      <td>100.961411</td>\n",
       "      <td>87.465832</td>\n",
       "      <td>104.792444</td>\n",
       "      <td>93.877256</td>\n",
       "      <td>119.777157</td>\n",
       "      <td>109.711116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor23  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...   86.571673   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...   86.320894   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...   86.068453   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...   85.814484   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...   85.559264   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  116.104149   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  116.135667   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  116.168216   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  116.202245   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  116.238161   \n",
       "\n",
       "        sensor24   sensor25   sensor26    sensor27    sensor28    sensor29  \\\n",
       "0     122.829726  58.707989  90.409519   85.072693  107.080571   81.030659   \n",
       "1     122.586872  58.617841  90.384621   84.864751  106.781760   81.133544   \n",
       "2     122.344712  58.532474  90.359670   84.653879  106.483520   81.236269   \n",
       "3     122.103192  58.451909  90.334759   84.440183  106.186037   81.338915   \n",
       "4     121.862268  58.375979  90.309994   84.223756  105.889654   81.441453   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438   93.524410  78.941929  49.400951  100.959254   87.581895  105.254566   \n",
       "2439   93.472193  79.132533  49.457799  100.963601   87.554310  105.141257   \n",
       "2440   93.418615  79.327325  49.513880  100.965453   87.525868  105.026491   \n",
       "2441   93.363889  79.526316  49.569455  100.964752   87.496464  104.910210   \n",
       "2442   93.308119  79.729482  49.624792  100.961411   87.465832  104.792444   \n",
       "\n",
       "        sensor30    sensor31    sensor32  \n",
       "0     104.832583   98.486310  114.350441  \n",
       "1     104.816294   98.554454  114.300289  \n",
       "2     104.800820   98.619946  114.252414  \n",
       "3     104.786250   98.682352  114.206770  \n",
       "4     104.772779   98.741457  114.163396  \n",
       "...          ...         ...         ...  \n",
       "2438   93.993383  120.348427  110.226970  \n",
       "2439   93.960316  120.207879  110.100697  \n",
       "2440   93.929907  120.065890  109.972625  \n",
       "2441   93.902218  119.922320  109.842729  \n",
       "2442   93.877256  119.777157  109.711116  \n",
       "\n",
       "[2443 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:32]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1158.8438 - val_loss: 910.5547\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 838.0851 - val_loss: 679.5717\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 559.9839 - val_loss: 412.8215\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 327.2825 - val_loss: 319.7539\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 173.8485 - val_loss: 121.9818\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 75.7618 - val_loss: 46.0146\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 33.1003 - val_loss: 43.8786\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 16.6420 - val_loss: 13.7682\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.8539 - val_loss: 7.7108\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.5250 - val_loss: 8.7187\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 3.6396 - val_loss: 2.5594\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.3686 - val_loss: 1.8292\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.8481 - val_loss: 7.0331\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.1083 - val_loss: 2.2880\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7597 - val_loss: 2.3182\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.4672 - val_loss: 3.7214\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.4441 - val_loss: 3.2991\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5744 - val_loss: 0.8677\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.6861 - val_loss: 0.7828\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.1685 - val_loss: 0.5259\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7727 - val_loss: 0.6807\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9711 - val_loss: 2.2144\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.8719 - val_loss: 1.3803\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7246 - val_loss: 0.6971\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7223 - val_loss: 0.5175\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7341 - val_loss: 0.5274\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9666 - val_loss: 2.5415\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.3793 - val_loss: 1.1625\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7455 - val_loss: 0.4382\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5413 - val_loss: 0.3718\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4002 - val_loss: 0.2929\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7598 - val_loss: 0.2708\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3946 - val_loss: 0.3814\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3651 - val_loss: 0.8711\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5998 - val_loss: 0.7326\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7550 - val_loss: 0.7996\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5611 - val_loss: 0.4341\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6641 - val_loss: 1.1973\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6735 - val_loss: 0.9366\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5077 - val_loss: 0.6086\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5302 - val_loss: 2.4053\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5903 - val_loss: 0.4720\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3237 - val_loss: 1.1636\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5556 - val_loss: 0.1871\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4713 - val_loss: 0.9310\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7667 - val_loss: 0.7566\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4629 - val_loss: 0.4462\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5440 - val_loss: 0.6122\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5873 - val_loss: 0.4600\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3016 - val_loss: 0.1766\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3251 - val_loss: 1.0183\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5694 - val_loss: 0.1575\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3376 - val_loss: 0.4379\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3441 - val_loss: 0.2213\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2751 - val_loss: 0.1475\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3924 - val_loss: 0.9827\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5637 - val_loss: 2.1133\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2617 - val_loss: 0.7833\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7190 - val_loss: 0.6291\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1775 - val_loss: 0.6125\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2985 - val_loss: 0.4605\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4923 - val_loss: 0.4379\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2292 - val_loss: 0.2372\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2399 - val_loss: 0.3792\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8800 - val_loss: 0.4749\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3898 - val_loss: 0.2316\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1766 - val_loss: 0.6234\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3112 - val_loss: 0.2606\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1524 - val_loss: 0.5699\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2146 - val_loss: 0.3388\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2026 - val_loss: 0.5607\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3106 - val_loss: 0.2754\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3998 - val_loss: 0.4524\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3209 - val_loss: 0.4320\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3727 - val_loss: 0.7001\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1759 - val_loss: 0.5043\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2815 - val_loss: 0.7363\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4365 - val_loss: 1.1129\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3812 - val_loss: 0.4352\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1171 - val_loss: 0.3032\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1499 - val_loss: 0.2088\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2151 - val_loss: 0.2539\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2260 - val_loss: 0.3033\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2524 - val_loss: 0.2895\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4798 - val_loss: 0.2784\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.14745499167188705\n",
      "Mean Absolute Error (MAE): 0.3087591838748865\n",
      "Root Mean Squared Error (RMSE): 0.3839986870705251\n",
      "Time taken: 382.3363449573517\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 17ms/step - loss: 1107.8538 - val_loss: 971.7940\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 813.1824 - val_loss: 705.0305\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 495.5030 - val_loss: 382.1832\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 259.0113 - val_loss: 229.7849\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 112.7362 - val_loss: 88.3455\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 47.8212 - val_loss: 38.7407\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 21.8410 - val_loss: 19.1721\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 12.2437 - val_loss: 9.4436\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.1959 - val_loss: 8.1673\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 5.2995 - val_loss: 4.4111\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.6044 - val_loss: 3.3072\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.1515 - val_loss: 1.8768\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.5813 - val_loss: 2.9799\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.8449 - val_loss: 1.5540\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.4798 - val_loss: 4.8633\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.9661 - val_loss: 1.9675\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0791 - val_loss: 0.9890\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1798 - val_loss: 1.5365\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8696 - val_loss: 0.3723\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8875 - val_loss: 0.9520\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5830 - val_loss: 1.6189\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9769 - val_loss: 3.4482\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1034 - val_loss: 1.4218\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9730 - val_loss: 1.0641\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8093 - val_loss: 1.5063\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3122 - val_loss: 1.2914\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7034 - val_loss: 0.5576\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7237 - val_loss: 0.9274\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7103 - val_loss: 24.7462\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7067 - val_loss: 1.1557\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5088 - val_loss: 0.5148\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4160 - val_loss: 0.8397\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4877 - val_loss: 1.5915\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5438 - val_loss: 1.2110\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8516 - val_loss: 0.4745\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5017 - val_loss: 0.7708\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.4297 - val_loss: 0.8658\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4829 - val_loss: 0.6466\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3360 - val_loss: 0.7464\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5209 - val_loss: 0.8364\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3489 - val_loss: 1.2060\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4297 - val_loss: 0.6454\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5025 - val_loss: 0.7961\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4192 - val_loss: 0.4902\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7728 - val_loss: 2.0887\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4538 - val_loss: 0.2978\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3978 - val_loss: 1.3054\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7340 - val_loss: 0.7939\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7001 - val_loss: 1.0224\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3636 - val_loss: 0.1256\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3238 - val_loss: 0.5203\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2925 - val_loss: 1.3963\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7085 - val_loss: 0.5391\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3524 - val_loss: 0.5225\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3291 - val_loss: 0.5209\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4494 - val_loss: 0.4404\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2571 - val_loss: 0.3974\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3780 - val_loss: 1.6509\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9377 - val_loss: 4.0138\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3648 - val_loss: 0.4290\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1890 - val_loss: 0.4550\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2963 - val_loss: 0.4734\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4478 - val_loss: 1.4644\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3523 - val_loss: 0.5527\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2894 - val_loss: 0.5564\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2401 - val_loss: 0.2547\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3992 - val_loss: 1.1976\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3538 - val_loss: 0.4391\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2482 - val_loss: 0.2759\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2139 - val_loss: 0.2131\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1960 - val_loss: 0.4535\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5826 - val_loss: 0.3219\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1776 - val_loss: 0.0657\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2854 - val_loss: 0.7642\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3754 - val_loss: 0.4902\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2030 - val_loss: 0.6650\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3190 - val_loss: 0.9593\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2131 - val_loss: 0.2799\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2357 - val_loss: 1.2726\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2954 - val_loss: 0.3592\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1827 - val_loss: 0.3488\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2307 - val_loss: 0.1527\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2347 - val_loss: 0.6282\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2846 - val_loss: 0.3797\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2888 - val_loss: 0.2192\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1980 - val_loss: 0.0949\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1967 - val_loss: 1.0548\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2611 - val_loss: 0.5028\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2169 - val_loss: 0.1896\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2031 - val_loss: 0.6402\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3406 - val_loss: 6.8882\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3909 - val_loss: 0.4608\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1127 - val_loss: 0.1390\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1160 - val_loss: 0.0638\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1311 - val_loss: 0.0991\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1573 - val_loss: 0.2205\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1720 - val_loss: 0.0751\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1654 - val_loss: 0.1159\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1468 - val_loss: 0.2204\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1942 - val_loss: 0.4058\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2361 - val_loss: 0.3199\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1698 - val_loss: 0.6221\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2375 - val_loss: 0.4188\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2152 - val_loss: 0.3100\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1733 - val_loss: 0.3934\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1603 - val_loss: 1.4664\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3519 - val_loss: 0.1981\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1146 - val_loss: 0.2603\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1817 - val_loss: 0.2595\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1285 - val_loss: 0.1832\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2023 - val_loss: 1.0442\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2094 - val_loss: 0.7425\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1094 - val_loss: 0.2620\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2313 - val_loss: 0.4658\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1408 - val_loss: 0.2168\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1511 - val_loss: 0.0973\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2277 - val_loss: 0.2103\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2089 - val_loss: 0.2021\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1040 - val_loss: 0.6868\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2328 - val_loss: 0.5538\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1346 - val_loss: 0.1083\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1019 - val_loss: 0.5355\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2458 - val_loss: 0.9325\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2176 - val_loss: 0.4811\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.06384123565898174\n",
      "Mean Absolute Error (MAE): 0.19140683449070836\n",
      "Root Mean Squared Error (RMSE): 0.25266823238979164\n",
      "Time taken: 543.4318714141846\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1122.8427 - val_loss: 882.5338\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 856.5890 - val_loss: 682.6583\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 559.0639 - val_loss: 413.6725\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 324.5712 - val_loss: 241.3170\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 167.6788 - val_loss: 118.6517\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 74.5800 - val_loss: 39.8034\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 35.0806 - val_loss: 22.8247\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 17.9814 - val_loss: 36.0696\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 9.7162 - val_loss: 9.5614\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.7082 - val_loss: 3.4861\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.3573 - val_loss: 2.5569\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5594 - val_loss: 4.0162\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 3.3329 - val_loss: 4.4893\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.2644 - val_loss: 5.3456\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7516 - val_loss: 1.6203\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5631 - val_loss: 3.1900\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.5193 - val_loss: 1.7908\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2584 - val_loss: 3.8567\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7038 - val_loss: 2.3417\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.9811 - val_loss: 1.8601\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5157 - val_loss: 2.5572\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.7436 - val_loss: 1.0334\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3524 - val_loss: 1.5265\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9190 - val_loss: 0.7595\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9943 - val_loss: 1.0651\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3523 - val_loss: 4.4632\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3562 - val_loss: 0.5064\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7939 - val_loss: 1.6372\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6901 - val_loss: 1.5796\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6573 - val_loss: 1.2095\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7874 - val_loss: 1.6636\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4733 - val_loss: 0.4778\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6298 - val_loss: 1.8972\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1578 - val_loss: 3.7142\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8260 - val_loss: 0.4879\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5859 - val_loss: 0.4985\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4796 - val_loss: 1.3863\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4565 - val_loss: 1.8453\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5846 - val_loss: 1.5845\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6065 - val_loss: 1.0714\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7943 - val_loss: 0.3009\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4618 - val_loss: 0.4009\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4259 - val_loss: 0.4700\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.6902 - val_loss: 1.3783\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4772 - val_loss: 1.7598\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3267 - val_loss: 0.9016\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2829 - val_loss: 0.1517\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2349 - val_loss: 0.6330\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4398 - val_loss: 1.8832\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6310 - val_loss: 0.6945\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3171 - val_loss: 0.3904\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4970 - val_loss: 0.5445\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3215 - val_loss: 0.6957\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4251 - val_loss: 0.4853\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5064 - val_loss: 1.2157\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4459 - val_loss: 0.5451\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3547 - val_loss: 0.4755\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6254 - val_loss: 0.6177\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4348 - val_loss: 0.6069\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2495 - val_loss: 0.1655\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3410 - val_loss: 0.2488\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4945 - val_loss: 1.4953\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3958 - val_loss: 0.4125\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4352 - val_loss: 0.5859\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3877 - val_loss: 0.4475\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2960 - val_loss: 0.8611\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4046 - val_loss: 0.2895\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2928 - val_loss: 0.7396\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3402 - val_loss: 0.4840\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3647 - val_loss: 0.5776\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2920 - val_loss: 1.1501\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2738 - val_loss: 0.4530\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2745 - val_loss: 0.2134\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2819 - val_loss: 0.3391\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3276 - val_loss: 0.4331\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.7020 - val_loss: 0.3254\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1768 - val_loss: 0.1517\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1360 - val_loss: 0.0964\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1075 - val_loss: 0.0901\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2061 - val_loss: 0.1636\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1695 - val_loss: 0.2966\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3308 - val_loss: 0.4201\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2523 - val_loss: 0.5004\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1720 - val_loss: 0.4680\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3058 - val_loss: 0.5325\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2726 - val_loss: 0.1444\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2355 - val_loss: 0.3041\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2148 - val_loss: 0.2090\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1697 - val_loss: 0.4853\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1564 - val_loss: 0.1488\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3099 - val_loss: 0.1864\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1956 - val_loss: 0.1929\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3359 - val_loss: 0.4611\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1927 - val_loss: 0.1751\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2262 - val_loss: 0.1405\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2349 - val_loss: 0.2933\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1494 - val_loss: 0.1999\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1431 - val_loss: 0.8559\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3026 - val_loss: 0.3771\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2070 - val_loss: 0.1790\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1082 - val_loss: 0.1463\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4492 - val_loss: 0.2185\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0895 - val_loss: 0.0734\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1113 - val_loss: 0.1355\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1957 - val_loss: 0.2343\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2112 - val_loss: 0.1875\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1838 - val_loss: 0.3857\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2190 - val_loss: 0.3485\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8007 - val_loss: 0.7226\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1144 - val_loss: 0.1264\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0913 - val_loss: 0.0998\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1037 - val_loss: 0.3169\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1795 - val_loss: 0.3904\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1272 - val_loss: 0.0971\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0974 - val_loss: 0.0528\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1558 - val_loss: 0.2074\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1351 - val_loss: 0.4133\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1756 - val_loss: 0.3229\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2062 - val_loss: 0.1995\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1326 - val_loss: 0.1270\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1131 - val_loss: 0.2039\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1889 - val_loss: 0.4942\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1180 - val_loss: 0.1159\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1365 - val_loss: 0.2859\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2641 - val_loss: 0.9236\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1072 - val_loss: 0.1641\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1739 - val_loss: 0.1516\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1014 - val_loss: 0.0903\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1867 - val_loss: 0.2475\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1502 - val_loss: 0.1586\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1356 - val_loss: 0.1284\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1083 - val_loss: 0.1144\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1900 - val_loss: 0.3362\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1776 - val_loss: 0.4217\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0953 - val_loss: 0.3356\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6728 - val_loss: 0.1234\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0818 - val_loss: 0.1236\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0833 - val_loss: 0.3159\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0583 - val_loss: 0.1445\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0759 - val_loss: 0.1610\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1587 - val_loss: 0.2135\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1121 - val_loss: 0.1892\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1693 - val_loss: 0.3648\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1995 - val_loss: 0.1279\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1048 - val_loss: 0.0459\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0875 - val_loss: 0.1705\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0736 - val_loss: 0.1062\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2398 - val_loss: 0.1960\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1083 - val_loss: 0.2525\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0698 - val_loss: 0.0496\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0993 - val_loss: 0.2981\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1147 - val_loss: 0.1841\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1657 - val_loss: 0.1482\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1380 - val_loss: 0.2917\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4927 - val_loss: 1.2221\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1087 - val_loss: 0.0586\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0607 - val_loss: 0.0462\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0623 - val_loss: 0.0605\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1390 - val_loss: 0.0658\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1467 - val_loss: 0.1846\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1639 - val_loss: 0.0351\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0641 - val_loss: 0.2252\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0860 - val_loss: 0.1078\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0698 - val_loss: 0.1359\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1179 - val_loss: 0.1491\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1130 - val_loss: 0.1083\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1408 - val_loss: 1.0876\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2240 - val_loss: 0.1040\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0742 - val_loss: 0.0475\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0537 - val_loss: 0.1905\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1358 - val_loss: 0.1833\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1210 - val_loss: 0.1472\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1667 - val_loss: 0.4593\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0989 - val_loss: 0.1456\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0735 - val_loss: 0.0973\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0859 - val_loss: 0.1067\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1896 - val_loss: 0.1520\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0780 - val_loss: 0.0700\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0577 - val_loss: 0.2021\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0637 - val_loss: 0.0712\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0930 - val_loss: 0.5564\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2118 - val_loss: 0.1180\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0768 - val_loss: 0.2647\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1088 - val_loss: 0.4844\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1129 - val_loss: 0.0442\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0681 - val_loss: 0.3589\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1005 - val_loss: 0.3260\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3968 - val_loss: 0.2387\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0615 - val_loss: 0.0595\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0665 - val_loss: 0.0692\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0606 - val_loss: 0.0503\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.03518031534138397\n",
      "Mean Absolute Error (MAE): 0.14175499349718634\n",
      "Root Mean Squared Error (RMSE): 0.187564163265225\n",
      "Time taken: 828.0174760818481\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 17ms/step - loss: 1168.7002 - val_loss: 962.0085\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 908.6467 - val_loss: 953.8331\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 717.8230 - val_loss: 653.8823\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 419.0301 - val_loss: 298.9807\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 202.0780 - val_loss: 143.8711\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 84.8996 - val_loss: 64.7263\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 35.7356 - val_loss: 24.2161\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 18.9567 - val_loss: 12.3079\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 9.1755 - val_loss: 9.2669\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.4134 - val_loss: 3.7065\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 4.0689 - val_loss: 3.2181\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.9467 - val_loss: 1.5055\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.1225 - val_loss: 1.5829\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.2186 - val_loss: 1.5573\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6777 - val_loss: 2.7252\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2149 - val_loss: 2.8240\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.3434 - val_loss: 3.7021\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6143 - val_loss: 2.0805\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0789 - val_loss: 0.8431\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.1373 - val_loss: 2.4106\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1551 - val_loss: 1.6278\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2704 - val_loss: 1.2668\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7483 - val_loss: 1.0996\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.2066 - val_loss: 1.5382\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0473 - val_loss: 0.5296\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6576 - val_loss: 1.0305\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1126 - val_loss: 0.8215\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.1280 - val_loss: 2.0032\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6257 - val_loss: 0.5398\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6664 - val_loss: 0.7586\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5562 - val_loss: 0.8093\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7438 - val_loss: 2.1538\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6309 - val_loss: 0.5670\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.1444 - val_loss: 1.2728\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4135 - val_loss: 0.4408\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2927 - val_loss: 0.6998\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5444 - val_loss: 0.6146\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6193 - val_loss: 0.5528\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6452 - val_loss: 1.3704\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4117 - val_loss: 0.6976\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5748 - val_loss: 1.5711\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3564 - val_loss: 0.8010\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0169 - val_loss: 0.8969\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.1570 - val_loss: 0.3381\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4283 - val_loss: 0.6687\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2763 - val_loss: 0.2507\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2386 - val_loss: 0.2892\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2776 - val_loss: 0.9784\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5269 - val_loss: 0.5335\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3126 - val_loss: 0.5349\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3404 - val_loss: 0.6080\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5035 - val_loss: 0.2287\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4768 - val_loss: 0.5906\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3420 - val_loss: 0.4958\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3791 - val_loss: 0.8471\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8379 - val_loss: 0.9182\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4668 - val_loss: 0.6653\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3602 - val_loss: 0.4172\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2041 - val_loss: 0.7622\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3293 - val_loss: 2.6223\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7114 - val_loss: 0.4342\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2544 - val_loss: 0.3033\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3013 - val_loss: 0.4742\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2332 - val_loss: 0.4506\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2753 - val_loss: 0.1461\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4632 - val_loss: 0.5129\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1730 - val_loss: 0.1513\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3412 - val_loss: 0.3000\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8748 - val_loss: 1.5740\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2230 - val_loss: 0.2886\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2286 - val_loss: 0.4268\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1166 - val_loss: 0.1219\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4457 - val_loss: 0.9264\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2913 - val_loss: 0.2582\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2770 - val_loss: 0.3394\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1420 - val_loss: 0.1567\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9482 - val_loss: 1.8391\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1952 - val_loss: 0.1225\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1218 - val_loss: 0.1003\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2381 - val_loss: 0.4356\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1359 - val_loss: 0.3573\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1488 - val_loss: 0.2033\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3888 - val_loss: 0.6798\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2062 - val_loss: 0.3235\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2045 - val_loss: 0.2560\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1969 - val_loss: 0.7640\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2637 - val_loss: 0.2910\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1371 - val_loss: 0.3023\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2839 - val_loss: 0.9500\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2896 - val_loss: 0.9526\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1711 - val_loss: 0.3247\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1937 - val_loss: 0.9321\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2279 - val_loss: 0.4472\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3606 - val_loss: 0.5134\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1952 - val_loss: 0.2663\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1368 - val_loss: 0.2346\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1263 - val_loss: 0.1868\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2132 - val_loss: 0.1395\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2434 - val_loss: 0.7609\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2369 - val_loss: 0.2084\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1265 - val_loss: 0.1430\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2589 - val_loss: 0.7460\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1761 - val_loss: 0.2819\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2285 - val_loss: 0.2635\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1362 - val_loss: 0.1405\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1842 - val_loss: 0.2826\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2223 - val_loss: 0.7538\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1457 - val_loss: 0.0927\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1710 - val_loss: 0.2046\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1723 - val_loss: 0.4440\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2204 - val_loss: 0.1088\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1208 - val_loss: 0.3584\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1678 - val_loss: 0.2441\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3343 - val_loss: 0.1346\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0916 - val_loss: 0.4159\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0977 - val_loss: 0.1652\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1784 - val_loss: 0.1100\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1190 - val_loss: 0.4791\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1410 - val_loss: 0.2470\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2905 - val_loss: 0.1240\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1712 - val_loss: 0.1653\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0813 - val_loss: 0.1785\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1602 - val_loss: 0.7444\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4208 - val_loss: 2.4422\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1883 - val_loss: 0.2021\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0629 - val_loss: 0.0629\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1304 - val_loss: 0.1957\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0977 - val_loss: 0.0748\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1145 - val_loss: 0.4662\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1748 - val_loss: 0.2452\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1388 - val_loss: 0.2944\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2090 - val_loss: 0.2926\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1093 - val_loss: 0.2506\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1164 - val_loss: 0.3817\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2342 - val_loss: 0.1973\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0687 - val_loss: 0.0487\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1021 - val_loss: 0.1040\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0716 - val_loss: 0.4144\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1307 - val_loss: 0.8817\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5510 - val_loss: 5.9653\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8822 - val_loss: 0.1271\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0548 - val_loss: 0.0304\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0502 - val_loss: 0.0471\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0518 - val_loss: 0.0265\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0596 - val_loss: 0.0582\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0532 - val_loss: 0.1050\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0844 - val_loss: 0.1019\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0830 - val_loss: 0.1011\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0864 - val_loss: 0.0962\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0869 - val_loss: 0.0914\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1241 - val_loss: 1.0465\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1303 - val_loss: 0.3060\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0871 - val_loss: 0.2141\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1017 - val_loss: 0.1489\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1310 - val_loss: 0.2969\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2044 - val_loss: 0.5745\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0634 - val_loss: 0.0766\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0945 - val_loss: 0.1176\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0999 - val_loss: 0.1858\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2206 - val_loss: 0.1424\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0687 - val_loss: 0.2124\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1117 - val_loss: 0.2346\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1099 - val_loss: 0.1929\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1520 - val_loss: 0.1669\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0711 - val_loss: 0.1668\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1444 - val_loss: 0.2126\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1347 - val_loss: 0.0734\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0946 - val_loss: 0.1213\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1042 - val_loss: 0.2141\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1525 - val_loss: 0.3569\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0924 - val_loss: 0.1693\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1002 - val_loss: 0.2720\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1257 - val_loss: 0.0600\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0751 - val_loss: 0.1026\n",
      "16/16 [==============================] - 1s 25ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.026553325343930564\n",
      "Mean Absolute Error (MAE): 0.1234724295295609\n",
      "Root Mean Squared Error (RMSE): 0.16295191113923937\n",
      "Time taken: 769.3674476146698\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1099.3632 - val_loss: 921.1718\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 814.6103 - val_loss: 958.8074\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 514.2949 - val_loss: 391.0320\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 315.1178 - val_loss: 252.7432\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 159.3015 - val_loss: 104.3403\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 63.5607 - val_loss: 38.9186\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 28.2079 - val_loss: 13.9252\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 12.7044 - val_loss: 8.5797\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 7.6576 - val_loss: 3.5273\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.0172 - val_loss: 3.4680\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5642 - val_loss: 5.5234\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.2547 - val_loss: 2.2833\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.1408 - val_loss: 2.8390\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5598 - val_loss: 7.0873\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.5581 - val_loss: 2.2218\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5881 - val_loss: 2.1176\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.9101 - val_loss: 2.1184\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4250 - val_loss: 1.9281\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8731 - val_loss: 1.4087\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2151 - val_loss: 2.1861\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7495 - val_loss: 2.1068\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0078 - val_loss: 0.6009\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8756 - val_loss: 0.6646\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0764 - val_loss: 0.8216\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6267 - val_loss: 1.5678\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0602 - val_loss: 2.5212\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0063 - val_loss: 0.7451\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7772 - val_loss: 0.7430\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8087 - val_loss: 1.7875\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8174 - val_loss: 2.4202\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1187 - val_loss: 0.9353\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7894 - val_loss: 0.5059\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5387 - val_loss: 1.4322\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6591 - val_loss: 0.8972\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8861 - val_loss: 1.2369\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7090 - val_loss: 0.3916\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7887 - val_loss: 1.9698\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6273 - val_loss: 1.5826\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4789 - val_loss: 0.4283\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4585 - val_loss: 1.4116\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7773 - val_loss: 1.2142\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3928 - val_loss: 0.5892\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7270 - val_loss: 1.8531\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4931 - val_loss: 0.4825\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3458 - val_loss: 1.0151\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6304 - val_loss: 0.7707\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6567 - val_loss: 0.8810\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1879 - val_loss: 0.2529\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1795 - val_loss: 0.2669\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2310 - val_loss: 0.2671\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6213 - val_loss: 2.0881\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5200 - val_loss: 0.5894\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2355 - val_loss: 0.5669\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2944 - val_loss: 0.3710\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5271 - val_loss: 1.2771\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5110 - val_loss: 0.3999\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3230 - val_loss: 0.3248\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3345 - val_loss: 0.4171\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3347 - val_loss: 0.8097\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3033 - val_loss: 0.3494\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5600 - val_loss: 0.3491\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.7561 - val_loss: 0.7956\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3307 - val_loss: 0.2060\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2229 - val_loss: 0.1748\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1971 - val_loss: 0.1615\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2315 - val_loss: 0.2994\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2133 - val_loss: 0.3167\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2355 - val_loss: 0.5481\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1884 - val_loss: 0.7884\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2939 - val_loss: 0.8135\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2891 - val_loss: 0.4020\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3868 - val_loss: 1.3866\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3939 - val_loss: 1.1448\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3131 - val_loss: 0.4995\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1940 - val_loss: 0.2252\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4456 - val_loss: 0.7641\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2829 - val_loss: 0.2050\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3094 - val_loss: 0.2185\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2051 - val_loss: 0.5773\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3126 - val_loss: 0.7086\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3189 - val_loss: 0.1369\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2836 - val_loss: 1.3657\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2984 - val_loss: 0.5276\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1701 - val_loss: 0.1982\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3683 - val_loss: 0.2776\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4351 - val_loss: 0.3283\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1168 - val_loss: 0.2261\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1750 - val_loss: 0.1967\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3166 - val_loss: 0.4450\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3451 - val_loss: 0.3501\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1938 - val_loss: 0.2930\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2595 - val_loss: 0.1228\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2558 - val_loss: 0.2997\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2675 - val_loss: 0.3142\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1930 - val_loss: 0.3518\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1792 - val_loss: 0.2722\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1504 - val_loss: 1.0909\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2750 - val_loss: 0.3124\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2204 - val_loss: 0.1278\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1508 - val_loss: 0.5257\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1988 - val_loss: 0.2067\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1558 - val_loss: 0.3637\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4414 - val_loss: 0.0898\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1959 - val_loss: 0.5495\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1293 - val_loss: 0.1298\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1571 - val_loss: 0.1968\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1146 - val_loss: 0.1240\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1971 - val_loss: 0.1666\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1557 - val_loss: 0.3017\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3474 - val_loss: 1.9717\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0611 - val_loss: 0.3839\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0880 - val_loss: 0.1751\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0697 - val_loss: 0.0463\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0773 - val_loss: 0.1114\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1004 - val_loss: 0.2020\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1590 - val_loss: 0.2052\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1304 - val_loss: 0.4786\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2662 - val_loss: 0.3707\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1130 - val_loss: 0.1158\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0793 - val_loss: 0.0985\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1594 - val_loss: 0.5763\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1518 - val_loss: 0.3935\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1769 - val_loss: 0.2291\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0936 - val_loss: 0.2966\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1480 - val_loss: 0.1275\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1699 - val_loss: 0.3305\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2666 - val_loss: 0.2102\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1578 - val_loss: 0.5114\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1285 - val_loss: 0.2525\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1044 - val_loss: 0.1471\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1610 - val_loss: 0.4226\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2263 - val_loss: 0.1853\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1225 - val_loss: 0.2413\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1529 - val_loss: 0.5147\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.0847 - val_loss: 0.2676\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1549 - val_loss: 0.2517\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1569 - val_loss: 0.1041\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1049 - val_loss: 0.1745\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1463 - val_loss: 0.2317\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1662 - val_loss: 0.1936\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1160 - val_loss: 0.1637\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1121 - val_loss: 0.1884\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1413 - val_loss: 0.2177\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.046337461550427776\n",
      "Mean Absolute Error (MAE): 0.16506965549669125\n",
      "Root Mean Squared Error (RMSE): 0.21526137960727598\n",
      "Time taken: 642.4521098136902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 32, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 32, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_2848\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.147455  0.308759  0.383999  382.336345\n",
      "1        2  0.063841  0.191407  0.252668  543.431871\n",
      "2        3  0.035180  0.141755  0.187564  828.017476\n",
      "3        4  0.026553  0.123472  0.162952  769.367448\n",
      "4        5  0.046337  0.165070  0.215261  642.452110\n",
      "5  Average  0.063873  0.186093  0.240489  633.121050\n",
      "Results saved to 'Sensors 32_PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 32_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 32_PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
