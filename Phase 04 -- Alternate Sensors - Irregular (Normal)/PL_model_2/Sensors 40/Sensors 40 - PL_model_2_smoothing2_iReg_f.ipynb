{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor31</th>\n",
       "      <th>sensor32</th>\n",
       "      <th>sensor33</th>\n",
       "      <th>sensor34</th>\n",
       "      <th>sensor35</th>\n",
       "      <th>sensor36</th>\n",
       "      <th>sensor37</th>\n",
       "      <th>sensor38</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>66.071604</td>\n",
       "      <td>68.156040</td>\n",
       "      <td>59.369021</td>\n",
       "      <td>66.661787</td>\n",
       "      <td>47.796351</td>\n",
       "      <td>66.452945</td>\n",
       "      <td>62.814714</td>\n",
       "      <td>55.850172</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>65.889377</td>\n",
       "      <td>68.318221</td>\n",
       "      <td>59.625879</td>\n",
       "      <td>66.585524</td>\n",
       "      <td>47.739907</td>\n",
       "      <td>66.339364</td>\n",
       "      <td>62.980079</td>\n",
       "      <td>56.044690</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>65.710188</td>\n",
       "      <td>68.479716</td>\n",
       "      <td>59.882320</td>\n",
       "      <td>66.510064</td>\n",
       "      <td>47.687821</td>\n",
       "      <td>66.224823</td>\n",
       "      <td>63.142738</td>\n",
       "      <td>56.236811</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>65.534390</td>\n",
       "      <td>68.640083</td>\n",
       "      <td>60.137843</td>\n",
       "      <td>66.435766</td>\n",
       "      <td>47.639670</td>\n",
       "      <td>66.108890</td>\n",
       "      <td>63.302618</td>\n",
       "      <td>56.426445</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>65.362021</td>\n",
       "      <td>68.798812</td>\n",
       "      <td>60.391916</td>\n",
       "      <td>66.362800</td>\n",
       "      <td>47.594836</td>\n",
       "      <td>65.991088</td>\n",
       "      <td>63.459689</td>\n",
       "      <td>56.613265</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>73.017234</td>\n",
       "      <td>68.733032</td>\n",
       "      <td>48.688796</td>\n",
       "      <td>67.889940</td>\n",
       "      <td>63.913929</td>\n",
       "      <td>61.103505</td>\n",
       "      <td>66.364609</td>\n",
       "      <td>51.335827</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>73.062015</td>\n",
       "      <td>68.582853</td>\n",
       "      <td>48.738075</td>\n",
       "      <td>67.984417</td>\n",
       "      <td>63.881597</td>\n",
       "      <td>60.883791</td>\n",
       "      <td>66.304346</td>\n",
       "      <td>51.483056</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>73.109695</td>\n",
       "      <td>68.434048</td>\n",
       "      <td>48.786405</td>\n",
       "      <td>68.085563</td>\n",
       "      <td>63.849112</td>\n",
       "      <td>60.663900</td>\n",
       "      <td>66.239792</td>\n",
       "      <td>51.629768</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>73.160283</td>\n",
       "      <td>68.286710</td>\n",
       "      <td>48.833291</td>\n",
       "      <td>68.192798</td>\n",
       "      <td>63.816530</td>\n",
       "      <td>60.444575</td>\n",
       "      <td>66.170796</td>\n",
       "      <td>51.775916</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>73.213698</td>\n",
       "      <td>68.140945</td>\n",
       "      <td>48.878434</td>\n",
       "      <td>68.305761</td>\n",
       "      <td>63.783890</td>\n",
       "      <td>60.226194</td>\n",
       "      <td>66.097308</td>\n",
       "      <td>51.921412</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor31   sensor32  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  66.071604  68.156040   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  65.889377  68.318221   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  65.710188  68.479716   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  65.534390  68.640083   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  65.362021  68.798812   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  73.017234  68.733032   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  73.062015  68.582853   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  73.109695  68.434048   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  73.160283  68.286710   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  73.213698  68.140945   \n",
       "\n",
       "       sensor33   sensor34   sensor35   sensor36   sensor37   sensor38  \\\n",
       "0     59.369021  66.661787  47.796351  66.452945  62.814714  55.850172   \n",
       "1     59.625879  66.585524  47.739907  66.339364  62.980079  56.044690   \n",
       "2     59.882320  66.510064  47.687821  66.224823  63.142738  56.236811   \n",
       "3     60.137843  66.435766  47.639670  66.108890  63.302618  56.426445   \n",
       "4     60.391916  66.362800  47.594836  65.991088  63.459689  56.613265   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  48.688796  67.889940  63.913929  61.103505  66.364609  51.335827   \n",
       "2439  48.738075  67.984417  63.881597  60.883791  66.304346  51.483056   \n",
       "2440  48.786405  68.085563  63.849112  60.663900  66.239792  51.629768   \n",
       "2441  48.833291  68.192798  63.816530  60.444575  66.170796  51.775916   \n",
       "2442  48.878434  68.305761  63.783890  60.226194  66.097308  51.921412   \n",
       "\n",
       "       sensor39   sensor40  \n",
       "0     59.015999  62.518813  \n",
       "1     59.062238  62.619356  \n",
       "2     59.111119  62.720162  \n",
       "3     59.162988  62.821366  \n",
       "4     59.218087  62.922934  \n",
       "...         ...        ...  \n",
       "2438  66.743266  68.095627  \n",
       "2439  66.884332  68.147865  \n",
       "2440  67.027974  68.198538  \n",
       "2441  67.173718  68.247988  \n",
       "2442  67.321086  68.296467  \n",
       "\n",
       "[2443 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:40]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 15s 19ms/step - loss: 1113.9434 - val_loss: 893.8196\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 923.2665 - val_loss: 899.4782\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 820.9247 - val_loss: 713.1741\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 466.2851 - val_loss: 354.6320\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 226.8296 - val_loss: 260.4531\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 96.8802 - val_loss: 59.2117\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 41.2539 - val_loss: 28.3913\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 18.4463 - val_loss: 13.7748\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 10.6622 - val_loss: 18.5660\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.0203 - val_loss: 21.0764\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 4.5779 - val_loss: 5.7678\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.8821 - val_loss: 6.8461\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 4.6274 - val_loss: 12.6439\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.2146 - val_loss: 5.0135\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.7964 - val_loss: 2.2718\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.7875 - val_loss: 1.8942\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5168 - val_loss: 3.7866\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6374 - val_loss: 5.4814\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.9024 - val_loss: 4.9784\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5760 - val_loss: 2.8632\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.4393 - val_loss: 5.0571\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.2989 - val_loss: 3.7798\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9584 - val_loss: 2.4999\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8894 - val_loss: 1.7576\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9463 - val_loss: 1.3952\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1041 - val_loss: 3.3554\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0831 - val_loss: 5.8742\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9132 - val_loss: 1.4159\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.3711 - val_loss: 12.4849\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1052 - val_loss: 1.5034\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9261 - val_loss: 5.0290\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2895 - val_loss: 2.4957\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7678 - val_loss: 1.0195\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8981 - val_loss: 1.7068\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3976 - val_loss: 6.8168\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7682 - val_loss: 1.6785\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5787 - val_loss: 0.9137\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8701 - val_loss: 1.0062\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6625 - val_loss: 0.9234\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6136 - val_loss: 1.0205\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0345 - val_loss: 1.1528\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.1041 - val_loss: 1.7454\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4812 - val_loss: 1.3681\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9082 - val_loss: 1.6905\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5949 - val_loss: 0.7288\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5780 - val_loss: 0.5971\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6323 - val_loss: 2.3335\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4808 - val_loss: 0.9175\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5486 - val_loss: 0.6529\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5510 - val_loss: 0.9010\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5790 - val_loss: 0.4980\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7865 - val_loss: 0.7734\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3526 - val_loss: 1.0498\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5678 - val_loss: 2.4229\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3651 - val_loss: 1.4454\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4148 - val_loss: 2.1179\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6756 - val_loss: 2.2087\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8224 - val_loss: 0.6622\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3409 - val_loss: 0.7290\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3920 - val_loss: 0.6824\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4005 - val_loss: 1.0523\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3462 - val_loss: 0.8198\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4391 - val_loss: 0.5148\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4012 - val_loss: 0.7163\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4853 - val_loss: 1.3674\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6681 - val_loss: 3.0147\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4370 - val_loss: 0.4553\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2114 - val_loss: 0.2552\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3454 - val_loss: 0.5664\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4319 - val_loss: 1.4330\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3913 - val_loss: 0.5071\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3215 - val_loss: 0.3552\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3698 - val_loss: 0.5158\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3829 - val_loss: 0.4779\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3646 - val_loss: 1.5204\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3215 - val_loss: 0.4258\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2843 - val_loss: 0.7619\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3227 - val_loss: 0.7181\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4079 - val_loss: 0.5107\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2661 - val_loss: 1.0143\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1835 - val_loss: 0.2666\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2464 - val_loss: 0.7707\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3285 - val_loss: 0.8014\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3268 - val_loss: 0.5189\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2818 - val_loss: 0.4766\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5210 - val_loss: 0.8341\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2489 - val_loss: 0.3918\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1672 - val_loss: 0.5297\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2955 - val_loss: 0.2462\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1758 - val_loss: 0.5720\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4204 - val_loss: 0.9758\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2416 - val_loss: 0.6871\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2152 - val_loss: 0.4061\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3551 - val_loss: 0.6152\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2143 - val_loss: 0.1890\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1577 - val_loss: 0.2736\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1985 - val_loss: 0.5323\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.4839 - val_loss: 0.2639\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1251 - val_loss: 0.1443\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0913 - val_loss: 0.1005\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0660 - val_loss: 0.0884\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0904 - val_loss: 0.1018\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1249 - val_loss: 0.1046\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0963 - val_loss: 0.1283\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1321 - val_loss: 0.1946\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1788 - val_loss: 1.1118\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1549 - val_loss: 0.4278\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2422 - val_loss: 0.4851\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1746 - val_loss: 0.2431\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1888 - val_loss: 0.3533\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2326 - val_loss: 0.4064\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2500 - val_loss: 0.2613\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1151 - val_loss: 0.3803\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1794 - val_loss: 0.4913\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3470 - val_loss: 0.3786\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1594 - val_loss: 0.4072\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.1402 - val_loss: 1.9464\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2990 - val_loss: 0.2458\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1502 - val_loss: 0.2432\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1976 - val_loss: 0.2802\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2064 - val_loss: 0.1511\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1380 - val_loss: 0.9894\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2502 - val_loss: 0.5825\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1261 - val_loss: 0.1087\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2007 - val_loss: 0.4870\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2374 - val_loss: 0.2597\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1802 - val_loss: 0.4969\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1824 - val_loss: 0.2619\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1168 - val_loss: 0.4066\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2257 - val_loss: 0.1649\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1894 - val_loss: 0.3808\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.08859710900708447\n",
      "Mean Absolute Error (MAE): 0.22588948360938277\n",
      "Root Mean Squared Error (RMSE): 0.29765266504280535\n",
      "Time taken: 641.5801029205322\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1096.1547 - val_loss: 956.2881\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 911.7245 - val_loss: 971.7571\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 819.1122 - val_loss: 796.8174\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 497.9032 - val_loss: 397.3245\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 283.5517 - val_loss: 238.7296\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 149.7072 - val_loss: 109.8495\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 72.2207 - val_loss: 66.1619\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 33.2824 - val_loss: 25.2855\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 16.2076 - val_loss: 19.5540\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 9.7718 - val_loss: 13.2142\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 6.3337 - val_loss: 7.9731\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 4.9363 - val_loss: 5.0068\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.9953 - val_loss: 10.0170\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.8287 - val_loss: 2.2871\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.8222 - val_loss: 3.9692\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.3252 - val_loss: 2.5930\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.4796 - val_loss: 11.5676\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.8996 - val_loss: 2.7153\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.0255 - val_loss: 8.1813\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.7008 - val_loss: 3.8046\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6141 - val_loss: 5.0206\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.8300 - val_loss: 2.2136\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.8880 - val_loss: 7.7428\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5440 - val_loss: 2.4364\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2412 - val_loss: 1.7876\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.2285 - val_loss: 1.4263\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.3692 - val_loss: 5.0108\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1420 - val_loss: 2.5180\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9152 - val_loss: 2.0905\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5532 - val_loss: 12.3773\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 7.3261 - val_loss: 1.9886\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7828 - val_loss: 0.6395\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5759 - val_loss: 0.7724\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5348 - val_loss: 0.6529\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4320 - val_loss: 0.6058\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5578 - val_loss: 1.1836\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6949 - val_loss: 1.5540\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7100 - val_loss: 1.9807\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6472 - val_loss: 3.2974\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0155 - val_loss: 4.2404\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8373 - val_loss: 0.7128\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8990 - val_loss: 1.2946\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8533 - val_loss: 1.1822\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8923 - val_loss: 1.3237\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6611 - val_loss: 1.0947\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6959 - val_loss: 0.6314\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7103 - val_loss: 0.8300\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.6928 - val_loss: 1.2066\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4212 - val_loss: 0.8485\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4676 - val_loss: 0.8025\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5675 - val_loss: 0.8395\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3559 - val_loss: 0.5888\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5377 - val_loss: 1.1405\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7392 - val_loss: 1.0206\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4377 - val_loss: 0.7504\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4533 - val_loss: 1.3105\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4283 - val_loss: 0.2799\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7320 - val_loss: 1.9138\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6037 - val_loss: 1.7068\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4011 - val_loss: 0.4436\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3540 - val_loss: 0.3083\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3655 - val_loss: 1.7007\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7050 - val_loss: 0.8305\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6478 - val_loss: 0.4791\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4248 - val_loss: 1.3017\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4494 - val_loss: 2.5013\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3006 - val_loss: 0.8815\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5353 - val_loss: 2.0009\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4756 - val_loss: 1.0668\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4123 - val_loss: 1.1082\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8395 - val_loss: 1.1651\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2921 - val_loss: 0.2335\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2204 - val_loss: 0.7417\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2965 - val_loss: 0.3328\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3578 - val_loss: 1.0296\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3384 - val_loss: 0.3362\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4677 - val_loss: 0.4541\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3170 - val_loss: 0.7200\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3354 - val_loss: 1.5865\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3721 - val_loss: 0.6733\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3977 - val_loss: 0.9193\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3521 - val_loss: 0.1949\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2194 - val_loss: 0.2652\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2828 - val_loss: 1.0031\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4938 - val_loss: 4.5107\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4840 - val_loss: 0.2020\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1491 - val_loss: 0.7888\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3397 - val_loss: 0.4612\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1982 - val_loss: 0.6123\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3305 - val_loss: 0.5802\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3308 - val_loss: 0.2877\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3238 - val_loss: 1.9632\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4455 - val_loss: 0.3592\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2898 - val_loss: 1.2961\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2601 - val_loss: 0.2563\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1803 - val_loss: 0.2866\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2127 - val_loss: 0.3504\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1705 - val_loss: 0.5802\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2922 - val_loss: 0.7448\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3012 - val_loss: 0.2403\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2751 - val_loss: 0.5130\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3446 - val_loss: 0.4268\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2065 - val_loss: 0.3773\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2627 - val_loss: 0.4857\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2099 - val_loss: 0.2690\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2424 - val_loss: 0.2202\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2565 - val_loss: 0.7122\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2722 - val_loss: 0.6429\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2677 - val_loss: 0.3602\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1907 - val_loss: 0.2679\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2660 - val_loss: 1.1251\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1652 - val_loss: 0.2274\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.1950966781032867\n",
      "Mean Absolute Error (MAE): 0.32559637663187857\n",
      "Root Mean Squared Error (RMSE): 0.44169749614785764\n",
      "Time taken: 526.6999852657318\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 18ms/step - loss: 1129.5143 - val_loss: 938.3552\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 920.8323 - val_loss: 930.7972\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 876.1490 - val_loss: 904.0822\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 650.8002 - val_loss: 1119.5917\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 374.4749 - val_loss: 335.1065\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 242.1691 - val_loss: 201.5250\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 128.2696 - val_loss: 97.7402\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 60.0056 - val_loss: 51.4433\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 28.4939 - val_loss: 20.0397\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 14.0353 - val_loss: 12.7969\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 7.8277 - val_loss: 9.7022\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 5.5468 - val_loss: 11.5920\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 4.7032 - val_loss: 12.1422\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.5704 - val_loss: 7.9206\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.0524 - val_loss: 4.7656\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.6525 - val_loss: 3.6197\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.0343 - val_loss: 3.7780\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.9824 - val_loss: 6.4441\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.2053 - val_loss: 7.9206\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7840 - val_loss: 3.2809\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.8077 - val_loss: 6.2422\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.7649 - val_loss: 6.8929\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.0362 - val_loss: 39.2928\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.3443 - val_loss: 2.0142\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3593 - val_loss: 2.0796\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1794 - val_loss: 1.7125\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.1608 - val_loss: 3.3837\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0806 - val_loss: 1.7334\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8523 - val_loss: 6.0083\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.5481 - val_loss: 1.6289\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8823 - val_loss: 2.3941\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8925 - val_loss: 1.1985\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 3.0017 - val_loss: 5.4805\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7882 - val_loss: 3.0595\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8991 - val_loss: 1.1224\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7974 - val_loss: 1.8585\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9106 - val_loss: 3.6322\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8887 - val_loss: 1.5634\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8287 - val_loss: 2.3997\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0435 - val_loss: 2.4343\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.6095 - val_loss: 9.1514\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0008 - val_loss: 2.6731\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.7312 - val_loss: 0.8722\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7180 - val_loss: 0.7726\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6359 - val_loss: 0.5543\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7151 - val_loss: 1.2214\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7145 - val_loss: 0.9715\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0844 - val_loss: 1.4281\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.0364 - val_loss: 0.5640\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4970 - val_loss: 0.8754\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4457 - val_loss: 0.6975\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6790 - val_loss: 4.1042\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8346 - val_loss: 2.2691\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4413 - val_loss: 1.6983\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.9942 - val_loss: 2.7828\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4122 - val_loss: 0.4759\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3249 - val_loss: 0.5127\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5896 - val_loss: 0.3333\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3490 - val_loss: 0.5492\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4259 - val_loss: 1.2323\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4847 - val_loss: 2.0254\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5601 - val_loss: 0.9575\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6662 - val_loss: 3.3738\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8196 - val_loss: 0.7981\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3784 - val_loss: 0.4744\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5520 - val_loss: 0.5964\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4235 - val_loss: 1.1313\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5314 - val_loss: 0.8710\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3103 - val_loss: 0.3043\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3490 - val_loss: 1.0469\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6484 - val_loss: 0.7704\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5596 - val_loss: 1.0385\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3429 - val_loss: 0.5582\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4089 - val_loss: 0.5529\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4963 - val_loss: 2.0847\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3973 - val_loss: 0.4188\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2985 - val_loss: 0.2884\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2990 - val_loss: 0.4700\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3338 - val_loss: 0.4649\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4314 - val_loss: 0.6381\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3410 - val_loss: 0.3037\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3335 - val_loss: 2.2567\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4893 - val_loss: 1.6539\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2777 - val_loss: 0.3414\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3931 - val_loss: 0.2798\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4487 - val_loss: 0.4146\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3295 - val_loss: 0.4878\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2161 - val_loss: 1.1738\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2805 - val_loss: 0.6626\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6087 - val_loss: 0.4404\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3210 - val_loss: 0.5079\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2821 - val_loss: 0.6456\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1846 - val_loss: 0.2512\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2219 - val_loss: 0.3460\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3102 - val_loss: 0.3839\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2271 - val_loss: 0.3615\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3705 - val_loss: 0.3642\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2630 - val_loss: 0.3897\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2563 - val_loss: 0.2953\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1553 - val_loss: 0.2138\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3573 - val_loss: 0.7819\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4365 - val_loss: 2.7202\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1904 - val_loss: 0.2184\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2026 - val_loss: 1.1847\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2788 - val_loss: 0.3716\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1226 - val_loss: 0.2830\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2251 - val_loss: 0.3120\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2480 - val_loss: 1.0658\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2214 - val_loss: 0.5423\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1932 - val_loss: 0.3147\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2237 - val_loss: 1.5555\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2340 - val_loss: 1.8109\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2353 - val_loss: 0.3779\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2261 - val_loss: 0.5430\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2484 - val_loss: 0.5035\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2145 - val_loss: 0.4426\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2273 - val_loss: 0.2023\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2210 - val_loss: 0.4516\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1843 - val_loss: 0.5976\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2281 - val_loss: 0.4958\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2892 - val_loss: 0.5425\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2041 - val_loss: 0.4801\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1307 - val_loss: 0.2989\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1904 - val_loss: 0.4131\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1044 - val_loss: 0.1280\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1880 - val_loss: 0.3574\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4082 - val_loss: 0.3064\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1084 - val_loss: 0.1574\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0894 - val_loss: 0.1228\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2024 - val_loss: 0.3211\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2545 - val_loss: 0.8841\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2055 - val_loss: 0.0875\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1548 - val_loss: 0.1365\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1167 - val_loss: 0.1658\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1954 - val_loss: 0.4577\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1842 - val_loss: 0.4430\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1491 - val_loss: 0.2262\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1648 - val_loss: 0.4957\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2033 - val_loss: 0.5629\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1165 - val_loss: 0.1483\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1229 - val_loss: 0.2012\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1991 - val_loss: 0.1662\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2601 - val_loss: 0.5020\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1133 - val_loss: 0.2252\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1124 - val_loss: 1.3732\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1301 - val_loss: 0.1678\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1971 - val_loss: 2.8257\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1877 - val_loss: 0.3578\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1111 - val_loss: 0.0946\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1392 - val_loss: 0.3801\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.8534 - val_loss: 1.4121\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1346 - val_loss: 0.0655\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0658 - val_loss: 0.0488\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0578 - val_loss: 0.0549\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0539 - val_loss: 0.0412\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0475 - val_loss: 0.0498\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0492 - val_loss: 0.0653\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0648 - val_loss: 0.0694\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1611 - val_loss: 0.3258\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1108 - val_loss: 0.1099\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0778 - val_loss: 0.0929\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1062 - val_loss: 0.1985\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1335 - val_loss: 0.1848\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0783 - val_loss: 0.1236\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1608 - val_loss: 0.3462\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0995 - val_loss: 0.0949\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1077 - val_loss: 0.2561\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1742 - val_loss: 0.1912\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1305 - val_loss: 0.1647\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0873 - val_loss: 0.1524\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1369 - val_loss: 0.3126\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2186 - val_loss: 0.1535\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0788 - val_loss: 0.1568\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1389 - val_loss: 0.4592\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1534 - val_loss: 0.2124\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0964 - val_loss: 0.3402\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1424 - val_loss: 0.4730\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1841 - val_loss: 0.2818\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1882 - val_loss: 0.2421\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1202 - val_loss: 0.1364\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0626 - val_loss: 0.0991\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1048 - val_loss: 0.2571\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2076 - val_loss: 0.6243\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1132 - val_loss: 0.2961\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1029 - val_loss: 0.1175\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.04118308111563049\n",
      "Mean Absolute Error (MAE): 0.14987220580308638\n",
      "Root Mean Squared Error (RMSE): 0.20293615034199916\n",
      "Time taken: 868.2035138607025\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 19ms/step - loss: 1152.9786 - val_loss: 944.5796\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 916.5309 - val_loss: 961.4110\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 780.5721 - val_loss: 823.6440\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 454.1867 - val_loss: 343.0791\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 247.9004 - val_loss: 212.0433\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 116.3658 - val_loss: 221.7426\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 50.9293 - val_loss: 37.0043\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 23.1610 - val_loss: 18.4062\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 11.8292 - val_loss: 14.8659\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 7.1545 - val_loss: 8.8811\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.1761 - val_loss: 6.1485\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.6229 - val_loss: 3.5872\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.1871 - val_loss: 3.6994\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.6512 - val_loss: 16.3794\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.4896 - val_loss: 2.9408\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.4746 - val_loss: 2.3971\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.7203 - val_loss: 2.8127\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.2587 - val_loss: 2.1752\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.5596 - val_loss: 2.4303\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.3726 - val_loss: 4.5759\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.4696 - val_loss: 1.5675\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.4830 - val_loss: 2.1590\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.1594 - val_loss: 1.3869\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0927 - val_loss: 2.4700\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.8747 - val_loss: 9.2881\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9768 - val_loss: 0.9830\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8656 - val_loss: 3.9201\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9142 - val_loss: 3.2035\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2944 - val_loss: 1.5015\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8394 - val_loss: 1.7575\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.0157 - val_loss: 2.1518\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0501 - val_loss: 1.5196\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.9321 - val_loss: 1.6428\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5801 - val_loss: 1.3951\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8805 - val_loss: 0.9735\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6116 - val_loss: 1.4400\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.4366 - val_loss: 101.0311\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 5.2284 - val_loss: 2.0760\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5442 - val_loss: 0.5397\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3604 - val_loss: 0.2973\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4177 - val_loss: 0.4838\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3109 - val_loss: 0.4667\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3082 - val_loss: 0.5504\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4577 - val_loss: 0.9638\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4620 - val_loss: 0.4062\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6983 - val_loss: 0.9015\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7627 - val_loss: 1.0943\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6874 - val_loss: 2.5157\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5948 - val_loss: 0.9251\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3903 - val_loss: 0.6158\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5382 - val_loss: 0.8625\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6470 - val_loss: 3.3514\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5367 - val_loss: 1.4266\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7414 - val_loss: 0.7277\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4092 - val_loss: 0.6359\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4707 - val_loss: 1.0699\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.4430 - val_loss: 0.6884\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5222 - val_loss: 0.4726\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4398 - val_loss: 0.6781\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2899 - val_loss: 0.2623\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7441 - val_loss: 163.5902\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2523 - val_loss: 0.2346\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2037 - val_loss: 0.4166\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2061 - val_loss: 0.4259\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2024 - val_loss: 0.2748\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2373 - val_loss: 0.4897\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3137 - val_loss: 1.7016\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.6438 - val_loss: 0.7501\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2290 - val_loss: 0.5102\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2583 - val_loss: 0.8559\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4771 - val_loss: 0.8096\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.8369 - val_loss: 3.9747\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2305 - val_loss: 0.8168\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2279 - val_loss: 0.2774\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1919 - val_loss: 0.1786\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2980 - val_loss: 0.6097\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5616 - val_loss: 1.8465\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6145 - val_loss: 0.4094\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2103 - val_loss: 0.7887\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1891 - val_loss: 0.3412\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3012 - val_loss: 0.5648\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4051 - val_loss: 1.8744\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3594 - val_loss: 1.6083\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3216 - val_loss: 0.3544\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1709 - val_loss: 0.5592\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2320 - val_loss: 1.0275\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2958 - val_loss: 0.9161\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3557 - val_loss: 0.7227\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3562 - val_loss: 0.3448\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2150 - val_loss: 0.7823\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2812 - val_loss: 1.6381\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2445 - val_loss: 0.7261\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3212 - val_loss: 0.5418\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2025 - val_loss: 0.4962\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2616 - val_loss: 0.4521\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2277 - val_loss: 0.3543\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2699 - val_loss: 0.3646\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2948 - val_loss: 0.8677\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2602 - val_loss: 0.6434\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2497 - val_loss: 0.6243\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2296 - val_loss: 0.4147\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1738 - val_loss: 0.2963\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3095 - val_loss: 0.5662\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2103 - val_loss: 0.1260\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2521 - val_loss: 0.8560\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2350 - val_loss: 0.1996\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1704 - val_loss: 0.6431\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2485 - val_loss: 0.4281\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1655 - val_loss: 0.1757\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2044 - val_loss: 0.6811\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3114 - val_loss: 0.2482\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1859 - val_loss: 0.7884\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2441 - val_loss: 0.4859\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1497 - val_loss: 0.1778\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1306 - val_loss: 0.4168\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2852 - val_loss: 1.1342\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.1896 - val_loss: 12.2218\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2676 - val_loss: 0.0767\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0686 - val_loss: 0.0575\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0593 - val_loss: 0.0662\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0601 - val_loss: 0.0571\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0538 - val_loss: 0.1443\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.0734 - val_loss: 0.1221\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1018 - val_loss: 0.0776\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0923 - val_loss: 0.1583\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1483 - val_loss: 0.1421\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1318 - val_loss: 0.2339\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1662 - val_loss: 0.0913\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0874 - val_loss: 0.1580\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2240 - val_loss: 0.2610\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1651 - val_loss: 0.3751\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1721 - val_loss: 0.1977\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1288 - val_loss: 0.2630\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1522 - val_loss: 0.3378\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2171 - val_loss: 1.0681\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1710 - val_loss: 0.2181\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1100 - val_loss: 0.3654\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1256 - val_loss: 0.3080\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3131 - val_loss: 3.9314\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3019 - val_loss: 0.3937\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0892 - val_loss: 0.2057\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.0984 - val_loss: 0.2828\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.0980 - val_loss: 0.1096\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1770 - val_loss: 0.5184\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1240 - val_loss: 0.1564\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1360 - val_loss: 0.3739\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2201 - val_loss: 0.7094\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1892 - val_loss: 1.3421\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1210 - val_loss: 0.4883\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1558 - val_loss: 0.2167\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1293 - val_loss: 0.2243\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.05692149772347486\n",
      "Mean Absolute Error (MAE): 0.16548726374708095\n",
      "Root Mean Squared Error (RMSE): 0.23858226615462194\n",
      "Time taken: 714.3391606807709\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 18ms/step - loss: 1164.0958 - val_loss: 921.1721\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 925.7625 - val_loss: 906.9379\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 887.9655 - val_loss: 827.6683\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 627.2416 - val_loss: 438.7551\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 320.6001 - val_loss: 229.8089\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 156.0673 - val_loss: 137.3987\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 65.8689 - val_loss: 44.8244\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 31.8198 - val_loss: 45.6369\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 16.6693 - val_loss: 19.1986\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.8433 - val_loss: 7.6228\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 6.5254 - val_loss: 9.6202\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 4.4196 - val_loss: 17.1366\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.7019 - val_loss: 3.8707\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.6868 - val_loss: 4.4816\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.3012 - val_loss: 4.9487\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.1397 - val_loss: 9.7360\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 2.9767 - val_loss: 1.6689\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.6355 - val_loss: 3.2652\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.6548 - val_loss: 2.9611\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 2.0266 - val_loss: 5.2425\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.5480 - val_loss: 1.9598\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.3649 - val_loss: 1.5751\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 1.6037 - val_loss: 31.7824\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 5.3504 - val_loss: 2.7751\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8180 - val_loss: 0.9596\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8594 - val_loss: 3.7210\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7647 - val_loss: 1.1200\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6758 - val_loss: 1.6449\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8997 - val_loss: 3.5853\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.7028 - val_loss: 2.5475\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8202 - val_loss: 0.7665\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.8506 - val_loss: 1.0637\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7673 - val_loss: 1.6553\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7490 - val_loss: 1.4793\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8635 - val_loss: 2.0770\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8246 - val_loss: 2.0837\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.8454 - val_loss: 1.0866\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.2387 - val_loss: 2.6156\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.6910 - val_loss: 0.7087\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.9205 - val_loss: 0.8056\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7641 - val_loss: 1.2576\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5391 - val_loss: 1.5221\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.6276 - val_loss: 2.4551\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7277 - val_loss: 1.3464\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.5761 - val_loss: 1.5202\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 3.1676 - val_loss: 2.3955\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6441 - val_loss: 0.3940\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2854 - val_loss: 0.3640\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3396 - val_loss: 0.5294\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3107 - val_loss: 0.3049\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3227 - val_loss: 1.3015\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3908 - val_loss: 0.7786\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7612 - val_loss: 1.3257\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.6204 - val_loss: 1.0118\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3982 - val_loss: 0.7410\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5884 - val_loss: 1.7079\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4175 - val_loss: 0.7006\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4071 - val_loss: 4.9466\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5252 - val_loss: 0.9243\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4139 - val_loss: 2.0890\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.5577 - val_loss: 0.5159\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4118 - val_loss: 1.1196\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2962 - val_loss: 0.5584\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.4213 - val_loss: 1.3551\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.9106 - val_loss: 15.2411\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7867 - val_loss: 0.3029\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1944 - val_loss: 0.2537\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1438 - val_loss: 0.3099\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2182 - val_loss: 0.3916\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2175 - val_loss: 0.2189\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2371 - val_loss: 0.1751\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3359 - val_loss: 0.3883\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3586 - val_loss: 1.3936\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3342 - val_loss: 1.8877\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3134 - val_loss: 0.3694\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3348 - val_loss: 0.9847\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3827 - val_loss: 0.4554\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3611 - val_loss: 0.3884\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2641 - val_loss: 1.1658\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3102 - val_loss: 0.5140\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3573 - val_loss: 1.8627\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4236 - val_loss: 1.7744\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.3419 - val_loss: 0.5223\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2739 - val_loss: 0.4802\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2158 - val_loss: 0.4167\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3776 - val_loss: 0.3743\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3279 - val_loss: 0.8490\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2694 - val_loss: 0.2617\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2768 - val_loss: 0.2306\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2380 - val_loss: 0.4224\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2595 - val_loss: 0.2187\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2376 - val_loss: 0.7489\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5027 - val_loss: 1.2819\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1708 - val_loss: 0.1137\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1987 - val_loss: 0.4932\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3455 - val_loss: 0.2670\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2299 - val_loss: 0.9726\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1887 - val_loss: 0.2007\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.3016 - val_loss: 0.8157\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2538 - val_loss: 0.1920\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.3295 - val_loss: 0.7762\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1578 - val_loss: 0.5236\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2269 - val_loss: 0.3252\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2158 - val_loss: 0.7363\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2243 - val_loss: 0.4961\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2936 - val_loss: 0.5246\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1861 - val_loss: 0.1894\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1510 - val_loss: 0.3929\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2406 - val_loss: 0.5481\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1784 - val_loss: 0.4178\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2515 - val_loss: 0.5400\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1735 - val_loss: 0.1311\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2111 - val_loss: 0.7387\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2307 - val_loss: 0.5228\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2075 - val_loss: 0.4362\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2794 - val_loss: 0.3116\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1837 - val_loss: 0.3070\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1446 - val_loss: 0.3622\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1841 - val_loss: 0.3250\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1727 - val_loss: 0.1896\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1775 - val_loss: 0.2218\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2633 - val_loss: 0.1951\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1404 - val_loss: 0.1588\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1448 - val_loss: 0.3406\n",
      "16/16 [==============================] - 1s 19ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.11378482806290235\n",
      "Mean Absolute Error (MAE): 0.2437628530060685\n",
      "Root Mean Squared Error (RMSE): 0.337320067684836\n",
      "Time taken: 584.6400957107544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 40, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 40, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 40, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 40, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_14040\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.088597  0.225889  0.297653  641.580103\n",
      "1        2  0.195097  0.325596  0.441697  526.699985\n",
      "2        3  0.041183  0.149872  0.202936  868.203514\n",
      "3        4  0.056921  0.165487  0.238582  714.339161\n",
      "4        5  0.113785  0.243763  0.337320  584.640096\n",
      "5  Average  0.099117  0.222122  0.303638  667.092572\n",
      "Results saved to 'Sensors 40_PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 40_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 40_PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
