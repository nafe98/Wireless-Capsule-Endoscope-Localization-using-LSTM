{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>54.843534</td>\n",
       "      <td>67.566064</td>\n",
       "      <td>72.373162</td>\n",
       "      <td>72.463056</td>\n",
       "      <td>66.168056</td>\n",
       "      <td>69.098539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>54.631252</td>\n",
       "      <td>67.545145</td>\n",
       "      <td>72.247857</td>\n",
       "      <td>72.492057</td>\n",
       "      <td>66.099594</td>\n",
       "      <td>69.203420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>54.417764</td>\n",
       "      <td>67.520912</td>\n",
       "      <td>72.121019</td>\n",
       "      <td>72.520560</td>\n",
       "      <td>66.027539</td>\n",
       "      <td>69.304422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>54.202488</td>\n",
       "      <td>67.493660</td>\n",
       "      <td>71.992534</td>\n",
       "      <td>72.547992</td>\n",
       "      <td>65.951660</td>\n",
       "      <td>69.401670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>53.984751</td>\n",
       "      <td>67.463973</td>\n",
       "      <td>71.862293</td>\n",
       "      <td>72.573869</td>\n",
       "      <td>65.872039</td>\n",
       "      <td>69.495287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>60.229853</td>\n",
       "      <td>45.476810</td>\n",
       "      <td>75.157908</td>\n",
       "      <td>74.634189</td>\n",
       "      <td>71.548064</td>\n",
       "      <td>72.737987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>60.264440</td>\n",
       "      <td>45.468933</td>\n",
       "      <td>75.240427</td>\n",
       "      <td>74.640917</td>\n",
       "      <td>71.707523</td>\n",
       "      <td>72.865926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>60.300991</td>\n",
       "      <td>45.465837</td>\n",
       "      <td>75.326575</td>\n",
       "      <td>74.648854</td>\n",
       "      <td>71.873750</td>\n",
       "      <td>72.992255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>60.339244</td>\n",
       "      <td>45.467099</td>\n",
       "      <td>75.416016</td>\n",
       "      <td>74.658202</td>\n",
       "      <td>72.046393</td>\n",
       "      <td>73.117147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>60.378959</td>\n",
       "      <td>45.472050</td>\n",
       "      <td>75.508297</td>\n",
       "      <td>74.668878</td>\n",
       "      <td>72.224964</td>\n",
       "      <td>73.240962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  54.843534  67.566064   \n",
       "1     67.636949  77.055207  61.417464  68.656037  54.631252  67.545145   \n",
       "2     67.468015  76.608876  61.529876  68.599884  54.417764  67.520912   \n",
       "3     67.304084  76.171754  61.636534  68.548849  54.202488  67.493660   \n",
       "4     67.145806  75.743710  61.738066  68.502746  53.984751  67.463973   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  60.229853  45.476810   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  60.264440  45.468933   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  60.300991  45.465837   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  60.339244  45.467099   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  60.378959  45.472050   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     72.373162  72.463056  66.168056  69.098539  \n",
       "1     72.247857  72.492057  66.099594  69.203420  \n",
       "2     72.121019  72.520560  66.027539  69.304422  \n",
       "3     71.992534  72.547992  65.951660  69.401670  \n",
       "4     71.862293  72.573869  65.872039  69.495287  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  75.157908  74.634189  71.548064  72.737987  \n",
       "2439  75.240427  74.640917  71.707523  72.865926  \n",
       "2440  75.326575  74.648854  71.873750  72.992255  \n",
       "2441  75.416016  74.658202  72.046393  73.117147  \n",
       "2442  75.508297  74.668878  72.224964  73.240962  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 1112.6857 - val_loss: 916.5208\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 742.1527 - val_loss: 613.3491\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 491.0328 - val_loss: 501.9774\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 347.2966 - val_loss: 474.4191\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 272.3093 - val_loss: 278.6398\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 175.6902 - val_loss: 130.7609\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 91.4290 - val_loss: 101.2863\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 51.6106 - val_loss: 45.0740\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 31.4095 - val_loss: 44.8778\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 20.6812 - val_loss: 56.8022\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.6789 - val_loss: 10.5144\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.6201 - val_loss: 8.9624\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5980 - val_loss: 7.2561\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9789 - val_loss: 19.5638\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.8916 - val_loss: 10.2430\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8974 - val_loss: 7.4500\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.5114 - val_loss: 4.4317\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4915 - val_loss: 6.5535\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.0546 - val_loss: 12.1494\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8282 - val_loss: 37.6597\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5719 - val_loss: 4.9078\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1986 - val_loss: 3.7383\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0240 - val_loss: 1.6708\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2926 - val_loss: 8.7652\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1129 - val_loss: 3.0797\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3814 - val_loss: 9.3793\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1948 - val_loss: 5.9429\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.9848 - val_loss: 37.5690\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.5279 - val_loss: 5.1151\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7436 - val_loss: 2.7202\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6179 - val_loss: 4.7948\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.0850 - val_loss: 1.6391\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7640 - val_loss: 3.1032\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0623 - val_loss: 0.9424\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0915 - val_loss: 2.7218\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.2956 - val_loss: 4.7431\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1348 - val_loss: 14.5768\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0976 - val_loss: 10.3728\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9777 - val_loss: 0.9127\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6491 - val_loss: 1.5343\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7311 - val_loss: 1.1505\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7081 - val_loss: 1.5955\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8393 - val_loss: 1.0846\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6382 - val_loss: 1.1008\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7985 - val_loss: 1.4126\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9140 - val_loss: 4.7145\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.6145 - val_loss: 2.0330\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5882 - val_loss: 0.5201\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4193 - val_loss: 1.1739\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6547 - val_loss: 0.4916\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5273 - val_loss: 2.0987\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1348 - val_loss: 0.9435\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1004 - val_loss: 1.5421\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6211 - val_loss: 1.3044\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4685 - val_loss: 1.2929\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4948 - val_loss: 1.3850\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0244 - val_loss: 4.3530\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7072 - val_loss: 8.3393\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6348 - val_loss: 0.9930\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2691 - val_loss: 0.4547\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3574 - val_loss: 0.8073\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3737 - val_loss: 0.3227\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3007 - val_loss: 1.8285\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4275 - val_loss: 0.8529\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4586 - val_loss: 0.8704\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4845 - val_loss: 0.9019\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4204 - val_loss: 0.6271\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5828 - val_loss: 1.3003\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4683 - val_loss: 0.2904\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4769 - val_loss: 1.7873\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6005 - val_loss: 1.4373\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0685 - val_loss: 1.5413\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2897 - val_loss: 0.7554\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6079 - val_loss: 0.3723\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4434 - val_loss: 1.7376\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2774 - val_loss: 0.8754\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2328 - val_loss: 0.6261\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4213 - val_loss: 2.3064\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3520 - val_loss: 0.4453\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3187 - val_loss: 0.3397\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3794 - val_loss: 0.4334\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3776 - val_loss: 0.5994\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5215 - val_loss: 0.3747\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2872 - val_loss: 1.0814\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4175 - val_loss: 0.6057\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3952 - val_loss: 0.8752\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2783 - val_loss: 3.1010\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5022 - val_loss: 0.3221\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1678 - val_loss: 0.1823\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1431 - val_loss: 0.1342\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1390 - val_loss: 0.2552\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1726 - val_loss: 0.3192\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1944 - val_loss: 0.2134\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1822 - val_loss: 0.2238\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1894 - val_loss: 0.3342\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3503 - val_loss: 1.1868\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2626 - val_loss: 0.3236\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2517 - val_loss: 0.2761\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2223 - val_loss: 1.8110\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3731 - val_loss: 1.9418\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3028 - val_loss: 0.3809\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3204 - val_loss: 0.6868\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2853 - val_loss: 1.2174\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6663 - val_loss: 3.1243\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3354 - val_loss: 0.1114\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1215 - val_loss: 0.1016\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1094 - val_loss: 0.0963\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0991 - val_loss: 0.0967\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0990 - val_loss: 0.2004\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1192 - val_loss: 0.2925\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1487 - val_loss: 0.1728\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2229 - val_loss: 0.4426\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1695 - val_loss: 0.1442\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1872 - val_loss: 1.0987\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3597 - val_loss: 0.9848\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2410 - val_loss: 0.2104\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2357 - val_loss: 0.4407\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2614 - val_loss: 0.1801\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1990 - val_loss: 0.2655\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2195 - val_loss: 0.2927\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2543 - val_loss: 1.4012\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2801 - val_loss: 0.3328\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7128 - val_loss: 179.8098\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5108 - val_loss: 0.1391\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1406 - val_loss: 0.1026\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0872 - val_loss: 0.2049\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0995 - val_loss: 0.1291\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0871 - val_loss: 0.0834\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1178 - val_loss: 0.1692\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1164 - val_loss: 0.1926\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1762 - val_loss: 0.3189\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1798 - val_loss: 0.2443\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3774 - val_loss: 39.8594\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8254 - val_loss: 0.1856\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1182 - val_loss: 0.0918\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1205 - val_loss: 0.1349\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1231 - val_loss: 0.1180\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0987 - val_loss: 0.1236\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1033 - val_loss: 0.1811\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1355 - val_loss: 0.1997\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1269 - val_loss: 0.1578\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1943 - val_loss: 0.3545\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2135 - val_loss: 0.2738\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2522 - val_loss: 0.2810\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1200 - val_loss: 0.2812\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1887 - val_loss: 0.6847\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2337 - val_loss: 0.9341\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1896 - val_loss: 0.2077\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1454 - val_loss: 0.3016\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1962 - val_loss: 0.4479\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1713 - val_loss: 0.2119\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2233 - val_loss: 0.2881\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9475 - val_loss: 2.6471\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1488 - val_loss: 0.0959\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0675 - val_loss: 0.1381\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0690 - val_loss: 0.0751\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0929 - val_loss: 0.1350\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1249 - val_loss: 0.2694\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1331 - val_loss: 0.2208\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1463 - val_loss: 0.3580\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1670 - val_loss: 0.3646\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1227 - val_loss: 0.1381\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1518 - val_loss: 0.2334\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7103 - val_loss: 1.2968\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6383 - val_loss: 0.1223\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0737 - val_loss: 0.0953\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0569 - val_loss: 0.0859\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0662 - val_loss: 0.0586\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0769 - val_loss: 0.1399\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1139 - val_loss: 0.2228\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1240 - val_loss: 0.1910\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1379 - val_loss: 0.3231\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1418 - val_loss: 0.1187\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1017 - val_loss: 0.1783\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1418 - val_loss: 0.2254\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1309 - val_loss: 0.2375\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2077 - val_loss: 0.3384\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1501 - val_loss: 0.3978\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1500 - val_loss: 0.4890\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1738 - val_loss: 0.3166\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1219 - val_loss: 1.0609\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1673 - val_loss: 0.2147\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1156 - val_loss: 0.4557\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1849 - val_loss: 0.4929\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1198 - val_loss: 1.0532\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1248 - val_loss: 0.1938\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1252 - val_loss: 0.6932\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1726 - val_loss: 0.6764\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1708 - val_loss: 0.4513\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1200 - val_loss: 0.2048\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1263 - val_loss: 0.1797\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1017 - val_loss: 0.9210\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2276 - val_loss: 0.1330\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1004 - val_loss: 0.1918\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1265 - val_loss: 0.4043\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1585 - val_loss: 0.2810\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1455 - val_loss: 0.3723\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1046 - val_loss: 0.2390\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.058549695978927396\n",
      "Mean Absolute Error (MAE): 0.17249949731271455\n",
      "Root Mean Squared Error (RMSE): 0.24197044443263602\n",
      "Time taken: 652.6752758026123\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1108.9539 - val_loss: 922.4147\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 744.7836 - val_loss: 629.8674\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 487.6818 - val_loss: 395.3224\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 343.1027 - val_loss: 304.1056\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 268.9966 - val_loss: 236.2888\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 180.0532 - val_loss: 137.0594\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 96.1094 - val_loss: 66.9681\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 49.4107 - val_loss: 40.9533\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 29.7625 - val_loss: 26.7045\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.2802 - val_loss: 47.8090\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.4585 - val_loss: 21.4297\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.7795 - val_loss: 16.3567\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8996 - val_loss: 8.5950\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6300 - val_loss: 6.9080\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.3293 - val_loss: 5.5584\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.4813 - val_loss: 8.6468\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.1645 - val_loss: 4.2292\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8138 - val_loss: 6.1253\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.6262 - val_loss: 5.9442\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4267 - val_loss: 3.7588\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.2119 - val_loss: 3.2305\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2226 - val_loss: 34.3028\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5185 - val_loss: 3.6618\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5829 - val_loss: 8.9726\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.8658 - val_loss: 2.1344\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9284 - val_loss: 5.6136\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3575 - val_loss: 1.3498\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.4159 - val_loss: 11.3262\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.8773 - val_loss: 3.3096\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3850 - val_loss: 4.4887\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.5141 - val_loss: 9.1024\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5549 - val_loss: 2.0282\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0279 - val_loss: 0.8892\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9236 - val_loss: 1.0667\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9707 - val_loss: 1.0685\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8757 - val_loss: 1.4905\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6329 - val_loss: 3.3484\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1830 - val_loss: 0.9769\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6735 - val_loss: 3.2521\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6844 - val_loss: 1.0232\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7330 - val_loss: 0.8815\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8552 - val_loss: 1.2185\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7225 - val_loss: 69.1620\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.5786 - val_loss: 0.6242\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6074 - val_loss: 0.4497\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5385 - val_loss: 0.5330\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6685 - val_loss: 0.7045\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4995 - val_loss: 0.9965\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5386 - val_loss: 1.0563\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5184 - val_loss: 0.6174\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0652 - val_loss: 7.4103\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7817 - val_loss: 0.5916\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6080 - val_loss: 0.9061\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5167 - val_loss: 0.6649\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5076 - val_loss: 0.8348\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4567 - val_loss: 0.5376\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9624 - val_loss: 0.7681\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4500 - val_loss: 0.5860\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4287 - val_loss: 4.1416\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2677 - val_loss: 0.4865\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3725 - val_loss: 0.3794\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3368 - val_loss: 0.5460\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3416 - val_loss: 0.5408\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3738 - val_loss: 1.0628\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5272 - val_loss: 0.8526\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4881 - val_loss: 0.5845\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5305 - val_loss: 0.6714\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3946 - val_loss: 1.3860\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4491 - val_loss: 0.7286\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3313 - val_loss: 0.7546\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4614 - val_loss: 3.2894\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4738 - val_loss: 0.6978\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4990 - val_loss: 4.3325\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4771 - val_loss: 0.2184\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2200 - val_loss: 0.2155\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2365 - val_loss: 0.2220\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2347 - val_loss: 0.3163\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2312 - val_loss: 0.3331\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2641 - val_loss: 0.9520\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4252 - val_loss: 2.7108\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3509 - val_loss: 0.3332\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7674 - val_loss: 0.5630\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2422 - val_loss: 0.3464\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2621 - val_loss: 0.2848\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3131 - val_loss: 0.4958\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3254 - val_loss: 1.0191\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4710 - val_loss: 0.4296\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2762 - val_loss: 0.8208\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2847 - val_loss: 0.3324\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3415 - val_loss: 1.4205\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3022 - val_loss: 0.7191\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3720 - val_loss: 1.0435\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3607 - val_loss: 0.7919\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2463 - val_loss: 1.3791\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3032 - val_loss: 0.1761\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1696 - val_loss: 0.1370\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1410 - val_loss: 0.1465\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1647 - val_loss: 0.1951\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1407 - val_loss: 0.1352\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1203 - val_loss: 0.2704\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1447 - val_loss: 0.1355\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1840 - val_loss: 0.5806\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2226 - val_loss: 0.7640\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2626 - val_loss: 1.5769\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2162 - val_loss: 0.3839\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2478 - val_loss: 0.2910\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2084 - val_loss: 0.3846\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4125 - val_loss: 0.8232\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2325 - val_loss: 1.5677\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1926 - val_loss: 0.2902\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3091 - val_loss: 0.7691\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3015 - val_loss: 0.2313\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1926 - val_loss: 0.8001\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3713 - val_loss: 0.4281\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1862 - val_loss: 0.4641\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1918 - val_loss: 0.5710\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9481 - val_loss: 41.0104\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1750 - val_loss: 0.1778\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1118 - val_loss: 0.1579\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1312 - val_loss: 0.2234\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1412 - val_loss: 0.2286\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1199 - val_loss: 0.1628\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1123 - val_loss: 0.2362\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1447 - val_loss: 0.2047\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1470 - val_loss: 0.1249\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1632 - val_loss: 0.9533\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1895 - val_loss: 0.2966\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2021 - val_loss: 0.2801\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1724 - val_loss: 1.1340\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2687 - val_loss: 0.2598\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1545 - val_loss: 0.6786\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2132 - val_loss: 0.4436\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1983 - val_loss: 0.4311\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1520 - val_loss: 0.2515\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8641 - val_loss: 0.2256\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1330 - val_loss: 0.1166\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0900 - val_loss: 0.0525\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0733 - val_loss: 0.0684\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0728 - val_loss: 0.1176\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0819 - val_loss: 0.0702\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1165 - val_loss: 0.2613\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0883 - val_loss: 0.1309\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0935 - val_loss: 0.1326\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1043 - val_loss: 0.1733\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1452 - val_loss: 0.2608\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1826 - val_loss: 0.4953\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1545 - val_loss: 0.1675\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1460 - val_loss: 0.2428\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1706 - val_loss: 0.3532\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1650 - val_loss: 1.0044\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1697 - val_loss: 0.2939\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1552 - val_loss: 0.1614\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1614 - val_loss: 0.2640\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1573 - val_loss: 0.6830\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1783 - val_loss: 0.2508\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1710 - val_loss: 0.4205\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1456 - val_loss: 0.4976\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1717 - val_loss: 0.8279\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1986 - val_loss: 0.1525\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8490 - val_loss: 0.4725\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1057 - val_loss: 0.0785\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0619 - val_loss: 0.1232\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0600 - val_loss: 1.4107\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0930 - val_loss: 0.0714\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0482 - val_loss: 0.0472\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0634 - val_loss: 0.0652\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0645 - val_loss: 0.1882\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1020 - val_loss: 0.2743\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1389 - val_loss: 0.4079\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1283 - val_loss: 0.2588\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1704 - val_loss: 0.2050\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1195 - val_loss: 0.4539\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1417 - val_loss: 0.2230\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1122 - val_loss: 0.2100\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1431 - val_loss: 0.4920\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2280 - val_loss: 0.6779\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1401 - val_loss: 0.1750\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1050 - val_loss: 1.5042\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1618 - val_loss: 0.3400\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1310 - val_loss: 0.3278\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1173 - val_loss: 0.2929\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2183 - val_loss: 0.7936\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1255 - val_loss: 0.7640\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1013 - val_loss: 0.1982\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1086 - val_loss: 0.8911\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1996 - val_loss: 0.3364\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0939 - val_loss: 0.4536\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1121 - val_loss: 0.3546\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1337 - val_loss: 0.7856\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3530 - val_loss: 0.2051\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0713 - val_loss: 0.0551\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0508 - val_loss: 0.0468\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0388 - val_loss: 0.0643\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0472 - val_loss: 0.0593\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0485 - val_loss: 0.0612\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0514 - val_loss: 0.0586\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0521 - val_loss: 0.0645\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1045 - val_loss: 0.1409\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0915 - val_loss: 0.0689\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0615 - val_loss: 0.0907\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.09074724043292633\n",
      "Mean Absolute Error (MAE): 0.21564183669499518\n",
      "Root Mean Squared Error (RMSE): 0.30124282635927835\n",
      "Time taken: 650.0979528427124\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1090.1947 - val_loss: 824.3654\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 619.4850 - val_loss: 542.3370\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 446.3468 - val_loss: 417.1813\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 346.7342 - val_loss: 394.4420\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 286.1753 - val_loss: 277.6387\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 212.0485 - val_loss: 263.8494\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 120.2803 - val_loss: 105.9305\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 58.0707 - val_loss: 70.4317\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 34.2869 - val_loss: 30.4570\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 19.4859 - val_loss: 60.5774\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.6828 - val_loss: 18.4104\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.1830 - val_loss: 18.2201\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4045 - val_loss: 7.2323\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.5909 - val_loss: 11.7223\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 6.9704 - val_loss: 6.1444\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.3346 - val_loss: 6.8279\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.1637 - val_loss: 16.4487\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.2570 - val_loss: 3.4814\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.9334 - val_loss: 6.8969\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.8392 - val_loss: 3.4271\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3550 - val_loss: 2.1456\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7809 - val_loss: 5.2770\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7733 - val_loss: 9.2936\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3179 - val_loss: 5.7638\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1523 - val_loss: 7.7379\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0367 - val_loss: 2.5864\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3925 - val_loss: 7.1131\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5153 - val_loss: 1.5674\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8598 - val_loss: 2.2950\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.7872 - val_loss: 1.4792\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0288 - val_loss: 1.4023\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6149 - val_loss: 5.1037\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0975 - val_loss: 1.9146\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3527 - val_loss: 0.9915\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8288 - val_loss: 1.7033\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7221 - val_loss: 0.9916\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9384 - val_loss: 10.6193\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0011 - val_loss: 30.6105\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3026 - val_loss: 6.7189\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8150 - val_loss: 3.0143\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6693 - val_loss: 1.8904\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7030 - val_loss: 2.7712\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7628 - val_loss: 0.5572\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7044 - val_loss: 2.1687\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8626 - val_loss: 2.0161\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9474 - val_loss: 0.8771\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2446 - val_loss: 1.7588\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6720 - val_loss: 1.3455\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5141 - val_loss: 0.5680\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4343 - val_loss: 0.6972\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7965 - val_loss: 3.9267\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7456 - val_loss: 1.0006\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4746 - val_loss: 1.0427\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4625 - val_loss: 1.2060\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4931 - val_loss: 0.6545\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7828 - val_loss: 2.0209\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5466 - val_loss: 1.0578\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5093 - val_loss: 1.4854\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2321 - val_loss: 1.0143\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4153 - val_loss: 0.3359\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3547 - val_loss: 0.3639\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2853 - val_loss: 0.1928\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3205 - val_loss: 0.7106\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6017 - val_loss: 0.9063\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8035 - val_loss: 0.5045\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4060 - val_loss: 0.2629\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2437 - val_loss: 0.3385\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2043 - val_loss: 0.3506\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3568 - val_loss: 0.3273\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2829 - val_loss: 0.6660\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3224 - val_loss: 0.6720\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3545 - val_loss: 0.5796\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2605 - val_loss: 0.3775\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3576 - val_loss: 0.6176\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4472 - val_loss: 1.3156\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5549 - val_loss: 1.6308\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3047 - val_loss: 0.2454\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1916 - val_loss: 0.1829\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1794 - val_loss: 0.1976\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2320 - val_loss: 0.2560\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2177 - val_loss: 0.3968\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2782 - val_loss: 0.8711\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2767 - val_loss: 0.3934\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2631 - val_loss: 0.8697\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4042 - val_loss: 0.3737\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3869 - val_loss: 0.5183\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3832 - val_loss: 0.4442\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5996 - val_loss: 5.7331\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3378 - val_loss: 0.5243\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1863 - val_loss: 0.7223\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2438 - val_loss: 0.1746\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1747 - val_loss: 0.2324\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1807 - val_loss: 0.1472\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1511 - val_loss: 0.1495\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1691 - val_loss: 0.2003\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1985 - val_loss: 0.3229\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1633 - val_loss: 0.4600\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2064 - val_loss: 0.2494\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2690 - val_loss: 0.5679\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6208 - val_loss: 0.3353\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1878 - val_loss: 0.2098\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2733 - val_loss: 0.3837\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1996 - val_loss: 0.3097\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2702 - val_loss: 0.2730\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3074 - val_loss: 0.5839\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1186 - val_loss: 8.6291\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3759 - val_loss: 0.1278\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1269 - val_loss: 0.1509\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0966 - val_loss: 0.1815\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0956 - val_loss: 0.0858\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0952 - val_loss: 0.1174\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1139 - val_loss: 0.2642\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1315 - val_loss: 0.2760\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1415 - val_loss: 0.5289\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2646 - val_loss: 0.8535\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1881 - val_loss: 0.3339\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1812 - val_loss: 0.2997\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2643 - val_loss: 0.8153\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2425 - val_loss: 1.3728\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2322 - val_loss: 0.2544\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2918 - val_loss: 0.8569\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3061 - val_loss: 0.2061\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1891 - val_loss: 0.5520\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1690 - val_loss: 0.5009\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3085 - val_loss: 0.4734\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2730 - val_loss: 0.5258\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5529 - val_loss: 2.0951\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1541 - val_loss: 0.1278\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1177 - val_loss: 0.2355\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1153 - val_loss: 0.1798\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1183 - val_loss: 0.1407\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1838 - val_loss: 0.1718\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2350 - val_loss: 0.5323\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1527 - val_loss: 0.4133\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1435 - val_loss: 21.2428\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6835 - val_loss: 0.1176\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0856 - val_loss: 0.1188\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0670 - val_loss: 0.1009\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0787 - val_loss: 0.1237\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0702 - val_loss: 0.0861\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.0857709464196111\n",
      "Mean Absolute Error (MAE): 0.2110549103413004\n",
      "Root Mean Squared Error (RMSE): 0.2928667724744668\n",
      "Time taken: 460.2457699775696\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1113.1506 - val_loss: 876.0159\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 698.4094 - val_loss: 553.8649\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 471.6911 - val_loss: 438.8360\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 340.0137 - val_loss: 382.3333\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 249.4681 - val_loss: 209.8654\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 149.7829 - val_loss: 154.7220\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.1113 - val_loss: 41.7217\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 37.0266 - val_loss: 34.1201\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 24.5456 - val_loss: 26.8570\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.5954 - val_loss: 31.5024\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9760 - val_loss: 11.0270\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 10.9964 - val_loss: 13.8439\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.8918 - val_loss: 8.0329\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.1766 - val_loss: 10.9496\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 5.3167 - val_loss: 6.5312\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.8411 - val_loss: 4.8487\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 4.6275 - val_loss: 4.0644\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.6793 - val_loss: 6.8323\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.9494 - val_loss: 8.9060\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.0812 - val_loss: 6.3203\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7501 - val_loss: 3.7596\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2644 - val_loss: 12.3518\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2555 - val_loss: 3.3675\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5682 - val_loss: 7.6340\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0226 - val_loss: 25.1390\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3406 - val_loss: 2.1715\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8763 - val_loss: 1.3623\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0144 - val_loss: 12.3029\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3528 - val_loss: 3.6453\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8168 - val_loss: 1.4919\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0984 - val_loss: 1.1319\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5997 - val_loss: 1.4812\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2894 - val_loss: 1.2054\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9896 - val_loss: 0.6235\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4278 - val_loss: 2.7189\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5784 - val_loss: 2.6299\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3250 - val_loss: 3.0167\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2453 - val_loss: 32.0592\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.8318 - val_loss: 3.3134\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0504 - val_loss: 0.9799\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6204 - val_loss: 0.9531\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7272 - val_loss: 0.9602\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0413 - val_loss: 2.0373\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1973 - val_loss: 0.7616\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7130 - val_loss: 1.2892\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6364 - val_loss: 1.4482\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2961 - val_loss: 34.5410\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3475 - val_loss: 0.6742\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4629 - val_loss: 0.5869\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7680 - val_loss: 3.1733\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4569 - val_loss: 0.4378\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5750 - val_loss: 0.9317\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5437 - val_loss: 1.2366\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0198 - val_loss: 0.7276\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5274 - val_loss: 1.1261\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.4050 - val_loss: 1.4701\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1598 - val_loss: 0.4526\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4253 - val_loss: 0.7794\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4125 - val_loss: 0.7023\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6542 - val_loss: 0.7124\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4533 - val_loss: 1.0905\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5103 - val_loss: 1.1488\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4453 - val_loss: 0.3547\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3977 - val_loss: 0.4245\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4553 - val_loss: 0.8298\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3565 - val_loss: 0.2582\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2906 - val_loss: 0.4209\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2628 - val_loss: 0.4123\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3525 - val_loss: 1.0566\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3991 - val_loss: 3.8935\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5798 - val_loss: 0.5493\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3188 - val_loss: 0.5485\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3332 - val_loss: 0.4575\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4373 - val_loss: 0.5861\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3493 - val_loss: 2.8278\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8495 - val_loss: 0.6218\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3635 - val_loss: 0.5059\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3233 - val_loss: 0.9164\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5510 - val_loss: 0.7981\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3925 - val_loss: 0.4168\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4310 - val_loss: 1.3201\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7279 - val_loss: 1.5247\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3399 - val_loss: 0.2675\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2392 - val_loss: 0.1957\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1771 - val_loss: 0.4556\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1915 - val_loss: 0.1539\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2534 - val_loss: 0.6124\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3903 - val_loss: 0.5451\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2707 - val_loss: 10.5138\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2069 - val_loss: 19.0736\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8126 - val_loss: 0.2342\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1676 - val_loss: 0.1241\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1548 - val_loss: 0.2702\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1447 - val_loss: 0.1149\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1355 - val_loss: 0.2097\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1744 - val_loss: 0.2098\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2047 - val_loss: 3.6250\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8111 - val_loss: 0.3004\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1865 - val_loss: 0.0932\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1291 - val_loss: 0.1274\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1279 - val_loss: 0.1925\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1380 - val_loss: 0.1183\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1514 - val_loss: 0.4285\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2140 - val_loss: 0.1566\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1850 - val_loss: 0.2393\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3187 - val_loss: 0.2682\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2664 - val_loss: 0.2527\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1518 - val_loss: 0.4207\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2960 - val_loss: 0.4573\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8434 - val_loss: 0.3662\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1817 - val_loss: 0.2878\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1266 - val_loss: 0.1503\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1617 - val_loss: 0.1638\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1725 - val_loss: 0.3175\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2011 - val_loss: 0.4768\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3224 - val_loss: 1.0393\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2756 - val_loss: 0.2563\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2403 - val_loss: 0.6502\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3074 - val_loss: 0.5720\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2381 - val_loss: 0.2440\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2376 - val_loss: 0.3223\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2328 - val_loss: 0.5234\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2416 - val_loss: 0.2567\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1623 - val_loss: 0.4998\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2821 - val_loss: 0.2852\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2165 - val_loss: 0.6053\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3756 - val_loss: 1.1634\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1934 - val_loss: 0.3816\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1651 - val_loss: 0.4045\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.0931935395569488\n",
      "Mean Absolute Error (MAE): 0.22785599564620138\n",
      "Root Mean Squared Error (RMSE): 0.30527616932369417\n",
      "Time taken: 417.8446555137634\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1069.5139 - val_loss: 934.6235\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 800.0370 - val_loss: 692.7623\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 548.2914 - val_loss: 528.4334\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 364.0186 - val_loss: 362.2165\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 265.3671 - val_loss: 247.9580\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 164.3211 - val_loss: 188.7697\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 85.4862 - val_loss: 69.1141\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 44.3644 - val_loss: 47.5493\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 26.4086 - val_loss: 35.5520\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.2260 - val_loss: 13.3342\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.6666 - val_loss: 28.8907\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.9481 - val_loss: 25.1969\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.2979 - val_loss: 7.3193\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7265 - val_loss: 9.2175\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8017 - val_loss: 11.7954\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.7302 - val_loss: 3.9156\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.9364 - val_loss: 5.4946\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.9668 - val_loss: 3.5352\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0753 - val_loss: 7.5929\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4623 - val_loss: 14.0016\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6950 - val_loss: 2.0829\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7968 - val_loss: 3.0455\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7807 - val_loss: 4.1492\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.6741 - val_loss: 2.2524\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5492 - val_loss: 2.8229\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5576 - val_loss: 9.7487\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3166 - val_loss: 1.2211\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1802 - val_loss: 2.7915\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3091 - val_loss: 1.9821\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2063 - val_loss: 1.9469\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8880 - val_loss: 1.7974\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1292 - val_loss: 1.3839\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9075 - val_loss: 3.3525\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7742 - val_loss: 1.4659\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3216 - val_loss: 2.7166\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8711 - val_loss: 3.4863\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7417 - val_loss: 1.3628\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7210 - val_loss: 1.8221\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.3171 - val_loss: 4.2712\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6810 - val_loss: 0.7806\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5080 - val_loss: 1.2477\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4775 - val_loss: 0.7203\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4920 - val_loss: 1.9083\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7438 - val_loss: 1.1322\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6342 - val_loss: 0.9798\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6320 - val_loss: 0.9116\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6616 - val_loss: 1.1931\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5911 - val_loss: 1.2362\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5156 - val_loss: 2.1946\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0283 - val_loss: 5.2322\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0404 - val_loss: 1.2483\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3850 - val_loss: 0.5599\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3029 - val_loss: 0.4565\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3968 - val_loss: 0.5838\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5807 - val_loss: 1.0768\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8106 - val_loss: 1.0543\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6152 - val_loss: 0.4033\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4098 - val_loss: 0.6899\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5923 - val_loss: 1.4401\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8390 - val_loss: 2.1089\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5874 - val_loss: 0.4643\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3383 - val_loss: 0.4573\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3188 - val_loss: 0.3874\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3270 - val_loss: 0.5470\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2915 - val_loss: 0.4163\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4273 - val_loss: 2.5778\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4296 - val_loss: 0.4693\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4054 - val_loss: 0.7991\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3923 - val_loss: 0.5255\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3580 - val_loss: 1.2615\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3801 - val_loss: 0.5225\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4905 - val_loss: 20.2667\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4953 - val_loss: 0.3392\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2176 - val_loss: 0.2216\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2123 - val_loss: 0.3216\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1871 - val_loss: 0.1811\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1730 - val_loss: 0.2354\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1836 - val_loss: 0.3517\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2706 - val_loss: 0.9792\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2879 - val_loss: 0.2479\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2870 - val_loss: 0.3466\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2977 - val_loss: 0.5502\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3364 - val_loss: 0.3870\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4372 - val_loss: 0.9569\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6931 - val_loss: 0.6283\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1852 - val_loss: 0.4198\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2918 - val_loss: 0.8322\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2703 - val_loss: 0.2679\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3170 - val_loss: 0.6138\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3209 - val_loss: 0.6469\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4737 - val_loss: 0.2952\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1942 - val_loss: 0.1759\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1413 - val_loss: 0.1618\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1462 - val_loss: 0.3136\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1703 - val_loss: 0.2492\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1940 - val_loss: 0.3736\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1829 - val_loss: 0.4039\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2689 - val_loss: 0.4258\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4237 - val_loss: 0.5230\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2757 - val_loss: 0.4682\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9555 - val_loss: 0.3088\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1880 - val_loss: 0.2064\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1502 - val_loss: 0.1645\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1494 - val_loss: 0.2361\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1925 - val_loss: 0.7597\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1898 - val_loss: 0.3630\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2049 - val_loss: 0.2101\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2924 - val_loss: 0.4733\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3285 - val_loss: 0.5291\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1694 - val_loss: 0.5753\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3368 - val_loss: 1.9836\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2845 - val_loss: 0.5224\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1768 - val_loss: 0.3520\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2556 - val_loss: 0.1636\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1882 - val_loss: 0.2944\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2915 - val_loss: 1.7197\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1773 - val_loss: 2.2663\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8144 - val_loss: 0.2613\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1213 - val_loss: 0.0987\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0791 - val_loss: 0.0517\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0736 - val_loss: 0.0818\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0801 - val_loss: 0.0752\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1085 - val_loss: 0.1394\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0878 - val_loss: 0.1096\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1409 - val_loss: 0.3470\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1628 - val_loss: 0.2188\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1136 - val_loss: 0.4524\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1890 - val_loss: 0.4849\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.1156 - val_loss: 0.3607\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1597 - val_loss: 0.1127\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1086 - val_loss: 0.0669\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0833 - val_loss: 0.0746\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0762 - val_loss: 0.0531\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0694 - val_loss: 0.0631\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0747 - val_loss: 0.1024\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0758 - val_loss: 0.1088\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1026 - val_loss: 0.1030\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1223 - val_loss: 0.1658\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1364 - val_loss: 0.2149\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0904 - val_loss: 0.1687\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1771 - val_loss: 0.2889\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1671 - val_loss: 0.1746\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2331 - val_loss: 0.8870\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2033 - val_loss: 0.3363\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1592 - val_loss: 0.2818\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1617 - val_loss: 0.3227\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1439 - val_loss: 0.5368\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2232 - val_loss: 0.2858\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2287 - val_loss: 0.2937\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1898 - val_loss: 0.2110\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.051739388799309366\n",
      "Mean Absolute Error (MAE): 0.16367599626706833\n",
      "Root Mean Squared Error (RMSE): 0.22746293939740903\n",
      "Time taken: 480.306654214859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_12852\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.058550  0.172499  0.241970  652.675276\n",
      "1        2  0.090747  0.215642  0.301243  650.097953\n",
      "2        3  0.085771  0.211055  0.292867  460.245770\n",
      "3        4  0.093194  0.227856  0.305276  417.844656\n",
      "4        5  0.051739  0.163676  0.227463  480.306654\n",
      "5  Average  0.076000  0.198146  0.273764  532.234062\n",
      "Results saved to 'Sensors 16_PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 16_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 16_PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
