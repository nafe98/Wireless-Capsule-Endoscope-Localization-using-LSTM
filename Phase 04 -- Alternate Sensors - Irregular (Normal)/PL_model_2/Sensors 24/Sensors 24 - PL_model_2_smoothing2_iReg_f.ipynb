{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "      <th>sensor17</th>\n",
       "      <th>sensor18</th>\n",
       "      <th>sensor19</th>\n",
       "      <th>sensor20</th>\n",
       "      <th>sensor21</th>\n",
       "      <th>sensor22</th>\n",
       "      <th>sensor23</th>\n",
       "      <th>sensor24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>66.168056</td>\n",
       "      <td>69.098539</td>\n",
       "      <td>63.774499</td>\n",
       "      <td>72.589663</td>\n",
       "      <td>49.926036</td>\n",
       "      <td>65.407693</td>\n",
       "      <td>69.594400</td>\n",
       "      <td>74.015551</td>\n",
       "      <td>61.617424</td>\n",
       "      <td>71.352761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>66.099594</td>\n",
       "      <td>69.203420</td>\n",
       "      <td>63.737188</td>\n",
       "      <td>72.606971</td>\n",
       "      <td>49.765625</td>\n",
       "      <td>65.608297</td>\n",
       "      <td>69.634335</td>\n",
       "      <td>74.075210</td>\n",
       "      <td>61.417515</td>\n",
       "      <td>71.408016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>66.027539</td>\n",
       "      <td>69.304422</td>\n",
       "      <td>63.702145</td>\n",
       "      <td>72.621562</td>\n",
       "      <td>49.607186</td>\n",
       "      <td>65.804715</td>\n",
       "      <td>69.676924</td>\n",
       "      <td>74.138272</td>\n",
       "      <td>61.218926</td>\n",
       "      <td>71.464419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>65.951660</td>\n",
       "      <td>69.401670</td>\n",
       "      <td>63.669579</td>\n",
       "      <td>72.633744</td>\n",
       "      <td>49.450559</td>\n",
       "      <td>65.997230</td>\n",
       "      <td>69.722282</td>\n",
       "      <td>74.204513</td>\n",
       "      <td>61.021928</td>\n",
       "      <td>71.521580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>65.872039</td>\n",
       "      <td>69.495287</td>\n",
       "      <td>63.639458</td>\n",
       "      <td>72.644198</td>\n",
       "      <td>49.295774</td>\n",
       "      <td>66.185965</td>\n",
       "      <td>69.770382</td>\n",
       "      <td>74.273575</td>\n",
       "      <td>60.826849</td>\n",
       "      <td>71.578954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>71.548064</td>\n",
       "      <td>72.737987</td>\n",
       "      <td>70.881042</td>\n",
       "      <td>69.021424</td>\n",
       "      <td>63.884099</td>\n",
       "      <td>50.325573</td>\n",
       "      <td>72.977259</td>\n",
       "      <td>72.937153</td>\n",
       "      <td>70.645923</td>\n",
       "      <td>62.372587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>71.707523</td>\n",
       "      <td>72.865926</td>\n",
       "      <td>70.809799</td>\n",
       "      <td>69.072015</td>\n",
       "      <td>63.801574</td>\n",
       "      <td>50.260081</td>\n",
       "      <td>72.854007</td>\n",
       "      <td>72.944675</td>\n",
       "      <td>70.774777</td>\n",
       "      <td>62.370815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>71.873750</td>\n",
       "      <td>72.992255</td>\n",
       "      <td>70.740291</td>\n",
       "      <td>69.126412</td>\n",
       "      <td>63.718818</td>\n",
       "      <td>50.197325</td>\n",
       "      <td>72.731412</td>\n",
       "      <td>72.956725</td>\n",
       "      <td>70.906763</td>\n",
       "      <td>62.371777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>72.046393</td>\n",
       "      <td>73.117147</td>\n",
       "      <td>70.672317</td>\n",
       "      <td>69.184942</td>\n",
       "      <td>63.635983</td>\n",
       "      <td>50.137200</td>\n",
       "      <td>72.609641</td>\n",
       "      <td>72.973140</td>\n",
       "      <td>71.042271</td>\n",
       "      <td>62.375594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>72.224964</td>\n",
       "      <td>73.240962</td>\n",
       "      <td>70.605619</td>\n",
       "      <td>69.247915</td>\n",
       "      <td>63.553026</td>\n",
       "      <td>50.079550</td>\n",
       "      <td>72.488558</td>\n",
       "      <td>72.993903</td>\n",
       "      <td>71.181410</td>\n",
       "      <td>62.382246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor15   sensor16  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  66.168056  69.098539   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  66.099594  69.203420   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  66.027539  69.304422   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  65.951660  69.401670   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  65.872039  69.495287   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  71.548064  72.737987   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  71.707523  72.865926   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  71.873750  72.992255   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  72.046393  73.117147   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  72.224964  73.240962   \n",
       "\n",
       "       sensor17   sensor18   sensor19   sensor20   sensor21   sensor22  \\\n",
       "0     63.774499  72.589663  49.926036  65.407693  69.594400  74.015551   \n",
       "1     63.737188  72.606971  49.765625  65.608297  69.634335  74.075210   \n",
       "2     63.702145  72.621562  49.607186  65.804715  69.676924  74.138272   \n",
       "3     63.669579  72.633744  49.450559  65.997230  69.722282  74.204513   \n",
       "4     63.639458  72.644198  49.295774  66.185965  69.770382  74.273575   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  70.881042  69.021424  63.884099  50.325573  72.977259  72.937153   \n",
       "2439  70.809799  69.072015  63.801574  50.260081  72.854007  72.944675   \n",
       "2440  70.740291  69.126412  63.718818  50.197325  72.731412  72.956725   \n",
       "2441  70.672317  69.184942  63.635983  50.137200  72.609641  72.973140   \n",
       "2442  70.605619  69.247915  63.553026  50.079550  72.488558  72.993903   \n",
       "\n",
       "       sensor23   sensor24  \n",
       "0     61.617424  71.352761  \n",
       "1     61.417515  71.408016  \n",
       "2     61.218926  71.464419  \n",
       "3     61.021928  71.521580  \n",
       "4     60.826849  71.578954  \n",
       "...         ...        ...  \n",
       "2438  70.645923  62.372587  \n",
       "2439  70.774777  62.370815  \n",
       "2440  70.906763  62.371777  \n",
       "2441  71.042271  62.375594  \n",
       "2442  71.181410  62.382246  \n",
       "\n",
       "[2443 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:24]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 12s 16ms/step - loss: 1106.9125 - val_loss: 942.3113\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 916.0068 - val_loss: 936.8646\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 876.3060 - val_loss: 880.9976\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 641.7604 - val_loss: 513.9247\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 362.4198 - val_loss: 262.2905\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 187.3855 - val_loss: 200.8505\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 93.9139 - val_loss: 108.1571\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 43.5866 - val_loss: 29.1165\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 23.3848 - val_loss: 21.4719\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 15.0044 - val_loss: 41.9773\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 10.6220 - val_loss: 6.6165\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.0007 - val_loss: 31.7077\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.5233 - val_loss: 7.1956\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2696 - val_loss: 114.1681\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8603 - val_loss: 16.1219\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.1009 - val_loss: 5.9109\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.8341 - val_loss: 4.6948\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.3984 - val_loss: 7.2263\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5726 - val_loss: 4.8741\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0364 - val_loss: 3.9525\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9650 - val_loss: 3.8013\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6628 - val_loss: 3.5214\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7490 - val_loss: 4.5313\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5399 - val_loss: 219.6291\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4226 - val_loss: 12.2491\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5643 - val_loss: 1.3304\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1860 - val_loss: 2.9210\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6234 - val_loss: 46.7249\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9253 - val_loss: 1.7163\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1675 - val_loss: 1.8611\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0825 - val_loss: 3.5932\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1961 - val_loss: 2.9936\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1757 - val_loss: 6.2552\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1878 - val_loss: 4.6226\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9346 - val_loss: 3.7793\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8213 - val_loss: 1.8743\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0026 - val_loss: 3.3803\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8441 - val_loss: 2.9223\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7380 - val_loss: 2.1965\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0144 - val_loss: 2.4140\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.0545 - val_loss: 1.3401\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7999 - val_loss: 3.6607\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7763 - val_loss: 1.0615\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5718 - val_loss: 1.9150\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5846 - val_loss: 4.3983\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9545 - val_loss: 1.4280\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6050 - val_loss: 2.3427\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5954 - val_loss: 1.7491\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7507 - val_loss: 0.5152\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7089 - val_loss: 0.6776\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6328 - val_loss: 4.0861\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5481 - val_loss: 2.2523\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5347 - val_loss: 1.1749\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7897 - val_loss: 1.8550\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7305 - val_loss: 0.9481\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4551 - val_loss: 1.2662\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7132 - val_loss: 1.5580\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4312 - val_loss: 0.9329\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7698 - val_loss: 26.8979\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5279 - val_loss: 0.8862\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5718 - val_loss: 0.6206\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4827 - val_loss: 0.6903\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3672 - val_loss: 3.0972\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5376 - val_loss: 0.5580\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8531 - val_loss: 4.4677\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4824 - val_loss: 1.8889\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3402 - val_loss: 0.6911\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4242 - val_loss: 0.6782\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4360 - val_loss: 0.9129\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2912 - val_loss: 0.6718\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5621 - val_loss: 2.8347\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3134 - val_loss: 1.4937\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3883 - val_loss: 2.6355\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3703 - val_loss: 0.4289\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3586 - val_loss: 7.4029\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4775 - val_loss: 0.4388\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3949 - val_loss: 1.0411\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4501 - val_loss: 0.8676\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2727 - val_loss: 2.1706\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2967 - val_loss: 0.5590\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2893 - val_loss: 3.4440\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5092 - val_loss: 1.0822\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2850 - val_loss: 2.6373\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3429 - val_loss: 1.1819\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2739 - val_loss: 0.2933\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2742 - val_loss: 2.2649\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3348 - val_loss: 0.3766\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3690 - val_loss: 0.7398\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2566 - val_loss: 2.4531\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4537 - val_loss: 0.8092\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3256 - val_loss: 1.4279\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2649 - val_loss: 0.3433\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2464 - val_loss: 0.4549\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3823 - val_loss: 0.9425\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2191 - val_loss: 0.4791\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3729 - val_loss: 3.4367\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.7402 - val_loss: 0.6380\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1944 - val_loss: 0.1133\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1235 - val_loss: 0.2105\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1275 - val_loss: 0.1634\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1235 - val_loss: 0.3500\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1848 - val_loss: 0.8671\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1762 - val_loss: 0.2746\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2234 - val_loss: 0.9983\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2497 - val_loss: 0.4345\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2125 - val_loss: 0.7774\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2483 - val_loss: 0.6402\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4068 - val_loss: 0.5529\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1438 - val_loss: 0.1981\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2473 - val_loss: 0.3850\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1641 - val_loss: 0.7596\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2787 - val_loss: 0.8938\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2273 - val_loss: 0.9580\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2211 - val_loss: 0.6038\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1712 - val_loss: 0.2569\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2621 - val_loss: 0.3049\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3868 - val_loss: 1.5189\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1695 - val_loss: 0.6665\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1737 - val_loss: 0.2208\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1530 - val_loss: 1.8165\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2030 - val_loss: 0.3750\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1920 - val_loss: 0.3085\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2186 - val_loss: 0.5572\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2435 - val_loss: 0.4996\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3521 - val_loss: 0.3223\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1486 - val_loss: 0.2495\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1961 - val_loss: 1.2453\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1446 - val_loss: 0.7734\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.1134926435115682\n",
      "Mean Absolute Error (MAE): 0.2525702006276191\n",
      "Root Mean Squared Error (RMSE): 0.3368866923931074\n",
      "Time taken: 488.9810416698456\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 1116.8289 - val_loss: 953.3746\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 898.9617 - val_loss: 910.3208\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 715.0687 - val_loss: 607.3289\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 468.1201 - val_loss: 374.7118\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 286.8345 - val_loss: 231.8336\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 155.7644 - val_loss: 107.4413\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 77.9287 - val_loss: 206.8001\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 39.2366 - val_loss: 71.1365\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 21.0952 - val_loss: 39.7448\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 12.9608 - val_loss: 49.7512\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.4170 - val_loss: 6.1496\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2801 - val_loss: 18.3344\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.2238 - val_loss: 5.2056\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.2899 - val_loss: 2.4626\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.7123 - val_loss: 3.4576\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.0376 - val_loss: 3.5094\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5619 - val_loss: 6.2329\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.9696 - val_loss: 3.2831\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9759 - val_loss: 2.0059\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2881 - val_loss: 5.3963\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0189 - val_loss: 2.6498\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.1216 - val_loss: 2.3879\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7730 - val_loss: 10.9226\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7902 - val_loss: 1.6133\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.5974 - val_loss: 12.2417\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.3919 - val_loss: 2.0171\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0741 - val_loss: 1.9903\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4418 - val_loss: 1.8525\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.2800 - val_loss: 0.7639\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9134 - val_loss: 0.7962\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4596 - val_loss: 2.5991\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1415 - val_loss: 3.3126\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.9681 - val_loss: 0.9993\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.9370 - val_loss: 26.1507\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.2487 - val_loss: 1.2577\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7172 - val_loss: 1.1713\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6859 - val_loss: 1.7043\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7861 - val_loss: 1.3131\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.1015 - val_loss: 2.7026\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7696 - val_loss: 0.7344\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7935 - val_loss: 1.6790\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6446 - val_loss: 0.7196\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4849 - val_loss: 1.2661\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7761 - val_loss: 0.6718\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7918 - val_loss: 1.2149\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7051 - val_loss: 1.3920\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6553 - val_loss: 1.6337\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.0977 - val_loss: 1.4915\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4473 - val_loss: 0.9207\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4114 - val_loss: 0.6262\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3805 - val_loss: 0.6674\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3870 - val_loss: 0.5252\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5991 - val_loss: 0.8756\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4727 - val_loss: 0.8937\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6323 - val_loss: 0.8626\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5575 - val_loss: 0.7442\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5316 - val_loss: 1.2833\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7025 - val_loss: 0.7235\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0185 - val_loss: 1.3504\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5586 - val_loss: 0.4937\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4205 - val_loss: 0.4883\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3520 - val_loss: 0.5767\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7012 - val_loss: 1.0243\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3836 - val_loss: 0.6722\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8699 - val_loss: 0.3894\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3537 - val_loss: 0.5235\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4470 - val_loss: 0.4876\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4229 - val_loss: 0.6245\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3927 - val_loss: 0.4718\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3971 - val_loss: 0.6223\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5284 - val_loss: 2.2683\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4727 - val_loss: 1.0105\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3041 - val_loss: 2.0743\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4117 - val_loss: 2.0063\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8502 - val_loss: 0.4679\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2320 - val_loss: 0.2500\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1831 - val_loss: 0.1764\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1708 - val_loss: 0.1984\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1822 - val_loss: 0.1975\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1884 - val_loss: 0.2587\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1799 - val_loss: 0.3353\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2812 - val_loss: 0.8123\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2629 - val_loss: 0.2361\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4025 - val_loss: 1.3538\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9416 - val_loss: 0.3014\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2043 - val_loss: 0.2271\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2041 - val_loss: 0.3149\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2950 - val_loss: 0.6569\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3496 - val_loss: 0.4988\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2736 - val_loss: 1.0785\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2854 - val_loss: 0.6213\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3012 - val_loss: 0.5554\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3264 - val_loss: 0.3647\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2219 - val_loss: 0.5604\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2689 - val_loss: 1.7217\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5571 - val_loss: 2.3309\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3358 - val_loss: 0.8332\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2399 - val_loss: 0.6083\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2180 - val_loss: 0.6731\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2650 - val_loss: 0.5139\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3500 - val_loss: 3.8176\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2752 - val_loss: 0.3705\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3450 - val_loss: 0.5318\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2737 - val_loss: 0.2893\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2068 - val_loss: 0.7362\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2546 - val_loss: 0.2801\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3468 - val_loss: 2.7438\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.1759370167219396\n",
      "Mean Absolute Error (MAE): 0.2997640674781636\n",
      "Root Mean Squared Error (RMSE): 0.41944846730193164\n",
      "Time taken: 405.5576846599579\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1151.0254 - val_loss: 915.7685\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 925.4937 - val_loss: 900.3397\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 814.7424 - val_loss: 695.5231\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 526.2740 - val_loss: 480.4409\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 345.7199 - val_loss: 323.8147\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 235.8717 - val_loss: 261.7751\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 126.5401 - val_loss: 109.0844\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 65.0133 - val_loss: 95.5795\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 34.2497 - val_loss: 48.3871\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.6410 - val_loss: 16.5097\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 12.7842 - val_loss: 17.5111\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.3888 - val_loss: 10.8584\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.7005 - val_loss: 10.2829\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.4754 - val_loss: 7.3384\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.7309 - val_loss: 10.4902\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.8727 - val_loss: 6.6164\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.8724 - val_loss: 8.5812\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.6693 - val_loss: 21.5907\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0592 - val_loss: 7.0978\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3012 - val_loss: 7.5871\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.8655 - val_loss: 23.3216\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.9267 - val_loss: 2.3822\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.4412 - val_loss: 22.7586\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.4765 - val_loss: 10.5708\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.8053 - val_loss: 1.4800\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6271 - val_loss: 28.1794\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4075 - val_loss: 4.6872\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2883 - val_loss: 0.9949\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2131 - val_loss: 1.3721\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4374 - val_loss: 2.0995\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5352 - val_loss: 3.4492\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4342 - val_loss: 1.7211\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.4204 - val_loss: 2.9720\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0285 - val_loss: 2.5073\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9756 - val_loss: 5.7965\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2690 - val_loss: 5.2316\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8842 - val_loss: 0.9856\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9042 - val_loss: 1.1909\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7076 - val_loss: 3.5547\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9230 - val_loss: 1.7022\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8876 - val_loss: 4.1517\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2882 - val_loss: 2.9900\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2768 - val_loss: 1.4817\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7024 - val_loss: 1.8659\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9667 - val_loss: 2.6120\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4729 - val_loss: 39.5647\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7911 - val_loss: 1.3209\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9408 - val_loss: 0.4292\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4135 - val_loss: 0.4929\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4512 - val_loss: 0.8013\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5529 - val_loss: 1.0498\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6549 - val_loss: 0.9933\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5664 - val_loss: 0.7907\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8239 - val_loss: 0.9292\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5458 - val_loss: 1.7706\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7644 - val_loss: 17.2494\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9237 - val_loss: 1.2898\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5177 - val_loss: 1.6422\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4849 - val_loss: 1.0251\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7150 - val_loss: 2.3380\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6582 - val_loss: 1.3076\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5741 - val_loss: 0.7775\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4214 - val_loss: 1.3207\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4930 - val_loss: 9.4856\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6648 - val_loss: 1.0546\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7628 - val_loss: 0.8753\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5156 - val_loss: 0.3393\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3695 - val_loss: 0.9139\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3654 - val_loss: 1.5443\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4367 - val_loss: 0.7787\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4603 - val_loss: 0.5957\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5970 - val_loss: 0.5607\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4052 - val_loss: 0.9766\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5223 - val_loss: 1.2030\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3599 - val_loss: 0.8518\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3839 - val_loss: 1.2172\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3060 - val_loss: 0.7660\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4887 - val_loss: 1.1881\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3849 - val_loss: 0.7302\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3966 - val_loss: 0.9097\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3433 - val_loss: 1.1142\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4873 - val_loss: 2.0342\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3242 - val_loss: 0.4678\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3661 - val_loss: 0.3637\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3543 - val_loss: 3.1700\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3855 - val_loss: 0.3182\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4506 - val_loss: 1.0681\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4307 - val_loss: 20.4744\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4575 - val_loss: 0.5458\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2128 - val_loss: 0.2048\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1771 - val_loss: 0.1769\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1503 - val_loss: 0.1565\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1461 - val_loss: 0.3292\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2026 - val_loss: 0.2269\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2035 - val_loss: 0.2658\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1915 - val_loss: 0.3634\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4277 - val_loss: 0.8575\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3085 - val_loss: 0.3303\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2001 - val_loss: 0.3831\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2737 - val_loss: 0.3573\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1774 - val_loss: 0.4633\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2027 - val_loss: 0.3314\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.6687 - val_loss: 0.6066\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1980 - val_loss: 0.2539\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1435 - val_loss: 0.2169\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1261 - val_loss: 0.1245\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1954 - val_loss: 0.3131\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1763 - val_loss: 0.1679\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1736 - val_loss: 0.2266\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2561 - val_loss: 0.6597\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2613 - val_loss: 0.7383\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1787 - val_loss: 0.3322\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2608 - val_loss: 3.0446\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2830 - val_loss: 0.5722\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4319 - val_loss: 0.7030\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1506 - val_loss: 0.1646\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1497 - val_loss: 0.3007\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2525 - val_loss: 0.8836\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3502 - val_loss: 0.5052\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2019 - val_loss: 0.2800\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2434 - val_loss: 0.2437\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2219 - val_loss: 2.1715\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2814 - val_loss: 0.3702\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2469 - val_loss: 0.1754\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1517 - val_loss: 0.2742\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2121 - val_loss: 0.4056\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2112 - val_loss: 0.4167\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3380 - val_loss: 0.7137\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2151 - val_loss: 0.7781\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3089 - val_loss: 0.3902\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1740 - val_loss: 0.1877\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1389 - val_loss: 0.3171\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2188 - val_loss: 0.3731\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1705 - val_loss: 0.3197\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2302 - val_loss: 0.3888\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2703 - val_loss: 0.3947\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.12414829972407508\n",
      "Mean Absolute Error (MAE): 0.26278681704112306\n",
      "Root Mean Squared Error (RMSE): 0.3523468457700098\n",
      "Time taken: 509.7592656612396\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1149.6071 - val_loss: 920.3321\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 919.6583 - val_loss: 893.5085\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 812.3561 - val_loss: 694.1967\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 605.3809 - val_loss: 544.4713\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 419.4658 - val_loss: 392.7543\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 300.9810 - val_loss: 272.9760\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 195.0507 - val_loss: 147.0986\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 99.6625 - val_loss: 97.3227\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 48.8452 - val_loss: 29.9656\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 23.8016 - val_loss: 24.6068\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 13.5843 - val_loss: 18.8300\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 10.1969 - val_loss: 8.3362\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.1792 - val_loss: 15.5123\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.9744 - val_loss: 8.2131\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.9428 - val_loss: 9.1922\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0654 - val_loss: 14.1847\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.2676 - val_loss: 8.3997\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.4130 - val_loss: 10.7047\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0431 - val_loss: 8.1573\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3770 - val_loss: 2.4862\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.6900 - val_loss: 5.2493\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.5078 - val_loss: 2.6933\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.0175 - val_loss: 2.8314\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4317 - val_loss: 3.7575\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5997 - val_loss: 1.2598\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.4972 - val_loss: 3.2628\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3693 - val_loss: 1.3451\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5559 - val_loss: 3.2908\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3530 - val_loss: 3.8854\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.1800 - val_loss: 1.3257\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2723 - val_loss: 1.9619\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5930 - val_loss: 2.0823\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0024 - val_loss: 10.9336\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.0813 - val_loss: 2.1563\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9560 - val_loss: 0.9361\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8600 - val_loss: 5.8166\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6380 - val_loss: 2.5772\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8530 - val_loss: 1.1470\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4064 - val_loss: 2.0291\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0500 - val_loss: 3.1605\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9921 - val_loss: 3.2794\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8544 - val_loss: 2.1929\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6737 - val_loss: 0.8046\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7238 - val_loss: 3.3839\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7837 - val_loss: 1.0855\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0682 - val_loss: 1.8263\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8565 - val_loss: 1.5876\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8009 - val_loss: 0.6140\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7641 - val_loss: 0.7493\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5537 - val_loss: 0.5653\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2909 - val_loss: 5.2512\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8960 - val_loss: 0.6410\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4752 - val_loss: 0.5454\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3544 - val_loss: 0.4605\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5585 - val_loss: 1.9664\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5676 - val_loss: 1.2836\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5139 - val_loss: 0.6529\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6879 - val_loss: 0.7840\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5852 - val_loss: 1.8759\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3742 - val_loss: 0.6955\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5005 - val_loss: 0.4146\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5926 - val_loss: 0.8295\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5494 - val_loss: 0.9967\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5192 - val_loss: 2.5745\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4450 - val_loss: 0.6764\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4461 - val_loss: 0.3844\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4885 - val_loss: 1.9173\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9079 - val_loss: 6.3833\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9446 - val_loss: 1.5353\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2682 - val_loss: 0.3570\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2529 - val_loss: 0.4049\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3029 - val_loss: 0.2633\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2818 - val_loss: 1.0130\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4186 - val_loss: 0.4707\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5246 - val_loss: 1.7128\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3376 - val_loss: 0.9004\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3782 - val_loss: 0.5163\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3190 - val_loss: 0.6766\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4245 - val_loss: 0.5407\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3996 - val_loss: 0.9168\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4324 - val_loss: 0.4979\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3386 - val_loss: 1.9167\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4417 - val_loss: 2.1979\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3196 - val_loss: 0.3506\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4105 - val_loss: 0.4560\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2517 - val_loss: 0.5168\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3486 - val_loss: 0.4631\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3397 - val_loss: 0.5419\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4174 - val_loss: 1.1128\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3365 - val_loss: 0.6786\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2657 - val_loss: 0.3476\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3502 - val_loss: 0.8888\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3663 - val_loss: 2.3541\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.8512 - val_loss: 0.2197\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1584 - val_loss: 0.1294\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1349 - val_loss: 0.2375\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1539 - val_loss: 0.1257\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1488 - val_loss: 0.1278\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1511 - val_loss: 0.2945\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2645 - val_loss: 0.4937\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2579 - val_loss: 0.1578\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1661 - val_loss: 0.1370\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3268 - val_loss: 0.4748\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3111 - val_loss: 0.5204\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3683 - val_loss: 0.7620\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2558 - val_loss: 0.4583\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1876 - val_loss: 0.3586\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2342 - val_loss: 0.5255\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3362 - val_loss: 0.4159\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2658 - val_loss: 0.6440\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.0759 - val_loss: 19.5848\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4862 - val_loss: 0.2517\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1217 - val_loss: 0.1336\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1058 - val_loss: 0.1952\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1301 - val_loss: 0.1245\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1315 - val_loss: 0.1494\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1390 - val_loss: 0.2634\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1387 - val_loss: 0.1768\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1738 - val_loss: 0.2602\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2011 - val_loss: 0.2497\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1975 - val_loss: 0.1459\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2456 - val_loss: 1.0295\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2551 - val_loss: 0.5153\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2515 - val_loss: 0.3803\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2314 - val_loss: 0.3932\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2517 - val_loss: 0.3406\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1431 - val_loss: 0.3787\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2606 - val_loss: 0.4225\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2966 - val_loss: 0.5297\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1634 - val_loss: 0.2912\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1947 - val_loss: 0.3898\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3092 - val_loss: 0.4319\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1540 - val_loss: 0.3550\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2644 - val_loss: 0.6727\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2581 - val_loss: 0.1928\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1320 - val_loss: 0.2321\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1829 - val_loss: 0.2773\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2540 - val_loss: 0.3709\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5028 - val_loss: 0.7325\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1404 - val_loss: 0.0967\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0920 - val_loss: 0.1233\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3147 - val_loss: 0.2618\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1447 - val_loss: 0.2408\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1457 - val_loss: 0.9273\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2883 - val_loss: 0.3596\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1216 - val_loss: 0.7183\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1661 - val_loss: 0.2629\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1890 - val_loss: 0.6456\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1804 - val_loss: 0.1841\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1699 - val_loss: 0.4455\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1979 - val_loss: 0.3024\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1654 - val_loss: 0.3284\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1165 - val_loss: 0.1871\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1503 - val_loss: 0.9659\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3283 - val_loss: 0.2378\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1548 - val_loss: 0.2628\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1525 - val_loss: 0.4636\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2081 - val_loss: 0.3753\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1347 - val_loss: 0.1926\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1919 - val_loss: 0.1748\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1318 - val_loss: 0.4646\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1447 - val_loss: 0.6934\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1556 - val_loss: 0.1431\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1550 - val_loss: 2.3066\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2128 - val_loss: 0.4514\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1988 - val_loss: 0.2924\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1865 - val_loss: 0.4214\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2115 - val_loss: 0.2133\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1089 - val_loss: 0.1194\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1696 - val_loss: 0.6589\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.09624886459398845\n",
      "Mean Absolute Error (MAE): 0.22676089188226078\n",
      "Root Mean Squared Error (RMSE): 0.3102400112718997\n",
      "Time taken: 636.1812665462494\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 15ms/step - loss: 1152.4984 - val_loss: 918.7264\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 849.7936 - val_loss: 757.8612\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 621.5644 - val_loss: 566.3522\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 431.4062 - val_loss: 445.2038\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 311.1690 - val_loss: 267.1168\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 205.1649 - val_loss: 267.3858\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 101.2384 - val_loss: 122.2848\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 49.2308 - val_loss: 49.3723\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 25.6139 - val_loss: 18.8808\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 15.6299 - val_loss: 22.9093\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 11.0290 - val_loss: 21.3085\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.5034 - val_loss: 39.1493\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.3228 - val_loss: 6.8660\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.7187 - val_loss: 4.0012\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7582 - val_loss: 18.0402\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.0494 - val_loss: 12.4550\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.8306 - val_loss: 4.7620\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5393 - val_loss: 6.2442\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.0308 - val_loss: 3.2246\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3056 - val_loss: 4.3998\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4505 - val_loss: 4.7440\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.5259 - val_loss: 4.0572\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.5441 - val_loss: 15.3628\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8028 - val_loss: 5.2058\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.3339 - val_loss: 3.8872\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6465 - val_loss: 3.5270\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2942 - val_loss: 6.1131\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9013 - val_loss: 2.4694\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3737 - val_loss: 2.6068\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.9905 - val_loss: 1.8672\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5021 - val_loss: 3.5188\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5397 - val_loss: 2.3223\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2592 - val_loss: 2.6905\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5604 - val_loss: 2.8446\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0547 - val_loss: 2.0371\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9198 - val_loss: 1.4865\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1174 - val_loss: 1.2887\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1343 - val_loss: 2.5366\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2949 - val_loss: 3.2451\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8979 - val_loss: 2.2264\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9291 - val_loss: 1.6856\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.1129 - val_loss: 3.1345\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0510 - val_loss: 1.3233\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7172 - val_loss: 1.3369\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5478 - val_loss: 0.7717\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5278 - val_loss: 2.0350\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8186 - val_loss: 1.7168\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6748 - val_loss: 1.0012\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8043 - val_loss: 3.0466\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6385 - val_loss: 0.9892\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1141 - val_loss: 16.2533\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0651 - val_loss: 1.1219\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5990 - val_loss: 1.6608\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5124 - val_loss: 0.9297\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6588 - val_loss: 0.6468\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5447 - val_loss: 2.5245\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5325 - val_loss: 1.8161\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5628 - val_loss: 2.8477\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5489 - val_loss: 0.9596\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2430 - val_loss: 2.5649\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5523 - val_loss: 0.7785\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3901 - val_loss: 1.0649\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5203 - val_loss: 0.5694\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4326 - val_loss: 0.8190\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4478 - val_loss: 4.1886\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5482 - val_loss: 1.3665\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5502 - val_loss: 0.5405\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5869 - val_loss: 1.0406\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4962 - val_loss: 0.9319\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4926 - val_loss: 1.1689\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4854 - val_loss: 0.9631\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4076 - val_loss: 1.5907\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5581 - val_loss: 2.3436\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4450 - val_loss: 0.7815\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3877 - val_loss: 0.7281\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4443 - val_loss: 1.9157\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4223 - val_loss: 0.9164\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3910 - val_loss: 1.2517\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3178 - val_loss: 0.9462\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4367 - val_loss: 1.2007\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3382 - val_loss: 0.6943\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3011 - val_loss: 0.6810\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4402 - val_loss: 0.6208\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3014 - val_loss: 0.5356\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2712 - val_loss: 0.6834\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3583 - val_loss: 0.9035\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5512 - val_loss: 1.4724\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2690 - val_loss: 0.3765\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3671 - val_loss: 0.9072\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3558 - val_loss: 3.3350\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3820 - val_loss: 0.3846\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2891 - val_loss: 1.4658\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2197 - val_loss: 0.3060\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2529 - val_loss: 0.2699\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4879 - val_loss: 0.5247\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4044 - val_loss: 0.4456\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4195 - val_loss: 0.5660\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1669 - val_loss: 0.2728\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2553 - val_loss: 0.4340\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2390 - val_loss: 0.8970\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2511 - val_loss: 0.5445\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2432 - val_loss: 5.1389\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3129 - val_loss: 0.6901\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2792 - val_loss: 1.1703\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1846 - val_loss: 0.3927\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3383 - val_loss: 1.5753\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1754 - val_loss: 0.6906\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2575 - val_loss: 0.2897\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2326 - val_loss: 0.6462\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3222 - val_loss: 1.4672\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1920 - val_loss: 0.2640\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1953 - val_loss: 0.2185\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2680 - val_loss: 1.0131\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2289 - val_loss: 0.2428\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2787 - val_loss: 6.4574\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3146 - val_loss: 0.6919\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2125 - val_loss: 0.2767\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2110 - val_loss: 1.4460\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1777 - val_loss: 0.2590\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2017 - val_loss: 0.5214\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2372 - val_loss: 0.5961\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2265 - val_loss: 0.4928\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2160 - val_loss: 0.5063\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1662 - val_loss: 0.4254\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1970 - val_loss: 0.7735\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1674 - val_loss: 0.3479\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1810 - val_loss: 0.6384\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2247 - val_loss: 0.3990\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2475 - val_loss: 0.6001\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2295 - val_loss: 0.8264\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1978 - val_loss: 0.3434\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1578 - val_loss: 0.3029\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1753 - val_loss: 0.1839\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2268 - val_loss: 0.3283\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1519 - val_loss: 0.2139\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1510 - val_loss: 0.2334\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1622 - val_loss: 0.2641\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1742 - val_loss: 0.6343\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2168 - val_loss: 0.3078\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1424 - val_loss: 0.1968\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1696 - val_loss: 3.1483\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1907 - val_loss: 0.2871\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2985 - val_loss: 0.9128\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1502 - val_loss: 0.2634\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1234 - val_loss: 0.5257\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2090 - val_loss: 0.4215\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3209 - val_loss: 0.3838\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1224 - val_loss: 0.2587\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1038 - val_loss: 0.1557\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1119 - val_loss: 0.2215\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1135 - val_loss: 0.3357\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2521 - val_loss: 0.7174\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1199 - val_loss: 0.2088\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1874 - val_loss: 0.7523\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1484 - val_loss: 0.1514\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1323 - val_loss: 0.3401\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1920 - val_loss: 0.1797\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1268 - val_loss: 0.2030\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1178 - val_loss: 0.2100\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1406 - val_loss: 0.4097\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1591 - val_loss: 0.2569\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1471 - val_loss: 0.3500\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1521 - val_loss: 0.2776\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1150 - val_loss: 0.4153\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1505 - val_loss: 0.1936\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1309 - val_loss: 0.3480\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1114 - val_loss: 0.1414\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1172 - val_loss: 0.3113\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1491 - val_loss: 0.4008\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1152 - val_loss: 0.4649\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1668 - val_loss: 0.1989\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1452 - val_loss: 0.2947\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1505 - val_loss: 0.7373\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1464 - val_loss: 0.1277\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0910 - val_loss: 0.3097\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1057 - val_loss: 0.1512\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1380 - val_loss: 0.6752\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1288 - val_loss: 0.5694\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0916 - val_loss: 0.1495\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1194 - val_loss: 0.4905\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1459 - val_loss: 0.3096\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1094 - val_loss: 0.2899\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1310 - val_loss: 0.1473\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0929 - val_loss: 0.1767\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0987 - val_loss: 0.2829\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1612 - val_loss: 0.2255\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1336 - val_loss: 0.7799\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1149 - val_loss: 0.2194\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0708 - val_loss: 0.1819\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0923 - val_loss: 0.2871\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1272 - val_loss: 0.4313\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1527 - val_loss: 0.4798\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1497 - val_loss: 0.2872\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1024 - val_loss: 0.1714\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0955 - val_loss: 0.3032\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1208 - val_loss: 0.4116\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0847 - val_loss: 0.1117\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1079 - val_loss: 0.1801\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1035 - val_loss: 0.2661\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1082 - val_loss: 0.2495\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.24954826778863837\n",
      "Mean Absolute Error (MAE): 0.35921861536368466\n",
      "Root Mean Squared Error (RMSE): 0.49954806354207637\n",
      "Time taken: 738.066374540329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 24, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 24, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 24, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 24, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_9168\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.113493  0.252570  0.336887  488.981042\n",
      "1        2  0.175937  0.299764  0.419448  405.557685\n",
      "2        3  0.124148  0.262787  0.352347  509.759266\n",
      "3        4  0.096249  0.226761  0.310240  636.181267\n",
      "4        5  0.249548  0.359219  0.499548  738.066375\n",
      "5  Average  0.151875  0.280220  0.383694  555.709127\n",
      "Results saved to 'Sensors 24_PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 24_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 24_PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
