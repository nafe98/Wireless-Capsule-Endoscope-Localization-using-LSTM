{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8  \n",
       "0     67.810228  77.510547  \n",
       "1     67.636949  77.055207  \n",
       "2     67.468015  76.608876  \n",
       "3     67.304084  76.171754  \n",
       "4     67.145806  75.743710  \n",
       "...         ...        ...  \n",
       "2438  69.085982  71.842437  \n",
       "2439  68.987498  71.977737  \n",
       "2440  68.891677  72.111296  \n",
       "2441  68.798594  72.242943  \n",
       "2442  68.708527  72.372422  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1054.1698 - val_loss: 788.8247\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 586.7299 - val_loss: 493.0639\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 431.8057 - val_loss: 396.1748\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 362.2932 - val_loss: 352.8174\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 274.2677 - val_loss: 219.2497\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 170.0526 - val_loss: 124.2194\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 98.1404 - val_loss: 71.1825\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 66.0716 - val_loss: 51.4947\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 51.0261 - val_loss: 55.8113\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 39.3626 - val_loss: 56.6424\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 36.6198 - val_loss: 36.0448\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 30.1323 - val_loss: 23.6078\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 28.6771 - val_loss: 40.5085\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 24.2556 - val_loss: 26.6507\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 22.2438 - val_loss: 17.7370\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 20.9574 - val_loss: 14.2347\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 18.8257 - val_loss: 20.7058\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.2780 - val_loss: 15.6376\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.3599 - val_loss: 22.5267\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.5071 - val_loss: 16.4303\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.5121 - val_loss: 34.4542\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.2752 - val_loss: 13.2876\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.8553 - val_loss: 15.3966\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.9476 - val_loss: 8.1230\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9030 - val_loss: 21.3877\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 9.9778 - val_loss: 12.0420\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.4042 - val_loss: 7.9180\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.7109 - val_loss: 13.7776\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.3196 - val_loss: 9.4208\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.2609 - val_loss: 11.2063\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.4876 - val_loss: 19.2404\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0945 - val_loss: 8.1863\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.2601 - val_loss: 13.8218\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 7.9083 - val_loss: 7.3621\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3699 - val_loss: 6.0690\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0864 - val_loss: 4.9228\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.3158 - val_loss: 4.4414\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4514 - val_loss: 8.4382\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2881 - val_loss: 2.8566\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7509 - val_loss: 3.1762\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2672 - val_loss: 4.7476\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5282 - val_loss: 3.3639\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.3425 - val_loss: 6.2996\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9264 - val_loss: 2.5972\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0349 - val_loss: 3.6276\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8760 - val_loss: 3.1378\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.6576 - val_loss: 5.4715\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1121 - val_loss: 5.8324\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4329 - val_loss: 1.4400\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.4623 - val_loss: 4.9055\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3598 - val_loss: 4.5587\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6476 - val_loss: 24.3417\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9773 - val_loss: 5.2868\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.1798 - val_loss: 4.5512\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6822 - val_loss: 4.7397\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.0503 - val_loss: 2.7959\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9690 - val_loss: 1.6665\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9005 - val_loss: 3.5052\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8368 - val_loss: 2.9950\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8040 - val_loss: 1.1308\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7334 - val_loss: 14.4188\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1598 - val_loss: 8.1395\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3069 - val_loss: 1.6321\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4089 - val_loss: 1.7381\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0819 - val_loss: 1.6260\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2796 - val_loss: 0.8371\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4695 - val_loss: 1.0355\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1613 - val_loss: 5.0870\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3813 - val_loss: 1.4776\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1642 - val_loss: 13.8843\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8675 - val_loss: 0.7815\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8568 - val_loss: 1.7464\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1344 - val_loss: 1.1203\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5813 - val_loss: 0.8325\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7077 - val_loss: 40.1380\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5267 - val_loss: 2.4154\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1457 - val_loss: 0.5787\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9774 - val_loss: 0.9603\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9041 - val_loss: 0.7086\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 19ms/step - loss: 2.2406 - val_loss: 2.3690\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.5922 - val_loss: 1.4765\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7631 - val_loss: 1.2356\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9961 - val_loss: 4.5545\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7583 - val_loss: 1.3205\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6316 - val_loss: 1.5536\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.8387 - val_loss: 0.9677\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6902 - val_loss: 2.3018\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.9016 - val_loss: 24.1017\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 1.7754 - val_loss: 2.3048\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 29s 89ms/step - loss: 0.7369 - val_loss: 0.4980\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.5360 - val_loss: 0.9357\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 4.9954 - val_loss: 83.8696\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 4.5537 - val_loss: 0.6434\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6746 - val_loss: 30.0854\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.7312 - val_loss: 7.3367\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4316 - val_loss: 0.4170\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.5234 - val_loss: 0.5966\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.6369 - val_loss: 1.1031\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.6815 - val_loss: 0.6069\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.5153 - val_loss: 0.3977\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.6920 - val_loss: 1.5172\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6865 - val_loss: 0.6177\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6578 - val_loss: 1.2033\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2251 - val_loss: 0.4207\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7395 - val_loss: 20.4811\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.1746 - val_loss: 5.6641\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5974 - val_loss: 3.7477\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7477 - val_loss: 2.5189\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5560 - val_loss: 2.3324\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6943 - val_loss: 0.4812\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7954 - val_loss: 0.4685\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4386 - val_loss: 0.3826\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5051 - val_loss: 0.4947\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4936 - val_loss: 0.5493\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8566 - val_loss: 0.7545\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4702 - val_loss: 1.6220\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.6183 - val_loss: 23.4867\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1411 - val_loss: 0.3350\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5722 - val_loss: 1.4170\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7298 - val_loss: 0.5074\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3768 - val_loss: 0.2700\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.3341 - val_loss: 2.7382\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7708 - val_loss: 0.6761\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3470 - val_loss: 0.3066\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.8535 - val_loss: 0.3735\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3168 - val_loss: 0.2174\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3025 - val_loss: 0.6215\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4120 - val_loss: 0.2732\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2880 - val_loss: 0.5141\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.7103 - val_loss: 39.1179\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6489 - val_loss: 0.3549\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3201 - val_loss: 0.6153\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3206 - val_loss: 0.2699\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4691 - val_loss: 0.4615\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5148 - val_loss: 0.3364\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2855 - val_loss: 0.3899\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4313 - val_loss: 0.3700\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2811 - val_loss: 0.6018\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5096 - val_loss: 3.4203\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2466 - val_loss: 0.6956\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3110 - val_loss: 0.3430\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3119 - val_loss: 0.3089\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2419 - val_loss: 0.2434\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5459 - val_loss: 0.2652\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4646 - val_loss: 5.7349\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.4324 - val_loss: 1.3453\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 3.0304 - val_loss: 23.5252\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.9255 - val_loss: 2.6051\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5850 - val_loss: 0.3407\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2326 - val_loss: 0.1936\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2459 - val_loss: 0.2412\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2756 - val_loss: 0.4518\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2738 - val_loss: 2.9515\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3290 - val_loss: 0.2542\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6034 - val_loss: 7.0336\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.6093 - val_loss: 0.2282\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.3386 - val_loss: 3.8116\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 2.0581 - val_loss: 0.3469\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 20ms/step - loss: 0.4891 - val_loss: 0.4762\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2360 - val_loss: 0.4563\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2502 - val_loss: 0.2368\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2507 - val_loss: 0.4636\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3767 - val_loss: 0.3008\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1991 - val_loss: 0.4394\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4029 - val_loss: 0.2599\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3964 - val_loss: 0.2491\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2813 - val_loss: 0.7095\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7128 - val_loss: 0.4209\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3425 - val_loss: 9.1983\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.8417 - val_loss: 0.2116\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8303 - val_loss: 1.5198\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2327 - val_loss: 0.1401\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2023 - val_loss: 0.1787\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2011 - val_loss: 0.1779\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1619 - val_loss: 0.2751\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2374 - val_loss: 3.0895\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.9471 - val_loss: 3.1901\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9701 - val_loss: 67.9770\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2451 - val_loss: 0.2188\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1938 - val_loss: 0.1306\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1752 - val_loss: 0.2023\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8523 - val_loss: 1.3869\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6199 - val_loss: 0.1347\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1318 - val_loss: 0.2731\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2743 - val_loss: 0.5973\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.1631 - val_loss: 0.2512\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.1834 - val_loss: 0.1488\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2091 - val_loss: 0.1550\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1907 - val_loss: 0.3135\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.5665 - val_loss: 4.8727\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5870 - val_loss: 0.1889\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4098 - val_loss: 5.7526\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4507 - val_loss: 0.2076\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1438 - val_loss: 0.1593\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5629 - val_loss: 0.1495\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3548 - val_loss: 0.2612\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2660 - val_loss: 0.3024\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2838 - val_loss: 0.2660\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.2302 - val_loss: 0.1575\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3720 - val_loss: 0.7824\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.7804166827935134\n",
      "Mean Absolute Error (MAE): 0.6294426054541613\n",
      "Root Mean Squared Error (RMSE): 0.8834119553150237\n",
      "Time taken: 1007.9628758430481\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 22ms/step - loss: 1012.5722 - val_loss: 678.0367\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 536.1427 - val_loss: 452.2048\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 388.5152 - val_loss: 382.6394\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 288.5044 - val_loss: 234.0997\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 180.1109 - val_loss: 149.5867\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 111.5747 - val_loss: 125.3694\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 74.4844 - val_loss: 67.6694\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 53.0806 - val_loss: 55.2804\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 42.8252 - val_loss: 38.0172\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 38.4075 - val_loss: 42.0410\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 34.9859 - val_loss: 36.8272\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 29.8633 - val_loss: 32.5140\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 28.0081 - val_loss: 32.8927\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 27.5376 - val_loss: 20.7752\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 23.6756 - val_loss: 23.0397\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.2509 - val_loss: 22.6989\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.9609 - val_loss: 38.1620\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 20.7929 - val_loss: 17.3342\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 18.7955 - val_loss: 14.8214\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 15.6630 - val_loss: 14.2694\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 16.3085 - val_loss: 24.7369\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 17.1511 - val_loss: 15.2901\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 13.6730 - val_loss: 19.8752\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 12.1496 - val_loss: 8.7724\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 10.1200 - val_loss: 14.0093\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 11.4036 - val_loss: 28.3239\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 11.4780 - val_loss: 16.2215\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 11.0341 - val_loss: 15.4541\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.8052 - val_loss: 9.7607\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.3810 - val_loss: 14.9758\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.8335 - val_loss: 7.3561\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.4729 - val_loss: 6.1764\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.4429 - val_loss: 6.4130\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 8.3691 - val_loss: 9.2234\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.1844 - val_loss: 6.2119\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.5289 - val_loss: 5.0228\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.9910 - val_loss: 12.9448\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.4675 - val_loss: 5.6529\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.0974 - val_loss: 4.3714\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 4.2370 - val_loss: 3.7913\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.7097 - val_loss: 4.1683\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.1270 - val_loss: 7.5992\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.2716 - val_loss: 5.2514\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.4627 - val_loss: 10.7368\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.4511 - val_loss: 3.3897\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9553 - val_loss: 2.5641\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.7192 - val_loss: 15.3997\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.0272 - val_loss: 4.5941\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6153 - val_loss: 9.0731\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.4645 - val_loss: 6.4003\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.7188 - val_loss: 1.7396\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.4513 - val_loss: 4.0021\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.4535 - val_loss: 2.8834\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8338 - val_loss: 4.2010\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1084 - val_loss: 2.6901\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9897 - val_loss: 1.8056\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3818 - val_loss: 1.9720\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.8310 - val_loss: 4.6383\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.1596 - val_loss: 7.0520\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5067 - val_loss: 1.2514\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5555 - val_loss: 1.8408\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.7458 - val_loss: 3.5857\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.9612 - val_loss: 11.6711\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.4451 - val_loss: 1.5303\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.9868 - val_loss: 3.0212\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.1670 - val_loss: 1.2790\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7512 - val_loss: 13.8333\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.5134 - val_loss: 2.1855\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7877 - val_loss: 9.3964\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.2869 - val_loss: 1.5897\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2219 - val_loss: 0.9833\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6569 - val_loss: 1.5086\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.7954 - val_loss: 2.0568\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.8096 - val_loss: 3.0796\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.0666 - val_loss: 0.7861\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.7477 - val_loss: 1.2230\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6274 - val_loss: 1.0463\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3533 - val_loss: 1.2939\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8780 - val_loss: 13.1843\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8487 - val_loss: 0.9556\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0174 - val_loss: 1.3101\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1694 - val_loss: 1.5232\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1742 - val_loss: 1.4518\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5037 - val_loss: 13.6050\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6864 - val_loss: 15.4276\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7116 - val_loss: 2.7953\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.3198 - val_loss: 4.0494\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8674 - val_loss: 6.9646\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8911 - val_loss: 0.8451\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7059 - val_loss: 2.5845\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7700 - val_loss: 0.3702\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3085 - val_loss: 11.0926\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9900 - val_loss: 0.8809\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6411 - val_loss: 1.2600\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0042 - val_loss: 1.3313\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6827 - val_loss: 0.7546\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0479 - val_loss: 1.1648\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8114 - val_loss: 1.3782\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7259 - val_loss: 0.5317\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0806 - val_loss: 6.2002\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0721 - val_loss: 0.6570\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7344 - val_loss: 0.3912\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4612 - val_loss: 0.7216\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0620 - val_loss: 0.9668\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5283 - val_loss: 0.7758\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1225 - val_loss: 0.4321\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8563 - val_loss: 1.2101\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6101 - val_loss: 0.5087\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7344 - val_loss: 0.6113\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1003 - val_loss: 1.0347\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2531 - val_loss: 9.9486\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0848 - val_loss: 0.7193\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2324 - val_loss: 0.4687\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4876 - val_loss: 0.3427\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4765 - val_loss: 1.6432\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5404 - val_loss: 0.8253\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9787 - val_loss: 1.1071\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6346 - val_loss: 0.3241\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7804 - val_loss: 0.7546\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5762 - val_loss: 5.3588\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5698 - val_loss: 0.5177\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3570 - val_loss: 0.4138\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0631 - val_loss: 0.4971\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4234 - val_loss: 0.5896\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6678 - val_loss: 1.2299\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3364 - val_loss: 0.8192\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.8487 - val_loss: 1.0140\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4402 - val_loss: 0.3049\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3405 - val_loss: 0.5632\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3463 - val_loss: 0.4906\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5838 - val_loss: 2.5298\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5300 - val_loss: 0.4428\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4195 - val_loss: 0.5775\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8524 - val_loss: 1.3633\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7363 - val_loss: 0.5186\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6619 - val_loss: 1.2504\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6546 - val_loss: 1.3852\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8263 - val_loss: 0.9609\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2034 - val_loss: 82.1735\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1810 - val_loss: 0.3587\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2851 - val_loss: 0.1605\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9151 - val_loss: 0.8829\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3735 - val_loss: 0.2131\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2482 - val_loss: 0.2064\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3090 - val_loss: 0.7380\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3692 - val_loss: 0.3931\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2974 - val_loss: 0.6897\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5666 - val_loss: 0.4918\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8017 - val_loss: 1.2876\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3270 - val_loss: 0.3406\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6315 - val_loss: 0.2644\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2689 - val_loss: 0.2457\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3035 - val_loss: 1.9121\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3180 - val_loss: 0.1687\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1474 - val_loss: 5.5677\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2247 - val_loss: 0.4336\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5025 - val_loss: 0.5168\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3526 - val_loss: 0.2742\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2206 - val_loss: 0.5615\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3970 - val_loss: 0.3717\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3679 - val_loss: 0.2499\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9843 - val_loss: 0.2413\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4464 - val_loss: 0.2530\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4228 - val_loss: 0.2641\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.8705 - val_loss: 0.7421\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 7s 22ms/step - loss: 0.2960 - val_loss: 0.3428\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.2987 - val_loss: 0.5016\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3483 - val_loss: 0.5454\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7948 - val_loss: 26.0751\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.6188 - val_loss: 0.5645\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3079 - val_loss: 0.2695\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.1630374563626267\n",
      "Mean Absolute Error (MAE): 0.2937886605287011\n",
      "Root Mean Squared Error (RMSE): 0.40377896968840105\n",
      "Time taken: 1017.6221783161163\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 21ms/step - loss: 1036.4900 - val_loss: 767.3196\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 621.5574 - val_loss: 542.1467\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 444.7666 - val_loss: 387.7171\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 337.8385 - val_loss: 259.5997\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 228.8484 - val_loss: 159.9663\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 141.7296 - val_loss: 123.6029\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 85.6385 - val_loss: 69.0729\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 59.5777 - val_loss: 56.8848\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 46.9266 - val_loss: 59.7385\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 39.9789 - val_loss: 54.0527\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 34.4623 - val_loss: 34.9493\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 31.8330 - val_loss: 53.0757\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 28.0150 - val_loss: 43.2533\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 26.0434 - val_loss: 22.0164\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 23.7306 - val_loss: 19.1608\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 22.3861 - val_loss: 18.7479\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 19.6804 - val_loss: 52.2589\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 20.3973 - val_loss: 16.6409\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.9202 - val_loss: 27.9757\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 16.3137 - val_loss: 14.9289\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 14.1878 - val_loss: 12.5253\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 13.1519 - val_loss: 12.0117\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 13.0708 - val_loss: 10.4485\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.5296 - val_loss: 13.9341\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.6566 - val_loss: 12.5722\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 10.4213 - val_loss: 12.1135\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 10.3295 - val_loss: 14.8464\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.9642 - val_loss: 15.2389\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.6433 - val_loss: 27.9502\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 9.2412 - val_loss: 9.8802\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 8.0778 - val_loss: 9.3051\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.4385 - val_loss: 7.5473\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.4459 - val_loss: 10.3990\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.0052 - val_loss: 11.2742\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.1173 - val_loss: 9.3249\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.1815 - val_loss: 5.7894\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.9212 - val_loss: 11.4364\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.7074 - val_loss: 4.6301\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.6663 - val_loss: 7.2450\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.6207 - val_loss: 4.1794\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.5250 - val_loss: 32.8932\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.8868 - val_loss: 4.2151\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8033 - val_loss: 2.2237\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.8020 - val_loss: 19.4345\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.8390 - val_loss: 43.9889\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9876 - val_loss: 2.8787\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.4832 - val_loss: 4.8099\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9636 - val_loss: 5.6066\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3817 - val_loss: 2.3923\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2607 - val_loss: 2.5806\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.0206 - val_loss: 11.0341\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.0163 - val_loss: 1.5577\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8099 - val_loss: 11.3664\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8086 - val_loss: 3.2229\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7810 - val_loss: 9.8892\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7934 - val_loss: 2.8311\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1741 - val_loss: 1.5787\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8490 - val_loss: 2.6244\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1715 - val_loss: 1.4733\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7325 - val_loss: 0.9513\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7306 - val_loss: 3.6946\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.0482 - val_loss: 157.6332\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3143 - val_loss: 1.9227\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3643 - val_loss: 3.3197\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4233 - val_loss: 8.4859\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.4339 - val_loss: 1.2609\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3744 - val_loss: 0.8668\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5330 - val_loss: 14.3327\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.4285 - val_loss: 16.3301\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7554 - val_loss: 1.2011\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3084 - val_loss: 1.0117\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8617 - val_loss: 0.7633\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6954 - val_loss: 1.6847\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7439 - val_loss: 1.7163\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2301 - val_loss: 1.2143\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0436 - val_loss: 1.3282\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0915 - val_loss: 0.6503\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0725 - val_loss: 7.4244\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6141 - val_loss: 2.8449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4559 - val_loss: 21.9938\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1452 - val_loss: 1.0026\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3075 - val_loss: 1.6443\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5367 - val_loss: 2.3440\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.2995 - val_loss: 0.7773\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5318 - val_loss: 0.6960\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0748 - val_loss: 0.6935\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3637 - val_loss: 0.6817\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5703 - val_loss: 0.6311\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3199 - val_loss: 2.9827\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8094 - val_loss: 1.0022\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3649 - val_loss: 2.3800\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8991 - val_loss: 1.5911\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6012 - val_loss: 0.6976\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5657 - val_loss: 0.7172\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9378 - val_loss: 1.9406\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5765 - val_loss: 0.4920\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.1384 - val_loss: 7.7146\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8411 - val_loss: 0.9174\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9246 - val_loss: 0.7524\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7305 - val_loss: 1.0540\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5647 - val_loss: 0.3783\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5668 - val_loss: 0.5267\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2247 - val_loss: 3.0718\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4989 - val_loss: 1.2441\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5199 - val_loss: 0.4157\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7085 - val_loss: 0.7315\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4923 - val_loss: 0.5036\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5479 - val_loss: 0.8950\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0535 - val_loss: 5.3708\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6216 - val_loss: 0.5984\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6365 - val_loss: 2.3009\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6202 - val_loss: 0.5126\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5000 - val_loss: 1.2680\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5184 - val_loss: 0.3716\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1784 - val_loss: 2.1809\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5980 - val_loss: 0.6247\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3862 - val_loss: 0.4341\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8534 - val_loss: 0.7115\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2606 - val_loss: 0.7659\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5005 - val_loss: 7.8003\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4287 - val_loss: 0.5966\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4331 - val_loss: 4.8164\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4683 - val_loss: 15.9760\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3520 - val_loss: 0.8412\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3460 - val_loss: 0.3977\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3691 - val_loss: 0.3350\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4361 - val_loss: 0.5433\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5777 - val_loss: 2.6510\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4384 - val_loss: 1.9555\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5924 - val_loss: 37.4423\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6529 - val_loss: 1.9048\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6632 - val_loss: 0.9213\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3680 - val_loss: 0.2151\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2578 - val_loss: 0.4635\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8898 - val_loss: 0.3075\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3043 - val_loss: 0.1982\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.3851 - val_loss: 1.4088\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7358 - val_loss: 0.6106\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5450 - val_loss: 0.2678\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3036 - val_loss: 0.5485\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2747 - val_loss: 0.2221\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3996 - val_loss: 0.6037\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3202 - val_loss: 0.2051\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3246 - val_loss: 0.3456\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2895 - val_loss: 1.3563\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3927 - val_loss: 0.7335\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 2.5413 - val_loss: 5.6104\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.2886 - val_loss: 0.4780\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2904 - val_loss: 0.1930\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0105 - val_loss: 2.0230\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4120 - val_loss: 0.2383\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.3368 - val_loss: 0.4457\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4979 - val_loss: 0.2829\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2369 - val_loss: 0.3463\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2474 - val_loss: 0.4292\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3432 - val_loss: 0.4907\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3783 - val_loss: 0.3784\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3804 - val_loss: 0.2789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2787 - val_loss: 0.5518\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 1.0777 - val_loss: 14.1754\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6605 - val_loss: 0.5656\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5905 - val_loss: 0.6825\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2509 - val_loss: 0.2793\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3170 - val_loss: 0.6085\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4793 - val_loss: 0.2113\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2456 - val_loss: 0.4578\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2452 - val_loss: 1.8235\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2599 - val_loss: 0.3076\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2149 - val_loss: 0.3815\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6348 - val_loss: 0.5067\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3612 - val_loss: 0.3999\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.0071 - val_loss: 7.7807\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7872 - val_loss: 0.3867\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3330 - val_loss: 0.3157\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3601 - val_loss: 0.4480\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4067 - val_loss: 0.5795\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2527 - val_loss: 0.4341\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2571 - val_loss: 0.2515\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6662 - val_loss: 0.4022\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.1891244280550605\n",
      "Mean Absolute Error (MAE): 0.3138465945832997\n",
      "Root Mean Squared Error (RMSE): 0.43488438469903756\n",
      "Time taken: 1034.0740039348602\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 21ms/step - loss: 1034.2592 - val_loss: 860.7092\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 559.1234 - val_loss: 490.1858\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 419.8174 - val_loss: 403.7510\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 353.5635 - val_loss: 363.5797\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 289.8117 - val_loss: 284.2372\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 202.8731 - val_loss: 166.6373\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 118.8462 - val_loss: 99.5362\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 77.6270 - val_loss: 71.5328\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 56.4001 - val_loss: 48.2076\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 43.6209 - val_loss: 47.0641\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 36.3263 - val_loss: 37.8899\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 33.2744 - val_loss: 32.6387\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 29.0912 - val_loss: 27.0698\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 27.8456 - val_loss: 40.4973\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 23.6517 - val_loss: 30.8373\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.5059 - val_loss: 16.7775\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.5118 - val_loss: 17.3685\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 20.1543 - val_loss: 31.5437\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 17.9216 - val_loss: 13.6247\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 16.8851 - val_loss: 14.4362\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 14.3273 - val_loss: 13.3114\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.6087 - val_loss: 14.2965\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.9267 - val_loss: 12.4125\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 12.8584 - val_loss: 19.0257\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.9392 - val_loss: 7.1057\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 10.0053 - val_loss: 10.3212\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.7167 - val_loss: 20.0327\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.7371 - val_loss: 11.6742\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.7562 - val_loss: 7.9259\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 11.3267 - val_loss: 9.4929\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.2231 - val_loss: 8.1701\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.4973 - val_loss: 5.8857\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 7.0096 - val_loss: 8.7035\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.1884 - val_loss: 10.1275\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.7526 - val_loss: 8.5500\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.7578 - val_loss: 7.3596\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.3405 - val_loss: 7.9173\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.2317 - val_loss: 6.9466\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.9924 - val_loss: 8.6730\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.2805 - val_loss: 7.2551\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.9847 - val_loss: 4.3902\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0884 - val_loss: 5.0362\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9853 - val_loss: 15.0913\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.3898 - val_loss: 7.9967\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.3858 - val_loss: 6.0818\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8608 - val_loss: 3.7515\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.1045 - val_loss: 8.8565\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6230 - val_loss: 3.2915\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5781 - val_loss: 3.7651\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.1093 - val_loss: 3.5817\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9595 - val_loss: 8.6970\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.1049 - val_loss: 7.5789\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0272 - val_loss: 1.9592\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5778 - val_loss: 2.1450\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.1395 - val_loss: 2.5972\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1411 - val_loss: 10.9182\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8084 - val_loss: 3.1691\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.8786 - val_loss: 7.0316\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8240 - val_loss: 2.5905\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6886 - val_loss: 4.0428\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6155 - val_loss: 1.7656\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0535 - val_loss: 2.1986\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9112 - val_loss: 2.7728\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9402 - val_loss: 3.2625\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.5345 - val_loss: 2.2319\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4438 - val_loss: 2.0344\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 3.2842 - val_loss: 35.5460\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 3.1878 - val_loss: 66.7700\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8172 - val_loss: 1.1371\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3723 - val_loss: 8.4769\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6289 - val_loss: 2.4599\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2677 - val_loss: 0.9497\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3987 - val_loss: 1.0468\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0808 - val_loss: 1.3993\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.2151 - val_loss: 1.4971\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5381 - val_loss: 0.9456\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.2706 - val_loss: 2.8895\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1149 - val_loss: 0.5996\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.0155 - val_loss: 11.5949\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 20ms/step - loss: 1.9073 - val_loss: 4.5293\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.7913 - val_loss: 5.4622\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2861 - val_loss: 2.4941\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9101 - val_loss: 0.7008\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7188 - val_loss: 18.8858\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9262 - val_loss: 1.5549\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9480 - val_loss: 0.5821\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4049 - val_loss: 0.6675\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1875 - val_loss: 5.5922\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.6863 - val_loss: 1.1941\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8579 - val_loss: 0.4176\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5522 - val_loss: 0.5271\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6771 - val_loss: 3.9229\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3020 - val_loss: 0.7422\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7568 - val_loss: 2.4832\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8605 - val_loss: 0.3498\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6518 - val_loss: 0.8713\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4770 - val_loss: 21.5493\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7407 - val_loss: 0.6056\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5106 - val_loss: 3.7296\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2633 - val_loss: 1.7621\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9481 - val_loss: 5.3327\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5549 - val_loss: 0.4204\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4784 - val_loss: 0.3729\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9249 - val_loss: 1.4341\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8858 - val_loss: 1.0295\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.4499 - val_loss: 0.6541\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5791 - val_loss: 0.4878\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9712 - val_loss: 1.2886\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6159 - val_loss: 0.4056\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5241 - val_loss: 0.5404\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5707 - val_loss: 0.6978\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.4703 - val_loss: 1.1224\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7404 - val_loss: 5.1182\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1502 - val_loss: 0.6025\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3723 - val_loss: 0.3917\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2992 - val_loss: 0.3479\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5460 - val_loss: 0.3425\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.7532 - val_loss: 27.8538\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7040 - val_loss: 1.0493\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.8683 - val_loss: 0.6102\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6097 - val_loss: 1.1378\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4193 - val_loss: 1.5477\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.7435 - val_loss: 3.4149\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7334 - val_loss: 0.3946\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8201 - val_loss: 0.6487\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3510 - val_loss: 0.4422\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4884 - val_loss: 0.8105\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7934 - val_loss: 0.2419\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0789 - val_loss: 7.4843\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7009 - val_loss: 0.7363\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2862 - val_loss: 23.8671\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1352 - val_loss: 0.2534\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3889 - val_loss: 0.7219\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9222 - val_loss: 1.4148\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5799 - val_loss: 0.3012\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3658 - val_loss: 0.4742\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3528 - val_loss: 0.2381\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2845 - val_loss: 0.5275\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2875 - val_loss: 0.3073\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3433 - val_loss: 0.3905\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.8350 - val_loss: 0.5195\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 7s 21ms/step - loss: 0.8732 - val_loss: 14.2833\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.3049 - val_loss: 0.5029\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2776 - val_loss: 0.3416\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5929 - val_loss: 3.4233\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4927 - val_loss: 0.7170\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7453 - val_loss: 0.9838\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4752 - val_loss: 0.2657\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3588 - val_loss: 1.2561\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0255 - val_loss: 1.2336\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4701 - val_loss: 2.8696\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2847 - val_loss: 0.2454\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0087 - val_loss: 14.2079\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9996 - val_loss: 0.3414\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2885 - val_loss: 0.2212\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2203 - val_loss: 0.1853\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4050 - val_loss: 5.8581\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.3822 - val_loss: 0.1721\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2144 - val_loss: 0.2216\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2280 - val_loss: 0.3484\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2456 - val_loss: 0.3961\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5829 - val_loss: 10.4920\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5938 - val_loss: 0.6615\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1034 - val_loss: 0.2351\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2023 - val_loss: 0.1281\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3083 - val_loss: 0.4636\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3507 - val_loss: 1.1089\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2353 - val_loss: 0.3240\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2405 - val_loss: 0.3372\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2777 - val_loss: 0.3125\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6358 - val_loss: 19.2410\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5144 - val_loss: 0.2300\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2800 - val_loss: 0.9345\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7894 - val_loss: 0.9629\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5499 - val_loss: 1.3210\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3230 - val_loss: 0.7292\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2907 - val_loss: 1.3311\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4318 - val_loss: 0.1637\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2216 - val_loss: 3.6436\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3901 - val_loss: 0.1768\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2991 - val_loss: 1.1791\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8192 - val_loss: 0.5269\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0200 - val_loss: 1.4784\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3248 - val_loss: 0.1868\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0108 - val_loss: 0.5535\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0017 - val_loss: 1.2880\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1822 - val_loss: 0.1731\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1868 - val_loss: 0.1985\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1553 - val_loss: 0.1557\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1491 - val_loss: 0.1850\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3070 - val_loss: 1.0159\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1884 - val_loss: 0.3732\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5269 - val_loss: 0.3955\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0136 - val_loss: 1.7360\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2242 - val_loss: 0.1681\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.12808131657240732\n",
      "Mean Absolute Error (MAE): 0.2558845749666963\n",
      "Root Mean Squared Error (RMSE): 0.3578845017214455\n",
      "Time taken: 1146.3543584346771\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 22ms/step - loss: 1056.1799 - val_loss: 951.0331\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 624.6609 - val_loss: 528.6909\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 443.0333 - val_loss: 392.5520\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 355.3135 - val_loss: 327.4078\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 275.0979 - val_loss: 265.5787\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 180.6971 - val_loss: 139.5197\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 105.7189 - val_loss: 98.2488\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 68.8601 - val_loss: 64.4712\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 49.6251 - val_loss: 45.1605\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 39.8856 - val_loss: 37.7040\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 36.4332 - val_loss: 52.5485\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 32.8493 - val_loss: 42.4660\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 26.2350 - val_loss: 27.4950\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 24.3809 - val_loss: 22.0367\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.8280 - val_loss: 34.7285\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 21.6521 - val_loss: 28.8971\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 18.1439 - val_loss: 24.8451\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 17.6168 - val_loss: 15.0633\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 15.0676 - val_loss: 19.1562\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 14.7517 - val_loss: 24.1604\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 13.8312 - val_loss: 17.7644\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 12.5480 - val_loss: 22.6313\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 11.6990 - val_loss: 20.1804\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 11.0509 - val_loss: 16.5290\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.7758 - val_loss: 11.6228\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 10.9434 - val_loss: 12.0261\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.4836 - val_loss: 13.8994\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 10.3588 - val_loss: 12.0696\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 8.1738 - val_loss: 9.5371\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.2494 - val_loss: 23.3878\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.9688 - val_loss: 8.4626\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 6.2193 - val_loss: 10.2449\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.2301 - val_loss: 8.4207\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.5413 - val_loss: 8.9903\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.0948 - val_loss: 7.3318\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 7.1167 - val_loss: 11.3282\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.8063 - val_loss: 11.1596\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.0643 - val_loss: 5.0983\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.9010 - val_loss: 13.9855\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.6084 - val_loss: 6.3771\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 4.7639 - val_loss: 3.5664\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.5904 - val_loss: 2.8600\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.4309 - val_loss: 3.9731\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.6504 - val_loss: 5.4471\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 5.0226 - val_loss: 15.6847\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0590 - val_loss: 14.4485\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 4.0214 - val_loss: 2.0749\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.3534 - val_loss: 2.5023\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.6100 - val_loss: 4.0558\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.1343 - val_loss: 3.4510\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6813 - val_loss: 2.4008\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.5310 - val_loss: 5.5336\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2544 - val_loss: 3.8064\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.4482 - val_loss: 2.7121\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9968 - val_loss: 3.8155\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7053 - val_loss: 1.6241\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1176 - val_loss: 2.7327\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.4255 - val_loss: 3.0761\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9930 - val_loss: 5.4612\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.9515 - val_loss: 3.1521\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1471 - val_loss: 2.9961\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.7841 - val_loss: 21.0962\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4472 - val_loss: 1.3866\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.7296 - val_loss: 3.2109\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7842 - val_loss: 1.3174\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7736 - val_loss: 3.1894\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0625 - val_loss: 5.1398\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.2444 - val_loss: 3.7470\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7035 - val_loss: 1.5547\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4643 - val_loss: 27.1868\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9352 - val_loss: 1.3894\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5806 - val_loss: 25.6591\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9848 - val_loss: 0.9990\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7687 - val_loss: 6.3722\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6964 - val_loss: 2.1774\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.9677 - val_loss: 1.4127\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.2694 - val_loss: 10.1672\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5791 - val_loss: 1.1931\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.2474 - val_loss: 1.0390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8815 - val_loss: 1.5231\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3241 - val_loss: 76.5331\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4248 - val_loss: 4.0047\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9010 - val_loss: 13.1657\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0103 - val_loss: 3.0413\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5189 - val_loss: 0.6855\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7681 - val_loss: 0.9941\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.9521 - val_loss: 6.3730\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6872 - val_loss: 0.4459\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9129 - val_loss: 2.9021\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7147 - val_loss: 24.2037\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.2405 - val_loss: 1.2359\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1923 - val_loss: 0.6546\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5938 - val_loss: 7.8069\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 1.4484 - val_loss: 4.5282\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.8495 - val_loss: 0.5586\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4056 - val_loss: 17.1040\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6943 - val_loss: 0.7440\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5620 - val_loss: 2.4232\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9877 - val_loss: 0.7837\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4837 - val_loss: 0.4813\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1041 - val_loss: 1.1348\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5003 - val_loss: 0.5179\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6396 - val_loss: 0.6864\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9831 - val_loss: 2.0562\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5795 - val_loss: 0.5287\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5055 - val_loss: 0.3966\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3469 - val_loss: 1.3302\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6141 - val_loss: 0.4478\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7556 - val_loss: 17.5896\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.6951 - val_loss: 1.4524\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4961 - val_loss: 0.3723\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3821 - val_loss: 0.3644\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0888 - val_loss: 40.4188\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.5555 - val_loss: 0.5844\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5049 - val_loss: 3.7563\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4802 - val_loss: 0.6306\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5558 - val_loss: 0.8185\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7948 - val_loss: 1.8135\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.1755 - val_loss: 22.2026\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7375 - val_loss: 0.3264\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4183 - val_loss: 0.2870\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3598 - val_loss: 1.0720\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7330 - val_loss: 0.9398\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9430 - val_loss: 0.8876\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0803 - val_loss: 78.8060\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2097 - val_loss: 0.9964\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4255 - val_loss: 0.4542\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2781 - val_loss: 0.2487\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2766 - val_loss: 0.2441\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3703 - val_loss: 0.3585\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.6034 - val_loss: 42.8115\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9790 - val_loss: 0.5702\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3392 - val_loss: 0.5603\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8454 - val_loss: 2.3785\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3924 - val_loss: 0.2911\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3329 - val_loss: 0.3091\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3104 - val_loss: 0.4455\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2305 - val_loss: 0.2986\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0987 - val_loss: 0.7546\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9178 - val_loss: 2.0130\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7200 - val_loss: 0.5956\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2606 - val_loss: 0.1787\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7700 - val_loss: 3.3131\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9564 - val_loss: 3.9465\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6577 - val_loss: 0.3162\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2394 - val_loss: 0.3394\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7753 - val_loss: 1.5853\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6924 - val_loss: 0.3366\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2477 - val_loss: 0.3194\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2649 - val_loss: 0.6570\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5368 - val_loss: 1.1055\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6832 - val_loss: 0.7726\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2458 - val_loss: 0.3929\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4916 - val_loss: 37.5974\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.6534 - val_loss: 0.9773\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4531 - val_loss: 0.2989\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2403 - val_loss: 0.5858\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2961 - val_loss: 0.5568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2088 - val_loss: 0.3648\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2629 - val_loss: 0.2106\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1909 - val_loss: 0.1874\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2634 - val_loss: 1.1022\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.6248 - val_loss: 2.2809\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 2.1697 - val_loss: 0.4725\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2847 - val_loss: 0.2271\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.1868 - val_loss: 0.2339\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1865 - val_loss: 0.7428\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3515 - val_loss: 0.5452\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3472 - val_loss: 0.9054\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8705 - val_loss: 1.3943\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2943 - val_loss: 0.9681\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8531 - val_loss: 0.1770\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2531 - val_loss: 2.7162\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4839 - val_loss: 0.8726\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3481 - val_loss: 0.3004\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1642 - val_loss: 0.3673\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1963 - val_loss: 0.1761\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1646 - val_loss: 0.2188\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5786 - val_loss: 1.0699\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.1865 - val_loss: 0.8342\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5069 - val_loss: 0.1777\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2332 - val_loss: 0.2555\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2266 - val_loss: 1.1510\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2263 - val_loss: 0.2324\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2524 - val_loss: 0.3239\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2403 - val_loss: 11.4138\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7970 - val_loss: 0.4321\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2146 - val_loss: 0.2158\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1591 - val_loss: 0.1304\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1615 - val_loss: 0.1893\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1931 - val_loss: 0.2020\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1922 - val_loss: 0.2093\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.5798 - val_loss: 1.8362\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3049 - val_loss: 0.1741\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1695 - val_loss: 0.1373\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1570 - val_loss: 0.1485\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5329 - val_loss: 0.4386\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2239 - val_loss: 1.3210\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1951 - val_loss: 0.1663\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8348 - val_loss: 0.8315\n",
      "16/16 [==============================] - 1s 10ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.8239300019840042\n",
      "Mean Absolute Error (MAE): 0.5897029952922277\n",
      "Root Mean Squared Error (RMSE): 0.9077059005999709\n",
      "Time taken: 1179.1761152744293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 512)            0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_14300\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.780417  0.629443  0.883412  1007.962876\n",
      "1        2  0.163037  0.293789  0.403779  1017.622178\n",
      "2        3  0.189124  0.313847  0.434884  1034.074004\n",
      "3        4  0.128081  0.255885  0.357885  1146.354358\n",
      "4        5  0.823930  0.589703  0.907706  1179.176115\n",
      "5  Average  0.416918  0.416533  0.597533  1077.037906\n",
      "Results saved to 'Sensors 8_PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 8_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 8_PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
