{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 22ms/step - loss: 1113.0892 - val_loss: 955.6058\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 912.9142 - val_loss: 951.5126\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 904.7828 - val_loss: 996.1735\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 797.3184 - val_loss: 762.7744\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 563.6332 - val_loss: 426.0662\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 271.6343 - val_loss: 217.9253\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 115.4342 - val_loss: 87.4109\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 51.4809 - val_loss: 193.7655\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 25.5856 - val_loss: 22.9930\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 16.9043 - val_loss: 14.1704\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.3866 - val_loss: 8.1420\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.7965 - val_loss: 7.8553\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.2531 - val_loss: 5.0408\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.3463 - val_loss: 8.0041\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.6407 - val_loss: 4.1201\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.5964 - val_loss: 5.5449\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7874 - val_loss: 9.0914\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.8077 - val_loss: 5.6529\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4505 - val_loss: 2.2708\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 2.1588 - val_loss: 5.4313\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0052 - val_loss: 4.2816\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5785 - val_loss: 2.0716\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.4258 - val_loss: 2.9424\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5650 - val_loss: 3.4040\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7394 - val_loss: 8.1060\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.6577 - val_loss: 4.2375\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.7623 - val_loss: 2.4848\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4402 - val_loss: 1.6610\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7884 - val_loss: 3.6134\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.2880 - val_loss: 4.1208\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3877 - val_loss: 1.9797\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3963 - val_loss: 2.7562\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9747 - val_loss: 1.3834\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3453 - val_loss: 5.9440\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.1032 - val_loss: 1.1555\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8088 - val_loss: 1.3583\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1148 - val_loss: 3.2428\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1898 - val_loss: 4.6822\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0940 - val_loss: 1.0257\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6147 - val_loss: 1.4497\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7806 - val_loss: 1.7738\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 4.7115 - val_loss: 3.7878\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7825 - val_loss: 0.8635\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4679 - val_loss: 0.5233\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4986 - val_loss: 0.6232\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4617 - val_loss: 0.5651\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5348 - val_loss: 0.6648\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5031 - val_loss: 0.6222\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1540 - val_loss: 2.7388\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4454 - val_loss: 0.9834\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7296 - val_loss: 1.0417\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7486 - val_loss: 3.3867\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8792 - val_loss: 2.7893\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6460 - val_loss: 1.2110\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6281 - val_loss: 1.0469\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6704 - val_loss: 1.1487\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7041 - val_loss: 1.0060\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5739 - val_loss: 0.5573\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4208 - val_loss: 0.5199\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8981 - val_loss: 2.9626\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6122 - val_loss: 1.0960\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5333 - val_loss: 0.7776\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6241 - val_loss: 1.1185\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5478 - val_loss: 1.0213\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4542 - val_loss: 1.4946\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5787 - val_loss: 1.8226\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7066 - val_loss: 1.3508\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7132 - val_loss: 0.5773\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3400 - val_loss: 0.5998\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3919 - val_loss: 1.4931\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.4965 - val_loss: 1.2280\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4831 - val_loss: 0.8739\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3467 - val_loss: 0.7033\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2999 - val_loss: 1.1024\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6732 - val_loss: 2.7871\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7336 - val_loss: 2.4306\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3397 - val_loss: 0.5344\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2981 - val_loss: 0.8712\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3718 - val_loss: 0.8001\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6014 - val_loss: 0.3824\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3099 - val_loss: 2.3801\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2615 - val_loss: 1.5316\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4898 - val_loss: 0.8150\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3511 - val_loss: 0.4762\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3641 - val_loss: 1.1210\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3728 - val_loss: 0.8667\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4766 - val_loss: 1.9540\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3652 - val_loss: 0.4724\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3726 - val_loss: 0.4091\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2809 - val_loss: 0.4882\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2183 - val_loss: 0.2988\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7851 - val_loss: 2.5721\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2940 - val_loss: 0.3249\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2084 - val_loss: 0.3223\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2740 - val_loss: 0.5787\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3663 - val_loss: 0.6026\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3041 - val_loss: 0.4283\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1982 - val_loss: 1.2462\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3894 - val_loss: 0.4348\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4056 - val_loss: 1.2176\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3331 - val_loss: 0.3742\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2578 - val_loss: 0.7213\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4245 - val_loss: 1.2381\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2565 - val_loss: 1.3704\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3464 - val_loss: 1.6350\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3151 - val_loss: 0.3507\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2171 - val_loss: 0.3903\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2376 - val_loss: 0.2311\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2175 - val_loss: 0.2787\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2873 - val_loss: 0.4836\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3456 - val_loss: 0.2756\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1411 - val_loss: 0.2225\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4589 - val_loss: 0.6409\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2845 - val_loss: 0.3317\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1663 - val_loss: 0.2213\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2654 - val_loss: 0.9586\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7492 - val_loss: 8.2190\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4534 - val_loss: 0.1529\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0938 - val_loss: 0.2027\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1096 - val_loss: 0.2249\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1457 - val_loss: 0.1474\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1991 - val_loss: 0.1125\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1293 - val_loss: 0.2269\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2366 - val_loss: 1.2236\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2658 - val_loss: 0.3568\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2754 - val_loss: 0.3500\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1829 - val_loss: 0.3976\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2103 - val_loss: 0.2991\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1578 - val_loss: 0.3067\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2021 - val_loss: 0.4501\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4716 - val_loss: 0.1719\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1349 - val_loss: 0.2070\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1397 - val_loss: 0.1807\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2087 - val_loss: 0.4484\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1806 - val_loss: 0.3958\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2352 - val_loss: 0.8356\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1907 - val_loss: 0.2336\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1879 - val_loss: 0.2194\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2801 - val_loss: 1.1433\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1625 - val_loss: 0.1204\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1966 - val_loss: 0.3376\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1763 - val_loss: 0.5211\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2715 - val_loss: 1.1983\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2009 - val_loss: 0.2627\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1707 - val_loss: 0.7693\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2163 - val_loss: 0.4529\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1348 - val_loss: 0.3331\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2236 - val_loss: 0.4718\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2342 - val_loss: 0.4803\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1520 - val_loss: 0.2511\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1536 - val_loss: 0.2721\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1614 - val_loss: 0.2555\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.11250532067252893\n",
      "Mean Absolute Error (MAE): 0.24718735437894476\n",
      "Root Mean Squared Error (RMSE): 0.3354181281214969\n",
      "Time taken: 872.5017220973969\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1099.2128 - val_loss: 920.5770\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 920.0570 - val_loss: 906.4085\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 847.8427 - val_loss: 787.6110\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 645.0028 - val_loss: 559.4764\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 325.1973 - val_loss: 210.3442\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 128.7072 - val_loss: 83.5963\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 54.1620 - val_loss: 36.7649\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 23.9611 - val_loss: 21.2615\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 13.5179 - val_loss: 35.9758\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 8.6474 - val_loss: 9.9935\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.8038 - val_loss: 6.5387\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.6321 - val_loss: 7.0261\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.7502 - val_loss: 4.1606\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 3.9615 - val_loss: 5.4229\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0062 - val_loss: 1.8706\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.8058 - val_loss: 5.1165\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 2.0687 - val_loss: 9.8596\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.3891 - val_loss: 1.8624\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7569 - val_loss: 8.8895\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2148 - val_loss: 1.4051\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4606 - val_loss: 1.9492\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.4794 - val_loss: 10.3682\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0408 - val_loss: 3.3030\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.1951 - val_loss: 1.2274\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2570 - val_loss: 1.5569\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6726 - val_loss: 3.0063\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1612 - val_loss: 2.1337\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7173 - val_loss: 6.0066\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.8789 - val_loss: 15.5778\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1247 - val_loss: 1.7649\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8027 - val_loss: 0.7973\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8556 - val_loss: 1.2478\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6806 - val_loss: 3.2242\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3542 - val_loss: 2.0946\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7002 - val_loss: 3.3080\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8649 - val_loss: 1.5633\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8232 - val_loss: 1.9865\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9075 - val_loss: 1.8052\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2727 - val_loss: 3.9196\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6034 - val_loss: 1.1436\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5691 - val_loss: 0.3862\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5881 - val_loss: 1.5734\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9163 - val_loss: 1.3347\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.4834 - val_loss: 1.0549\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.9060 - val_loss: 110.8263\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7650 - val_loss: 0.7691\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3054 - val_loss: 0.7067\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2993 - val_loss: 0.3205\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3444 - val_loss: 0.2399\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4258 - val_loss: 1.9510\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7312 - val_loss: 0.5940\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5337 - val_loss: 0.6221\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4908 - val_loss: 1.1443\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0140 - val_loss: 2.5877\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5220 - val_loss: 1.1385\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4594 - val_loss: 0.7953\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5684 - val_loss: 0.7675\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5529 - val_loss: 1.1577\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.8614 - val_loss: 2.2816\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6354 - val_loss: 0.5501\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3559 - val_loss: 2.5281\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4116 - val_loss: 1.9117\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4666 - val_loss: 2.1640\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4850 - val_loss: 1.3703\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6359 - val_loss: 1.1408\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4309 - val_loss: 1.0060\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5490 - val_loss: 3.8972\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4895 - val_loss: 0.3237\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3803 - val_loss: 0.6070\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3183 - val_loss: 0.4907\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8973 - val_loss: 2.8198\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4302 - val_loss: 0.3298\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2810 - val_loss: 1.7238\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4589 - val_loss: 0.9018\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2748 - val_loss: 0.5669\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4635 - val_loss: 0.4475\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2784 - val_loss: 1.6718\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3633 - val_loss: 1.5044\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4817 - val_loss: 2.5437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.23987426757575447\n",
      "Mean Absolute Error (MAE): 0.367829615357917\n",
      "Root Mean Squared Error (RMSE): 0.4897696066271921\n",
      "Time taken: 457.979994058609\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 20ms/step - loss: 1112.8296 - val_loss: 931.2522\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 912.0496 - val_loss: 939.0148\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 827.9625 - val_loss: 809.6221\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 644.9786 - val_loss: 558.9903\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 353.3359 - val_loss: 265.5838\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 152.1971 - val_loss: 147.6993\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 66.2209 - val_loss: 99.3561\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 34.0874 - val_loss: 56.6632\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 19.5948 - val_loss: 27.6806\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 12.4518 - val_loss: 28.5646\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 8.6683 - val_loss: 32.0007\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.9094 - val_loss: 5.1355\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.3376 - val_loss: 8.5155\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 3.6705 - val_loss: 8.9872\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.0638 - val_loss: 5.4005\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.9584 - val_loss: 6.3414\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.6770 - val_loss: 5.3371\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2235 - val_loss: 9.4923\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.0290 - val_loss: 2.2812\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.3404 - val_loss: 9.0145\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.1042 - val_loss: 2.6598\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7010 - val_loss: 4.6893\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.6415 - val_loss: 3.0824\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.7457 - val_loss: 3.4892\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.3135 - val_loss: 1.2286\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1855 - val_loss: 13.0787\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9495 - val_loss: 1.2747\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.0379 - val_loss: 1.1263\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1991 - val_loss: 2.7411\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0825 - val_loss: 1.5566\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4373 - val_loss: 9.0335\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6110 - val_loss: 0.9293\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9982 - val_loss: 4.7394\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.1121 - val_loss: 0.7570\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9922 - val_loss: 1.3933\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8614 - val_loss: 0.9528\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7332 - val_loss: 1.2217\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7912 - val_loss: 2.1408\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9359 - val_loss: 4.9699\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1509 - val_loss: 1.0012\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9960 - val_loss: 0.8404\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.0597 - val_loss: 1.2229\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.1305 - val_loss: 10.2568\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8603 - val_loss: 0.5312\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5154 - val_loss: 0.9105\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3964 - val_loss: 0.3443\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4675 - val_loss: 0.5064\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4707 - val_loss: 0.7109\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5484 - val_loss: 1.2871\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8637 - val_loss: 1.7760\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6793 - val_loss: 0.9555\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7570 - val_loss: 1.5573\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5645 - val_loss: 0.6744\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6972 - val_loss: 1.5629\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4620 - val_loss: 0.3598\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6022 - val_loss: 3.1833\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9307 - val_loss: 1.1588\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6306 - val_loss: 1.7928\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4440 - val_loss: 1.0343\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6885 - val_loss: 1.5371\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4665 - val_loss: 0.6053\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4791 - val_loss: 0.5204\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6842 - val_loss: 0.7200\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5342 - val_loss: 2.8036\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7780 - val_loss: 0.3342\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5165 - val_loss: 0.3121\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3136 - val_loss: 1.0296\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9185 - val_loss: 1.9793\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5142 - val_loss: 0.8536\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3628 - val_loss: 0.7079\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2705 - val_loss: 0.2423\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2680 - val_loss: 0.8934\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9716 - val_loss: 1.4653\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3381 - val_loss: 0.4986\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3186 - val_loss: 0.4571\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3089 - val_loss: 0.4783\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3694 - val_loss: 0.4981\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4622 - val_loss: 2.3508\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3891 - val_loss: 0.3088\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3278 - val_loss: 0.6373\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3669 - val_loss: 0.5335\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4536 - val_loss: 1.4871\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3565 - val_loss: 1.0546\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3564 - val_loss: 0.7027\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3550 - val_loss: 0.7050\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2454 - val_loss: 0.2986\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3113 - val_loss: 0.3521\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2455 - val_loss: 0.5701\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5739 - val_loss: 0.8312\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3479 - val_loss: 0.4598\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2821 - val_loss: 0.2374\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2121 - val_loss: 0.3199\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1791 - val_loss: 0.2796\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4458 - val_loss: 0.3780\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.1677 - val_loss: 0.1814\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1387 - val_loss: 0.1246\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1071 - val_loss: 0.2491\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1003 - val_loss: 0.1807\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.0923 - val_loss: 0.1835\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1919 - val_loss: 0.2015\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1398 - val_loss: 0.2019\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.1750 - val_loss: 1.0867\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2225 - val_loss: 0.4904\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2284 - val_loss: 0.3302\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2320 - val_loss: 0.2902\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2834 - val_loss: 0.3913\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3280 - val_loss: 0.2997\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1970 - val_loss: 0.9300\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3120 - val_loss: 0.7179\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2824 - val_loss: 0.2290\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1773 - val_loss: 0.2200\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2222 - val_loss: 0.3684\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2835 - val_loss: 1.3633\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2671 - val_loss: 0.5057\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1883 - val_loss: 0.1915\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3853 - val_loss: 0.8504\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1427 - val_loss: 0.4746\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3225 - val_loss: 1.0286\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2733 - val_loss: 0.2542\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1345 - val_loss: 0.1575\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2213 - val_loss: 0.2346\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1986 - val_loss: 0.5537\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2645 - val_loss: 0.3687\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2265 - val_loss: 0.3489\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2489 - val_loss: 0.2653\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1790 - val_loss: 0.2893\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.1246289572692576\n",
      "Mean Absolute Error (MAE): 0.26852816423266807\n",
      "Root Mean Squared Error (RMSE): 0.3530282669550097\n",
      "Time taken: 724.1200749874115\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 21ms/step - loss: 1116.1693 - val_loss: 893.2218\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 929.7738 - val_loss: 884.3204\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 877.7698 - val_loss: 1014.0842\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 684.9012 - val_loss: 640.9161\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 345.1942 - val_loss: 230.3507\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 128.6903 - val_loss: 84.5586\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 54.4669 - val_loss: 75.0534\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 24.1385 - val_loss: 16.8626\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 12.5931 - val_loss: 10.2388\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 9.0360 - val_loss: 7.7623\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 6.7030 - val_loss: 11.0331\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.9948 - val_loss: 4.9693\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.2444 - val_loss: 113.9136\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.1618 - val_loss: 4.3850\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.8330 - val_loss: 8.3057\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 5.6162 - val_loss: 6.5989\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.7041 - val_loss: 2.5631\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.0829 - val_loss: 5.7027\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.7311 - val_loss: 3.6572\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 1.8294 - val_loss: 8.0943\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5821 - val_loss: 2.8858\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.6570 - val_loss: 3.6024\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5568 - val_loss: 11.4056\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.8774 - val_loss: 1.8298\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 4.2240 - val_loss: 1.6303\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0693 - val_loss: 1.1276\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.9272 - val_loss: 2.3223\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 2.2067 - val_loss: 6.7264\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.7559 - val_loss: 0.9720\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2587 - val_loss: 1.9939\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8913 - val_loss: 0.8203\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4229 - val_loss: 28.4531\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0643 - val_loss: 17.7771\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.1601 - val_loss: 2.4113\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.0997 - val_loss: 2.2172\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7162 - val_loss: 2.7683\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.6969 - val_loss: 0.8819\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7795 - val_loss: 1.1617\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5862 - val_loss: 0.7439\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5978 - val_loss: 1.4239\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 20ms/step - loss: 0.9560 - val_loss: 4.6460\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.9671 - val_loss: 5.6996\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.7101 - val_loss: 0.8333\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 7s 20ms/step - loss: 0.9245 - val_loss: 1.6493\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.7105 - val_loss: 1.9646\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6777 - val_loss: 4.0936\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0750 - val_loss: 1.9100\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8164 - val_loss: 2.0259\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.4740 - val_loss: 2.5787\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5918 - val_loss: 0.6910\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3422 - val_loss: 0.8790\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3809 - val_loss: 1.3601\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4541 - val_loss: 0.9146\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4029 - val_loss: 0.6948\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6917 - val_loss: 0.9574\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4913 - val_loss: 2.2177\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5370 - val_loss: 2.1752\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7913 - val_loss: 0.5816\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3979 - val_loss: 0.5998\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5663 - val_loss: 4.3123\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4784 - val_loss: 0.9295\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8360 - val_loss: 0.9325\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5765 - val_loss: 1.1608\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4417 - val_loss: 1.3398\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4292 - val_loss: 1.4758\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4649 - val_loss: 1.9691\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8014 - val_loss: 3.6134\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5059 - val_loss: 0.6849\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3083 - val_loss: 0.6417\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.9340 - val_loss: 0.6824\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2734 - val_loss: 0.4026\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2527 - val_loss: 0.6158\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4607 - val_loss: 0.8041\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4160 - val_loss: 1.2614\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4002 - val_loss: 0.2631\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3980 - val_loss: 1.0375\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3891 - val_loss: 0.6697\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2843 - val_loss: 0.9390\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7411 - val_loss: 0.7385\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3531 - val_loss: 0.4695\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2961 - val_loss: 1.4192\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4167 - val_loss: 2.5501\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6233 - val_loss: 0.6890\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2699 - val_loss: 0.2878\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4726 - val_loss: 3.0989\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3636 - val_loss: 0.3462\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4404 - val_loss: 2.1332\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4384 - val_loss: 0.9622\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2356 - val_loss: 0.7882\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2291 - val_loss: 5.4695\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6064 - val_loss: 0.7017\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2496 - val_loss: 0.5955\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2313 - val_loss: 0.4867\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3508 - val_loss: 0.6403\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3800 - val_loss: 0.9133\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3114 - val_loss: 0.4589\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2150 - val_loss: 0.4530\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3774 - val_loss: 1.2557\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2879 - val_loss: 0.4145\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2993 - val_loss: 1.0902\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.4629 - val_loss: 0.7095\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2190 - val_loss: 0.8507\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2366 - val_loss: 0.3767\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2846 - val_loss: 7.5830\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4684 - val_loss: 0.4600\n",
      "16/16 [==============================] - 1s 6ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.2630914209668027\n",
      "Mean Absolute Error (MAE): 0.3788709228207708\n",
      "Root Mean Squared Error (RMSE): 0.5129243813339377\n",
      "Time taken: 609.5319304466248\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 21ms/step - loss: 1086.6855 - val_loss: 932.3878\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 916.8555 - val_loss: 929.4986\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 871.7759 - val_loss: 838.5776\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 689.9271 - val_loss: 606.8787\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 388.9822 - val_loss: 280.3689\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 167.9574 - val_loss: 140.8877\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 68.4523 - val_loss: 118.1360\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 29.7013 - val_loss: 35.5384\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 16.4671 - val_loss: 51.8537\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 9.9258 - val_loss: 9.5500\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 6.4991 - val_loss: 7.0144\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 5.0859 - val_loss: 4.8717\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 4.5053 - val_loss: 7.3143\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.2114 - val_loss: 2.6681\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.0009 - val_loss: 5.8268\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.6053 - val_loss: 7.1698\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.2884 - val_loss: 9.4072\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 2.4701 - val_loss: 10.8220\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5667 - val_loss: 3.7071\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5838 - val_loss: 2.1694\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.8344 - val_loss: 3.6966\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.5581 - val_loss: 1.4403\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3708 - val_loss: 2.9087\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 1.4149 - val_loss: 4.4766\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.3283 - val_loss: 2.9133\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.4830 - val_loss: 4.4477\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2111 - val_loss: 4.6847\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0428 - val_loss: 1.3309\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2830 - val_loss: 2.0390\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0139 - val_loss: 2.3466\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.5177 - val_loss: 1.0769\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.8838 - val_loss: 1.2694\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.2612 - val_loss: 10.2486\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9054 - val_loss: 1.0501\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0389 - val_loss: 1.7837\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 3.1041 - val_loss: 3.6314\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6295 - val_loss: 0.7857\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.6712 - val_loss: 1.0872\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6660 - val_loss: 0.9252\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5941 - val_loss: 0.9744\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.9465 - val_loss: 2.2749\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5960 - val_loss: 0.7545\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5064 - val_loss: 1.0016\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0663 - val_loss: 3.4945\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8453 - val_loss: 0.7995\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5776 - val_loss: 0.7673\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7086 - val_loss: 2.3945\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 1.0288 - val_loss: 3.7741\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4231 - val_loss: 1.3213\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4797 - val_loss: 1.5200\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7942 - val_loss: 1.5776\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4830 - val_loss: 1.0304\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4180 - val_loss: 0.4127\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 1.0385 - val_loss: 2.3436\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.7277 - val_loss: 1.9441\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3961 - val_loss: 1.5406\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.4973 - val_loss: 1.4049\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4657 - val_loss: 1.1600\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.6036 - val_loss: 2.4556\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.8286 - val_loss: 0.4486\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3735 - val_loss: 0.7380\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3613 - val_loss: 0.5211\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5138 - val_loss: 0.8957\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.7138 - val_loss: 5.8234\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3535 - val_loss: 0.5491\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.6101 - val_loss: 1.2561\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3484 - val_loss: 0.2805\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2747 - val_loss: 0.4496\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4018 - val_loss: 0.8788\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7763 - val_loss: 3.0055\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4660 - val_loss: 0.4381\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3447 - val_loss: 1.1155\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3268 - val_loss: 0.3137\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.5364 - val_loss: 1.0779\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.3953 - val_loss: 0.7289\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.5796 - val_loss: 0.9751\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3280 - val_loss: 0.4479\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2696 - val_loss: 0.4917\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5332 - val_loss: 1.8191\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4112 - val_loss: 0.4108\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2540 - val_loss: 0.2394\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3055 - val_loss: 0.3514\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3546 - val_loss: 0.8139\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3037 - val_loss: 2.8928\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3674 - val_loss: 0.6407\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.3030 - val_loss: 0.5901\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2203 - val_loss: 0.7203\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3534 - val_loss: 1.9587\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.5280 - val_loss: 1.3765\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3447 - val_loss: 0.2328\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1654 - val_loss: 0.3674\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.4561 - val_loss: 2.8639\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.3295 - val_loss: 1.1720\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1920 - val_loss: 0.5353\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2472 - val_loss: 2.8560\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3089 - val_loss: 0.3260\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4216 - val_loss: 0.2413\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2365 - val_loss: 0.3245\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2830 - val_loss: 0.5984\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3138 - val_loss: 2.1820\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2489 - val_loss: 0.9943\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2113 - val_loss: 0.2381\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2781 - val_loss: 0.3965\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4201 - val_loss: 1.1859\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1932 - val_loss: 0.3247\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1958 - val_loss: 0.2651\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1951 - val_loss: 0.5729\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2478 - val_loss: 1.3404\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2550 - val_loss: 0.2771\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 6s 19ms/step - loss: 0.2146 - val_loss: 0.3630\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2946 - val_loss: 0.6329\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2962 - val_loss: 1.5029\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.7483 - val_loss: 0.1301\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1231 - val_loss: 0.0931\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1082 - val_loss: 0.1099\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1113 - val_loss: 0.1937\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2139 - val_loss: 0.5491\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1086 - val_loss: 0.1338\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2335 - val_loss: 0.3227\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.2246 - val_loss: 0.6152\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.4114 - val_loss: 1.0967\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1679 - val_loss: 0.5505\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2090 - val_loss: 0.4637\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1599 - val_loss: 0.2755\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2683 - val_loss: 0.8219\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.2509 - val_loss: 1.4587\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3011 - val_loss: 0.1870\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1097 - val_loss: 0.2689\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1866 - val_loss: 0.6630\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2246 - val_loss: 1.8543\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3084 - val_loss: 0.2734\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1232 - val_loss: 0.1589\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1929 - val_loss: 0.3729\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2254 - val_loss: 0.3157\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2130 - val_loss: 1.8626\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.2220 - val_loss: 0.3421\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1680 - val_loss: 0.2832\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1707 - val_loss: 0.1576\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1431 - val_loss: 0.4479\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1638 - val_loss: 0.1993\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.3667 - val_loss: 0.4594\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1381 - val_loss: 0.1349\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1354 - val_loss: 0.3511\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 6s 18ms/step - loss: 0.1879 - val_loss: 0.3657\n",
      "16/16 [==============================] - 1s 7ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.09306156549762919\n",
      "Mean Absolute Error (MAE): 0.2309166785573308\n",
      "Root Mean Squared Error (RMSE): 0.305059937549376\n",
      "Time taken: 827.2211761474609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 48, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 48, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 48, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 48, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_21348\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.112505  0.247187  0.335418  872.501722\n",
      "1        2  0.239874  0.367830  0.489770  457.979994\n",
      "2        3  0.124629  0.268528  0.353028  724.120075\n",
      "3        4  0.263091  0.378871  0.512924  609.531930\n",
      "4        5  0.093062  0.230917  0.305060  827.221176\n",
      "5  Average  0.166632  0.298667  0.399240  698.270980\n",
      "Results saved to 'Sensors 48_PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('Sensors 48_PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'Sensors 48_PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
