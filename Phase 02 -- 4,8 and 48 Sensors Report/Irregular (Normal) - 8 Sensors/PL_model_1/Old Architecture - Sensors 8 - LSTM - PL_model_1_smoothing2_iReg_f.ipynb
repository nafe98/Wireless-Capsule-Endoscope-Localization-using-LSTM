{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf63fe",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090b68f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     105.960735  134.734917  \n",
       "1     105.788181  134.546280  \n",
       "2     105.613823  134.358052  \n",
       "3     105.437718  134.170555  \n",
       "4     105.260017  133.984101  \n",
       "...          ...         ...  \n",
       "2438  128.827778  113.779812  \n",
       "2439  128.842679  113.832694  \n",
       "2440  128.857569  113.886728  \n",
       "2441  128.872267  113.942389  \n",
       "2442  128.886554  113.999895  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 13s 7ms/step - loss: 1375.5240 - val_loss: 1262.4741\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1213.4388 - val_loss: 1163.9435\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1128.0569 - val_loss: 1090.6406\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1064.2249 - val_loss: 1035.9373\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1016.7841 - val_loss: 995.5220\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 982.2020 - val_loss: 966.6422\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 957.8575 - val_loss: 946.7023\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 941.5357 - val_loss: 933.9656\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 931.2685 - val_loss: 926.1588\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 925.2036 - val_loss: 921.7779\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 920.1068 - val_loss: 914.5901\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 904.3306 - val_loss: 876.0230\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 843.0862 - val_loss: 809.3302\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 787.4183 - val_loss: 759.9201\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 737.1555 - val_loss: 713.3278\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 684.3588 - val_loss: 653.4791\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 632.5141 - val_loss: 611.7692\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 576.8760 - val_loss: 551.9620\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 528.5624 - val_loss: 513.4337\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 487.3633 - val_loss: 462.7428\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 448.2422 - val_loss: 440.0942\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 409.3230 - val_loss: 388.1982\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 372.8644 - val_loss: 353.7081\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 336.0922 - val_loss: 318.6861\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 301.6848 - val_loss: 286.9561\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 271.4041 - val_loss: 260.2749\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 241.9580 - val_loss: 227.0962\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 213.4369 - val_loss: 201.1828\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 188.4026 - val_loss: 177.4086\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 167.9621 - val_loss: 162.2161\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 146.3530 - val_loss: 137.9149\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 127.3853 - val_loss: 121.9042\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 112.0559 - val_loss: 103.5187\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 95.9141 - val_loss: 90.2614\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 81.8106 - val_loss: 78.7454\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 72.2455 - val_loss: 69.2247\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 61.1628 - val_loss: 57.8350\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 53.3048 - val_loss: 54.5971\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 45.4182 - val_loss: 49.0585\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 39.3207 - val_loss: 37.7865\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 35.1097 - val_loss: 34.4916\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 28.2196 - val_loss: 27.2309\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 24.3104 - val_loss: 22.4758\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 21.3824 - val_loss: 20.3752\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 17.7163 - val_loss: 17.5846\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 15.6850 - val_loss: 15.6138\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 13.6482 - val_loss: 14.2114\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 19.6788 - val_loss: 13.8082\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 9.8408 - val_loss: 9.4127\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 7.9787 - val_loss: 7.2816\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 7.1472 - val_loss: 6.6804\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 6.2470 - val_loss: 7.2358\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.3454 - val_loss: 4.4928\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.2125 - val_loss: 4.3054\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.3302 - val_loss: 61.6048\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.0723 - val_loss: 3.3603\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.0139 - val_loss: 2.4804\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8840 - val_loss: 2.4393\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5324 - val_loss: 3.5732\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.4606 - val_loss: 2.2524\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.3788 - val_loss: 1.8825\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.8833 - val_loss: 1.3556\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8804 - val_loss: 6.0226\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.5349 - val_loss: 2.5064\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2230 - val_loss: 1.5538\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.8790 - val_loss: 1.4648\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0319 - val_loss: 1.0768\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0419 - val_loss: 2.3473\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9715 - val_loss: 2.3812\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.9850 - val_loss: 1.1231\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.9362 - val_loss: 0.8360\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9406 - val_loss: 0.9808\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.8205 - val_loss: 2.1472\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7333 - val_loss: 0.3743\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3251 - val_loss: 0.3644\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3707 - val_loss: 0.9811\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.0454 - val_loss: 0.6260\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9053 - val_loss: 0.9566\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.1846 - val_loss: 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7964 - val_loss: 0.3528\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9130 - val_loss: 0.3521\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7452 - val_loss: 0.4413\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1389 - val_loss: 2.8285\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.5158 - val_loss: 1.3821\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0322 - val_loss: 1.6466\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9062 - val_loss: 1.0437\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9688 - val_loss: 2.1681\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8782 - val_loss: 0.6262\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.5677 - val_loss: 0.5437\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3977 - val_loss: 0.3643\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6517 - val_loss: 1.0746\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6001 - val_loss: 0.3303\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5982 - val_loss: 0.4673\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0468 - val_loss: 0.6535\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7955 - val_loss: 0.4046\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.0777 - val_loss: 1.0684\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 3.9727 - val_loss: 16.7361\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.3248 - val_loss: 0.4078\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6755 - val_loss: 0.4866\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6177 - val_loss: 0.4229\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7939 - val_loss: 0.5673\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8467 - val_loss: 0.9052\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0244 - val_loss: 0.4985\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9437 - val_loss: 0.3794\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3956 - val_loss: 1.0215\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6325 - val_loss: 0.3854\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5090 - val_loss: 0.4456\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9472 - val_loss: 1.1227\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7364 - val_loss: 2.2130\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.6207 - val_loss: 1.2935\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4911 - val_loss: 0.3386\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2229 - val_loss: 0.3953\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4892 - val_loss: 0.1165\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2665 - val_loss: 1.1002\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8151 - val_loss: 3.6985\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3733 - val_loss: 0.2632\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8513 - val_loss: 0.6700\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6127 - val_loss: 1.0567\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4810 - val_loss: 0.8982\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 3.2610 - val_loss: 0.8571\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3504 - val_loss: 0.2113\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.2717 - val_loss: 0.2473\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5156 - val_loss: 0.1634\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5441 - val_loss: 0.3732\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5195 - val_loss: 1.2101\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.0441 - val_loss: 0.4216\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.8711 - val_loss: 0.3594\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 1.7011 - val_loss: 0.3547\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 0.3418 - val_loss: 0.4532\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2302 - val_loss: 0.3656\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4952 - val_loss: 0.1749\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.7937 - val_loss: 0.9648\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9998 - val_loss: 0.8609\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0109 - val_loss: 0.2345\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5230 - val_loss: 1.1581\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.6207 - val_loss: 0.3192\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5306 - val_loss: 0.7695\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7532 - val_loss: 1.3270\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 2.8693 - val_loss: 0.2251\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1869 - val_loss: 0.1012\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2339 - val_loss: 0.0928\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3020 - val_loss: 0.4377\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5085 - val_loss: 0.7004\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.0791 - val_loss: 0.5264\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.9106 - val_loss: 0.1458\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2761 - val_loss: 0.3486\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3988 - val_loss: 0.3016\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2411 - val_loss: 0.2673\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1796 - val_loss: 0.0702\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5635 - val_loss: 1.8926\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2790 - val_loss: 0.1755\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.1487 - val_loss: 0.0542\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4785 - val_loss: 0.4302\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3756 - val_loss: 0.6075\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5328 - val_loss: 0.5659\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6007 - val_loss: 0.3162\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4844 - val_loss: 0.2714\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4865 - val_loss: 0.1233\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2533 - val_loss: 0.4018\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.8818 - val_loss: 3.7385\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5064 - val_loss: 0.1337\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1230 - val_loss: 0.0879\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0918 - val_loss: 0.0815\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0898 - val_loss: 0.0887\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2495 - val_loss: 0.5980\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3484 - val_loss: 0.4328\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2094 - val_loss: 0.1090\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2831 - val_loss: 0.1071\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7772 - val_loss: 0.8765\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5828 - val_loss: 0.9717\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3843 - val_loss: 0.1457\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0664 - val_loss: 0.3991\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1491 - val_loss: 0.0878\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4513 - val_loss: 0.5634\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5542 - val_loss: 1.2678\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2706 - val_loss: 0.1581\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2091 - val_loss: 0.7915\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5753 - val_loss: 2.0976\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.1382 - val_loss: 0.1385\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1031 - val_loss: 0.0593\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0949 - val_loss: 0.0694\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0959 - val_loss: 0.2885\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3458 - val_loss: 0.3663\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4921 - val_loss: 0.1742\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.1623 - val_loss: 0.1486\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4141 - val_loss: 0.4314\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5062 - val_loss: 0.5756\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.5717 - val_loss: 0.3284\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8484 - val_loss: 0.8754\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2244 - val_loss: 0.1513\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3517 - val_loss: 0.2083\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4422 - val_loss: 0.4633\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4388 - val_loss: 0.3347\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3003 - val_loss: 0.6094\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 6.5773 - val_loss: 2.4436\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5491 - val_loss: 0.1006\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.0899 - val_loss: 0.0707\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0626 - val_loss: 0.0449\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0754 - val_loss: 0.0701\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1237 - val_loss: 0.4250\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.42381922493488783\n",
      "Mean Absolute Error (MAE): 0.4460961725414485\n",
      "Root Mean Squared Error (RMSE): 0.6510139974953594\n",
      "Time taken: 512.2824854850769\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 9ms/step - loss: 1374.9236 - val_loss: 1268.4711\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1213.1276 - val_loss: 1168.2220\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1128.1826 - val_loss: 1092.8167\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1062.9373 - val_loss: 1034.8348\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1012.2648 - val_loss: 987.1562\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 969.5149 - val_loss: 948.8148\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 925.7524 - val_loss: 901.9845\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 880.3183 - val_loss: 856.4427\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 832.9520 - val_loss: 809.5579\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 780.8578 - val_loss: 754.3599\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 724.6824 - val_loss: 701.4356\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 669.9988 - val_loss: 644.1821\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 618.1068 - val_loss: 594.9408\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 569.8778 - val_loss: 545.7705\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 522.4545 - val_loss: 506.6392\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 479.9334 - val_loss: 460.5546\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 440.7868 - val_loss: 422.8719\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 401.3707 - val_loss: 385.0368\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 364.6389 - val_loss: 347.6998\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 328.9008 - val_loss: 315.4303\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 297.5204 - val_loss: 285.2408\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 268.2212 - val_loss: 256.9303\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 240.6957 - val_loss: 233.6508\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 215.8163 - val_loss: 206.6423\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 192.1271 - val_loss: 184.9933\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 170.9837 - val_loss: 164.5437\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 151.1321 - val_loss: 145.0917\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 132.9875 - val_loss: 127.7475\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 117.4761 - val_loss: 112.0598\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 101.7524 - val_loss: 101.1153\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 90.1791 - val_loss: 85.4556\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 77.2607 - val_loss: 76.1074\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 67.0473 - val_loss: 64.0246\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 57.6750 - val_loss: 55.7389\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 49.8909 - val_loss: 49.1605\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 43.0068 - val_loss: 41.7729\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 37.4205 - val_loss: 36.1218\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 31.9863 - val_loss: 30.5014\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 26.4579 - val_loss: 25.4012\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 22.5575 - val_loss: 21.8102\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 20.0516 - val_loss: 19.0795\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 17.3376 - val_loss: 16.4260\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 14.9376 - val_loss: 13.9058\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 11.8807 - val_loss: 11.9904\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 10.9318 - val_loss: 12.9294\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.3728 - val_loss: 9.2957\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.8196 - val_loss: 8.6841\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.3587 - val_loss: 6.7869\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.3038 - val_loss: 5.3001\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.7070 - val_loss: 5.6731\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.4879 - val_loss: 3.4803\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.4930 - val_loss: 4.6260\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.2209 - val_loss: 2.6847\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.2175 - val_loss: 2.4272\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8125 - val_loss: 2.4375\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6981 - val_loss: 2.0612\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0996 - val_loss: 2.1280\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6792 - val_loss: 1.3274\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0777 - val_loss: 2.0963\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9383 - val_loss: 0.8707\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.7439 - val_loss: 1.6870\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0837 - val_loss: 1.0064\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8309 - val_loss: 1.2234\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2509 - val_loss: 0.8155\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7196 - val_loss: 0.9525\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8539 - val_loss: 0.7413\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5882 - val_loss: 0.4122\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6695 - val_loss: 1.3988\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1232 - val_loss: 1.3982\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7882 - val_loss: 0.7073\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7478 - val_loss: 0.5688\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0619 - val_loss: 0.4685\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6143 - val_loss: 0.4637\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9730 - val_loss: 5.7647\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1892 - val_loss: 0.3286\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4954 - val_loss: 0.6019\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7791 - val_loss: 0.8814\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5569 - val_loss: 0.9846\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9115 - val_loss: 0.3729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3983 - val_loss: 0.6760\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.9949 - val_loss: 0.4629\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0248 - val_loss: 0.3265\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3656 - val_loss: 0.4716\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 1.2969 - val_loss: 0.2529\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.3914 - val_loss: 0.3770\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5125 - val_loss: 0.6660\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9413 - val_loss: 0.6565\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.2036 - val_loss: 3.4160\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.4488 - val_loss: 0.2937\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2356 - val_loss: 0.1485\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2264 - val_loss: 0.9054\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6072 - val_loss: 0.5740\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6629 - val_loss: 1.1433\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3998 - val_loss: 2.5282\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9464 - val_loss: 0.3295\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8291 - val_loss: 0.8481\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5069 - val_loss: 0.1323\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3730 - val_loss: 0.5348\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4089 - val_loss: 0.1651\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5718 - val_loss: 0.1963\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8324 - val_loss: 0.4147\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5531 - val_loss: 0.1930\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7371 - val_loss: 0.4634\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3418 - val_loss: 0.5513\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4452 - val_loss: 0.4704\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8648 - val_loss: 0.6323\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2584 - val_loss: 0.3113\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3725 - val_loss: 0.4284\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.9282 - val_loss: 0.3044\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1010 - val_loss: 0.1446\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0988 - val_loss: 0.1123\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2047 - val_loss: 0.4646\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2519 - val_loss: 0.2275\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4365 - val_loss: 0.6009\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0554 - val_loss: 1.6762\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5267 - val_loss: 0.2532\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2950 - val_loss: 0.2033\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3330 - val_loss: 0.2904\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8164 - val_loss: 2.5642\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3738 - val_loss: 0.5807\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5013 - val_loss: 0.8934\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2614 - val_loss: 0.6384\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5855 - val_loss: 0.2141\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7820 - val_loss: 0.1539\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2488 - val_loss: 0.3503\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5818 - val_loss: 0.2649\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5607 - val_loss: 0.2655\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3769 - val_loss: 0.2648\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4038 - val_loss: 0.1338\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7029 - val_loss: 0.4820\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3256 - val_loss: 0.7295\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5331 - val_loss: 0.3254\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5223 - val_loss: 0.3149\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3193 - val_loss: 0.2481\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3662 - val_loss: 0.2442\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2976 - val_loss: 0.2178\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.3127 - val_loss: 2.8769\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4445 - val_loss: 0.1206\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0948 - val_loss: 0.0789\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0854 - val_loss: 0.6152\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0906 - val_loss: 0.0554\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1175 - val_loss: 1.0449\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3782 - val_loss: 0.1921\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2473 - val_loss: 0.7976\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8821 - val_loss: 0.4908\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2834 - val_loss: 0.3645\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3410 - val_loss: 1.2507\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6035 - val_loss: 0.6906\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2312 - val_loss: 0.2183\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2061 - val_loss: 0.2030\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4341 - val_loss: 2.3765\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8748 - val_loss: 0.2060\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4692 - val_loss: 1.1661\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3993 - val_loss: 0.6693\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1699 - val_loss: 0.1393\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1343 - val_loss: 0.2608\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7313 - val_loss: 0.8599\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3131 - val_loss: 0.1024\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 3.1274 - val_loss: 8.7785\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6087 - val_loss: 0.1111\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0992 - val_loss: 0.3009\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1014 - val_loss: 0.0725\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1021 - val_loss: 0.1437\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1571 - val_loss: 0.2719\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6416 - val_loss: 0.1600\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2498 - val_loss: 0.1982\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2609 - val_loss: 0.1531\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4482 - val_loss: 0.3640\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4832 - val_loss: 0.2649\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1608 - val_loss: 0.1549\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4702 - val_loss: 0.1590\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4949 - val_loss: 0.3015\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1948 - val_loss: 0.9054\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2857 - val_loss: 0.3725\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3186 - val_loss: 0.2465\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2321 - val_loss: 0.3213\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4711 - val_loss: 0.3033\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3575 - val_loss: 0.3443\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2566 - val_loss: 1.0592\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5202 - val_loss: 0.0786\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1635 - val_loss: 0.5615\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5296 - val_loss: 0.4710\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2405 - val_loss: 0.1959\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3489 - val_loss: 3.9213\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.5986 - val_loss: 0.1638\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.0619 - val_loss: 0.0386\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0621 - val_loss: 0.0789\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1140 - val_loss: 0.3203\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2744 - val_loss: 0.1398\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2071 - val_loss: 0.1400\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6973 - val_loss: 0.5160\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2133 - val_loss: 0.0620\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.1351 - val_loss: 0.0855\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.5523 - val_loss: 0.2731\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4272 - val_loss: 0.0844\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1384 - val_loss: 0.2007\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1596 - val_loss: 0.4104\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8619 - val_loss: 0.3688\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.1542 - val_loss: 0.0542\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1787 - val_loss: 0.2337\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.23379669109010948\n",
      "Mean Absolute Error (MAE): 0.3558886252447408\n",
      "Root Mean Squared Error (RMSE): 0.4835252745101433\n",
      "Time taken: 479.6885368824005\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 9ms/step - loss: 1389.0394 - val_loss: 1282.6519\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1213.4352 - val_loss: 1164.4016\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1111.5031 - val_loss: 1069.6074\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1021.8766 - val_loss: 986.7532\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 945.9343 - val_loss: 915.9849\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 881.1920 - val_loss: 856.5290\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 826.2755 - val_loss: 805.4979\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 776.9180 - val_loss: 759.8464\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 733.2192 - val_loss: 718.5894\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 687.1807 - val_loss: 671.5254\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 647.9789 - val_loss: 631.6132\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 604.2016 - val_loss: 589.4299\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 556.9358 - val_loss: 535.8385\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 506.1052 - val_loss: 487.5756\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 463.2770 - val_loss: 446.7173\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 426.6149 - val_loss: 415.0276\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 386.3612 - val_loss: 370.0572\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 352.1550 - val_loss: 336.6363\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 317.2088 - val_loss: 303.1789\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 287.6729 - val_loss: 272.9011\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 257.8929 - val_loss: 247.8134\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 231.4952 - val_loss: 220.8167\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 207.5373 - val_loss: 195.0140\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 182.4829 - val_loss: 173.2960\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 161.0955 - val_loss: 150.4395\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 142.5153 - val_loss: 132.1773\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 124.5779 - val_loss: 116.2200\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 109.1955 - val_loss: 102.3516\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 94.9713 - val_loss: 92.9134\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 82.2442 - val_loss: 77.8860\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 72.8434 - val_loss: 66.4873\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 61.1842 - val_loss: 60.4810\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 53.9690 - val_loss: 49.9209\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 45.5150 - val_loss: 43.6757\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 39.7122 - val_loss: 37.1787\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 35.1674 - val_loss: 33.4153\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 29.3756 - val_loss: 26.0612\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 24.2186 - val_loss: 22.5267\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 23.1570 - val_loss: 18.7720\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 17.6847 - val_loss: 16.0107\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 15.3461 - val_loss: 16.6664\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 12.5432 - val_loss: 12.1002\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 11.7586 - val_loss: 12.1051\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.6747 - val_loss: 46.4333\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.8639 - val_loss: 7.1263\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 7.5118 - val_loss: 11.0255\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.7097 - val_loss: 4.8328\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.7285 - val_loss: 4.7459\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.3662 - val_loss: 3.9624\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.6272 - val_loss: 2.9719\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.7506 - val_loss: 2.7147\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.2945 - val_loss: 11.0396\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.4222 - val_loss: 2.0156\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 1.9915 - val_loss: 2.5517\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.0277 - val_loss: 5.3111\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.9127 - val_loss: 6.5865\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.9935 - val_loss: 1.1351\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0202 - val_loss: 0.9461\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4191 - val_loss: 1.7087\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8660 - val_loss: 3.3591\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1574 - val_loss: 2.7163\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3463 - val_loss: 1.7139\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.5478 - val_loss: 1.4829\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9709 - val_loss: 0.8042\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0151 - val_loss: 2.0635\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 6.1596 - val_loss: 0.7379\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3836 - val_loss: 0.5262\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7676 - val_loss: 0.5790\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6357 - val_loss: 0.9292\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9419 - val_loss: 1.1673\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.8726 - val_loss: 1.7198\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5201 - val_loss: 0.7276\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9003 - val_loss: 2.0980\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9397 - val_loss: 0.3084\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2695 - val_loss: 0.5875\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7288 - val_loss: 2.9451\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8589 - val_loss: 0.9787\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7912 - val_loss: 1.4079\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.4386 - val_loss: 2.4730\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6919 - val_loss: 0.4727\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5643 - val_loss: 0.7416\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5608 - val_loss: 0.2099\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5253 - val_loss: 2.6709\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8910 - val_loss: 2.4938\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7913 - val_loss: 0.2380\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7330 - val_loss: 0.3252\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8295 - val_loss: 0.7393\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.4915 - val_loss: 10.4468\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 2.0048 - val_loss: 0.3441\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3737 - val_loss: 0.2238\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3619 - val_loss: 0.4992\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3589 - val_loss: 0.2326\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5974 - val_loss: 1.4872\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5508 - val_loss: 0.5620\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7275 - val_loss: 0.6812\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1998 - val_loss: 0.5157\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4726 - val_loss: 0.4681\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1405 - val_loss: 0.8481\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7256 - val_loss: 2.1192\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 4.9368 - val_loss: 5.2840\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6181 - val_loss: 0.1908\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1718 - val_loss: 0.1658\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2986 - val_loss: 0.1594\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3319 - val_loss: 0.5667\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7015 - val_loss: 1.4238\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3868 - val_loss: 0.4165\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3531 - val_loss: 0.2739\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.7397 - val_loss: 0.8376\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3264 - val_loss: 0.1816\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1527 - val_loss: 0.1491\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3262 - val_loss: 0.9411\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4106 - val_loss: 0.3778\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6156 - val_loss: 0.9744\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6286 - val_loss: 0.3405\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6919 - val_loss: 1.1523\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8434 - val_loss: 0.4223\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8007 - val_loss: 0.6156\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8380 - val_loss: 0.1439\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4185 - val_loss: 0.6869\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8820 - val_loss: 0.7464\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4028 - val_loss: 2.0288\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1331 - val_loss: 1.4810\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7864 - val_loss: 0.1876\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2674 - val_loss: 1.3551\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8904 - val_loss: 0.3627\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7274 - val_loss: 0.4623\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3469 - val_loss: 0.2447\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7243 - val_loss: 0.2454\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6955 - val_loss: 0.2888\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6725 - val_loss: 0.2223\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3457 - val_loss: 0.3323\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.3290 - val_loss: 0.4815\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2804 - val_loss: 0.1904\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1357 - val_loss: 0.1817\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5090 - val_loss: 1.3830\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3803 - val_loss: 0.0982\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3667 - val_loss: 0.1818\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2680 - val_loss: 0.5804\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6016 - val_loss: 0.6327\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7495 - val_loss: 0.3847\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4835 - val_loss: 2.2102\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7276 - val_loss: 0.1650\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3097 - val_loss: 0.5012\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7472 - val_loss: 0.9299\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.4308 - val_loss: 0.2323\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3592 - val_loss: 2.9838\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.2647 - val_loss: 0.1183\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1403 - val_loss: 0.1510\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2880 - val_loss: 0.3996\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3001 - val_loss: 0.1731\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2252 - val_loss: 0.2352\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6122 - val_loss: 1.9835\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6256 - val_loss: 0.2333\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3527 - val_loss: 0.5477\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3794 - val_loss: 0.3707\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8982 - val_loss: 14.3249\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6591 - val_loss: 0.0901\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1102 - val_loss: 0.1694\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2186 - val_loss: 0.6849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8689 - val_loss: 0.1349\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2257 - val_loss: 0.2755\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5487 - val_loss: 1.9751\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3483 - val_loss: 3.2579\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.9800 - val_loss: 0.1878\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1762 - val_loss: 0.1844\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1286 - val_loss: 0.1654\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1615 - val_loss: 0.1145\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7381 - val_loss: 0.2261\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3934 - val_loss: 0.2061\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5171 - val_loss: 0.1523\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3172 - val_loss: 0.1194\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2313 - val_loss: 1.3595\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5600 - val_loss: 0.5738\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.5336 - val_loss: 0.3121\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2199 - val_loss: 0.1118\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1273 - val_loss: 0.1676\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1839 - val_loss: 0.1438\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1860 - val_loss: 0.3395\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2719 - val_loss: 0.2439\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4266 - val_loss: 0.7210\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5180 - val_loss: 0.2473\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1554 - val_loss: 0.3671\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9917 - val_loss: 1.2091\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3216 - val_loss: 1.6188\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2899 - val_loss: 0.3549\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6307 - val_loss: 0.4163\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.2891 - val_loss: 0.2770\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1941 - val_loss: 0.0583\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3448 - val_loss: 0.2759\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6115 - val_loss: 0.1758\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4654 - val_loss: 0.6704\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.3719 - val_loss: 0.1844\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 0.2993 - val_loss: 0.1873\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6102 - val_loss: 2.7294\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.3386 - val_loss: 0.1359\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6589 - val_loss: 1.5243\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4873 - val_loss: 0.2738\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1385 - val_loss: 0.2291\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4370 - val_loss: 0.1058\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5009 - val_loss: 0.3233\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.3200306746501576\n",
      "Mean Absolute Error (MAE): 0.39577250354834265\n",
      "Root Mean Squared Error (RMSE): 0.5657125371159434\n",
      "Time taken: 465.5777428150177\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 7ms/step - loss: 1392.2640 - val_loss: 1283.6277\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1220.7881 - val_loss: 1177.0853\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1123.1981 - val_loss: 1086.5414\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1039.3135 - val_loss: 1007.1213\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 963.3209 - val_loss: 941.1292\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 901.0148 - val_loss: 878.6241\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 843.3939 - val_loss: 822.4869\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 787.2343 - val_loss: 764.9708\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 727.9744 - val_loss: 701.2277\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 672.4518 - val_loss: 648.5490\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 621.9575 - val_loss: 606.4072\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 577.5629 - val_loss: 557.4764\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 535.7960 - val_loss: 516.2199\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 493.8454 - val_loss: 474.8227\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 455.2740 - val_loss: 437.6224\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 419.9102 - val_loss: 398.7866\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 381.6167 - val_loss: 364.1709\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 348.0997 - val_loss: 327.7893\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 313.5578 - val_loss: 296.6827\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 282.4091 - val_loss: 265.6888\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 252.7818 - val_loss: 236.3184\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 227.0600 - val_loss: 210.7820\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 202.2937 - val_loss: 186.8884\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 181.1027 - val_loss: 163.6799\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 155.8976 - val_loss: 141.7188\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 135.9603 - val_loss: 124.1020\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 118.5330 - val_loss: 109.8210\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 102.2529 - val_loss: 92.6598\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 89.6447 - val_loss: 78.3168\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 77.2840 - val_loss: 67.1839\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 66.6697 - val_loss: 57.1555\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 56.5331 - val_loss: 48.4368\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 48.9738 - val_loss: 42.7790\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 40.7074 - val_loss: 33.6280\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 34.7045 - val_loss: 39.5888\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 29.3891 - val_loss: 23.3742\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 24.6224 - val_loss: 23.0582\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 20.5433 - val_loss: 16.7415\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 16.9934 - val_loss: 14.4166\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 14.8238 - val_loss: 12.2755\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 14.4245 - val_loss: 16.8056\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.7166 - val_loss: 8.0713\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.6083 - val_loss: 6.6324\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.3052 - val_loss: 5.5541\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.8120 - val_loss: 4.9526\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 5.1224 - val_loss: 5.1497\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.0393 - val_loss: 4.4441\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.0057 - val_loss: 2.6674\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 2.9096 - val_loss: 2.1189\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 2.3798 - val_loss: 4.6633\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 6.3483 - val_loss: 2.6852\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5051 - val_loss: 1.0639\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3371 - val_loss: 2.0398\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.3963 - val_loss: 0.9275\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.1526 - val_loss: 0.8007\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7272 - val_loss: 1.4404\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.0636 - val_loss: 1.1126\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1530 - val_loss: 0.7015\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 1.0353 - val_loss: 1.4633\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9420 - val_loss: 2.4672\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.6632 - val_loss: 0.6463\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5532 - val_loss: 0.5617\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4336 - val_loss: 0.7308\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7808 - val_loss: 0.3550\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5921 - val_loss: 0.5093\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0131 - val_loss: 0.9267\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9877 - val_loss: 0.5697\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7950 - val_loss: 1.0539\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.5957 - val_loss: 1.0770\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6931 - val_loss: 0.4642\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.2974 - val_loss: 0.6310\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4341 - val_loss: 0.3813\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6142 - val_loss: 0.8649\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0683 - val_loss: 0.9800\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7493 - val_loss: 0.3338\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 6.7087 - val_loss: 0.5062\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2544 - val_loss: 0.2014\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2044 - val_loss: 0.2194\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2380 - val_loss: 0.7205\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5242 - val_loss: 2.7527\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8316 - val_loss: 0.4314\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7709 - val_loss: 1.2565\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6787 - val_loss: 0.7485\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.0356 - val_loss: 1.1336\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7722 - val_loss: 1.3414\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7451 - val_loss: 0.3920\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.8711 - val_loss: 5.4714\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9335 - val_loss: 0.2497\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2407 - val_loss: 0.1622\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1663 - val_loss: 0.1461\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5488 - val_loss: 0.3447\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2161 - val_loss: 0.1901\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9837 - val_loss: 0.4943\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1623 - val_loss: 0.8706\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2750 - val_loss: 0.3352\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5542 - val_loss: 0.7049\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6444 - val_loss: 0.3869\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9330 - val_loss: 0.1827\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2061 - val_loss: 0.7064\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.4202 - val_loss: 0.2344\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3634 - val_loss: 0.9351\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5469 - val_loss: 0.3521\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6370 - val_loss: 0.3359\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5572 - val_loss: 0.6631\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8408 - val_loss: 0.5551\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5471 - val_loss: 1.5627\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7781 - val_loss: 0.1995\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 4.6115 - val_loss: 0.2661\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1620 - val_loss: 0.0869\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1440 - val_loss: 0.1110\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4838 - val_loss: 0.5702\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4396 - val_loss: 0.7172\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3338 - val_loss: 0.1660\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5999 - val_loss: 4.1743\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1729 - val_loss: 0.3089\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1960 - val_loss: 0.1599\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4643 - val_loss: 0.1400\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5890 - val_loss: 0.3040\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3514 - val_loss: 0.1234\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4277 - val_loss: 0.2646\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5289 - val_loss: 0.3837\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3770 - val_loss: 1.5759\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 3.5518 - val_loss: 0.1644\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.1467 - val_loss: 0.1201\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7912 - val_loss: 0.5001\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1807 - val_loss: 0.0724\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1829 - val_loss: 0.2097\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5184 - val_loss: 0.4542\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6991 - val_loss: 0.6724\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1938 - val_loss: 0.0989\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1780 - val_loss: 0.3279\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2779 - val_loss: 0.7267\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7560 - val_loss: 0.5312\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9337 - val_loss: 0.1381\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3132 - val_loss: 0.1346\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4479 - val_loss: 0.9587\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6360 - val_loss: 0.4034\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6210 - val_loss: 0.5875\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7531 - val_loss: 0.2757\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2682 - val_loss: 0.8578\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1214 - val_loss: 14.6417\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.9150 - val_loss: 0.0938\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1095 - val_loss: 0.0590\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1430 - val_loss: 0.9180\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2202 - val_loss: 0.2372\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3614 - val_loss: 0.0742\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2864 - val_loss: 0.1204\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5625 - val_loss: 0.6623\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4732 - val_loss: 0.2446\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5989 - val_loss: 1.1409\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6511 - val_loss: 0.3837\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 0.2903 - val_loss: 0.2795\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2913 - val_loss: 0.4681\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3989 - val_loss: 0.2141\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2817 - val_loss: 0.0850\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6361 - val_loss: 0.4731\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3404 - val_loss: 1.9407\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.2515 - val_loss: 0.1032\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0911 - val_loss: 0.0804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0748 - val_loss: 0.0686\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1043 - val_loss: 0.0542\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2534 - val_loss: 0.4388\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.2827 - val_loss: 0.3925\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5863 - val_loss: 0.3170\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4251 - val_loss: 0.5552\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3819 - val_loss: 0.1037\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1667 - val_loss: 0.2499\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5731 - val_loss: 0.2847\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2202 - val_loss: 0.1072\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6895 - val_loss: 0.4197\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2449 - val_loss: 0.3519\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1079 - val_loss: 1.6300\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2201 - val_loss: 0.1076\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1124 - val_loss: 0.3118\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5246 - val_loss: 0.4150\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5782 - val_loss: 0.2118\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1429 - val_loss: 0.2312\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2807 - val_loss: 0.0976\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4616 - val_loss: 0.6210\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3883 - val_loss: 0.4864\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5398 - val_loss: 0.2726\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3444 - val_loss: 0.0998\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1509 - val_loss: 0.0686\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1804 - val_loss: 0.3180\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.2450 - val_loss: 0.2033\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1088 - val_loss: 0.0605\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.0598 - val_loss: 0.0750\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0483 - val_loss: 0.0439\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0521 - val_loss: 0.0947\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3420 - val_loss: 0.3463\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2963 - val_loss: 0.5438\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1206 - val_loss: 0.1808\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2112 - val_loss: 0.3469\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4109 - val_loss: 0.6000\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1129 - val_loss: 0.0473\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0544 - val_loss: 0.0624\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1208 - val_loss: 0.1738\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4074 - val_loss: 1.7417\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2845 - val_loss: 0.1777\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1667 - val_loss: 0.2239\n",
      "16/16 [==============================] - 1s 2ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.21876570407747994\n",
      "Mean Absolute Error (MAE): 0.3257890239171637\n",
      "Root Mean Squared Error (RMSE): 0.46772396141044553\n",
      "Time taken: 430.1376996040344\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 8ms/step - loss: 1348.8553 - val_loss: 1219.4207\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1185.3064 - val_loss: 1126.9347\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1105.6919 - val_loss: 1059.9493\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1047.2185 - val_loss: 1010.6135\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1004.4965 - val_loss: 974.6747\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 973.9070 - val_loss: 949.4138\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 952.3919 - val_loss: 931.3627\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 935.3360 - val_loss: 913.3464\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 916.4445 - val_loss: 881.7247\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 859.8527 - val_loss: 812.7112\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 802.5810 - val_loss: 762.4130\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 753.3132 - val_loss: 713.0822\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 700.7807 - val_loss: 661.3051\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 647.5307 - val_loss: 609.9892\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 597.2615 - val_loss: 565.4667\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 553.0909 - val_loss: 523.0412\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 513.2603 - val_loss: 486.7123\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 475.8047 - val_loss: 449.9578\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 440.2454 - val_loss: 415.4802\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 404.8277 - val_loss: 378.8753\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 367.7498 - val_loss: 340.4117\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 332.2382 - val_loss: 310.0077\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 298.7227 - val_loss: 275.8643\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 267.6754 - val_loss: 247.2406\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 239.6102 - val_loss: 220.5375\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 214.9963 - val_loss: 205.3325\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 192.1776 - val_loss: 174.8984\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 167.1329 - val_loss: 153.3503\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 147.3119 - val_loss: 132.3962\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 126.6734 - val_loss: 114.6010\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 109.0335 - val_loss: 101.4966\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 94.8301 - val_loss: 83.8301\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 81.0892 - val_loss: 72.9927\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 69.8712 - val_loss: 63.3519\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 58.7381 - val_loss: 52.4642\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 51.5968 - val_loss: 46.9656\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 43.3645 - val_loss: 38.2730\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 38.5478 - val_loss: 32.4022\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 30.9695 - val_loss: 27.4148\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 27.0419 - val_loss: 22.8634\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 22.7779 - val_loss: 21.0828\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 20.9087 - val_loss: 16.1472\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 16.1758 - val_loss: 14.2268\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 13.5513 - val_loss: 14.2377\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 12.7096 - val_loss: 19.7202\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 10.8216 - val_loss: 8.6134\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 9.1250 - val_loss: 7.3115\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 8.8460 - val_loss: 17.2207\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.6899 - val_loss: 4.9199\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.9125 - val_loss: 5.7380\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.3675 - val_loss: 3.6277\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.8339 - val_loss: 4.7232\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 3.6054 - val_loss: 2.2315\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 3.9059 - val_loss: 2.3944\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.0965 - val_loss: 1.5492\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.8145 - val_loss: 1.7144\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.7023 - val_loss: 1.3862\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4627 - val_loss: 1.2181\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 6.3039 - val_loss: 1.6909\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.0106 - val_loss: 1.1794\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.4625 - val_loss: 2.0406\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.9594 - val_loss: 1.1105\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1.1066 - val_loss: 1.2078\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9239 - val_loss: 1.9358\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.6961 - val_loss: 1.8039\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 12.0013 - val_loss: 1.1260\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.7170 - val_loss: 0.8373\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5526 - val_loss: 0.5089\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4803 - val_loss: 1.0063\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.6072 - val_loss: 0.5037\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8248 - val_loss: 1.1100\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8356 - val_loss: 1.3205\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3122 - val_loss: 1.9268\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9227 - val_loss: 0.4240\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8499 - val_loss: 8.6627\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.7652 - val_loss: 0.6794\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5711 - val_loss: 0.3134\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4964 - val_loss: 0.4254\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5966 - val_loss: 1.6337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7529 - val_loss: 0.9635\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1835 - val_loss: 1.3514\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 1.2700 - val_loss: 0.2308\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0015 - val_loss: 6.3518\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4520 - val_loss: 0.3374\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4301 - val_loss: 0.3500\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5315 - val_loss: 0.9391\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 1.5377 - val_loss: 0.9732\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7954 - val_loss: 1.2378\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5681 - val_loss: 1.1067\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.6280 - val_loss: 0.8918\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4973 - val_loss: 0.2196\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2940 - val_loss: 1.3732\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9094 - val_loss: 0.8537\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5902 - val_loss: 0.8413\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3883 - val_loss: 0.4388\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.8938 - val_loss: 1.7349\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5940 - val_loss: 0.1375\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.5181 - val_loss: 2.0448\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8645 - val_loss: 0.3585\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3383 - val_loss: 0.3808\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2670 - val_loss: 0.1491\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2580 - val_loss: 0.6637\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4584 - val_loss: 0.4260\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4473 - val_loss: 0.1563\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.3638 - val_loss: 0.3523\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5506 - val_loss: 0.3410\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5994 - val_loss: 0.3129\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4015 - val_loss: 0.4273\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.6574 - val_loss: 1.0072\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6085 - val_loss: 1.0003\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5605 - val_loss: 0.8646\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3968 - val_loss: 0.1312\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.3539 - val_loss: 13.3159\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7337 - val_loss: 0.1572\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1940 - val_loss: 0.1286\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7773 - val_loss: 0.2484\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.7174 - val_loss: 0.3276\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2791 - val_loss: 0.2249\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2178 - val_loss: 0.3021\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.8029 - val_loss: 0.4446\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.8386 - val_loss: 0.2757\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4303 - val_loss: 0.4508\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7268 - val_loss: 0.5520\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 3.3280 - val_loss: 0.2196\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1327 - val_loss: 0.1013\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1702 - val_loss: 0.2971\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2983 - val_loss: 0.5051\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3151 - val_loss: 0.1301\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4414 - val_loss: 0.5778\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7789 - val_loss: 0.3774\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3792 - val_loss: 0.3883\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3360 - val_loss: 0.2074\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7492 - val_loss: 0.8068\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3101 - val_loss: 0.1512\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4900 - val_loss: 1.5042\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9980 - val_loss: 8.9674\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.4681 - val_loss: 0.1802\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1381 - val_loss: 0.0984\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1347 - val_loss: 0.1328\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2143 - val_loss: 0.2524\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5952 - val_loss: 0.2153\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2915 - val_loss: 0.0977\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2738 - val_loss: 0.3750\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.9344 - val_loss: 0.3241\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2066 - val_loss: 0.3147\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8744 - val_loss: 2.4677\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3775 - val_loss: 0.0694\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2150 - val_loss: 0.2106\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1567 - val_loss: 0.2423\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2433 - val_loss: 0.4725\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.6901 - val_loss: 0.7002\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3279 - val_loss: 0.3780\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4349 - val_loss: 0.3865\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5771 - val_loss: 0.2947\n",
      "Epoch 155/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3184 - val_loss: 0.3254\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3754 - val_loss: 0.4953\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4136 - val_loss: 0.1884\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0759 - val_loss: 0.6324\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5603 - val_loss: 0.1430\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1508 - val_loss: 0.0528\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4201 - val_loss: 0.4907\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4559 - val_loss: 0.7985\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3652 - val_loss: 0.4361\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4195 - val_loss: 0.5610\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.3986 - val_loss: 0.2022\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1458 - val_loss: 0.0689\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1888 - val_loss: 0.1397\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1036 - val_loss: 0.2788\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5823 - val_loss: 0.1086\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 0.2057 - val_loss: 0.1106\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2530 - val_loss: 0.8611\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.6781 - val_loss: 0.1374\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.3894 - val_loss: 0.2829\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 0.4381 - val_loss: 0.3667\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 4.3598 - val_loss: 0.1860\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1019 - val_loss: 0.0727\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0683 - val_loss: 0.0492\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0679 - val_loss: 0.0874\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.0920 - val_loss: 0.0757\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.1612 - val_loss: 0.0816\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 0.5502 - val_loss: 0.2440\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1026 - val_loss: 0.1150\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3516 - val_loss: 0.6227\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.7804 - val_loss: 1.9296\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.4530 - val_loss: 0.1671\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2660 - val_loss: 0.2130\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3566 - val_loss: 2.2093\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.4817 - val_loss: 0.1027\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1370 - val_loss: 0.0815\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3608 - val_loss: 0.5460\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 3.3852 - val_loss: 0.2497\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.1396 - val_loss: 0.0578\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0949 - val_loss: 0.0560\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0695 - val_loss: 0.0663\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1098 - val_loss: 0.0852\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2572 - val_loss: 0.2538\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.5884 - val_loss: 0.2029\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3599 - val_loss: 0.0527\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.3458 - val_loss: 0.7178\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.2909 - val_loss: 0.1223\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.12238713274686104\n",
      "Mean Absolute Error (MAE): 0.2604169335080433\n",
      "Root Mean Squared Error (RMSE): 0.34983872390983395\n",
      "Time taken: 444.094051361084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_9104\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.423819  0.446096  0.651014  512.282485\n",
      "1        2  0.233797  0.355889  0.483525  479.688537\n",
      "2        3  0.320031  0.395773  0.565713  465.577743\n",
      "3        4  0.218766  0.325789  0.467724  430.137700\n",
      "4        5  0.122387  0.260417  0.349839  444.094051\n",
      "5  Average  0.263760  0.356793  0.503563  466.356103\n",
      "Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
