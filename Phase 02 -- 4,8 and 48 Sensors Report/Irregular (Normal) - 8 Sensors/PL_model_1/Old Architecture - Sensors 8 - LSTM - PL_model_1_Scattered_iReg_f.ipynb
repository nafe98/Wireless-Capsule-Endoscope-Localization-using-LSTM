{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_Scattered_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "              45          46          47  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "      <td>89.453295</td>\n",
       "      <td>97.318384</td>\n",
       "      <td>...</td>\n",
       "      <td>81.685404</td>\n",
       "      <td>84.830110</td>\n",
       "      <td>86.513881</td>\n",
       "      <td>81.048996</td>\n",
       "      <td>114.964811</td>\n",
       "      <td>120.010616</td>\n",
       "      <td>103.909997</td>\n",
       "      <td>133.568532</td>\n",
       "      <td>57.626093</td>\n",
       "      <td>109.708209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "      <td>92.314999</td>\n",
       "      <td>112.314087</td>\n",
       "      <td>...</td>\n",
       "      <td>81.526583</td>\n",
       "      <td>92.908051</td>\n",
       "      <td>94.438277</td>\n",
       "      <td>89.628271</td>\n",
       "      <td>114.498751</td>\n",
       "      <td>106.887589</td>\n",
       "      <td>99.505693</td>\n",
       "      <td>128.544662</td>\n",
       "      <td>67.730350</td>\n",
       "      <td>113.436964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "      <td>99.182335</td>\n",
       "      <td>106.232463</td>\n",
       "      <td>...</td>\n",
       "      <td>75.930487</td>\n",
       "      <td>82.432658</td>\n",
       "      <td>87.572150</td>\n",
       "      <td>90.919428</td>\n",
       "      <td>116.186110</td>\n",
       "      <td>121.150696</td>\n",
       "      <td>96.193748</td>\n",
       "      <td>134.116483</td>\n",
       "      <td>68.863500</td>\n",
       "      <td>116.446807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "      <td>86.763744</td>\n",
       "      <td>106.168512</td>\n",
       "      <td>...</td>\n",
       "      <td>79.984057</td>\n",
       "      <td>99.957787</td>\n",
       "      <td>93.313344</td>\n",
       "      <td>84.668294</td>\n",
       "      <td>111.953201</td>\n",
       "      <td>119.676628</td>\n",
       "      <td>106.414441</td>\n",
       "      <td>137.948662</td>\n",
       "      <td>69.634344</td>\n",
       "      <td>114.024685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "      <td>83.559242</td>\n",
       "      <td>103.091764</td>\n",
       "      <td>...</td>\n",
       "      <td>75.279364</td>\n",
       "      <td>87.349475</td>\n",
       "      <td>97.655142</td>\n",
       "      <td>89.118820</td>\n",
       "      <td>126.637608</td>\n",
       "      <td>114.886056</td>\n",
       "      <td>101.361093</td>\n",
       "      <td>126.482809</td>\n",
       "      <td>66.133931</td>\n",
       "      <td>109.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "      <td>107.105731</td>\n",
       "      <td>96.441980</td>\n",
       "      <td>...</td>\n",
       "      <td>91.496394</td>\n",
       "      <td>121.729389</td>\n",
       "      <td>87.948166</td>\n",
       "      <td>77.602308</td>\n",
       "      <td>127.656991</td>\n",
       "      <td>114.668824</td>\n",
       "      <td>127.756278</td>\n",
       "      <td>109.362652</td>\n",
       "      <td>102.983525</td>\n",
       "      <td>78.077730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "      <td>104.631338</td>\n",
       "      <td>98.998328</td>\n",
       "      <td>...</td>\n",
       "      <td>92.880258</td>\n",
       "      <td>108.747017</td>\n",
       "      <td>88.541794</td>\n",
       "      <td>75.344392</td>\n",
       "      <td>125.557441</td>\n",
       "      <td>111.031434</td>\n",
       "      <td>134.494231</td>\n",
       "      <td>116.813742</td>\n",
       "      <td>112.599318</td>\n",
       "      <td>79.992646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "      <td>106.966013</td>\n",
       "      <td>96.617547</td>\n",
       "      <td>...</td>\n",
       "      <td>89.648431</td>\n",
       "      <td>106.485343</td>\n",
       "      <td>93.400271</td>\n",
       "      <td>71.177932</td>\n",
       "      <td>123.918015</td>\n",
       "      <td>105.789520</td>\n",
       "      <td>127.670906</td>\n",
       "      <td>109.512188</td>\n",
       "      <td>104.166149</td>\n",
       "      <td>83.022547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "      <td>105.897605</td>\n",
       "      <td>91.914775</td>\n",
       "      <td>...</td>\n",
       "      <td>86.126272</td>\n",
       "      <td>106.959002</td>\n",
       "      <td>88.494586</td>\n",
       "      <td>63.991014</td>\n",
       "      <td>129.409898</td>\n",
       "      <td>109.907911</td>\n",
       "      <td>126.391262</td>\n",
       "      <td>111.268189</td>\n",
       "      <td>100.508162</td>\n",
       "      <td>70.592735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "      <td>97.022346</td>\n",
       "      <td>99.972913</td>\n",
       "      <td>...</td>\n",
       "      <td>88.589209</td>\n",
       "      <td>107.322913</td>\n",
       "      <td>86.795897</td>\n",
       "      <td>75.659668</td>\n",
       "      <td>122.322131</td>\n",
       "      <td>117.782888</td>\n",
       "      <td>126.797409</td>\n",
       "      <td>117.722182</td>\n",
       "      <td>110.106607</td>\n",
       "      <td>76.549859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     103.508252  125.193887   89.453295   97.318384  ...  81.685404   \n",
       "1     106.645699  137.372609   92.314999  112.314087  ...  81.526583   \n",
       "2     103.789337  135.667714   99.182335  106.232463  ...  75.930487   \n",
       "3     102.460744  129.928887   86.763744  106.168512  ...  79.984057   \n",
       "4     116.786233  139.061346   83.559242  103.091764  ...  75.279364   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  123.942335  108.196626  107.105731   96.441980  ...  91.496394   \n",
       "2439  136.835759  113.267986  104.631338   98.998328  ...  92.880258   \n",
       "2440  129.875574  120.944104  106.966013   96.617547  ...  89.648431   \n",
       "2441  125.361425  123.071554  105.897605   91.914775  ...  86.126272   \n",
       "2442  127.958184  113.784393   97.022346   99.972913  ...  88.589209   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      84.830110  86.513881  81.048996  114.964811  120.010616  103.909997   \n",
       "1      92.908051  94.438277  89.628271  114.498751  106.887589   99.505693   \n",
       "2      82.432658  87.572150  90.919428  116.186110  121.150696   96.193748   \n",
       "3      99.957787  93.313344  84.668294  111.953201  119.676628  106.414441   \n",
       "4      87.349475  97.655142  89.118820  126.637608  114.886056  101.361093   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  121.729389  87.948166  77.602308  127.656991  114.668824  127.756278   \n",
       "2439  108.747017  88.541794  75.344392  125.557441  111.031434  134.494231   \n",
       "2440  106.485343  93.400271  71.177932  123.918015  105.789520  127.670906   \n",
       "2441  106.959002  88.494586  63.991014  129.409898  109.907911  126.391262   \n",
       "2442  107.322913  86.795897  75.659668  122.322131  117.782888  126.797409   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     133.568532   57.626093  109.708209  \n",
       "1     128.544662   67.730350  113.436964  \n",
       "2     134.116483   68.863500  116.446807  \n",
       "3     137.948662   69.634344  114.024685  \n",
       "4     126.482809   66.133931  109.168340  \n",
       "...          ...         ...         ...  \n",
       "2438  109.362652  102.983525   78.077730  \n",
       "2439  116.813742  112.599318   79.992646  \n",
       "2440  109.512188  104.166149   83.022547  \n",
       "2441  111.268189  100.508162   70.592735  \n",
       "2442  117.722182  110.106607   76.549859  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.870132</td>\n",
       "      <td>134.252574</td>\n",
       "      <td>72.801390</td>\n",
       "      <td>108.446568</td>\n",
       "      <td>130.203502</td>\n",
       "      <td>150.760073</td>\n",
       "      <td>103.508252</td>\n",
       "      <td>125.193887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.656974</td>\n",
       "      <td>133.421922</td>\n",
       "      <td>76.037698</td>\n",
       "      <td>115.578180</td>\n",
       "      <td>124.237134</td>\n",
       "      <td>144.797812</td>\n",
       "      <td>106.645699</td>\n",
       "      <td>137.372609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.889598</td>\n",
       "      <td>140.640194</td>\n",
       "      <td>71.553137</td>\n",
       "      <td>106.871465</td>\n",
       "      <td>123.654012</td>\n",
       "      <td>148.446127</td>\n",
       "      <td>103.789337</td>\n",
       "      <td>135.667714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.508164</td>\n",
       "      <td>130.788193</td>\n",
       "      <td>73.179060</td>\n",
       "      <td>122.566756</td>\n",
       "      <td>128.558120</td>\n",
       "      <td>144.121205</td>\n",
       "      <td>102.460744</td>\n",
       "      <td>129.928887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.073940</td>\n",
       "      <td>127.217426</td>\n",
       "      <td>69.730286</td>\n",
       "      <td>115.690253</td>\n",
       "      <td>120.920018</td>\n",
       "      <td>148.666096</td>\n",
       "      <td>116.786233</td>\n",
       "      <td>139.061346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>126.520010</td>\n",
       "      <td>110.917507</td>\n",
       "      <td>102.822108</td>\n",
       "      <td>62.853700</td>\n",
       "      <td>149.747652</td>\n",
       "      <td>127.099840</td>\n",
       "      <td>123.942335</td>\n",
       "      <td>108.196626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>122.656116</td>\n",
       "      <td>114.785269</td>\n",
       "      <td>98.718438</td>\n",
       "      <td>76.449249</td>\n",
       "      <td>152.660776</td>\n",
       "      <td>140.174139</td>\n",
       "      <td>136.835759</td>\n",
       "      <td>113.267986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>126.597748</td>\n",
       "      <td>119.491178</td>\n",
       "      <td>105.538634</td>\n",
       "      <td>75.394449</td>\n",
       "      <td>145.766322</td>\n",
       "      <td>143.595590</td>\n",
       "      <td>129.875574</td>\n",
       "      <td>120.944104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>139.564043</td>\n",
       "      <td>109.141995</td>\n",
       "      <td>106.936201</td>\n",
       "      <td>78.218026</td>\n",
       "      <td>144.561741</td>\n",
       "      <td>141.507291</td>\n",
       "      <td>125.361425</td>\n",
       "      <td>123.071554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>132.013398</td>\n",
       "      <td>115.430773</td>\n",
       "      <td>105.461899</td>\n",
       "      <td>71.804618</td>\n",
       "      <td>151.624598</td>\n",
       "      <td>143.982949</td>\n",
       "      <td>127.958184</td>\n",
       "      <td>113.784393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     101.870132  134.252574   72.801390  108.446568  130.203502  150.760073   \n",
       "1     101.656974  133.421922   76.037698  115.578180  124.237134  144.797812   \n",
       "2     102.889598  140.640194   71.553137  106.871465  123.654012  148.446127   \n",
       "3     100.508164  130.788193   73.179060  122.566756  128.558120  144.121205   \n",
       "4     104.073940  127.217426   69.730286  115.690253  120.920018  148.666096   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  126.520010  110.917507  102.822108   62.853700  149.747652  127.099840   \n",
       "2439  122.656116  114.785269   98.718438   76.449249  152.660776  140.174139   \n",
       "2440  126.597748  119.491178  105.538634   75.394449  145.766322  143.595590   \n",
       "2441  139.564043  109.141995  106.936201   78.218026  144.561741  141.507291   \n",
       "2442  132.013398  115.430773  105.461899   71.804618  151.624598  143.982949   \n",
       "\n",
       "         sensor7     sensor8  \n",
       "0     103.508252  125.193887  \n",
       "1     106.645699  137.372609  \n",
       "2     103.789337  135.667714  \n",
       "3     102.460744  129.928887  \n",
       "4     116.786233  139.061346  \n",
       "...          ...         ...  \n",
       "2438  123.942335  108.196626  \n",
       "2439  136.835759  113.267986  \n",
       "2440  129.875574  120.944104  \n",
       "2441  125.361425  123.071554  \n",
       "2442  127.958184  113.784393  \n",
       "\n",
       "[2443 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:8]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 12ms/step - loss: 1377.2533 - val_loss: 1250.9250\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1184.8077 - val_loss: 1117.4603\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 1073.5950 - val_loss: 1022.7543\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 989.6253 - val_loss: 949.0942\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 919.0845 - val_loss: 883.7564\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 860.3633 - val_loss: 826.9417\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 807.5776 - val_loss: 779.1963\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 765.0389 - val_loss: 740.1092\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 723.9340 - val_loss: 701.0577\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 688.8762 - val_loss: 665.4986\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 654.5029 - val_loss: 634.6768\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 622.1132 - val_loss: 596.3680\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 581.4222 - val_loss: 554.7175\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 540.0266 - val_loss: 513.1669\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 494.9422 - val_loss: 469.4037\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 462.7439 - val_loss: 436.5738\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 429.6461 - val_loss: 404.7766\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 401.1669 - val_loss: 375.3354\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 372.5880 - val_loss: 348.9676\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 344.1505 - val_loss: 319.9858\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 316.5382 - val_loss: 293.7958\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 290.1314 - val_loss: 270.9934\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 263.2059 - val_loss: 246.6026\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 238.9812 - val_loss: 221.6022\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 218.5243 - val_loss: 203.4678\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 199.5409 - val_loss: 181.7191\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 179.4409 - val_loss: 169.4303\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 165.9288 - val_loss: 149.7476\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 149.9979 - val_loss: 137.9882\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 135.9855 - val_loss: 126.6231\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 124.6368 - val_loss: 118.1627\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 114.8952 - val_loss: 107.8331\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 108.5381 - val_loss: 100.0263\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 97.2513 - val_loss: 98.4253\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 91.5472 - val_loss: 88.1535\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 86.1815 - val_loss: 92.3049\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 82.1594 - val_loss: 76.6913\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 76.3775 - val_loss: 88.0113\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 74.5145 - val_loss: 77.7937\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 69.1672 - val_loss: 68.6192\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 64.3673 - val_loss: 68.5518\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 63.8229 - val_loss: 61.4511\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 60.5835 - val_loss: 69.3649\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 58.1319 - val_loss: 60.3109\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 57.2912 - val_loss: 56.9922\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 55.2813 - val_loss: 59.2124\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 52.3785 - val_loss: 52.0027\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 52.6940 - val_loss: 53.3027\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 50.4083 - val_loss: 52.7150\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 49.0986 - val_loss: 64.9504\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 48.7977 - val_loss: 57.8865\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 47.7560 - val_loss: 47.3107\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 47.8869 - val_loss: 55.1467\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 45.2435 - val_loss: 49.6773\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 46.1111 - val_loss: 49.3622\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 44.2852 - val_loss: 47.9096\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 43.0888 - val_loss: 41.1485\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 40.6099 - val_loss: 40.2704\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 39.8014 - val_loss: 44.6093\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 40.0568 - val_loss: 45.7893\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 38.5073 - val_loss: 39.8358\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 37.1236 - val_loss: 45.5257\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 36.0568 - val_loss: 40.7637\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 36.7786 - val_loss: 44.1722\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 35.2881 - val_loss: 36.8364\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 36.3093 - val_loss: 36.2892\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 34.4128 - val_loss: 36.4426\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 35.5584 - val_loss: 38.3845\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 33.3928 - val_loss: 35.4413\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.5878 - val_loss: 38.3262\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 33.5996 - val_loss: 41.1605\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.9546 - val_loss: 34.5812\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.9186 - val_loss: 33.5637\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 33.3736 - val_loss: 35.6454\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.9299 - val_loss: 33.4359\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.2461 - val_loss: 34.0163\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.4863 - val_loss: 42.1778\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 30.7384 - val_loss: 42.4790\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.3303 - val_loss: 35.0850\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.7503 - val_loss: 31.8477\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.1740 - val_loss: 37.5475\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.4076 - val_loss: 39.1965\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.2896 - val_loss: 31.9941\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.8945 - val_loss: 36.4494\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.8104 - val_loss: 34.1872\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.9481 - val_loss: 34.1210\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.4895 - val_loss: 36.0234\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.6518 - val_loss: 33.0001\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.8102 - val_loss: 31.3627\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.5364 - val_loss: 32.7036\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.0814 - val_loss: 31.7633\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.3844 - val_loss: 30.0076\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.1442 - val_loss: 33.2756\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.2568 - val_loss: 32.4451\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.7257 - val_loss: 32.5856\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.7690 - val_loss: 30.8292\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.8478 - val_loss: 31.2390\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.8358 - val_loss: 31.9502\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.2126 - val_loss: 32.0257\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.7625 - val_loss: 30.4150\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.0387 - val_loss: 34.1550\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.6500 - val_loss: 31.2834\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.3794 - val_loss: 46.3030\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.5223 - val_loss: 32.0495\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.7624 - val_loss: 32.2858\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.9437 - val_loss: 32.9737\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.1226 - val_loss: 29.7964\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.5102 - val_loss: 32.2013\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.5627 - val_loss: 29.3256\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.7479 - val_loss: 32.8854\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.3131 - val_loss: 29.6248\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.5814 - val_loss: 32.5277\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.2343 - val_loss: 31.2322\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.5155 - val_loss: 30.6499\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.1722 - val_loss: 29.6526\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.8418 - val_loss: 33.5501\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.8993 - val_loss: 39.4653\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.1161 - val_loss: 29.6903\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.3220 - val_loss: 30.8996\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.8785 - val_loss: 29.3204\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.1224 - val_loss: 28.5826\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.9482 - val_loss: 33.6260\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.0888 - val_loss: 28.7880\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 25.5926 - val_loss: 33.0117\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.7032 - val_loss: 32.7346\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.0788 - val_loss: 29.1301\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.1341 - val_loss: 36.9713\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.7710 - val_loss: 28.4833\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.7603 - val_loss: 29.5383\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.1185 - val_loss: 28.6304\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.7820 - val_loss: 30.9212\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.6687 - val_loss: 30.0587\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.0869 - val_loss: 28.9676\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.3607 - val_loss: 29.2267\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.9534 - val_loss: 31.8684\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.3297 - val_loss: 32.7231\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.2487 - val_loss: 33.8420\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.2937 - val_loss: 30.1105\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.6375 - val_loss: 35.0854\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.3708 - val_loss: 30.0893\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.5643 - val_loss: 30.4097\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.5220 - val_loss: 36.8960\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.0242 - val_loss: 31.3552\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.7385 - val_loss: 33.1119\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.2235 - val_loss: 30.8677\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.0416 - val_loss: 28.9906\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.2589 - val_loss: 30.2833\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.8566 - val_loss: 30.4528\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.5543 - val_loss: 28.7570\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 21.3380 - val_loss: 29.7598\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 21.7230 - val_loss: 43.2865\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.6605 - val_loss: 28.2365\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.1160 - val_loss: 30.9553\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 20.6920 - val_loss: 29.3697\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 18ms/step - loss: 22.3591 - val_loss: 31.4628\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.4052 - val_loss: 30.5490\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 21.5779 - val_loss: 31.4057\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.6586 - val_loss: 32.5337\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.7257 - val_loss: 29.6268\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.9178 - val_loss: 29.9813\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.5708 - val_loss: 30.1979\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.6702 - val_loss: 31.7621\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.9689 - val_loss: 29.6286\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.5785 - val_loss: 32.7188\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.7439 - val_loss: 32.3405\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.3988 - val_loss: 35.2518\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.5201 - val_loss: 30.2442\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.2963 - val_loss: 31.6280\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.9213 - val_loss: 28.2813\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.9779 - val_loss: 31.8737\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.4822 - val_loss: 29.6731\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.2530 - val_loss: 29.7404\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.7418 - val_loss: 29.9979\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.7347 - val_loss: 28.4288\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.6687 - val_loss: 38.8146\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.6613 - val_loss: 29.0900\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.3471 - val_loss: 30.6908\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.7341 - val_loss: 30.6858\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.6568 - val_loss: 29.2297\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.7540 - val_loss: 28.9024\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.3050 - val_loss: 31.3255\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.7624 - val_loss: 30.9304\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 19.0614 - val_loss: 30.2178\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 19.0061 - val_loss: 28.8494\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 19.7097 - val_loss: 34.2728\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 18.2909 - val_loss: 29.9173\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 19.7784 - val_loss: 31.0015\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.2480 - val_loss: 29.3858\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.5728 - val_loss: 30.6887\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.8673 - val_loss: 28.3773\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.6422 - val_loss: 29.9136\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.1351 - val_loss: 32.1520\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.3620 - val_loss: 29.2516\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.5814 - val_loss: 28.9711\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.1260 - val_loss: 30.0864\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.6082 - val_loss: 28.2733\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 16.9432 - val_loss: 30.7844\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.6416 - val_loss: 31.9692\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.1225 - val_loss: 32.4217\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.0307 - val_loss: 30.7427\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 30.762556901752692\n",
      "Mean Absolute Error (MAE): 3.8105336015837707\n",
      "Root Mean Squared Error (RMSE): 5.546400355343336\n",
      "Time taken: 1294.389091014862\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 10s 17ms/step - loss: 1400.7018 - val_loss: 1287.4718\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 1233.5316 - val_loss: 1182.4025\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1144.5535 - val_loss: 1105.3071\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1074.3533 - val_loss: 1039.5883\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 1012.2349 - val_loss: 979.9725\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 952.4310 - val_loss: 924.4801\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 899.7727 - val_loss: 874.4446\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 852.4387 - val_loss: 831.4929\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 803.7263 - val_loss: 781.2549\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 750.5703 - val_loss: 725.3466\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 698.7510 - val_loss: 677.5805\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 652.6536 - val_loss: 636.6945\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 607.7924 - val_loss: 587.8719\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 567.3521 - val_loss: 550.3699\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 530.6080 - val_loss: 515.3415\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 494.7218 - val_loss: 479.4619\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 462.7473 - val_loss: 450.0700\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 431.9189 - val_loss: 416.6180\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 400.2615 - val_loss: 390.2960\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 369.4404 - val_loss: 361.0388\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 341.5538 - val_loss: 328.5978\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 311.4747 - val_loss: 298.8932\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 284.4317 - val_loss: 275.1953\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 261.4393 - val_loss: 253.5890\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 239.0026 - val_loss: 236.8349\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 216.4023 - val_loss: 212.5982\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 197.0919 - val_loss: 189.9423\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 178.7087 - val_loss: 173.2826\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 162.3759 - val_loss: 157.6839\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 150.3276 - val_loss: 143.7498\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 134.2923 - val_loss: 132.3152\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 122.5525 - val_loss: 120.9384\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 111.8407 - val_loss: 111.0849\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 102.0443 - val_loss: 101.5655\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 91.9141 - val_loss: 91.0588\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 84.2930 - val_loss: 84.4203\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 78.2196 - val_loss: 75.5424\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 71.7563 - val_loss: 70.3705\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 66.5340 - val_loss: 69.2697\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 61.3139 - val_loss: 65.1835\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 57.3898 - val_loss: 72.4692\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 55.7539 - val_loss: 55.4028\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 51.1526 - val_loss: 53.0518\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 49.9715 - val_loss: 53.5039\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 47.5886 - val_loss: 47.9352\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 43.6739 - val_loss: 46.6757\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 44.3039 - val_loss: 45.5710\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 40.9588 - val_loss: 49.2728\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 39.9301 - val_loss: 42.8165\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.7134 - val_loss: 41.2244\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 38.2270 - val_loss: 40.9899\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 36.8502 - val_loss: 41.4570\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 35.3213 - val_loss: 39.8350\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 35.8733 - val_loss: 39.6634\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 34.3291 - val_loss: 38.4874\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.5835 - val_loss: 35.9293\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 33.6099 - val_loss: 34.7317\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 32.5358 - val_loss: 36.7491\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 32.0009 - val_loss: 35.3003\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.9901 - val_loss: 36.9298\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 31.1474 - val_loss: 33.9280\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 30.4335 - val_loss: 34.6011\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 29.6932 - val_loss: 37.0331\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.1888 - val_loss: 36.1709\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 30.3284 - val_loss: 33.8016\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 29.4988 - val_loss: 32.4509\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 29.5858 - val_loss: 35.3784\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 29.4106 - val_loss: 35.7185\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 29.0947 - val_loss: 40.4100\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 29.2406 - val_loss: 32.7395\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 27.6357 - val_loss: 31.1639\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 28.4548 - val_loss: 35.1341\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 28.4512 - val_loss: 37.1244\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 28.4290 - val_loss: 33.1191\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 27.0816 - val_loss: 34.6598\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 29.2530 - val_loss: 33.2657\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 28.0518 - val_loss: 31.6388\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 5s 14ms/step - loss: 26.3989 - val_loss: 32.7925\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 26.1772 - val_loss: 30.6826\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 27.3298 - val_loss: 33.6642\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 27.2661 - val_loss: 44.3244\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 26.5342 - val_loss: 40.5971\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 27.2940 - val_loss: 30.8120\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 25.2390 - val_loss: 34.4236\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 26.0671 - val_loss: 31.6232\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.4959 - val_loss: 30.8550\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.9752 - val_loss: 31.0864\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.8430 - val_loss: 34.1982\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 26.0739 - val_loss: 33.8531\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 25.8252 - val_loss: 30.5301\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 26.2545 - val_loss: 31.8560\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 25.3880 - val_loss: 32.0784\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 24.8742 - val_loss: 34.7209\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 24.6288 - val_loss: 34.3043\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 24.7543 - val_loss: 40.3888\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 25.6002 - val_loss: 35.9519\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 24.4908 - val_loss: 34.7836\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 25.4204 - val_loss: 35.0000\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 24.9138 - val_loss: 28.6273\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 24.3343 - val_loss: 34.8745\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 25.3633 - val_loss: 33.1679\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 24.5156 - val_loss: 30.2757\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.4753 - val_loss: 30.5272\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.7359 - val_loss: 30.9436\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.9264 - val_loss: 32.3841\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 24.1979 - val_loss: 29.9435\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.5537 - val_loss: 36.0413\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.4489 - val_loss: 34.9000\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.9648 - val_loss: 32.6144\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.8931 - val_loss: 30.9458\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 23.9087 - val_loss: 31.2228\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.9244 - val_loss: 33.4742\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.2903 - val_loss: 34.9707\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.9471 - val_loss: 33.9854\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 22.6609 - val_loss: 30.8834\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.0909 - val_loss: 34.0774\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.3294 - val_loss: 30.5409\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 21.4063 - val_loss: 34.8391\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 23.6665 - val_loss: 34.0480\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.8135 - val_loss: 30.5151\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 23.9256 - val_loss: 29.6499\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.3811 - val_loss: 29.5329\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 21.2837 - val_loss: 32.7593\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.8428 - val_loss: 30.1233\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.9340 - val_loss: 30.9407\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.7943 - val_loss: 30.4911\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.4747 - val_loss: 35.8438\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.0345 - val_loss: 31.9698\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 23.0262 - val_loss: 32.0193\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 22.0895 - val_loss: 35.1176\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 20.5772 - val_loss: 29.9591\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.4721 - val_loss: 31.7625\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 21.4610 - val_loss: 29.2950\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 20.6695 - val_loss: 30.1405\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 20.7594 - val_loss: 33.4093\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 21.8713 - val_loss: 32.0950\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 22.6928 - val_loss: 34.1571\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.6992 - val_loss: 31.2426\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.6418 - val_loss: 30.1871\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.4367 - val_loss: 35.5946\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.5211 - val_loss: 33.1617\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 19.5370 - val_loss: 31.7465\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.0511 - val_loss: 31.1144\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 19.5112 - val_loss: 30.4873\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.2054 - val_loss: 30.5732\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 19.6867 - val_loss: 30.1589\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 19.1874 - val_loss: 33.2529\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.8506 - val_loss: 31.0996\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.3403 - val_loss: 30.9046\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 19.6417 - val_loss: 33.1286\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.3593 - val_loss: 30.1772\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 20.3688 - val_loss: 36.3954\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.4027 - val_loss: 32.2013\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.9926 - val_loss: 30.0210\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 14ms/step - loss: 19.4174 - val_loss: 32.5761\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.1228 - val_loss: 31.9021\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.6408 - val_loss: 34.1207\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.4468 - val_loss: 31.5098\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.4182 - val_loss: 30.8478\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 18.1463 - val_loss: 32.7807\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 19.0386 - val_loss: 31.4306\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.4866 - val_loss: 36.4604\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 20.7828 - val_loss: 30.5535\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.9331 - val_loss: 33.3322\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.4390 - val_loss: 32.0334\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.0532 - val_loss: 30.6625\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 18.2359 - val_loss: 31.9435\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 18.0784 - val_loss: 31.5593\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 17.0866 - val_loss: 35.1686\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 17.7647 - val_loss: 31.7511\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 18.1176 - val_loss: 31.6576\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 17.1509 - val_loss: 30.9188\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.1782 - val_loss: 31.4324\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 16.8156 - val_loss: 34.9602\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 18.7909 - val_loss: 34.2941\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 17.6661 - val_loss: 32.0250\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 17.6232 - val_loss: 31.7843\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 16.9778 - val_loss: 31.0121\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.5993 - val_loss: 38.1330\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 17.0116 - val_loss: 32.1082\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 5s 14ms/step - loss: 16.5257 - val_loss: 33.4523\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 16.4112 - val_loss: 32.2375\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 16.7959 - val_loss: 30.8423\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.4924 - val_loss: 32.5004\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 17.6969 - val_loss: 33.7255\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.4584 - val_loss: 31.9238\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0383 - val_loss: 32.8469\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7934 - val_loss: 32.7578\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0709 - val_loss: 33.0897\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0222 - val_loss: 30.8393\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.0972 - val_loss: 32.2896\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.3092 - val_loss: 30.8686\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 16.5174 - val_loss: 33.0170\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 16.1974 - val_loss: 31.6746\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7677 - val_loss: 42.2650\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.2542 - val_loss: 33.0300\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.1615 - val_loss: 32.7739\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.7586 - val_loss: 31.7884\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 15.5971 - val_loss: 31.0448\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 15.0452 - val_loss: 33.4914\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 33.45899659243948\n",
      "Mean Absolute Error (MAE): 3.976147712055713\n",
      "Root Mean Squared Error (RMSE): 5.7843752119342575\n",
      "Time taken: 1142.4811537265778\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 21ms/step - loss: 1386.5380 - val_loss: 1250.0662\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1218.8514 - val_loss: 1142.7260\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 1121.5385 - val_loss: 1056.0416\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 1045.1796 - val_loss: 988.2572\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 978.7657 - val_loss: 922.9890\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 921.7620 - val_loss: 875.9683\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 874.6463 - val_loss: 828.0631\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 830.3440 - val_loss: 785.7349\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 787.5928 - val_loss: 744.3919\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 746.4864 - val_loss: 706.2775\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 706.8829 - val_loss: 669.7849\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 668.6444 - val_loss: 636.7030\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 632.8298 - val_loss: 597.2798\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 589.0958 - val_loss: 555.4204\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 549.8395 - val_loss: 518.9604\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 509.5321 - val_loss: 476.8179\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 468.2174 - val_loss: 433.5765\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 430.6354 - val_loss: 414.4832\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 395.3018 - val_loss: 368.4149\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 365.4525 - val_loss: 337.3553\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 334.2318 - val_loss: 309.3947\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 302.9698 - val_loss: 280.2264\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 276.7494 - val_loss: 254.5055\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 254.3433 - val_loss: 242.6792\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 233.3512 - val_loss: 214.3839\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 213.5557 - val_loss: 195.3813\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 194.1647 - val_loss: 178.6096\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 176.6051 - val_loss: 167.2730\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 161.8239 - val_loss: 150.0347\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 148.7752 - val_loss: 133.8176\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 135.6414 - val_loss: 126.3556\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 124.0285 - val_loss: 113.0672\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 115.5164 - val_loss: 115.7152\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 105.4727 - val_loss: 102.8089\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 98.4339 - val_loss: 93.2640\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 91.5048 - val_loss: 83.5607\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 86.2180 - val_loss: 82.3783\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 78.1268 - val_loss: 78.0546\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 76.1405 - val_loss: 68.3952\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 69.3870 - val_loss: 77.1861\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 66.9384 - val_loss: 67.2417\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 66.0097 - val_loss: 65.0765\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 62.7433 - val_loss: 62.9133\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 60.8473 - val_loss: 57.2044\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 56.2354 - val_loss: 57.9254\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 55.2346 - val_loss: 50.9598\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 54.4008 - val_loss: 54.3626\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 49.7460 - val_loss: 51.1296\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 50.0033 - val_loss: 54.1398\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 49.6002 - val_loss: 45.3463\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 46.0622 - val_loss: 44.1121\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 46.1379 - val_loss: 55.9408\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 44.3506 - val_loss: 43.2426\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 43.0726 - val_loss: 52.4423\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 42.4847 - val_loss: 47.7815\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 40.2802 - val_loss: 40.8853\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 40.8516 - val_loss: 38.5338\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 38.9351 - val_loss: 38.4167\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 38.4312 - val_loss: 39.1397\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 37.7799 - val_loss: 49.4328\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 37.6777 - val_loss: 36.5883\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 36.6290 - val_loss: 40.1834\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 35.7908 - val_loss: 37.6454\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 35.1004 - val_loss: 38.0133\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 35.0605 - val_loss: 37.5946\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 36.2798 - val_loss: 41.2277\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 34.7172 - val_loss: 36.7463\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 34.7699 - val_loss: 34.1844\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.9239 - val_loss: 36.3978\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.8091 - val_loss: 34.3505\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.7261 - val_loss: 43.2020\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 33.0207 - val_loss: 32.9642\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 33.1112 - val_loss: 33.6892\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.4341 - val_loss: 34.1617\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.1671 - val_loss: 34.6950\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 32.6469 - val_loss: 33.2812\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.1349 - val_loss: 34.8852\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 17ms/step - loss: 30.5152 - val_loss: 31.5619\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.5368 - val_loss: 37.4820\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.8738 - val_loss: 36.3637\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.5010 - val_loss: 31.6921\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 31.2355 - val_loss: 51.5018\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.4211 - val_loss: 34.3858\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.6155 - val_loss: 53.9360\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 30.0638 - val_loss: 30.3019\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.9596 - val_loss: 33.6051\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.1404 - val_loss: 31.3428\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.6481 - val_loss: 30.6107\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.5551 - val_loss: 32.4617\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.5092 - val_loss: 36.5268\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.3986 - val_loss: 33.0798\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.9967 - val_loss: 30.3840\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 29.3043 - val_loss: 32.5903\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 28.0649 - val_loss: 30.6729\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.9377 - val_loss: 33.9318\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.6588 - val_loss: 30.6491\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.7799 - val_loss: 33.5100\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.7564 - val_loss: 32.2972\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.4602 - val_loss: 29.8104\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.6969 - val_loss: 36.7534\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 27.1646 - val_loss: 29.3456\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.4219 - val_loss: 33.9991\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.6301 - val_loss: 29.2914\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.5412 - val_loss: 29.4853\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.5350 - val_loss: 32.0853\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.4520 - val_loss: 29.6630\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.7908 - val_loss: 31.3687\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.1621 - val_loss: 34.7290\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 27.0519 - val_loss: 32.7996\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.2814 - val_loss: 29.7008\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.1092 - val_loss: 32.1237\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 26.6661 - val_loss: 31.2030\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 25.9416 - val_loss: 33.0597\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.9550 - val_loss: 33.4389\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 25.7881 - val_loss: 31.1063\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 26.1628 - val_loss: 32.2855\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.5538 - val_loss: 30.4194\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.7646 - val_loss: 28.9226\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.6631 - val_loss: 28.3888\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.7427 - val_loss: 28.6435\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.9884 - val_loss: 33.1204\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.6323 - val_loss: 28.5955\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.3228 - val_loss: 30.1667\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.1886 - val_loss: 28.2136\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.7852 - val_loss: 29.8392\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.1569 - val_loss: 29.1179\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.7853 - val_loss: 28.9314\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.3399 - val_loss: 31.8504\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.4671 - val_loss: 29.3491\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.6277 - val_loss: 32.6785\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.7178 - val_loss: 34.5827\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.1042 - val_loss: 33.5787\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.3030 - val_loss: 36.3753\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.1371 - val_loss: 32.9749\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.9778 - val_loss: 29.3616\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.0695 - val_loss: 31.9646\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.4839 - val_loss: 31.9620\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.8638 - val_loss: 29.9348\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.0860 - val_loss: 32.5345\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.9223 - val_loss: 31.0382\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 24.0523 - val_loss: 33.1598\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 24.1389 - val_loss: 33.2683\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.4668 - val_loss: 28.8134\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.0149 - val_loss: 30.7479\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.9990 - val_loss: 30.7605\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.0533 - val_loss: 39.9008\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.6320 - val_loss: 29.0759\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.5990 - val_loss: 29.2159\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 8s 20ms/step - loss: 22.1059 - val_loss: 33.9928\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 23.1427 - val_loss: 31.1583\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.6058 - val_loss: 28.9201\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.4929 - val_loss: 27.9655\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 22.0820 - val_loss: 29.6314\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 22.4718 - val_loss: 29.3030\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 7s 19ms/step - loss: 21.9440 - val_loss: 31.0740\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.3938 - val_loss: 34.8686\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 21.9257 - val_loss: 28.2990\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 21.1380 - val_loss: 29.3236\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.0117 - val_loss: 37.3636\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.8405 - val_loss: 30.1601\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.3611 - val_loss: 28.0116\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.0766 - val_loss: 31.9552\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.3906 - val_loss: 29.3303\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.0421 - val_loss: 31.6461\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.1806 - val_loss: 32.1971\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.0892 - val_loss: 29.2693\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.5934 - val_loss: 29.8834\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.0931 - val_loss: 28.5304\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.1082 - val_loss: 29.9922\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.5542 - val_loss: 33.2995\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.2545 - val_loss: 30.7609\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.8934 - val_loss: 29.0580\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 21.3795 - val_loss: 31.9733\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 20.5543 - val_loss: 30.8909\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 19.2102 - val_loss: 30.6602\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.9841 - val_loss: 29.7354\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.5249 - val_loss: 32.5265\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.4845 - val_loss: 30.7119\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.0102 - val_loss: 30.9816\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.0197 - val_loss: 28.7533\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.0872 - val_loss: 31.0310\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.9119 - val_loss: 28.3834\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.8239 - val_loss: 35.5629\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.2068 - val_loss: 28.5130\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.6858 - val_loss: 31.6260\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.8240 - val_loss: 28.8804\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.0439 - val_loss: 30.2284\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.1739 - val_loss: 29.9657\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.1574 - val_loss: 30.5050\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.8997 - val_loss: 31.7924\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 18.5860 - val_loss: 32.2735\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.6275 - val_loss: 28.5970\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.8156 - val_loss: 36.5587\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.4201 - val_loss: 30.3085\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.5885 - val_loss: 30.4055\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.4732 - val_loss: 29.4297\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.3806 - val_loss: 28.6817\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.0298 - val_loss: 29.6544\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.8551 - val_loss: 32.9160\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.1814 - val_loss: 30.6932\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 30.70194121898454\n",
      "Mean Absolute Error (MAE): 3.818375234030805\n",
      "Root Mean Squared Error (RMSE): 5.540933244407889\n",
      "Time taken: 1371.4043643474579\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1352.0183 - val_loss: 1277.5793\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1191.4156 - val_loss: 1180.5414\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1108.5089 - val_loss: 1109.5708\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1047.1418 - val_loss: 1057.1443\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1001.8472 - val_loss: 1018.4426\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 969.1844 - val_loss: 991.0693\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 946.5109 - val_loss: 972.5133\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 931.5146 - val_loss: 960.5502\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 922.3185 - val_loss: 953.4711\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 917.1227 - val_loss: 949.6537\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 914.4277 - val_loss: 947.7570\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 913.1658 - val_loss: 946.9171\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.7407 - val_loss: 946.5853\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.5250 - val_loss: 946.5331\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.5231 - val_loss: 946.5146\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.5123 - val_loss: 946.5405\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4604 - val_loss: 946.5408\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4554 - val_loss: 946.5382\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 912.4855 - val_loss: 946.5408\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 888.7864 - val_loss: 853.4549\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 798.5386 - val_loss: 791.7562\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 742.3725 - val_loss: 739.0538\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 696.9835 - val_loss: 731.0214\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 657.8759 - val_loss: 659.0402\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 619.5976 - val_loss: 618.5144\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 579.9372 - val_loss: 570.9785\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 541.5168 - val_loss: 531.3256\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 495.3112 - val_loss: 488.2497\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 455.4687 - val_loss: 459.6702\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 422.5721 - val_loss: 418.4928\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 393.9334 - val_loss: 385.7294\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 364.0801 - val_loss: 356.5662\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 334.7639 - val_loss: 333.5107\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 308.1148 - val_loss: 309.6730\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 286.2818 - val_loss: 284.8890\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 262.4583 - val_loss: 283.7173\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 243.7495 - val_loss: 234.0440\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 220.4534 - val_loss: 212.7240\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 202.1650 - val_loss: 199.2411\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 185.0901 - val_loss: 179.4524\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 170.8371 - val_loss: 162.1511\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 155.2058 - val_loss: 149.6646\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 144.1933 - val_loss: 139.6233\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 131.8688 - val_loss: 129.5476\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 122.6076 - val_loss: 113.7683\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 113.2231 - val_loss: 106.0683\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 106.4405 - val_loss: 99.5349\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 98.9121 - val_loss: 94.6900\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 92.4666 - val_loss: 96.5508\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 89.1355 - val_loss: 83.1984\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 80.2903 - val_loss: 83.9092\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 78.0145 - val_loss: 75.6487\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 74.3184 - val_loss: 69.3581\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 72.2281 - val_loss: 77.8692\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 71.4382 - val_loss: 64.9839\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.1680 - val_loss: 71.6980\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.2040 - val_loss: 56.7637\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 63.1184 - val_loss: 73.8298\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 59.7446 - val_loss: 56.6429\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 60.2115 - val_loss: 56.4224\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 56.5185 - val_loss: 63.4131\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 56.2365 - val_loss: 54.7758\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 54.3147 - val_loss: 56.4073\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.7489 - val_loss: 59.3339\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 52.6105 - val_loss: 49.4748\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 51.4711 - val_loss: 48.0743\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 50.3903 - val_loss: 50.6913\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 50.5516 - val_loss: 47.7188\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.6011 - val_loss: 54.8256\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 49.3231 - val_loss: 51.0944\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.3213 - val_loss: 46.6876\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.0443 - val_loss: 42.7528\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.5750 - val_loss: 45.6342\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 46.4260 - val_loss: 44.7882\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 45.3111 - val_loss: 50.4351\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.6091 - val_loss: 46.4110\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.1043 - val_loss: 40.0614\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 17ms/step - loss: 42.0218 - val_loss: 42.9443\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.5843 - val_loss: 49.5417\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 40.2929 - val_loss: 41.8202\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 39.5831 - val_loss: 38.2300\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.9523 - val_loss: 39.8500\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 36.9441 - val_loss: 38.0859\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.6728 - val_loss: 42.3326\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.3772 - val_loss: 43.2906\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 35.1555 - val_loss: 49.5807\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 35.1526 - val_loss: 45.3346\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.9871 - val_loss: 34.6955\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.9189 - val_loss: 38.3710\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.8898 - val_loss: 38.1163\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.3006 - val_loss: 35.4816\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.9966 - val_loss: 42.8661\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.8033 - val_loss: 36.1382\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.8517 - val_loss: 36.9164\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.2560 - val_loss: 49.7308\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.4154 - val_loss: 32.9936\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.3910 - val_loss: 36.9438\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.7710 - val_loss: 34.5573\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.1875 - val_loss: 33.1054\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.7454 - val_loss: 37.3702\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.5362 - val_loss: 34.8426\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.1698 - val_loss: 35.9848\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.0786 - val_loss: 38.8690\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.3157 - val_loss: 34.8119\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.7138 - val_loss: 33.2221\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.0531 - val_loss: 31.4124\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.5303 - val_loss: 40.2687\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.1379 - val_loss: 33.1178\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.7722 - val_loss: 33.0606\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.2070 - val_loss: 34.7192\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.1364 - val_loss: 30.8748\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.5673 - val_loss: 34.8389\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.0973 - val_loss: 35.1249\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.2055 - val_loss: 36.9300\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.2648 - val_loss: 31.8481\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.0426 - val_loss: 29.6277\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.7135 - val_loss: 33.1459\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.5131 - val_loss: 30.3707\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.8862 - val_loss: 31.3996\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.4437 - val_loss: 31.3430\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.3774 - val_loss: 32.2408\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.2574 - val_loss: 33.3348\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.8975 - val_loss: 32.1646\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.7701 - val_loss: 38.9935\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.6715 - val_loss: 35.3226\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.9017 - val_loss: 30.5607\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.4083 - val_loss: 32.6350\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.8601 - val_loss: 35.4072\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.1337 - val_loss: 31.7938\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.1567 - val_loss: 39.1397\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1767 - val_loss: 30.5046\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.7128 - val_loss: 36.0728\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9961 - val_loss: 32.7999\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.9717 - val_loss: 32.3518\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.9478 - val_loss: 31.8822\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 24.1046 - val_loss: 31.8626\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 24.5769 - val_loss: 35.7711\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.3837 - val_loss: 30.6872\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.0172 - val_loss: 37.2151\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 25.0850 - val_loss: 30.6053\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.5796 - val_loss: 33.8030\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 25.0195 - val_loss: 32.2534\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.2783 - val_loss: 37.3604\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.4488 - val_loss: 32.6131\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.0534 - val_loss: 31.7035\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.4973 - val_loss: 32.0725\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.3804 - val_loss: 33.9449\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9417 - val_loss: 32.2691\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9477 - val_loss: 36.9720\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9186 - val_loss: 33.2988\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.0600 - val_loss: 31.1991\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.9128 - val_loss: 33.4068\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.4779 - val_loss: 31.0996\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.7152 - val_loss: 33.5024\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 23.8840 - val_loss: 30.8093\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.1610 - val_loss: 31.9961\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.3156 - val_loss: 31.7121\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.1955 - val_loss: 32.6448\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.4483 - val_loss: 30.5700\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.2273 - val_loss: 32.3593\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.7547 - val_loss: 32.5028\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.3079 - val_loss: 33.5043\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.7286 - val_loss: 30.2697\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.3749 - val_loss: 32.2907\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.2235 - val_loss: 32.0992\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.0953 - val_loss: 31.1558\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.2027 - val_loss: 30.6656\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.2249 - val_loss: 32.4322\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4943 - val_loss: 33.3286\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.5201 - val_loss: 34.7566\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 21.5188 - val_loss: 33.9293\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4480 - val_loss: 35.2997\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.2872 - val_loss: 30.7321\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.0627 - val_loss: 30.0794\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.5738 - val_loss: 31.8151\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3029 - val_loss: 34.3583\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 21.3587 - val_loss: 32.9158\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 21.0512 - val_loss: 30.3006\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.5895 - val_loss: 35.6470\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3054 - val_loss: 32.8079\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4100 - val_loss: 32.1926\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.0619 - val_loss: 30.8563\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.8403 - val_loss: 29.9718\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3055 - val_loss: 30.1964\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.9209 - val_loss: 39.4353\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3375 - val_loss: 36.5791\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.3261 - val_loss: 30.0147\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.4698 - val_loss: 32.1551\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.0893 - val_loss: 33.9012\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.6726 - val_loss: 33.2978\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.1106 - val_loss: 32.6530\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.0873 - val_loss: 35.1528\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.4000 - val_loss: 31.9383\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.5595 - val_loss: 31.9662\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.4563 - val_loss: 32.6086\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.1354 - val_loss: 31.0652\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 18.0770 - val_loss: 34.9207\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 19.1705 - val_loss: 35.4799\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.7971 - val_loss: 32.3137\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 18.0818 - val_loss: 32.1806\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 32.16079361253658\n",
      "Mean Absolute Error (MAE): 3.9394843551468512\n",
      "Root Mean Squared Error (RMSE): 5.671048722461885\n",
      "Time taken: 1292.1589386463165\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 11s 19ms/step - loss: 1383.2659 - val_loss: 1265.3861\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1204.7135 - val_loss: 1144.5723\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1103.7258 - val_loss: 1052.0084\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 1018.7441 - val_loss: 973.4088\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 946.3658 - val_loss: 907.7002\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 885.5010 - val_loss: 852.7779\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 834.2805 - val_loss: 805.5449\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 792.2163 - val_loss: 766.6058\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 752.8892 - val_loss: 730.8708\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 716.3323 - val_loss: 694.0897\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 683.5674 - val_loss: 662.3961\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 649.5256 - val_loss: 629.1550\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 616.4839 - val_loss: 599.7565\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 584.2733 - val_loss: 565.7856\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 548.8849 - val_loss: 527.6232\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 514.5011 - val_loss: 492.8724\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 479.2645 - val_loss: 462.0165\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 443.5115 - val_loss: 430.0529\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 409.7718 - val_loss: 394.7969\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 378.8840 - val_loss: 366.8632\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 347.8403 - val_loss: 337.3661\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 318.6022 - val_loss: 307.7682\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 291.8669 - val_loss: 289.3994\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 267.7592 - val_loss: 268.7961\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 244.8507 - val_loss: 247.0515\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 226.1937 - val_loss: 223.5505\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 205.3619 - val_loss: 225.0433\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 190.9023 - val_loss: 189.2194\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 171.0275 - val_loss: 167.4899\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 156.5019 - val_loss: 162.5525\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 143.2243 - val_loss: 141.5475\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 133.5153 - val_loss: 133.5105\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 122.0135 - val_loss: 138.1125\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 111.8781 - val_loss: 114.3963\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 102.1093 - val_loss: 122.0778\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 96.9215 - val_loss: 96.6362\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 90.6491 - val_loss: 94.7818\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 85.5886 - val_loss: 85.8187\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 79.4724 - val_loss: 94.9838\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 75.6776 - val_loss: 76.8350\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 70.6084 - val_loss: 78.7964\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 66.8412 - val_loss: 68.2487\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 63.8150 - val_loss: 79.4107\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 64.5443 - val_loss: 65.4395\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 60.7929 - val_loss: 64.6063\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 57.0119 - val_loss: 61.7393\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 56.9231 - val_loss: 63.3858\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 53.8854 - val_loss: 59.2276\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 53.4596 - val_loss: 54.4690\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 50.8851 - val_loss: 52.7361\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 48.9946 - val_loss: 69.5238\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 48.9340 - val_loss: 50.4722\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.5980 - val_loss: 49.6609\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 47.3277 - val_loss: 55.5606\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 48.0731 - val_loss: 50.9254\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 44.2444 - val_loss: 47.0881\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 43.2918 - val_loss: 43.3480\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 42.2455 - val_loss: 54.4131\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 42.2936 - val_loss: 48.1965\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 41.0345 - val_loss: 42.1038\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.7013 - val_loss: 42.5176\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 40.7454 - val_loss: 42.3423\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 39.3173 - val_loss: 40.9073\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 38.3301 - val_loss: 46.7218\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 37.4146 - val_loss: 48.6826\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.6441 - val_loss: 44.7109\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 36.5420 - val_loss: 39.7281\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 36.0953 - val_loss: 36.7887\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.9065 - val_loss: 38.6968\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 34.1621 - val_loss: 39.4644\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 33.7486 - val_loss: 38.6081\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 33.5378 - val_loss: 35.9995\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 34.3323 - val_loss: 37.7888\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.7285 - val_loss: 38.7089\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 33.0240 - val_loss: 38.7627\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.1164 - val_loss: 32.8058\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 32.0155 - val_loss: 34.8690\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 32.2869 - val_loss: 34.5841\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.6370 - val_loss: 34.2277\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.0559 - val_loss: 37.2547\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 31.0360 - val_loss: 32.9137\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 30.1908 - val_loss: 33.9036\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 30.2900 - val_loss: 32.6421\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.6223 - val_loss: 32.4875\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 30.1361 - val_loss: 34.2570\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 30.2918 - val_loss: 33.1999\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.1682 - val_loss: 32.2473\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.0565 - val_loss: 31.0406\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 30.5394 - val_loss: 32.5383\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.1161 - val_loss: 35.9846\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 28.8291 - val_loss: 31.5663\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 29.9511 - val_loss: 32.2245\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.7018 - val_loss: 31.0457\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 29.0223 - val_loss: 31.7401\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.7416 - val_loss: 32.9567\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.8287 - val_loss: 32.4475\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 27.9509 - val_loss: 30.5769\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 28.0797 - val_loss: 29.7760\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 28.5654 - val_loss: 36.4774\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.3907 - val_loss: 30.5645\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.8021 - val_loss: 32.3789\n",
      "Epoch 102/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.4639 - val_loss: 33.0460\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.5485 - val_loss: 29.3976\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.0617 - val_loss: 31.0414\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1907 - val_loss: 36.2276\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 27.2026 - val_loss: 30.1852\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 28.0729 - val_loss: 30.9039\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.9190 - val_loss: 31.9781\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 26.4090 - val_loss: 35.6439\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.1941 - val_loss: 30.0364\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.5423 - val_loss: 31.6570\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.5194 - val_loss: 34.5065\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.0748 - val_loss: 29.1517\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 26.7709 - val_loss: 33.3200\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.4197 - val_loss: 31.0048\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.3573 - val_loss: 29.0175\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.8703 - val_loss: 30.9404\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.9966 - val_loss: 39.3626\n",
      "Epoch 119/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.1789 - val_loss: 32.3124\n",
      "Epoch 120/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.4278 - val_loss: 33.9866\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.6990 - val_loss: 34.2919\n",
      "Epoch 122/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 24.5453 - val_loss: 30.9287\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 25.6544 - val_loss: 30.2551\n",
      "Epoch 124/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.2336 - val_loss: 30.4491\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 25.1010 - val_loss: 30.3967\n",
      "Epoch 126/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 24.2441 - val_loss: 29.8499\n",
      "Epoch 127/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.3577 - val_loss: 30.0549\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.9960 - val_loss: 28.7237\n",
      "Epoch 129/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.8465 - val_loss: 31.8304\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.4081 - val_loss: 29.3335\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.1993 - val_loss: 29.7635\n",
      "Epoch 132/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.8297 - val_loss: 30.8483\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.3250 - val_loss: 29.2711\n",
      "Epoch 134/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 23.4499 - val_loss: 28.6487\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 24.1852 - val_loss: 33.9892\n",
      "Epoch 136/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.7617 - val_loss: 31.6052\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 23.1368 - val_loss: 33.5999\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.8818 - val_loss: 31.4646\n",
      "Epoch 139/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.4400 - val_loss: 35.4056\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.3833 - val_loss: 34.4378\n",
      "Epoch 141/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.9470 - val_loss: 41.2174\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 23.7942 - val_loss: 30.5464\n",
      "Epoch 143/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.0646 - val_loss: 31.8202\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.5458 - val_loss: 30.0315\n",
      "Epoch 145/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.6226 - val_loss: 31.7473\n",
      "Epoch 146/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 22.9065 - val_loss: 32.6163\n",
      "Epoch 147/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.0538 - val_loss: 33.3479\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.3481 - val_loss: 28.9904\n",
      "Epoch 149/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.5246 - val_loss: 36.3452\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.3967 - val_loss: 31.5160\n",
      "Epoch 151/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.5226 - val_loss: 33.8282\n",
      "Epoch 152/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.8855 - val_loss: 31.8842\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.2876 - val_loss: 30.4341\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.5451 - val_loss: 32.0593\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 6s 16ms/step - loss: 21.4466 - val_loss: 30.9758\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 22.0016 - val_loss: 29.4548\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.6300 - val_loss: 43.6109\n",
      "Epoch 158/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 21.5050 - val_loss: 30.9974\n",
      "Epoch 159/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.6305 - val_loss: 29.8997\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.3277 - val_loss: 30.0880\n",
      "Epoch 161/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.0698 - val_loss: 33.8724\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 22.5476 - val_loss: 32.5504\n",
      "Epoch 163/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.7275 - val_loss: 32.9778\n",
      "Epoch 164/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.6090 - val_loss: 31.4171\n",
      "Epoch 165/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.2488 - val_loss: 32.5561\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.9692 - val_loss: 30.7934\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.9179 - val_loss: 31.7080\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.8865 - val_loss: 32.0661\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 21.7494 - val_loss: 30.2635\n",
      "Epoch 170/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.5336 - val_loss: 29.4630\n",
      "Epoch 171/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 19.7283 - val_loss: 29.6693\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.2211 - val_loss: 32.0280\n",
      "Epoch 173/200\n",
      "391/391 [==============================] - 6s 16ms/step - loss: 20.2873 - val_loss: 31.1159\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 20.5038 - val_loss: 34.1727\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.6486 - val_loss: 30.5452\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 19.8388 - val_loss: 30.8435\n",
      "Epoch 177/200\n",
      "391/391 [==============================] - 7s 18ms/step - loss: 20.0996 - val_loss: 30.5136\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.0923 - val_loss: 33.2541\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.5924 - val_loss: 31.9028\n",
      "Epoch 180/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.3278 - val_loss: 30.0643\n",
      "Epoch 181/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.9205 - val_loss: 30.6841\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - 6s 17ms/step - loss: 18.8621 - val_loss: 31.4794\n",
      "Epoch 183/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.5087 - val_loss: 33.6490\n",
      "Epoch 184/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 20.0941 - val_loss: 31.1674\n",
      "Epoch 185/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.0991 - val_loss: 33.8542\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.3980 - val_loss: 30.3078\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.2795 - val_loss: 30.9425\n",
      "Epoch 188/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.5621 - val_loss: 30.6477\n",
      "Epoch 189/200\n",
      "391/391 [==============================] - 7s 19ms/step - loss: 19.4023 - val_loss: 31.0241\n",
      "Epoch 190/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.6271 - val_loss: 31.8984\n",
      "Epoch 191/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.7033 - val_loss: 29.8663\n",
      "Epoch 192/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.0688 - val_loss: 33.2589\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.3330 - val_loss: 31.3617\n",
      "Epoch 194/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.1498 - val_loss: 31.1822\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.5849 - val_loss: 34.7063\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.6572 - val_loss: 31.9673\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 19.5234 - val_loss: 30.5340\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 18.1590 - val_loss: 30.5971\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.5484 - val_loss: 30.4558\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - 7s 17ms/step - loss: 17.6321 - val_loss: 33.5557\n",
      "16/16 [==============================] - 1s 9ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 33.571987890993626\n",
      "Mean Absolute Error (MAE): 3.9995373567980064\n",
      "Root Mean Squared Error (RMSE): 5.7941339206989015\n",
      "Time taken: 1299.9168276786804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3))\n",
    "\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=5, validation_data=(X_test, y_test))\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 8, 512)            1052672   \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 8, 256)            787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,037,635\n",
      "Trainable params: 2,037,635\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_10936\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold        MSE       MAE      RMSE   Time taken\n",
      "0        1  30.762557  3.810534  5.546400  1294.389091\n",
      "1        2  33.458997  3.976148  5.784375  1142.481154\n",
      "2        3  30.701941  3.818375  5.540933  1371.404364\n",
      "3        4  32.160794  3.939484  5.671049  1292.158939\n",
      "4        5  33.571988  3.999537  5.794134  1299.916828\n",
      "5  Average  32.131255  3.908816  5.667378  1280.070075\n",
      "Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('DL_Result_PL_model_1_Scattered_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'DL_Result_PL_model_1_Scattered_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
