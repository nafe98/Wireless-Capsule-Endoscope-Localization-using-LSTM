{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f_obese.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.504459</td>\n",
       "      <td>86.399597</td>\n",
       "      <td>79.080492</td>\n",
       "      <td>87.102449</td>\n",
       "      <td>86.771802</td>\n",
       "      <td>89.147780</td>\n",
       "      <td>85.664094</td>\n",
       "      <td>91.059360</td>\n",
       "      <td>83.770213</td>\n",
       "      <td>88.197770</td>\n",
       "      <td>...</td>\n",
       "      <td>81.239405</td>\n",
       "      <td>78.048687</td>\n",
       "      <td>84.186534</td>\n",
       "      <td>79.617342</td>\n",
       "      <td>88.756527</td>\n",
       "      <td>87.301170</td>\n",
       "      <td>84.126977</td>\n",
       "      <td>87.795904</td>\n",
       "      <td>67.633187</td>\n",
       "      <td>78.108970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.519259</td>\n",
       "      <td>86.593020</td>\n",
       "      <td>79.101701</td>\n",
       "      <td>86.823706</td>\n",
       "      <td>86.758531</td>\n",
       "      <td>89.178741</td>\n",
       "      <td>85.666877</td>\n",
       "      <td>90.877329</td>\n",
       "      <td>83.697448</td>\n",
       "      <td>88.085323</td>\n",
       "      <td>...</td>\n",
       "      <td>81.155170</td>\n",
       "      <td>78.370631</td>\n",
       "      <td>84.187540</td>\n",
       "      <td>79.793881</td>\n",
       "      <td>88.724417</td>\n",
       "      <td>87.359682</td>\n",
       "      <td>84.163713</td>\n",
       "      <td>87.768586</td>\n",
       "      <td>67.625241</td>\n",
       "      <td>78.155214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.537091</td>\n",
       "      <td>86.782692</td>\n",
       "      <td>79.118162</td>\n",
       "      <td>86.546519</td>\n",
       "      <td>86.749143</td>\n",
       "      <td>89.213281</td>\n",
       "      <td>85.665636</td>\n",
       "      <td>90.703549</td>\n",
       "      <td>83.626904</td>\n",
       "      <td>87.974585</td>\n",
       "      <td>...</td>\n",
       "      <td>81.075011</td>\n",
       "      <td>78.688196</td>\n",
       "      <td>84.187046</td>\n",
       "      <td>79.965533</td>\n",
       "      <td>88.695040</td>\n",
       "      <td>87.420345</td>\n",
       "      <td>84.200228</td>\n",
       "      <td>87.739953</td>\n",
       "      <td>67.616693</td>\n",
       "      <td>78.199453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.557419</td>\n",
       "      <td>86.968268</td>\n",
       "      <td>79.130002</td>\n",
       "      <td>86.270870</td>\n",
       "      <td>86.743626</td>\n",
       "      <td>89.251556</td>\n",
       "      <td>85.660263</td>\n",
       "      <td>90.537891</td>\n",
       "      <td>83.558588</td>\n",
       "      <td>87.865334</td>\n",
       "      <td>...</td>\n",
       "      <td>80.998904</td>\n",
       "      <td>79.000898</td>\n",
       "      <td>84.184876</td>\n",
       "      <td>80.132052</td>\n",
       "      <td>88.668364</td>\n",
       "      <td>87.483563</td>\n",
       "      <td>84.236669</td>\n",
       "      <td>87.710200</td>\n",
       "      <td>67.608228</td>\n",
       "      <td>78.241218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.579760</td>\n",
       "      <td>87.149185</td>\n",
       "      <td>79.137413</td>\n",
       "      <td>85.996588</td>\n",
       "      <td>86.742103</td>\n",
       "      <td>89.293636</td>\n",
       "      <td>85.650554</td>\n",
       "      <td>90.379940</td>\n",
       "      <td>83.492659</td>\n",
       "      <td>87.757402</td>\n",
       "      <td>...</td>\n",
       "      <td>80.927080</td>\n",
       "      <td>79.308376</td>\n",
       "      <td>84.181101</td>\n",
       "      <td>80.293286</td>\n",
       "      <td>88.644123</td>\n",
       "      <td>87.549464</td>\n",
       "      <td>84.272948</td>\n",
       "      <td>87.679623</td>\n",
       "      <td>67.600649</td>\n",
       "      <td>78.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>92.020713</td>\n",
       "      <td>87.437522</td>\n",
       "      <td>78.460299</td>\n",
       "      <td>75.652257</td>\n",
       "      <td>91.864426</td>\n",
       "      <td>93.264508</td>\n",
       "      <td>87.532459</td>\n",
       "      <td>84.719985</td>\n",
       "      <td>82.388790</td>\n",
       "      <td>83.227223</td>\n",
       "      <td>...</td>\n",
       "      <td>78.864388</td>\n",
       "      <td>80.495710</td>\n",
       "      <td>80.470719</td>\n",
       "      <td>81.113167</td>\n",
       "      <td>91.693359</td>\n",
       "      <td>91.156410</td>\n",
       "      <td>84.697309</td>\n",
       "      <td>85.748841</td>\n",
       "      <td>76.423079</td>\n",
       "      <td>71.173074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>92.240183</td>\n",
       "      <td>87.534982</td>\n",
       "      <td>78.470896</td>\n",
       "      <td>75.771393</td>\n",
       "      <td>91.834236</td>\n",
       "      <td>93.460502</td>\n",
       "      <td>87.600872</td>\n",
       "      <td>84.600752</td>\n",
       "      <td>82.230454</td>\n",
       "      <td>83.128281</td>\n",
       "      <td>...</td>\n",
       "      <td>78.829764</td>\n",
       "      <td>80.388767</td>\n",
       "      <td>80.310960</td>\n",
       "      <td>81.125860</td>\n",
       "      <td>91.847410</td>\n",
       "      <td>91.294340</td>\n",
       "      <td>84.606032</td>\n",
       "      <td>85.729828</td>\n",
       "      <td>76.590800</td>\n",
       "      <td>71.336844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>92.461814</td>\n",
       "      <td>87.630229</td>\n",
       "      <td>78.486618</td>\n",
       "      <td>75.894408</td>\n",
       "      <td>91.807209</td>\n",
       "      <td>93.653126</td>\n",
       "      <td>87.672303</td>\n",
       "      <td>84.480406</td>\n",
       "      <td>82.073045</td>\n",
       "      <td>83.028264</td>\n",
       "      <td>...</td>\n",
       "      <td>78.797137</td>\n",
       "      <td>80.284453</td>\n",
       "      <td>80.149074</td>\n",
       "      <td>81.137419</td>\n",
       "      <td>92.003863</td>\n",
       "      <td>91.430279</td>\n",
       "      <td>84.514645</td>\n",
       "      <td>85.711048</td>\n",
       "      <td>76.759045</td>\n",
       "      <td>71.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.685277</td>\n",
       "      <td>87.722960</td>\n",
       "      <td>78.507761</td>\n",
       "      <td>76.021213</td>\n",
       "      <td>91.783334</td>\n",
       "      <td>93.842537</td>\n",
       "      <td>87.746920</td>\n",
       "      <td>84.359183</td>\n",
       "      <td>81.916747</td>\n",
       "      <td>82.926738</td>\n",
       "      <td>...</td>\n",
       "      <td>78.767266</td>\n",
       "      <td>80.182617</td>\n",
       "      <td>79.985102</td>\n",
       "      <td>81.147198</td>\n",
       "      <td>92.162897</td>\n",
       "      <td>91.563879</td>\n",
       "      <td>84.422441</td>\n",
       "      <td>85.691773</td>\n",
       "      <td>76.928062</td>\n",
       "      <td>71.674240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.909981</td>\n",
       "      <td>87.812998</td>\n",
       "      <td>78.534559</td>\n",
       "      <td>76.151518</td>\n",
       "      <td>91.762590</td>\n",
       "      <td>94.029071</td>\n",
       "      <td>87.824930</td>\n",
       "      <td>84.237114</td>\n",
       "      <td>81.761408</td>\n",
       "      <td>82.823075</td>\n",
       "      <td>...</td>\n",
       "      <td>78.740971</td>\n",
       "      <td>80.082791</td>\n",
       "      <td>79.819085</td>\n",
       "      <td>81.154675</td>\n",
       "      <td>92.324398</td>\n",
       "      <td>91.694908</td>\n",
       "      <td>84.328841</td>\n",
       "      <td>85.671334</td>\n",
       "      <td>77.098275</td>\n",
       "      <td>71.847731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     83.504459  86.399597  79.080492  87.102449  86.771802  89.147780   \n",
       "1     83.519259  86.593020  79.101701  86.823706  86.758531  89.178741   \n",
       "2     83.537091  86.782692  79.118162  86.546519  86.749143  89.213281   \n",
       "3     83.557419  86.968268  79.130002  86.270870  86.743626  89.251556   \n",
       "4     83.579760  87.149185  79.137413  85.996588  86.742103  89.293636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  92.020713  87.437522  78.460299  75.652257  91.864426  93.264508   \n",
       "2439  92.240183  87.534982  78.470896  75.771393  91.834236  93.460502   \n",
       "2440  92.461814  87.630229  78.486618  75.894408  91.807209  93.653126   \n",
       "2441  92.685277  87.722960  78.507761  76.021213  91.783334  93.842537   \n",
       "2442  92.909981  87.812998  78.534559  76.151518  91.762590  94.029071   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     85.664094  91.059360  83.770213  88.197770  ...  81.239405  78.048687   \n",
       "1     85.666877  90.877329  83.697448  88.085323  ...  81.155170  78.370631   \n",
       "2     85.665636  90.703549  83.626904  87.974585  ...  81.075011  78.688196   \n",
       "3     85.660263  90.537891  83.558588  87.865334  ...  80.998904  79.000898   \n",
       "4     85.650554  90.379940  83.492659  87.757402  ...  80.927080  79.308376   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.532459  84.719985  82.388790  83.227223  ...  78.864388  80.495710   \n",
       "2439  87.600872  84.600752  82.230454  83.128281  ...  78.829764  80.388767   \n",
       "2440  87.672303  84.480406  82.073045  83.028264  ...  78.797137  80.284453   \n",
       "2441  87.746920  84.359183  81.916747  82.926738  ...  78.767266  80.182617   \n",
       "2442  87.824930  84.237114  81.761408  82.823075  ...  78.740971  80.082791   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     84.186534  79.617342  88.756527  87.301170  84.126977  87.795904   \n",
       "1     84.187540  79.793881  88.724417  87.359682  84.163713  87.768586   \n",
       "2     84.187046  79.965533  88.695040  87.420345  84.200228  87.739953   \n",
       "3     84.184876  80.132052  88.668364  87.483563  84.236669  87.710200   \n",
       "4     84.181101  80.293286  88.644123  87.549464  84.272948  87.679623   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  80.470719  81.113167  91.693359  91.156410  84.697309  85.748841   \n",
       "2439  80.310960  81.125860  91.847410  91.294340  84.606032  85.729828   \n",
       "2440  80.149074  81.137419  92.003863  91.430279  84.514645  85.711048   \n",
       "2441  79.985102  81.147198  92.162897  91.563879  84.422441  85.691773   \n",
       "2442  79.819085  81.154675  92.324398  91.694908  84.328841  85.671334   \n",
       "\n",
       "             46         47  \n",
       "0     67.633187  78.108970  \n",
       "1     67.625241  78.155214  \n",
       "2     67.616693  78.199453  \n",
       "3     67.608228  78.241218  \n",
       "4     67.600649  78.280300  \n",
       "...         ...        ...  \n",
       "2438  76.423079  71.173074  \n",
       "2439  76.590800  71.336844  \n",
       "2440  76.759045  71.503928  \n",
       "2441  76.928062  71.674240  \n",
       "2442  77.098275  71.847731  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.504459</td>\n",
       "      <td>86.399597</td>\n",
       "      <td>79.080492</td>\n",
       "      <td>87.102449</td>\n",
       "      <td>86.771802</td>\n",
       "      <td>89.147780</td>\n",
       "      <td>85.664094</td>\n",
       "      <td>91.059360</td>\n",
       "      <td>83.770213</td>\n",
       "      <td>88.197770</td>\n",
       "      <td>...</td>\n",
       "      <td>81.239405</td>\n",
       "      <td>78.048687</td>\n",
       "      <td>84.186534</td>\n",
       "      <td>79.617342</td>\n",
       "      <td>88.756527</td>\n",
       "      <td>87.301170</td>\n",
       "      <td>84.126977</td>\n",
       "      <td>87.795904</td>\n",
       "      <td>67.633187</td>\n",
       "      <td>78.108970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.519259</td>\n",
       "      <td>86.593020</td>\n",
       "      <td>79.101701</td>\n",
       "      <td>86.823706</td>\n",
       "      <td>86.758531</td>\n",
       "      <td>89.178741</td>\n",
       "      <td>85.666877</td>\n",
       "      <td>90.877329</td>\n",
       "      <td>83.697448</td>\n",
       "      <td>88.085323</td>\n",
       "      <td>...</td>\n",
       "      <td>81.155170</td>\n",
       "      <td>78.370631</td>\n",
       "      <td>84.187540</td>\n",
       "      <td>79.793881</td>\n",
       "      <td>88.724417</td>\n",
       "      <td>87.359682</td>\n",
       "      <td>84.163713</td>\n",
       "      <td>87.768586</td>\n",
       "      <td>67.625241</td>\n",
       "      <td>78.155214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.537091</td>\n",
       "      <td>86.782692</td>\n",
       "      <td>79.118162</td>\n",
       "      <td>86.546519</td>\n",
       "      <td>86.749143</td>\n",
       "      <td>89.213281</td>\n",
       "      <td>85.665636</td>\n",
       "      <td>90.703549</td>\n",
       "      <td>83.626904</td>\n",
       "      <td>87.974585</td>\n",
       "      <td>...</td>\n",
       "      <td>81.075011</td>\n",
       "      <td>78.688196</td>\n",
       "      <td>84.187046</td>\n",
       "      <td>79.965533</td>\n",
       "      <td>88.695040</td>\n",
       "      <td>87.420345</td>\n",
       "      <td>84.200228</td>\n",
       "      <td>87.739953</td>\n",
       "      <td>67.616693</td>\n",
       "      <td>78.199453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.557419</td>\n",
       "      <td>86.968268</td>\n",
       "      <td>79.130002</td>\n",
       "      <td>86.270870</td>\n",
       "      <td>86.743626</td>\n",
       "      <td>89.251556</td>\n",
       "      <td>85.660263</td>\n",
       "      <td>90.537891</td>\n",
       "      <td>83.558588</td>\n",
       "      <td>87.865334</td>\n",
       "      <td>...</td>\n",
       "      <td>80.998904</td>\n",
       "      <td>79.000898</td>\n",
       "      <td>84.184876</td>\n",
       "      <td>80.132052</td>\n",
       "      <td>88.668364</td>\n",
       "      <td>87.483563</td>\n",
       "      <td>84.236669</td>\n",
       "      <td>87.710200</td>\n",
       "      <td>67.608228</td>\n",
       "      <td>78.241218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.579760</td>\n",
       "      <td>87.149185</td>\n",
       "      <td>79.137413</td>\n",
       "      <td>85.996588</td>\n",
       "      <td>86.742103</td>\n",
       "      <td>89.293636</td>\n",
       "      <td>85.650554</td>\n",
       "      <td>90.379940</td>\n",
       "      <td>83.492659</td>\n",
       "      <td>87.757402</td>\n",
       "      <td>...</td>\n",
       "      <td>80.927080</td>\n",
       "      <td>79.308376</td>\n",
       "      <td>84.181101</td>\n",
       "      <td>80.293286</td>\n",
       "      <td>88.644123</td>\n",
       "      <td>87.549464</td>\n",
       "      <td>84.272948</td>\n",
       "      <td>87.679623</td>\n",
       "      <td>67.600649</td>\n",
       "      <td>78.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>92.020713</td>\n",
       "      <td>87.437522</td>\n",
       "      <td>78.460299</td>\n",
       "      <td>75.652257</td>\n",
       "      <td>91.864426</td>\n",
       "      <td>93.264508</td>\n",
       "      <td>87.532459</td>\n",
       "      <td>84.719985</td>\n",
       "      <td>82.388790</td>\n",
       "      <td>83.227223</td>\n",
       "      <td>...</td>\n",
       "      <td>78.864388</td>\n",
       "      <td>80.495710</td>\n",
       "      <td>80.470719</td>\n",
       "      <td>81.113167</td>\n",
       "      <td>91.693359</td>\n",
       "      <td>91.156410</td>\n",
       "      <td>84.697309</td>\n",
       "      <td>85.748841</td>\n",
       "      <td>76.423079</td>\n",
       "      <td>71.173074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>92.240183</td>\n",
       "      <td>87.534982</td>\n",
       "      <td>78.470896</td>\n",
       "      <td>75.771393</td>\n",
       "      <td>91.834236</td>\n",
       "      <td>93.460502</td>\n",
       "      <td>87.600872</td>\n",
       "      <td>84.600752</td>\n",
       "      <td>82.230454</td>\n",
       "      <td>83.128281</td>\n",
       "      <td>...</td>\n",
       "      <td>78.829764</td>\n",
       "      <td>80.388767</td>\n",
       "      <td>80.310960</td>\n",
       "      <td>81.125860</td>\n",
       "      <td>91.847410</td>\n",
       "      <td>91.294340</td>\n",
       "      <td>84.606032</td>\n",
       "      <td>85.729828</td>\n",
       "      <td>76.590800</td>\n",
       "      <td>71.336844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>92.461814</td>\n",
       "      <td>87.630229</td>\n",
       "      <td>78.486618</td>\n",
       "      <td>75.894408</td>\n",
       "      <td>91.807209</td>\n",
       "      <td>93.653126</td>\n",
       "      <td>87.672303</td>\n",
       "      <td>84.480406</td>\n",
       "      <td>82.073045</td>\n",
       "      <td>83.028264</td>\n",
       "      <td>...</td>\n",
       "      <td>78.797137</td>\n",
       "      <td>80.284453</td>\n",
       "      <td>80.149074</td>\n",
       "      <td>81.137419</td>\n",
       "      <td>92.003863</td>\n",
       "      <td>91.430279</td>\n",
       "      <td>84.514645</td>\n",
       "      <td>85.711048</td>\n",
       "      <td>76.759045</td>\n",
       "      <td>71.503928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.685277</td>\n",
       "      <td>87.722960</td>\n",
       "      <td>78.507761</td>\n",
       "      <td>76.021213</td>\n",
       "      <td>91.783334</td>\n",
       "      <td>93.842537</td>\n",
       "      <td>87.746920</td>\n",
       "      <td>84.359183</td>\n",
       "      <td>81.916747</td>\n",
       "      <td>82.926738</td>\n",
       "      <td>...</td>\n",
       "      <td>78.767266</td>\n",
       "      <td>80.182617</td>\n",
       "      <td>79.985102</td>\n",
       "      <td>81.147198</td>\n",
       "      <td>92.162897</td>\n",
       "      <td>91.563879</td>\n",
       "      <td>84.422441</td>\n",
       "      <td>85.691773</td>\n",
       "      <td>76.928062</td>\n",
       "      <td>71.674240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.909981</td>\n",
       "      <td>87.812998</td>\n",
       "      <td>78.534559</td>\n",
       "      <td>76.151518</td>\n",
       "      <td>91.762590</td>\n",
       "      <td>94.029071</td>\n",
       "      <td>87.824930</td>\n",
       "      <td>84.237114</td>\n",
       "      <td>81.761408</td>\n",
       "      <td>82.823075</td>\n",
       "      <td>...</td>\n",
       "      <td>78.740971</td>\n",
       "      <td>80.082791</td>\n",
       "      <td>79.819085</td>\n",
       "      <td>81.154675</td>\n",
       "      <td>92.324398</td>\n",
       "      <td>91.694908</td>\n",
       "      <td>84.328841</td>\n",
       "      <td>85.671334</td>\n",
       "      <td>77.098275</td>\n",
       "      <td>71.847731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     83.504459  86.399597  79.080492  87.102449  86.771802  89.147780   \n",
       "1     83.519259  86.593020  79.101701  86.823706  86.758531  89.178741   \n",
       "2     83.537091  86.782692  79.118162  86.546519  86.749143  89.213281   \n",
       "3     83.557419  86.968268  79.130002  86.270870  86.743626  89.251556   \n",
       "4     83.579760  87.149185  79.137413  85.996588  86.742103  89.293636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  92.020713  87.437522  78.460299  75.652257  91.864426  93.264508   \n",
       "2439  92.240183  87.534982  78.470896  75.771393  91.834236  93.460502   \n",
       "2440  92.461814  87.630229  78.486618  75.894408  91.807209  93.653126   \n",
       "2441  92.685277  87.722960  78.507761  76.021213  91.783334  93.842537   \n",
       "2442  92.909981  87.812998  78.534559  76.151518  91.762590  94.029071   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     85.664094  91.059360  83.770213  88.197770  ...  81.239405  78.048687   \n",
       "1     85.666877  90.877329  83.697448  88.085323  ...  81.155170  78.370631   \n",
       "2     85.665636  90.703549  83.626904  87.974585  ...  81.075011  78.688196   \n",
       "3     85.660263  90.537891  83.558588  87.865334  ...  80.998904  79.000898   \n",
       "4     85.650554  90.379940  83.492659  87.757402  ...  80.927080  79.308376   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.532459  84.719985  82.388790  83.227223  ...  78.864388  80.495710   \n",
       "2439  87.600872  84.600752  82.230454  83.128281  ...  78.829764  80.388767   \n",
       "2440  87.672303  84.480406  82.073045  83.028264  ...  78.797137  80.284453   \n",
       "2441  87.746920  84.359183  81.916747  82.926738  ...  78.767266  80.182617   \n",
       "2442  87.824930  84.237114  81.761408  82.823075  ...  78.740971  80.082791   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     84.186534  79.617342  88.756527  87.301170  84.126977  87.795904   \n",
       "1     84.187540  79.793881  88.724417  87.359682  84.163713  87.768586   \n",
       "2     84.187046  79.965533  88.695040  87.420345  84.200228  87.739953   \n",
       "3     84.184876  80.132052  88.668364  87.483563  84.236669  87.710200   \n",
       "4     84.181101  80.293286  88.644123  87.549464  84.272948  87.679623   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  80.470719  81.113167  91.693359  91.156410  84.697309  85.748841   \n",
       "2439  80.310960  81.125860  91.847410  91.294340  84.606032  85.729828   \n",
       "2440  80.149074  81.137419  92.003863  91.430279  84.514645  85.711048   \n",
       "2441  79.985102  81.147198  92.162897  91.563879  84.422441  85.691773   \n",
       "2442  79.819085  81.154675  92.324398  91.694908  84.328841  85.671334   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     67.633187  78.108970  \n",
       "1     67.625241  78.155214  \n",
       "2     67.616693  78.199453  \n",
       "3     67.608228  78.241218  \n",
       "4     67.600649  78.280300  \n",
       "...         ...        ...  \n",
       "2438  76.423079  71.173074  \n",
       "2439  76.590800  71.336844  \n",
       "2440  76.759045  71.503928  \n",
       "2441  76.928062  71.674240  \n",
       "2442  77.098275  71.847731  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.504459</td>\n",
       "      <td>86.399597</td>\n",
       "      <td>79.080492</td>\n",
       "      <td>87.102449</td>\n",
       "      <td>86.771802</td>\n",
       "      <td>89.147780</td>\n",
       "      <td>85.664094</td>\n",
       "      <td>91.059360</td>\n",
       "      <td>83.770213</td>\n",
       "      <td>88.197770</td>\n",
       "      <td>78.822435</td>\n",
       "      <td>84.500231</td>\n",
       "      <td>91.754463</td>\n",
       "      <td>94.353728</td>\n",
       "      <td>88.904557</td>\n",
       "      <td>89.165864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83.519259</td>\n",
       "      <td>86.593020</td>\n",
       "      <td>79.101701</td>\n",
       "      <td>86.823706</td>\n",
       "      <td>86.758531</td>\n",
       "      <td>89.178741</td>\n",
       "      <td>85.666877</td>\n",
       "      <td>90.877329</td>\n",
       "      <td>83.697448</td>\n",
       "      <td>88.085323</td>\n",
       "      <td>78.839395</td>\n",
       "      <td>84.321200</td>\n",
       "      <td>91.753070</td>\n",
       "      <td>94.245762</td>\n",
       "      <td>88.745741</td>\n",
       "      <td>89.045237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.537091</td>\n",
       "      <td>86.782692</td>\n",
       "      <td>79.118162</td>\n",
       "      <td>86.546519</td>\n",
       "      <td>86.749143</td>\n",
       "      <td>89.213281</td>\n",
       "      <td>85.665636</td>\n",
       "      <td>90.703549</td>\n",
       "      <td>83.626904</td>\n",
       "      <td>87.974585</td>\n",
       "      <td>78.859580</td>\n",
       "      <td>84.148078</td>\n",
       "      <td>91.752078</td>\n",
       "      <td>94.136583</td>\n",
       "      <td>88.588362</td>\n",
       "      <td>88.929631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.557419</td>\n",
       "      <td>86.968268</td>\n",
       "      <td>79.130002</td>\n",
       "      <td>86.270870</td>\n",
       "      <td>86.743626</td>\n",
       "      <td>89.251556</td>\n",
       "      <td>85.660263</td>\n",
       "      <td>90.537891</td>\n",
       "      <td>83.558588</td>\n",
       "      <td>87.865334</td>\n",
       "      <td>78.883775</td>\n",
       "      <td>83.980482</td>\n",
       "      <td>91.751261</td>\n",
       "      <td>94.025694</td>\n",
       "      <td>88.432962</td>\n",
       "      <td>88.819258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.579760</td>\n",
       "      <td>87.149185</td>\n",
       "      <td>79.137413</td>\n",
       "      <td>85.996588</td>\n",
       "      <td>86.742103</td>\n",
       "      <td>89.293636</td>\n",
       "      <td>85.650554</td>\n",
       "      <td>90.379940</td>\n",
       "      <td>83.492659</td>\n",
       "      <td>87.757402</td>\n",
       "      <td>78.912787</td>\n",
       "      <td>83.817811</td>\n",
       "      <td>91.750430</td>\n",
       "      <td>93.912555</td>\n",
       "      <td>88.280093</td>\n",
       "      <td>88.714110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>92.020713</td>\n",
       "      <td>87.437522</td>\n",
       "      <td>78.460299</td>\n",
       "      <td>75.652257</td>\n",
       "      <td>91.864426</td>\n",
       "      <td>93.264508</td>\n",
       "      <td>87.532459</td>\n",
       "      <td>84.719985</td>\n",
       "      <td>82.388790</td>\n",
       "      <td>83.227223</td>\n",
       "      <td>79.743088</td>\n",
       "      <td>73.424011</td>\n",
       "      <td>89.816247</td>\n",
       "      <td>90.581672</td>\n",
       "      <td>83.943868</td>\n",
       "      <td>85.674314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>92.240183</td>\n",
       "      <td>87.534982</td>\n",
       "      <td>78.470896</td>\n",
       "      <td>75.771393</td>\n",
       "      <td>91.834236</td>\n",
       "      <td>93.460502</td>\n",
       "      <td>87.600872</td>\n",
       "      <td>84.600752</td>\n",
       "      <td>82.230454</td>\n",
       "      <td>83.128281</td>\n",
       "      <td>79.803199</td>\n",
       "      <td>73.258739</td>\n",
       "      <td>89.808417</td>\n",
       "      <td>90.615763</td>\n",
       "      <td>83.714857</td>\n",
       "      <td>85.658978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>92.461814</td>\n",
       "      <td>87.630229</td>\n",
       "      <td>78.486618</td>\n",
       "      <td>75.894408</td>\n",
       "      <td>91.807209</td>\n",
       "      <td>93.653126</td>\n",
       "      <td>87.672303</td>\n",
       "      <td>84.480406</td>\n",
       "      <td>82.073045</td>\n",
       "      <td>83.028264</td>\n",
       "      <td>79.865621</td>\n",
       "      <td>73.092812</td>\n",
       "      <td>89.802887</td>\n",
       "      <td>90.649790</td>\n",
       "      <td>83.481950</td>\n",
       "      <td>85.638714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.685277</td>\n",
       "      <td>87.722960</td>\n",
       "      <td>78.507761</td>\n",
       "      <td>76.021213</td>\n",
       "      <td>91.783334</td>\n",
       "      <td>93.842537</td>\n",
       "      <td>87.746920</td>\n",
       "      <td>84.359183</td>\n",
       "      <td>81.916747</td>\n",
       "      <td>82.926738</td>\n",
       "      <td>79.930845</td>\n",
       "      <td>72.925945</td>\n",
       "      <td>89.799774</td>\n",
       "      <td>90.683558</td>\n",
       "      <td>83.245121</td>\n",
       "      <td>85.612735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.909981</td>\n",
       "      <td>87.812998</td>\n",
       "      <td>78.534559</td>\n",
       "      <td>76.151518</td>\n",
       "      <td>91.762590</td>\n",
       "      <td>94.029071</td>\n",
       "      <td>87.824930</td>\n",
       "      <td>84.237114</td>\n",
       "      <td>81.761408</td>\n",
       "      <td>82.823075</td>\n",
       "      <td>79.999161</td>\n",
       "      <td>72.758029</td>\n",
       "      <td>89.799036</td>\n",
       "      <td>90.717114</td>\n",
       "      <td>83.004421</td>\n",
       "      <td>85.580637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     83.504459  86.399597  79.080492  87.102449  86.771802  89.147780   \n",
       "1     83.519259  86.593020  79.101701  86.823706  86.758531  89.178741   \n",
       "2     83.537091  86.782692  79.118162  86.546519  86.749143  89.213281   \n",
       "3     83.557419  86.968268  79.130002  86.270870  86.743626  89.251556   \n",
       "4     83.579760  87.149185  79.137413  85.996588  86.742103  89.293636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  92.020713  87.437522  78.460299  75.652257  91.864426  93.264508   \n",
       "2439  92.240183  87.534982  78.470896  75.771393  91.834236  93.460502   \n",
       "2440  92.461814  87.630229  78.486618  75.894408  91.807209  93.653126   \n",
       "2441  92.685277  87.722960  78.507761  76.021213  91.783334  93.842537   \n",
       "2442  92.909981  87.812998  78.534559  76.151518  91.762590  94.029071   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     85.664094  91.059360  83.770213  88.197770  78.822435  84.500231   \n",
       "1     85.666877  90.877329  83.697448  88.085323  78.839395  84.321200   \n",
       "2     85.665636  90.703549  83.626904  87.974585  78.859580  84.148078   \n",
       "3     85.660263  90.537891  83.558588  87.865334  78.883775  83.980482   \n",
       "4     85.650554  90.379940  83.492659  87.757402  78.912787  83.817811   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  87.532459  84.719985  82.388790  83.227223  79.743088  73.424011   \n",
       "2439  87.600872  84.600752  82.230454  83.128281  79.803199  73.258739   \n",
       "2440  87.672303  84.480406  82.073045  83.028264  79.865621  73.092812   \n",
       "2441  87.746920  84.359183  81.916747  82.926738  79.930845  72.925945   \n",
       "2442  87.824930  84.237114  81.761408  82.823075  79.999161  72.758029   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     91.754463  94.353728  88.904557  89.165864  \n",
       "1     91.753070  94.245762  88.745741  89.045237  \n",
       "2     91.752078  94.136583  88.588362  88.929631  \n",
       "3     91.751261  94.025694  88.432962  88.819258  \n",
       "4     91.750430  93.912555  88.280093  88.714110  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  89.816247  90.581672  83.943868  85.674314  \n",
       "2439  89.808417  90.615763  83.714857  85.658978  \n",
       "2440  89.802887  90.649790  83.481950  85.638714  \n",
       "2441  89.799774  90.683558  83.245121  85.612735  \n",
       "2442  89.799036  90.717114  83.004421  85.580637  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1083.6963 - val_loss: 931.9446\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 840.7921 - val_loss: 754.8726\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 591.6756 - val_loss: 489.7480\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 420.5528 - val_loss: 406.6468\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 363.8575 - val_loss: 377.3927\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 342.0525 - val_loss: 386.0910\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 327.9668 - val_loss: 332.0466\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 313.4213 - val_loss: 447.9805\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 291.8472 - val_loss: 290.2959\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 273.6004 - val_loss: 349.9629\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 238.7081 - val_loss: 325.0978\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 176.0544 - val_loss: 174.6541\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 106.5405 - val_loss: 176.4309\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 76.4335 - val_loss: 105.6752\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 58.5950 - val_loss: 213.9980\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 45.7252 - val_loss: 94.4059\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 39.4835 - val_loss: 78.3758\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 33.1205 - val_loss: 51.0887\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 25.4491 - val_loss: 151.8560\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 24.0980 - val_loss: 118.4756\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.1689 - val_loss: 135.6744\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 16.4114 - val_loss: 71.3005\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 24.3740 - val_loss: 24.9419\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.1130 - val_loss: 16.9723\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 16.5134 - val_loss: 462.8877\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.2262 - val_loss: 34.5293\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 7.9000 - val_loss: 16.6955\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.4458 - val_loss: 8.6212\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4034 - val_loss: 6.2383\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.2242 - val_loss: 55.3173\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.6139 - val_loss: 132.8214\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.2214 - val_loss: 13.0896\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.5221 - val_loss: 135.1354\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.5796 - val_loss: 10.0439\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3231 - val_loss: 14.3226\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7655 - val_loss: 6.0716\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.7591 - val_loss: 6.1967\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4420 - val_loss: 20.8520\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.5340 - val_loss: 2.1077\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1581 - val_loss: 12.2418\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 23.7977 - val_loss: 45.8688\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3956 - val_loss: 8.8434\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0877 - val_loss: 1.9530\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8053 - val_loss: 5.8612\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8523 - val_loss: 1.5597\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4430 - val_loss: 1.1279\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0483 - val_loss: 2.2384\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3556 - val_loss: 6.2554\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.1572 - val_loss: 114.3979\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.2632 - val_loss: 2.7613\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4716 - val_loss: 2.2268\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.1894 - val_loss: 16.9150\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0366 - val_loss: 6.2762\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8964 - val_loss: 1.1269\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7267 - val_loss: 0.9222\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6737 - val_loss: 0.9336\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8884 - val_loss: 1.7983\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6414 - val_loss: 1.0111\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9628 - val_loss: 2.9801\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8090 - val_loss: 1.2550\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0487 - val_loss: 3.6446\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.9020 - val_loss: 254.4446\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1158 - val_loss: 1.1709\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6723 - val_loss: 1.4697\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6004 - val_loss: 1.9570\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4611 - val_loss: 2.2400\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9120 - val_loss: 1152.2225\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.5695 - val_loss: 4.1288\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7194 - val_loss: 0.6611\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4833 - val_loss: 0.3148\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3811 - val_loss: 0.6412\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4989 - val_loss: 0.8316\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7881 - val_loss: 0.8219\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4676 - val_loss: 0.9311\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5238 - val_loss: 0.4413\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3951 - val_loss: 0.9881\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4464 - val_loss: 0.9914\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5363 - val_loss: 4.2332\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.8372 - val_loss: 2.3224\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 7.0712 - val_loss: 32.3665\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7758 - val_loss: 0.5757\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4388 - val_loss: 0.7134\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2900 - val_loss: 0.3500\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2893 - val_loss: 0.3407\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3029 - val_loss: 0.2902\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2915 - val_loss: 1.1267\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3021 - val_loss: 0.5138\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.0776 - val_loss: 28.1934\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1836 - val_loss: 0.6124\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5007 - val_loss: 0.7414\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3288 - val_loss: 0.2683\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2687 - val_loss: 0.6468\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2910 - val_loss: 0.3858\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2802 - val_loss: 0.2131\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2469 - val_loss: 0.4149\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2920 - val_loss: 0.7824\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.9404 - val_loss: 3.0041\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4771 - val_loss: 0.3141\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2915 - val_loss: 0.3363\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2469 - val_loss: 0.2830\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2710 - val_loss: 0.3966\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2241 - val_loss: 0.3364\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2473 - val_loss: 1.9339\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2191 - val_loss: 0.2493\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2645 - val_loss: 0.5732\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4174 - val_loss: 0.5246\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3396 - val_loss: 0.5555\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2546 - val_loss: 0.4576\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4828 - val_loss: 1.1487\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3152 - val_loss: 2.3443\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3690 - val_loss: 4.0267\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4734 - val_loss: 7.3950\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3022 - val_loss: 5.2028\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.4339 - val_loss: 41.1652\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4859 - val_loss: 0.9377\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1998 - val_loss: 0.1874\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1733 - val_loss: 0.3631\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1536 - val_loss: 0.4082\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8920 - val_loss: 0.3844\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1638 - val_loss: 0.2479\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1313 - val_loss: 0.1753\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1491 - val_loss: 0.7530\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1797 - val_loss: 0.4224\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1700 - val_loss: 1.1899\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2669 - val_loss: 0.4328\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2096 - val_loss: 0.7330\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7154 - val_loss: 1.0418\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2416 - val_loss: 0.8936\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2510 - val_loss: 0.5395\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2276 - val_loss: 3.6657\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0895 - val_loss: 484.1957\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1346 - val_loss: 0.3242\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1938 - val_loss: 3.2978\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1430 - val_loss: 0.2712\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1617 - val_loss: 0.2374\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1046 - val_loss: 0.2185\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1327 - val_loss: 0.1914\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1332 - val_loss: 0.2299\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1166 - val_loss: 0.3369\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1550 - val_loss: 0.3119\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1520 - val_loss: 1.0078\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2442 - val_loss: 0.9988\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.7843 - val_loss: 0.9911\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3295 - val_loss: 0.4906\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1845 - val_loss: 0.1659\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2564 - val_loss: 0.2460\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1289 - val_loss: 0.1418\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1123 - val_loss: 0.1040\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1097 - val_loss: 0.2733\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1025 - val_loss: 0.1156\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1072 - val_loss: 1.7445\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1422 - val_loss: 0.2245\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1487 - val_loss: 0.1696\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.8061 - val_loss: 0.4132\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1686 - val_loss: 0.1649\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1207 - val_loss: 0.1266\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0996 - val_loss: 0.1356\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0948 - val_loss: 0.1435\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1028 - val_loss: 0.1013\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1076 - val_loss: 0.1359\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4092 - val_loss: 129.6317\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3811 - val_loss: 0.9658\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1575 - val_loss: 0.1101\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1376 - val_loss: 0.1173\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0945 - val_loss: 0.1683\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0883 - val_loss: 0.0794\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0780 - val_loss: 0.1104\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0893 - val_loss: 0.3235\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0890 - val_loss: 0.0994\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1077 - val_loss: 0.1999\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1169 - val_loss: 0.5042\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1197 - val_loss: 0.3906\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1495 - val_loss: 0.7782\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1564 - val_loss: 0.3293\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2831 - val_loss: 0.5133\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2209 - val_loss: 0.2892\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1891 - val_loss: 8.8740\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2292 - val_loss: 3.5760\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1718 - val_loss: 0.5911\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2353 - val_loss: 31.1785\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2903 - val_loss: 0.7727\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2882 - val_loss: 0.3220\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1284 - val_loss: 0.8800\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1893 - val_loss: 0.7268\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9077 - val_loss: 18.9744\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2154 - val_loss: 0.1675\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1757 - val_loss: 0.1721\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0831 - val_loss: 0.1760\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0850 - val_loss: 0.0992\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0729 - val_loss: 0.1443\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0802 - val_loss: 0.1655\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.0991 - val_loss: 0.8754\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0855 - val_loss: 0.1453\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0943 - val_loss: 0.1966\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1190 - val_loss: 0.2640\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1159 - val_loss: 0.6975\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.0793684628650829\n",
      "Mean Absolute Error (MAE): 0.21183735037185583\n",
      "Root Mean Squared Error (RMSE): 0.2817240899622943\n",
      "Time taken: 540.061802148819\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1105.8354 - val_loss: 917.5106\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 700.6711 - val_loss: 574.6603\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 479.0597 - val_loss: 440.9754\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 387.2776 - val_loss: 408.9539\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 353.4980 - val_loss: 355.6177\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 336.7851 - val_loss: 353.5403\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 319.3314 - val_loss: 399.3641\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 301.5883 - val_loss: 346.9244\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 277.1650 - val_loss: 331.5826\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 231.3239 - val_loss: 320.9414\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 145.4548 - val_loss: 114.4869\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 100.6993 - val_loss: 110.5110\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.8384 - val_loss: 209.6301\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 63.0648 - val_loss: 95.1401\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 49.7074 - val_loss: 147.3076\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 44.3660 - val_loss: 51.6919\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 37.1886 - val_loss: 73.8546\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 31.8111 - val_loss: 143.1056\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 24.0790 - val_loss: 30.6832\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.0258 - val_loss: 29.5547\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.6650 - val_loss: 86.7757\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.1905 - val_loss: 41.0628\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.4664 - val_loss: 96.9006\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 19.9510 - val_loss: 73.0038\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 12.9390 - val_loss: 16.2442\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3199 - val_loss: 20.0229\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 8.0001 - val_loss: 91.1345\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.5233 - val_loss: 38.9130\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 8.0131 - val_loss: 16.5183\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.3934 - val_loss: 59.8547\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.9579 - val_loss: 9.8009\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1095 - val_loss: 19.4803\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3885 - val_loss: 10.2690\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.7608 - val_loss: 5.8885\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.5190 - val_loss: 285.6550\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.8511 - val_loss: 29.6211\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7942 - val_loss: 15.7418\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.1358 - val_loss: 2.3105\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.4847 - val_loss: 3.8917\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4738 - val_loss: 2.9060\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.8957 - val_loss: 25.4011\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0553 - val_loss: 160.6811\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4758 - val_loss: 1.0876\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2532 - val_loss: 1.9583\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0164 - val_loss: 2.9703\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8644 - val_loss: 1.3166\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5805 - val_loss: 37.8042\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.1562 - val_loss: 22.4540\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9250 - val_loss: 0.9172\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6600 - val_loss: 1.8754\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6896 - val_loss: 4.6624\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7783 - val_loss: 1.0153\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.0851 - val_loss: 69.1860\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1814 - val_loss: 2.7168\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.5724 - val_loss: 1.6235\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8502 - val_loss: 1.8346\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5952 - val_loss: 0.7345\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5649 - val_loss: 0.5940\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5480 - val_loss: 5.8892\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5955 - val_loss: 1.0984\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5321 - val_loss: 25.6757\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5777 - val_loss: 4.1326\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5446 - val_loss: 0.8903\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.2950 - val_loss: 105.5734\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1941 - val_loss: 1.2504\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9820 - val_loss: 1.0949\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4610 - val_loss: 0.4872\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4132 - val_loss: 0.6988\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3902 - val_loss: 0.4954\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4312 - val_loss: 0.5772\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4147 - val_loss: 1.7813\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5339 - val_loss: 1.7285\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6202 - val_loss: 9.9326\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 14.4961 - val_loss: 15.9151\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7944 - val_loss: 0.6583\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4350 - val_loss: 0.6971\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3645 - val_loss: 4.4455\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3237 - val_loss: 0.5236\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3091 - val_loss: 0.3562\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3106 - val_loss: 0.5194\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3363 - val_loss: 0.6560\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3102 - val_loss: 0.7361\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3306 - val_loss: 1.3045\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4170 - val_loss: 15.7372\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.7802 - val_loss: 7.8268\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.2437 - val_loss: 0.5584\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4085 - val_loss: 0.4695\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3169 - val_loss: 0.4247\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2612 - val_loss: 0.3709\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2539 - val_loss: 0.3085\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2514 - val_loss: 0.3718\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2512 - val_loss: 0.5106\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3404 - val_loss: 0.3702\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3155 - val_loss: 0.5900\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3831 - val_loss: 0.6110\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3603 - val_loss: 1.2522\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4512 - val_loss: 0.8160\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2952 - val_loss: 364.4553\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 6.1808 - val_loss: 0.8550\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3277 - val_loss: 0.2315\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2713 - val_loss: 0.5775\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2111 - val_loss: 0.6130\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1916 - val_loss: 0.2411\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2183 - val_loss: 0.1922\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1756 - val_loss: 0.2257\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1997 - val_loss: 0.5281\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2155 - val_loss: 0.2674\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2344 - val_loss: 29.9643\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3033 - val_loss: 1.8465\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4552 - val_loss: 2.8290\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.0562 - val_loss: 1.3388\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2834 - val_loss: 0.4157\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2064 - val_loss: 0.2495\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1573 - val_loss: 0.2581\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2138 - val_loss: 0.6381\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1586 - val_loss: 0.3327\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1902 - val_loss: 0.3113\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1907 - val_loss: 0.3990\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 6.1636 - val_loss: 549.8157\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.9078 - val_loss: 0.3011\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2819 - val_loss: 0.2845\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1673 - val_loss: 0.2576\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1449 - val_loss: 0.2043\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1366 - val_loss: 1.1830\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1376 - val_loss: 0.1109\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1263 - val_loss: 0.2203\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1391 - val_loss: 0.3468\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1915 - val_loss: 0.2117\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2061 - val_loss: 0.3769\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2368 - val_loss: 0.3578\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2023 - val_loss: 0.2894\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2002 - val_loss: 0.2947\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2481 - val_loss: 2.8934\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7654 - val_loss: 571.3765\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.2671 - val_loss: 0.7860\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2581 - val_loss: 0.2040\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1812 - val_loss: 0.2369\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1291 - val_loss: 0.1598\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1427 - val_loss: 1.4076\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1235 - val_loss: 0.1413\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1282 - val_loss: 0.2083\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1345 - val_loss: 0.3337\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1566 - val_loss: 0.3259\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2618 - val_loss: 0.6972\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1910 - val_loss: 0.4729\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2206 - val_loss: 0.4079\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2816 - val_loss: 0.5702\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3243 - val_loss: 0.3386\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2394 - val_loss: 10.1634\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3167 - val_loss: 1.7610\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3278 - val_loss: 873.8502\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.6374 - val_loss: 0.5592\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3070 - val_loss: 0.2043\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1655 - val_loss: 0.1313\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1424 - val_loss: 0.1672\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.11097084283503224\n",
      "Mean Absolute Error (MAE): 0.24428457841745954\n",
      "Root Mean Squared Error (RMSE): 0.3331228644735036\n",
      "Time taken: 434.97280955314636\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1129.3755 - val_loss: 898.3810\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 700.3840 - val_loss: 625.7425\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 473.2581 - val_loss: 447.4777\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 379.8009 - val_loss: 402.5293\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 348.0453 - val_loss: 368.9191\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 332.5753 - val_loss: 351.5952\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 312.6796 - val_loss: 315.3252\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 293.2278 - val_loss: 377.2377\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 269.0731 - val_loss: 791.3798\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 220.0325 - val_loss: 367.7221\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 143.2575 - val_loss: 153.6935\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 94.1994 - val_loss: 132.1887\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 76.6396 - val_loss: 121.0669\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 66.8769 - val_loss: 170.1785\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 52.0906 - val_loss: 368.5643\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 41.6035 - val_loss: 83.1628\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 37.7324 - val_loss: 74.8266\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 33.4555 - val_loss: 114.5945\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 25.0964 - val_loss: 124.1375\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 25.3752 - val_loss: 86.3064\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 20.0090 - val_loss: 37.9601\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.1069 - val_loss: 73.8699\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.4375 - val_loss: 16.6910\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.8667 - val_loss: 53.5005\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 11.2293 - val_loss: 21.8746\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.8536 - val_loss: 24.6943\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.8086 - val_loss: 67.3324\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2622 - val_loss: 18.2543\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.6184 - val_loss: 91.8409\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.7690 - val_loss: 16.0854\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.8792 - val_loss: 13.6650\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.6533 - val_loss: 29.7363\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1886 - val_loss: 30.3535\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.4403 - val_loss: 14.1731\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.1740 - val_loss: 4.0093\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.9022 - val_loss: 2.8108\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.0798 - val_loss: 12.2449\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.8705 - val_loss: 2.9777\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6336 - val_loss: 47.6216\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 26.8397 - val_loss: 44.2715\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3537 - val_loss: 89.2130\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.6746 - val_loss: 12.9037\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.4058 - val_loss: 2.1264\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.1572 - val_loss: 338.3011\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9612 - val_loss: 2.9328\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1652 - val_loss: 1.2122\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9394 - val_loss: 44.5899\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.1094 - val_loss: 10.0233\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.5301 - val_loss: 1.5251\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.6806 - val_loss: 36.4002\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0561 - val_loss: 2.3402\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9402 - val_loss: 2.6049\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0913 - val_loss: 23.0935\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3437 - val_loss: 1.4891\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7648 - val_loss: 1.2789\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7108 - val_loss: 1.6298\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.0568 - val_loss: 1.3275\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6242 - val_loss: 1.4065\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5928 - val_loss: 1.3818\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4774 - val_loss: 2.8841\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5713 - val_loss: 1.3320\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6124 - val_loss: 0.8546\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6430 - val_loss: 0.9939\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9156 - val_loss: 1.8201\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 9.6357 - val_loss: 400.5454\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.3904 - val_loss: 0.6408\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5224 - val_loss: 0.6589\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4678 - val_loss: 1.1936\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4138 - val_loss: 0.5853\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3502 - val_loss: 1.7337\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5627 - val_loss: 0.6991\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4108 - val_loss: 0.9713\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4680 - val_loss: 16.5939\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.2134 - val_loss: 19.6809\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8656 - val_loss: 0.9404\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3695 - val_loss: 2.7743\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3253 - val_loss: 0.4127\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3073 - val_loss: 0.3218\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3116 - val_loss: 0.3970\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3509 - val_loss: 9.9581\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3095 - val_loss: 2.3666\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6887 - val_loss: 2.4938\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3875 - val_loss: 0.8754\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7505 - val_loss: 2.5881\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5331 - val_loss: 3.0712\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5876 - val_loss: 5.1155\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3362 - val_loss: 1.4177\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4841 - val_loss: 1.2469\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.7545 - val_loss: 11.8014\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5987 - val_loss: 0.3724\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3150 - val_loss: 0.3323\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3326 - val_loss: 1.7572\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2187 - val_loss: 0.3462\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1903 - val_loss: 0.2442\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1891 - val_loss: 0.5290\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2272 - val_loss: 0.6178\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3340 - val_loss: 0.3839\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2529 - val_loss: 0.6021\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2791 - val_loss: 0.3226\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2841 - val_loss: 1.3824\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7490 - val_loss: 47.6890\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 8.6614 - val_loss: 2.0753\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3642 - val_loss: 0.3455\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2266 - val_loss: 0.3091\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2070 - val_loss: 0.3322\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1920 - val_loss: 0.3293\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1901 - val_loss: 0.3140\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2421 - val_loss: 0.3810\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2508 - val_loss: 0.3905\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2433 - val_loss: 0.2823\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2083 - val_loss: 0.3589\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2335 - val_loss: 0.5828\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4846 - val_loss: 223.7335\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8333 - val_loss: 0.6533\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3091 - val_loss: 0.3041\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1956 - val_loss: 0.2281\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2177 - val_loss: 0.2903\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1835 - val_loss: 0.2421\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1504 - val_loss: 0.4368\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1597 - val_loss: 0.2578\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1603 - val_loss: 2.7209\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1939 - val_loss: 0.5170\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1864 - val_loss: 0.3226\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2017 - val_loss: 0.4755\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.3022 - val_loss: 274.5863\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1664 - val_loss: 1.1001\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2587 - val_loss: 0.2299\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1671 - val_loss: 0.1940\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1842 - val_loss: 0.3017\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1357 - val_loss: 0.1925\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1339 - val_loss: 0.1447\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1222 - val_loss: 0.5813\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1272 - val_loss: 0.2420\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1964 - val_loss: 0.2591\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1537 - val_loss: 0.2888\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2196 - val_loss: 0.5134\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1622 - val_loss: 0.3132\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2925 - val_loss: 0.9197\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2590 - val_loss: 1.1014\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3096 - val_loss: 0.8041\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4551 - val_loss: 4.6532\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3440 - val_loss: 267.3979\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3311 - val_loss: 0.4308\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1727 - val_loss: 0.2670\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1297 - val_loss: 0.1098\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1151 - val_loss: 0.1197\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1173 - val_loss: 0.2838\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1510 - val_loss: 0.2370\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1201 - val_loss: 0.1659\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1077 - val_loss: 0.1367\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1746 - val_loss: 0.1966\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1139 - val_loss: 0.3029\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1864 - val_loss: 0.2726\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1867 - val_loss: 0.4303\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1845 - val_loss: 0.6642\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2200 - val_loss: 0.3219\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3002 - val_loss: 106.5785\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.2086 - val_loss: 1.6600\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3217 - val_loss: 0.2470\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1385 - val_loss: 0.1805\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0998 - val_loss: 0.1152\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1028 - val_loss: 0.0925\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0890 - val_loss: 0.1600\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0849 - val_loss: 0.1113\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0956 - val_loss: 0.1170\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0932 - val_loss: 0.2961\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1182 - val_loss: 0.1475\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1268 - val_loss: 0.3024\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1517 - val_loss: 0.1558\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1243 - val_loss: 0.1784\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1336 - val_loss: 0.3524\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.1695 - val_loss: 9.4389\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2779 - val_loss: 0.1842\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1378 - val_loss: 0.2326\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0962 - val_loss: 0.1124\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1038 - val_loss: 0.1234\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0762 - val_loss: 0.1127\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0839 - val_loss: 0.6082\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2618 - val_loss: 0.2196\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0771 - val_loss: 0.0985\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0800 - val_loss: 0.1452\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0768 - val_loss: 0.1729\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2698 - val_loss: 119.2660\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8765 - val_loss: 0.2390\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1365 - val_loss: 0.1241\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0966 - val_loss: 0.1179\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0814 - val_loss: 0.1013\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0712 - val_loss: 0.2141\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0790 - val_loss: 0.1437\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3163 - val_loss: 0.2940\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1019 - val_loss: 0.1874\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0917 - val_loss: 0.2589\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.09253385633742033\n",
      "Mean Absolute Error (MAE): 0.22359471076643025\n",
      "Root Mean Squared Error (RMSE): 0.3041937808986573\n",
      "Time taken: 532.9879403114319\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1118.7894 - val_loss: 922.6889\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 917.5754 - val_loss: 891.6311\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 791.8134 - val_loss: 691.2285\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 568.9506 - val_loss: 481.4103\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 422.7891 - val_loss: 380.3495\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 361.8071 - val_loss: 356.2707\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 336.6795 - val_loss: 351.2390\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 317.5113 - val_loss: 372.1026\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 297.8656 - val_loss: 324.0883\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 276.4391 - val_loss: 280.9040\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 247.2146 - val_loss: 310.4808\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 192.1275 - val_loss: 412.5857\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 127.0230 - val_loss: 174.1331\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 86.8902 - val_loss: 129.4944\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 59.6520 - val_loss: 54.1106\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 47.5195 - val_loss: 156.3216\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 39.8983 - val_loss: 43.5026\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 29.4652 - val_loss: 34.4520\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 25.0166 - val_loss: 75.9338\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 22.7478 - val_loss: 55.9467\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.3146 - val_loss: 41.6403\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.0909 - val_loss: 25.8753\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.7036 - val_loss: 16.0652\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9026 - val_loss: 8.2441\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.7834 - val_loss: 6.3388\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.4970 - val_loss: 37.7071\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.8853 - val_loss: 21.2462\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.6904 - val_loss: 14.0183\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0175 - val_loss: 6.5991\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0631 - val_loss: 3.4621\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3879 - val_loss: 18.5094\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6446 - val_loss: 6.4042\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4854 - val_loss: 22.5622\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.1245 - val_loss: 3.6511\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5196 - val_loss: 6.4720\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8797 - val_loss: 2.1314\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6426 - val_loss: 19.2767\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.3555 - val_loss: 2.7984\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 12.3168 - val_loss: 79.8985\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8289 - val_loss: 9.8634\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3000 - val_loss: 1.4108\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1294 - val_loss: 1.0539\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9379 - val_loss: 1.0053\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6441 - val_loss: 1.3361\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9585 - val_loss: 4.9509\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8955 - val_loss: 7.1656\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3308 - val_loss: 35.6395\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.9586 - val_loss: 91.0186\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1459 - val_loss: 1.5136\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 27.1289 - val_loss: 34.6184\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9259 - val_loss: 2.2556\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5520 - val_loss: 2.0117\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7781 - val_loss: 1.7757\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6586 - val_loss: 2.2939\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6686 - val_loss: 1.0951\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5973 - val_loss: 3.6803\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5955 - val_loss: 6.4218\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8684 - val_loss: 11.8399\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6386 - val_loss: 0.7907\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5811 - val_loss: 12.3129\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6846 - val_loss: 1.6758\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8518 - val_loss: 1.8993\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.6270 - val_loss: 1.7945\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8870 - val_loss: 0.7153\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5887 - val_loss: 1.5210\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4816 - val_loss: 0.5452\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3977 - val_loss: 0.5442\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3941 - val_loss: 1.5381\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4995 - val_loss: 1.1296\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3815 - val_loss: 0.5195\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5175 - val_loss: 1.9715\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5125 - val_loss: 1.0082\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4487 - val_loss: 0.6359\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.1990 - val_loss: 274.9829\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1903 - val_loss: 0.5236\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4270 - val_loss: 2.7328\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3353 - val_loss: 0.9350\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2876 - val_loss: 0.8078\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2555 - val_loss: 0.2436\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2760 - val_loss: 0.4860\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2885 - val_loss: 0.3529\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2568 - val_loss: 2.2130\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3270 - val_loss: 1.4007\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3271 - val_loss: 0.5961\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4242 - val_loss: 1.0191\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.3242 - val_loss: 131.0145\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0557 - val_loss: 0.5776\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3681 - val_loss: 1.6370\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2655 - val_loss: 0.2830\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2517 - val_loss: 0.4365\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3032 - val_loss: 0.5027\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2049 - val_loss: 0.3411\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2021 - val_loss: 0.2215\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2101 - val_loss: 0.6110\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2155 - val_loss: 0.3124\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2456 - val_loss: 0.4352\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3341 - val_loss: 1.6385\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8103 - val_loss: 184.1095\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4512 - val_loss: 0.5517\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2586 - val_loss: 0.3846\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1957 - val_loss: 0.1928\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1777 - val_loss: 0.1811\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1667 - val_loss: 0.1899\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1900 - val_loss: 0.2422\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2070 - val_loss: 0.3795\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1864 - val_loss: 0.3107\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2480 - val_loss: 0.8629\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2723 - val_loss: 1.3715\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4296 - val_loss: 0.5440\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.6618 - val_loss: 3.3403\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4501 - val_loss: 0.2592\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2491 - val_loss: 0.4487\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1895 - val_loss: 0.1525\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1500 - val_loss: 0.1639\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1563 - val_loss: 0.3365\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.6113 - val_loss: 1.0346\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2241 - val_loss: 0.4876\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1637 - val_loss: 0.2163\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1616 - val_loss: 0.3501\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1606 - val_loss: 0.2732\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1994 - val_loss: 0.4191\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2715 - val_loss: 29.5529\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8076 - val_loss: 0.6519\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2538 - val_loss: 0.9280\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1647 - val_loss: 0.1701\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1439 - val_loss: 0.2037\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1495 - val_loss: 0.2320\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1437 - val_loss: 0.3730\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1578 - val_loss: 0.2993\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2361 - val_loss: 0.4166\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2373 - val_loss: 0.5220\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2163 - val_loss: 0.5167\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2607 - val_loss: 0.4635\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2273 - val_loss: 0.5191\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8039 - val_loss: 0.5959\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1833 - val_loss: 0.1794\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1541 - val_loss: 0.1350\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1172 - val_loss: 0.1819\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1094 - val_loss: 0.2302\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1195 - val_loss: 0.1755\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1280 - val_loss: 0.4501\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2260 - val_loss: 0.3001\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1702 - val_loss: 0.4656\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2195 - val_loss: 0.4450\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2660 - val_loss: 0.6385\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2491 - val_loss: 0.9283\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2791 - val_loss: 362.5778\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1949 - val_loss: 0.2599\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1806 - val_loss: 0.1697\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1532 - val_loss: 0.1024\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1031 - val_loss: 0.0966\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0947 - val_loss: 0.0866\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0862 - val_loss: 0.0850\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0855 - val_loss: 0.0838\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0804 - val_loss: 0.1484\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0980 - val_loss: 0.1486\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0954 - val_loss: 0.1030\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1107 - val_loss: 0.1707\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1139 - val_loss: 0.2481\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1781 - val_loss: 0.2773\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1272 - val_loss: 0.3287\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3101 - val_loss: 108.5696\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5759 - val_loss: 0.2108\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1238 - val_loss: 0.1377\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1040 - val_loss: 0.2736\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0860 - val_loss: 0.1080\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0827 - val_loss: 0.0979\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0881 - val_loss: 0.1000\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1029 - val_loss: 0.2862\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1247 - val_loss: 0.5195\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1292 - val_loss: 0.4511\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1877 - val_loss: 0.4194\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2194 - val_loss: 0.3228\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1412 - val_loss: 0.3786\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2009 - val_loss: 0.7316\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3054 - val_loss: 0.4102\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1659 - val_loss: 0.7364\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1489 - val_loss: 1.0455\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2464 - val_loss: 0.2306\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.9126 - val_loss: 7.7748\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3393 - val_loss: 0.2483\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1144 - val_loss: 0.1044\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0846 - val_loss: 0.0686\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0788 - val_loss: 0.1174\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0614 - val_loss: 0.0809\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0696 - val_loss: 0.1072\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0666 - val_loss: 0.0581\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0638 - val_loss: 0.0957\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0717 - val_loss: 0.0815\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5465 - val_loss: 1.0746\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1333 - val_loss: 0.1241\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0770 - val_loss: 0.1103\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0732 - val_loss: 0.1027\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0706 - val_loss: 0.0611\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1316 - val_loss: 0.0980\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0810 - val_loss: 0.2649\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.0960 - val_loss: 0.2852\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1227 - val_loss: 0.2895\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1095 - val_loss: 0.2503\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1249 - val_loss: 0.3579\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.3579846134356006\n",
      "Mean Absolute Error (MAE): 0.4417930455673858\n",
      "Root Mean Squared Error (RMSE): 0.5983181540247635\n",
      "Time taken: 569.8587725162506\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1098.1040 - val_loss: 928.6140\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 844.8721 - val_loss: 759.5196\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 589.0789 - val_loss: 512.2327\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 424.7986 - val_loss: 456.4283\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 363.6706 - val_loss: 351.9528\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 335.5234 - val_loss: 392.4351\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 315.6241 - val_loss: 348.6798\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 297.6520 - val_loss: 291.9502\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 277.1599 - val_loss: 303.2971\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 238.1277 - val_loss: 308.8095\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 173.6551 - val_loss: 143.1980\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 114.9284 - val_loss: 115.3175\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 79.9209 - val_loss: 98.6218\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 58.9311 - val_loss: 56.0127\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 52.4088 - val_loss: 72.3800\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 41.6438 - val_loss: 132.4190\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 31.8394 - val_loss: 37.3092\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 28.9967 - val_loss: 60.6311\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 25.4330 - val_loss: 40.4156\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 19.9895 - val_loss: 58.7840\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 28.1894 - val_loss: 62.7078\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 19.7024 - val_loss: 20.7458\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.8710 - val_loss: 36.9721\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.3398 - val_loss: 23.2366\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.2577 - val_loss: 7.1567\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.7702 - val_loss: 117.5872\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.8422 - val_loss: 14.4370\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.9832 - val_loss: 20.5337\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 8.9372 - val_loss: 43.9819\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7286 - val_loss: 3.8456\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1273 - val_loss: 11.9352\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.7765 - val_loss: 34.8115\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.3080 - val_loss: 26.1249\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1235 - val_loss: 3.8569\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9199 - val_loss: 36.5624\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5010 - val_loss: 3.3999\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3550 - val_loss: 19.8797\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 18.7121 - val_loss: 9.6604\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.2443 - val_loss: 48.4755\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.9318 - val_loss: 2.9390\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9442 - val_loss: 5.1304\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.7423 - val_loss: 34.4582\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4191 - val_loss: 2.9164\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 8.7746 - val_loss: 2.3048\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.4239 - val_loss: 4.4201\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3120 - val_loss: 1.8923\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1333 - val_loss: 3.8943\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.9387 - val_loss: 29.6502\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3797 - val_loss: 1.8380\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1687 - val_loss: 1.8941\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8644 - val_loss: 2.1631\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8132 - val_loss: 1.3956\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0757 - val_loss: 2.1875\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7962 - val_loss: 1.0201\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7446 - val_loss: 4.0922\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7746 - val_loss: 1.2534\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4661 - val_loss: 69.8981\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3352 - val_loss: 151.1783\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9397 - val_loss: 1.5143\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6796 - val_loss: 0.6519\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6516 - val_loss: 1.7089\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7896 - val_loss: 0.9249\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.6886 - val_loss: 3.2468\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7277 - val_loss: 10.3946\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5453 - val_loss: 0.8358\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4533 - val_loss: 0.7100\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5840 - val_loss: 0.8481\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.3981 - val_loss: 46.7035\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.3575 - val_loss: 1.3915\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5986 - val_loss: 0.6235\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4855 - val_loss: 0.3653\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4239 - val_loss: 0.6459\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4246 - val_loss: 0.7691\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4816 - val_loss: 0.3776\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3581 - val_loss: 2.1722\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4384 - val_loss: 10.6706\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8765 - val_loss: 44.0148\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.6423 - val_loss: 1.7184\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7838 - val_loss: 0.4982\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4259 - val_loss: 0.5110\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3420 - val_loss: 0.5124\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3624 - val_loss: 0.3371\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3794 - val_loss: 0.5035\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3213 - val_loss: 2.0707\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3742 - val_loss: 0.4092\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7394 - val_loss: 124.6808\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0302 - val_loss: 0.9045\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4560 - val_loss: 0.6060\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3782 - val_loss: 0.5192\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4994 - val_loss: 8.4585\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3761 - val_loss: 2.2968\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8355 - val_loss: 166.3955\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0054 - val_loss: 0.6988\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3147 - val_loss: 0.3920\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3322 - val_loss: 0.4058\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2790 - val_loss: 4.2218\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4360 - val_loss: 0.3339\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2723 - val_loss: 1.2515\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3328 - val_loss: 0.6068\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4404 - val_loss: 0.5612\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3437 - val_loss: 1.1764\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2881 - val_loss: 2.0742\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.9870 - val_loss: 59.5302\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4987 - val_loss: 0.3768\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2329 - val_loss: 0.3017\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2082 - val_loss: 0.2518\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1982 - val_loss: 0.2895\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2113 - val_loss: 0.1688\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1817 - val_loss: 0.3572\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2781 - val_loss: 0.6722\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2566 - val_loss: 0.3815\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2895 - val_loss: 40.4297\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4813 - val_loss: 1.3181\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3307 - val_loss: 1.1025\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3219 - val_loss: 0.8449\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2588 - val_loss: 1.2296\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6697 - val_loss: 72.7880\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.5235 - val_loss: 1.1696\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3809 - val_loss: 0.2612\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2453 - val_loss: 0.5270\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1838 - val_loss: 0.2182\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4074 - val_loss: 25.5998\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2903 - val_loss: 0.1766\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1471 - val_loss: 0.1501\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1191 - val_loss: 0.1302\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1320 - val_loss: 0.1279\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1503 - val_loss: 0.3356\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1662 - val_loss: 0.1669\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.1892 - val_loss: 0.2204\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.5713 - val_loss: 182.4693\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.5019 - val_loss: 0.2970\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2576 - val_loss: 0.4713\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1922 - val_loss: 0.1305\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1625 - val_loss: 0.1850\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1302 - val_loss: 0.3130\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1323 - val_loss: 0.1160\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1286 - val_loss: 0.2181\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1385 - val_loss: 0.2521\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1374 - val_loss: 0.1883\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2961 - val_loss: 280.3208\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.4139 - val_loss: 0.4066\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1846 - val_loss: 0.3280\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1298 - val_loss: 0.1377\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1159 - val_loss: 0.1655\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1190 - val_loss: 0.1465\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1377 - val_loss: 0.1434\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1310 - val_loss: 0.3145\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1421 - val_loss: 0.5365\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1988 - val_loss: 5.7497\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1977 - val_loss: 0.3258\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1233 - val_loss: 0.1392\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1253 - val_loss: 0.2356\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1249 - val_loss: 0.2517\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2534 - val_loss: 0.3581\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1700 - val_loss: 0.3723\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1978 - val_loss: 0.5314\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1922 - val_loss: 0.7280\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3688 - val_loss: 2.7416\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2891 - val_loss: 0.5508\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1809 - val_loss: 1.0666\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2679 - val_loss: 0.6820\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.3667 - val_loss: 41.0088\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2623 - val_loss: 0.2686\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1278 - val_loss: 0.1177\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1094 - val_loss: 0.1860\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1013 - val_loss: 0.1167\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.11603632961293879\n",
      "Mean Absolute Error (MAE): 0.2511886229894434\n",
      "Root Mean Squared Error (RMSE): 0.3406410568515469\n",
      "Time taken: 459.8357081413269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_18624\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.079368  0.211837  0.281724  540.061802\n",
      "1        2  0.110971  0.244285  0.333123  434.972810\n",
      "2        3  0.092534  0.223595  0.304194  532.987940\n",
      "3        4  0.357985  0.441793  0.598318  569.858773\n",
      "4        5  0.116036  0.251189  0.340641  459.835708\n",
      "5  Average  0.151379  0.274540  0.371600  507.543407\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f_obese.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_iReg_f_obese.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f_obese.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
