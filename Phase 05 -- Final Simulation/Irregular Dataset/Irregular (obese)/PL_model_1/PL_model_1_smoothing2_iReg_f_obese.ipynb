{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f_obese.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.743085</td>\n",
       "      <td>245.587438</td>\n",
       "      <td>154.771840</td>\n",
       "      <td>186.323260</td>\n",
       "      <td>267.056984</td>\n",
       "      <td>281.794220</td>\n",
       "      <td>213.136940</td>\n",
       "      <td>233.416212</td>\n",
       "      <td>202.821461</td>\n",
       "      <td>215.055444</td>\n",
       "      <td>...</td>\n",
       "      <td>169.147656</td>\n",
       "      <td>175.680869</td>\n",
       "      <td>195.231478</td>\n",
       "      <td>187.637399</td>\n",
       "      <td>244.296045</td>\n",
       "      <td>240.707677</td>\n",
       "      <td>198.439601</td>\n",
       "      <td>223.215212</td>\n",
       "      <td>108.578469</td>\n",
       "      <td>150.342019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.875959</td>\n",
       "      <td>245.628403</td>\n",
       "      <td>154.990000</td>\n",
       "      <td>186.452474</td>\n",
       "      <td>266.774556</td>\n",
       "      <td>281.682030</td>\n",
       "      <td>212.844259</td>\n",
       "      <td>233.292498</td>\n",
       "      <td>203.094102</td>\n",
       "      <td>215.228612</td>\n",
       "      <td>...</td>\n",
       "      <td>169.469071</td>\n",
       "      <td>175.677374</td>\n",
       "      <td>195.428247</td>\n",
       "      <td>187.875116</td>\n",
       "      <td>243.940958</td>\n",
       "      <td>240.313042</td>\n",
       "      <td>198.337504</td>\n",
       "      <td>223.071305</td>\n",
       "      <td>108.490446</td>\n",
       "      <td>150.358478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.011181</td>\n",
       "      <td>245.670578</td>\n",
       "      <td>155.208309</td>\n",
       "      <td>186.581710</td>\n",
       "      <td>266.493200</td>\n",
       "      <td>281.567210</td>\n",
       "      <td>212.552198</td>\n",
       "      <td>233.165031</td>\n",
       "      <td>203.367043</td>\n",
       "      <td>215.404428</td>\n",
       "      <td>...</td>\n",
       "      <td>169.790500</td>\n",
       "      <td>175.672988</td>\n",
       "      <td>195.625443</td>\n",
       "      <td>188.111875</td>\n",
       "      <td>243.586884</td>\n",
       "      <td>239.920057</td>\n",
       "      <td>198.240067</td>\n",
       "      <td>222.930539</td>\n",
       "      <td>108.401175</td>\n",
       "      <td>150.371969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224.148595</td>\n",
       "      <td>245.714444</td>\n",
       "      <td>155.426454</td>\n",
       "      <td>186.711006</td>\n",
       "      <td>266.212869</td>\n",
       "      <td>281.449838</td>\n",
       "      <td>212.260816</td>\n",
       "      <td>233.033756</td>\n",
       "      <td>203.640209</td>\n",
       "      <td>215.582574</td>\n",
       "      <td>...</td>\n",
       "      <td>170.112037</td>\n",
       "      <td>175.668061</td>\n",
       "      <td>195.823263</td>\n",
       "      <td>188.347696</td>\n",
       "      <td>243.233929</td>\n",
       "      <td>239.528739</td>\n",
       "      <td>198.147536</td>\n",
       "      <td>222.793100</td>\n",
       "      <td>108.310803</td>\n",
       "      <td>150.382686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224.288038</td>\n",
       "      <td>245.760404</td>\n",
       "      <td>155.644190</td>\n",
       "      <td>186.840358</td>\n",
       "      <td>265.933706</td>\n",
       "      <td>281.329994</td>\n",
       "      <td>211.970027</td>\n",
       "      <td>232.898766</td>\n",
       "      <td>203.913603</td>\n",
       "      <td>215.762856</td>\n",
       "      <td>...</td>\n",
       "      <td>170.433821</td>\n",
       "      <td>175.662771</td>\n",
       "      <td>196.021985</td>\n",
       "      <td>188.582581</td>\n",
       "      <td>242.882111</td>\n",
       "      <td>239.139085</td>\n",
       "      <td>198.060040</td>\n",
       "      <td>222.659032</td>\n",
       "      <td>108.219520</td>\n",
       "      <td>150.390880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.491893</td>\n",
       "      <td>228.347280</td>\n",
       "      <td>163.325436</td>\n",
       "      <td>140.725997</td>\n",
       "      <td>289.325607</td>\n",
       "      <td>279.420575</td>\n",
       "      <td>235.391196</td>\n",
       "      <td>220.497413</td>\n",
       "      <td>211.044658</td>\n",
       "      <td>205.695511</td>\n",
       "      <td>...</td>\n",
       "      <td>174.913838</td>\n",
       "      <td>195.123629</td>\n",
       "      <td>194.102154</td>\n",
       "      <td>170.352117</td>\n",
       "      <td>253.804827</td>\n",
       "      <td>238.780862</td>\n",
       "      <td>223.348777</td>\n",
       "      <td>213.205734</td>\n",
       "      <td>141.658729</td>\n",
       "      <td>109.645137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.356194</td>\n",
       "      <td>228.299296</td>\n",
       "      <td>163.396140</td>\n",
       "      <td>140.781240</td>\n",
       "      <td>289.263115</td>\n",
       "      <td>279.352105</td>\n",
       "      <td>235.529317</td>\n",
       "      <td>220.497198</td>\n",
       "      <td>210.910581</td>\n",
       "      <td>205.530370</td>\n",
       "      <td>...</td>\n",
       "      <td>174.881507</td>\n",
       "      <td>194.997915</td>\n",
       "      <td>194.137131</td>\n",
       "      <td>170.297434</td>\n",
       "      <td>253.794552</td>\n",
       "      <td>238.893328</td>\n",
       "      <td>223.134036</td>\n",
       "      <td>213.184004</td>\n",
       "      <td>141.996996</td>\n",
       "      <td>109.875721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.221940</td>\n",
       "      <td>228.253433</td>\n",
       "      <td>163.467314</td>\n",
       "      <td>140.838753</td>\n",
       "      <td>289.200562</td>\n",
       "      <td>279.283755</td>\n",
       "      <td>235.669179</td>\n",
       "      <td>220.493546</td>\n",
       "      <td>210.775845</td>\n",
       "      <td>205.365708</td>\n",
       "      <td>...</td>\n",
       "      <td>174.851815</td>\n",
       "      <td>194.869596</td>\n",
       "      <td>194.174712</td>\n",
       "      <td>170.241666</td>\n",
       "      <td>253.784674</td>\n",
       "      <td>239.004938</td>\n",
       "      <td>222.919171</td>\n",
       "      <td>213.161838</td>\n",
       "      <td>142.336676</td>\n",
       "      <td>110.108562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.089132</td>\n",
       "      <td>228.209953</td>\n",
       "      <td>163.539226</td>\n",
       "      <td>140.898725</td>\n",
       "      <td>289.137701</td>\n",
       "      <td>279.215639</td>\n",
       "      <td>235.811103</td>\n",
       "      <td>220.486423</td>\n",
       "      <td>210.640521</td>\n",
       "      <td>205.201545</td>\n",
       "      <td>...</td>\n",
       "      <td>174.824357</td>\n",
       "      <td>194.738693</td>\n",
       "      <td>194.215230</td>\n",
       "      <td>170.184822</td>\n",
       "      <td>253.775163</td>\n",
       "      <td>239.115735</td>\n",
       "      <td>222.704098</td>\n",
       "      <td>213.139343</td>\n",
       "      <td>142.677461</td>\n",
       "      <td>110.343690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>239.957675</td>\n",
       "      <td>228.169041</td>\n",
       "      <td>163.612031</td>\n",
       "      <td>140.961407</td>\n",
       "      <td>289.074300</td>\n",
       "      <td>279.147979</td>\n",
       "      <td>235.955342</td>\n",
       "      <td>220.476014</td>\n",
       "      <td>210.504516</td>\n",
       "      <td>205.038096</td>\n",
       "      <td>...</td>\n",
       "      <td>174.798854</td>\n",
       "      <td>194.605222</td>\n",
       "      <td>194.258957</td>\n",
       "      <td>170.126913</td>\n",
       "      <td>253.765884</td>\n",
       "      <td>239.225797</td>\n",
       "      <td>222.488912</td>\n",
       "      <td>213.116608</td>\n",
       "      <td>143.019033</td>\n",
       "      <td>110.581220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     223.743085  245.587438  154.771840  186.323260  267.056984  281.794220   \n",
       "1     223.875959  245.628403  154.990000  186.452474  266.774556  281.682030   \n",
       "2     224.011181  245.670578  155.208309  186.581710  266.493200  281.567210   \n",
       "3     224.148595  245.714444  155.426454  186.711006  266.212869  281.449838   \n",
       "4     224.288038  245.760404  155.644190  186.840358  265.933706  281.329994   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.491893  228.347280  163.325436  140.725997  289.325607  279.420575   \n",
       "2439  240.356194  228.299296  163.396140  140.781240  289.263115  279.352105   \n",
       "2440  240.221940  228.253433  163.467314  140.838753  289.200562  279.283755   \n",
       "2441  240.089132  228.209953  163.539226  140.898725  289.137701  279.215639   \n",
       "2442  239.957675  228.169041  163.612031  140.961407  289.074300  279.147979   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     213.136940  233.416212  202.821461  215.055444  ...  169.147656   \n",
       "1     212.844259  233.292498  203.094102  215.228612  ...  169.469071   \n",
       "2     212.552198  233.165031  203.367043  215.404428  ...  169.790500   \n",
       "3     212.260816  233.033756  203.640209  215.582574  ...  170.112037   \n",
       "4     211.970027  232.898766  203.913603  215.762856  ...  170.433821   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  235.391196  220.497413  211.044658  205.695511  ...  174.913838   \n",
       "2439  235.529317  220.497198  210.910581  205.530370  ...  174.881507   \n",
       "2440  235.669179  220.493546  210.775845  205.365708  ...  174.851815   \n",
       "2441  235.811103  220.486423  210.640521  205.201545  ...  174.824357   \n",
       "2442  235.955342  220.476014  210.504516  205.038096  ...  174.798854   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     175.680869  195.231478  187.637399  244.296045  240.707677  198.439601   \n",
       "1     175.677374  195.428247  187.875116  243.940958  240.313042  198.337504   \n",
       "2     175.672988  195.625443  188.111875  243.586884  239.920057  198.240067   \n",
       "3     175.668061  195.823263  188.347696  243.233929  239.528739  198.147536   \n",
       "4     175.662771  196.021985  188.582581  242.882111  239.139085  198.060040   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  195.123629  194.102154  170.352117  253.804827  238.780862  223.348777   \n",
       "2439  194.997915  194.137131  170.297434  253.794552  238.893328  223.134036   \n",
       "2440  194.869596  194.174712  170.241666  253.784674  239.004938  222.919171   \n",
       "2441  194.738693  194.215230  170.184822  253.775163  239.115735  222.704098   \n",
       "2442  194.605222  194.258957  170.126913  253.765884  239.225797  222.488912   \n",
       "\n",
       "              45          46          47  \n",
       "0     223.215212  108.578469  150.342019  \n",
       "1     223.071305  108.490446  150.358478  \n",
       "2     222.930539  108.401175  150.371969  \n",
       "3     222.793100  108.310803  150.382686  \n",
       "4     222.659032  108.219520  150.390880  \n",
       "...          ...         ...         ...  \n",
       "2438  213.205734  141.658729  109.645137  \n",
       "2439  213.184004  141.996996  109.875721  \n",
       "2440  213.161838  142.336676  110.108562  \n",
       "2441  213.139343  142.677461  110.343690  \n",
       "2442  213.116608  143.019033  110.581220  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.743085</td>\n",
       "      <td>245.587438</td>\n",
       "      <td>154.771840</td>\n",
       "      <td>186.323260</td>\n",
       "      <td>267.056984</td>\n",
       "      <td>281.794220</td>\n",
       "      <td>213.136940</td>\n",
       "      <td>233.416212</td>\n",
       "      <td>202.821461</td>\n",
       "      <td>215.055444</td>\n",
       "      <td>...</td>\n",
       "      <td>169.147656</td>\n",
       "      <td>175.680869</td>\n",
       "      <td>195.231478</td>\n",
       "      <td>187.637399</td>\n",
       "      <td>244.296045</td>\n",
       "      <td>240.707677</td>\n",
       "      <td>198.439601</td>\n",
       "      <td>223.215212</td>\n",
       "      <td>108.578469</td>\n",
       "      <td>150.342019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.875959</td>\n",
       "      <td>245.628403</td>\n",
       "      <td>154.990000</td>\n",
       "      <td>186.452474</td>\n",
       "      <td>266.774556</td>\n",
       "      <td>281.682030</td>\n",
       "      <td>212.844259</td>\n",
       "      <td>233.292498</td>\n",
       "      <td>203.094102</td>\n",
       "      <td>215.228612</td>\n",
       "      <td>...</td>\n",
       "      <td>169.469071</td>\n",
       "      <td>175.677374</td>\n",
       "      <td>195.428247</td>\n",
       "      <td>187.875116</td>\n",
       "      <td>243.940958</td>\n",
       "      <td>240.313042</td>\n",
       "      <td>198.337504</td>\n",
       "      <td>223.071305</td>\n",
       "      <td>108.490446</td>\n",
       "      <td>150.358478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.011181</td>\n",
       "      <td>245.670578</td>\n",
       "      <td>155.208309</td>\n",
       "      <td>186.581710</td>\n",
       "      <td>266.493200</td>\n",
       "      <td>281.567210</td>\n",
       "      <td>212.552198</td>\n",
       "      <td>233.165031</td>\n",
       "      <td>203.367043</td>\n",
       "      <td>215.404428</td>\n",
       "      <td>...</td>\n",
       "      <td>169.790500</td>\n",
       "      <td>175.672988</td>\n",
       "      <td>195.625443</td>\n",
       "      <td>188.111875</td>\n",
       "      <td>243.586884</td>\n",
       "      <td>239.920057</td>\n",
       "      <td>198.240067</td>\n",
       "      <td>222.930539</td>\n",
       "      <td>108.401175</td>\n",
       "      <td>150.371969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224.148595</td>\n",
       "      <td>245.714444</td>\n",
       "      <td>155.426454</td>\n",
       "      <td>186.711006</td>\n",
       "      <td>266.212869</td>\n",
       "      <td>281.449838</td>\n",
       "      <td>212.260816</td>\n",
       "      <td>233.033756</td>\n",
       "      <td>203.640209</td>\n",
       "      <td>215.582574</td>\n",
       "      <td>...</td>\n",
       "      <td>170.112037</td>\n",
       "      <td>175.668061</td>\n",
       "      <td>195.823263</td>\n",
       "      <td>188.347696</td>\n",
       "      <td>243.233929</td>\n",
       "      <td>239.528739</td>\n",
       "      <td>198.147536</td>\n",
       "      <td>222.793100</td>\n",
       "      <td>108.310803</td>\n",
       "      <td>150.382686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224.288038</td>\n",
       "      <td>245.760404</td>\n",
       "      <td>155.644190</td>\n",
       "      <td>186.840358</td>\n",
       "      <td>265.933706</td>\n",
       "      <td>281.329994</td>\n",
       "      <td>211.970027</td>\n",
       "      <td>232.898766</td>\n",
       "      <td>203.913603</td>\n",
       "      <td>215.762856</td>\n",
       "      <td>...</td>\n",
       "      <td>170.433821</td>\n",
       "      <td>175.662771</td>\n",
       "      <td>196.021985</td>\n",
       "      <td>188.582581</td>\n",
       "      <td>242.882111</td>\n",
       "      <td>239.139085</td>\n",
       "      <td>198.060040</td>\n",
       "      <td>222.659032</td>\n",
       "      <td>108.219520</td>\n",
       "      <td>150.390880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.491893</td>\n",
       "      <td>228.347280</td>\n",
       "      <td>163.325436</td>\n",
       "      <td>140.725997</td>\n",
       "      <td>289.325607</td>\n",
       "      <td>279.420575</td>\n",
       "      <td>235.391196</td>\n",
       "      <td>220.497413</td>\n",
       "      <td>211.044658</td>\n",
       "      <td>205.695511</td>\n",
       "      <td>...</td>\n",
       "      <td>174.913838</td>\n",
       "      <td>195.123629</td>\n",
       "      <td>194.102154</td>\n",
       "      <td>170.352117</td>\n",
       "      <td>253.804827</td>\n",
       "      <td>238.780862</td>\n",
       "      <td>223.348777</td>\n",
       "      <td>213.205734</td>\n",
       "      <td>141.658729</td>\n",
       "      <td>109.645137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.356194</td>\n",
       "      <td>228.299296</td>\n",
       "      <td>163.396140</td>\n",
       "      <td>140.781240</td>\n",
       "      <td>289.263115</td>\n",
       "      <td>279.352105</td>\n",
       "      <td>235.529317</td>\n",
       "      <td>220.497198</td>\n",
       "      <td>210.910581</td>\n",
       "      <td>205.530370</td>\n",
       "      <td>...</td>\n",
       "      <td>174.881507</td>\n",
       "      <td>194.997915</td>\n",
       "      <td>194.137131</td>\n",
       "      <td>170.297434</td>\n",
       "      <td>253.794552</td>\n",
       "      <td>238.893328</td>\n",
       "      <td>223.134036</td>\n",
       "      <td>213.184004</td>\n",
       "      <td>141.996996</td>\n",
       "      <td>109.875721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.221940</td>\n",
       "      <td>228.253433</td>\n",
       "      <td>163.467314</td>\n",
       "      <td>140.838753</td>\n",
       "      <td>289.200562</td>\n",
       "      <td>279.283755</td>\n",
       "      <td>235.669179</td>\n",
       "      <td>220.493546</td>\n",
       "      <td>210.775845</td>\n",
       "      <td>205.365708</td>\n",
       "      <td>...</td>\n",
       "      <td>174.851815</td>\n",
       "      <td>194.869596</td>\n",
       "      <td>194.174712</td>\n",
       "      <td>170.241666</td>\n",
       "      <td>253.784674</td>\n",
       "      <td>239.004938</td>\n",
       "      <td>222.919171</td>\n",
       "      <td>213.161838</td>\n",
       "      <td>142.336676</td>\n",
       "      <td>110.108562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.089132</td>\n",
       "      <td>228.209953</td>\n",
       "      <td>163.539226</td>\n",
       "      <td>140.898725</td>\n",
       "      <td>289.137701</td>\n",
       "      <td>279.215639</td>\n",
       "      <td>235.811103</td>\n",
       "      <td>220.486423</td>\n",
       "      <td>210.640521</td>\n",
       "      <td>205.201545</td>\n",
       "      <td>...</td>\n",
       "      <td>174.824357</td>\n",
       "      <td>194.738693</td>\n",
       "      <td>194.215230</td>\n",
       "      <td>170.184822</td>\n",
       "      <td>253.775163</td>\n",
       "      <td>239.115735</td>\n",
       "      <td>222.704098</td>\n",
       "      <td>213.139343</td>\n",
       "      <td>142.677461</td>\n",
       "      <td>110.343690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>239.957675</td>\n",
       "      <td>228.169041</td>\n",
       "      <td>163.612031</td>\n",
       "      <td>140.961407</td>\n",
       "      <td>289.074300</td>\n",
       "      <td>279.147979</td>\n",
       "      <td>235.955342</td>\n",
       "      <td>220.476014</td>\n",
       "      <td>210.504516</td>\n",
       "      <td>205.038096</td>\n",
       "      <td>...</td>\n",
       "      <td>174.798854</td>\n",
       "      <td>194.605222</td>\n",
       "      <td>194.258957</td>\n",
       "      <td>170.126913</td>\n",
       "      <td>253.765884</td>\n",
       "      <td>239.225797</td>\n",
       "      <td>222.488912</td>\n",
       "      <td>213.116608</td>\n",
       "      <td>143.019033</td>\n",
       "      <td>110.581220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     223.743085  245.587438  154.771840  186.323260  267.056984  281.794220   \n",
       "1     223.875959  245.628403  154.990000  186.452474  266.774556  281.682030   \n",
       "2     224.011181  245.670578  155.208309  186.581710  266.493200  281.567210   \n",
       "3     224.148595  245.714444  155.426454  186.711006  266.212869  281.449838   \n",
       "4     224.288038  245.760404  155.644190  186.840358  265.933706  281.329994   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.491893  228.347280  163.325436  140.725997  289.325607  279.420575   \n",
       "2439  240.356194  228.299296  163.396140  140.781240  289.263115  279.352105   \n",
       "2440  240.221940  228.253433  163.467314  140.838753  289.200562  279.283755   \n",
       "2441  240.089132  228.209953  163.539226  140.898725  289.137701  279.215639   \n",
       "2442  239.957675  228.169041  163.612031  140.961407  289.074300  279.147979   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     213.136940  233.416212  202.821461  215.055444  ...  169.147656   \n",
       "1     212.844259  233.292498  203.094102  215.228612  ...  169.469071   \n",
       "2     212.552198  233.165031  203.367043  215.404428  ...  169.790500   \n",
       "3     212.260816  233.033756  203.640209  215.582574  ...  170.112037   \n",
       "4     211.970027  232.898766  203.913603  215.762856  ...  170.433821   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  235.391196  220.497413  211.044658  205.695511  ...  174.913838   \n",
       "2439  235.529317  220.497198  210.910581  205.530370  ...  174.881507   \n",
       "2440  235.669179  220.493546  210.775845  205.365708  ...  174.851815   \n",
       "2441  235.811103  220.486423  210.640521  205.201545  ...  174.824357   \n",
       "2442  235.955342  220.476014  210.504516  205.038096  ...  174.798854   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     175.680869  195.231478  187.637399  244.296045  240.707677  198.439601   \n",
       "1     175.677374  195.428247  187.875116  243.940958  240.313042  198.337504   \n",
       "2     175.672988  195.625443  188.111875  243.586884  239.920057  198.240067   \n",
       "3     175.668061  195.823263  188.347696  243.233929  239.528739  198.147536   \n",
       "4     175.662771  196.021985  188.582581  242.882111  239.139085  198.060040   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  195.123629  194.102154  170.352117  253.804827  238.780862  223.348777   \n",
       "2439  194.997915  194.137131  170.297434  253.794552  238.893328  223.134036   \n",
       "2440  194.869596  194.174712  170.241666  253.784674  239.004938  222.919171   \n",
       "2441  194.738693  194.215230  170.184822  253.775163  239.115735  222.704098   \n",
       "2442  194.605222  194.258957  170.126913  253.765884  239.225797  222.488912   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     223.215212  108.578469  150.342019  \n",
       "1     223.071305  108.490446  150.358478  \n",
       "2     222.930539  108.401175  150.371969  \n",
       "3     222.793100  108.310803  150.382686  \n",
       "4     222.659032  108.219520  150.390880  \n",
       "...          ...         ...         ...  \n",
       "2438  213.205734  141.658729  109.645137  \n",
       "2439  213.184004  141.996996  109.875721  \n",
       "2440  213.161838  142.336676  110.108562  \n",
       "2441  213.139343  142.677461  110.343690  \n",
       "2442  213.116608  143.019033  110.581220  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.743085</td>\n",
       "      <td>245.587438</td>\n",
       "      <td>154.771840</td>\n",
       "      <td>186.323260</td>\n",
       "      <td>267.056984</td>\n",
       "      <td>281.794220</td>\n",
       "      <td>213.136940</td>\n",
       "      <td>233.416212</td>\n",
       "      <td>202.821461</td>\n",
       "      <td>215.055444</td>\n",
       "      <td>167.541488</td>\n",
       "      <td>178.784265</td>\n",
       "      <td>251.622217</td>\n",
       "      <td>256.867334</td>\n",
       "      <td>221.523587</td>\n",
       "      <td>234.912080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.875959</td>\n",
       "      <td>245.628403</td>\n",
       "      <td>154.990000</td>\n",
       "      <td>186.452474</td>\n",
       "      <td>266.774556</td>\n",
       "      <td>281.682030</td>\n",
       "      <td>212.844259</td>\n",
       "      <td>233.292498</td>\n",
       "      <td>203.094102</td>\n",
       "      <td>215.228612</td>\n",
       "      <td>167.642420</td>\n",
       "      <td>179.081244</td>\n",
       "      <td>251.272735</td>\n",
       "      <td>256.729369</td>\n",
       "      <td>221.274181</td>\n",
       "      <td>234.445764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224.011181</td>\n",
       "      <td>245.670578</td>\n",
       "      <td>155.208309</td>\n",
       "      <td>186.581710</td>\n",
       "      <td>266.493200</td>\n",
       "      <td>281.567210</td>\n",
       "      <td>212.552198</td>\n",
       "      <td>233.165031</td>\n",
       "      <td>203.367043</td>\n",
       "      <td>215.404428</td>\n",
       "      <td>167.745248</td>\n",
       "      <td>179.376102</td>\n",
       "      <td>250.927682</td>\n",
       "      <td>256.590563</td>\n",
       "      <td>221.025559</td>\n",
       "      <td>233.983725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>224.148595</td>\n",
       "      <td>245.714444</td>\n",
       "      <td>155.426454</td>\n",
       "      <td>186.711006</td>\n",
       "      <td>266.212869</td>\n",
       "      <td>281.449838</td>\n",
       "      <td>212.260816</td>\n",
       "      <td>233.033756</td>\n",
       "      <td>203.640209</td>\n",
       "      <td>215.582574</td>\n",
       "      <td>167.849870</td>\n",
       "      <td>179.668907</td>\n",
       "      <td>250.587079</td>\n",
       "      <td>256.450662</td>\n",
       "      <td>220.777616</td>\n",
       "      <td>233.526214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224.288038</td>\n",
       "      <td>245.760404</td>\n",
       "      <td>155.644190</td>\n",
       "      <td>186.840358</td>\n",
       "      <td>265.933706</td>\n",
       "      <td>281.329994</td>\n",
       "      <td>211.970027</td>\n",
       "      <td>232.898766</td>\n",
       "      <td>203.913603</td>\n",
       "      <td>215.762856</td>\n",
       "      <td>167.956272</td>\n",
       "      <td>179.959641</td>\n",
       "      <td>250.250877</td>\n",
       "      <td>256.309416</td>\n",
       "      <td>220.530344</td>\n",
       "      <td>233.073294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.491893</td>\n",
       "      <td>228.347280</td>\n",
       "      <td>163.325436</td>\n",
       "      <td>140.725997</td>\n",
       "      <td>289.325607</td>\n",
       "      <td>279.420575</td>\n",
       "      <td>235.391196</td>\n",
       "      <td>220.497413</td>\n",
       "      <td>211.044658</td>\n",
       "      <td>205.695511</td>\n",
       "      <td>162.080521</td>\n",
       "      <td>149.818471</td>\n",
       "      <td>267.608044</td>\n",
       "      <td>262.881371</td>\n",
       "      <td>231.681003</td>\n",
       "      <td>226.686987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.356194</td>\n",
       "      <td>228.299296</td>\n",
       "      <td>163.396140</td>\n",
       "      <td>140.781240</td>\n",
       "      <td>289.263115</td>\n",
       "      <td>279.352105</td>\n",
       "      <td>235.529317</td>\n",
       "      <td>220.497198</td>\n",
       "      <td>210.910581</td>\n",
       "      <td>205.530370</td>\n",
       "      <td>162.294814</td>\n",
       "      <td>149.778755</td>\n",
       "      <td>267.622757</td>\n",
       "      <td>262.866899</td>\n",
       "      <td>231.663910</td>\n",
       "      <td>226.829957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.221940</td>\n",
       "      <td>228.253433</td>\n",
       "      <td>163.467314</td>\n",
       "      <td>140.838753</td>\n",
       "      <td>289.200562</td>\n",
       "      <td>279.283755</td>\n",
       "      <td>235.669179</td>\n",
       "      <td>220.493546</td>\n",
       "      <td>210.775845</td>\n",
       "      <td>205.365708</td>\n",
       "      <td>162.512681</td>\n",
       "      <td>149.737821</td>\n",
       "      <td>267.640554</td>\n",
       "      <td>262.851723</td>\n",
       "      <td>231.646888</td>\n",
       "      <td>226.975243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.089132</td>\n",
       "      <td>228.209953</td>\n",
       "      <td>163.539226</td>\n",
       "      <td>140.898725</td>\n",
       "      <td>289.137701</td>\n",
       "      <td>279.215639</td>\n",
       "      <td>235.811103</td>\n",
       "      <td>220.486423</td>\n",
       "      <td>210.640521</td>\n",
       "      <td>205.201545</td>\n",
       "      <td>162.733987</td>\n",
       "      <td>149.695575</td>\n",
       "      <td>267.661706</td>\n",
       "      <td>262.836162</td>\n",
       "      <td>231.629391</td>\n",
       "      <td>227.122726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>239.957675</td>\n",
       "      <td>228.169041</td>\n",
       "      <td>163.612031</td>\n",
       "      <td>140.961407</td>\n",
       "      <td>289.074300</td>\n",
       "      <td>279.147979</td>\n",
       "      <td>235.955342</td>\n",
       "      <td>220.476014</td>\n",
       "      <td>210.504516</td>\n",
       "      <td>205.038096</td>\n",
       "      <td>162.958676</td>\n",
       "      <td>149.651770</td>\n",
       "      <td>267.686305</td>\n",
       "      <td>262.820620</td>\n",
       "      <td>231.611029</td>\n",
       "      <td>227.272211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     223.743085  245.587438  154.771840  186.323260  267.056984  281.794220   \n",
       "1     223.875959  245.628403  154.990000  186.452474  266.774556  281.682030   \n",
       "2     224.011181  245.670578  155.208309  186.581710  266.493200  281.567210   \n",
       "3     224.148595  245.714444  155.426454  186.711006  266.212869  281.449838   \n",
       "4     224.288038  245.760404  155.644190  186.840358  265.933706  281.329994   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.491893  228.347280  163.325436  140.725997  289.325607  279.420575   \n",
       "2439  240.356194  228.299296  163.396140  140.781240  289.263115  279.352105   \n",
       "2440  240.221940  228.253433  163.467314  140.838753  289.200562  279.283755   \n",
       "2441  240.089132  228.209953  163.539226  140.898725  289.137701  279.215639   \n",
       "2442  239.957675  228.169041  163.612031  140.961407  289.074300  279.147979   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10    sensor11    sensor12  \\\n",
       "0     213.136940  233.416212  202.821461  215.055444  167.541488  178.784265   \n",
       "1     212.844259  233.292498  203.094102  215.228612  167.642420  179.081244   \n",
       "2     212.552198  233.165031  203.367043  215.404428  167.745248  179.376102   \n",
       "3     212.260816  233.033756  203.640209  215.582574  167.849870  179.668907   \n",
       "4     211.970027  232.898766  203.913603  215.762856  167.956272  179.959641   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  235.391196  220.497413  211.044658  205.695511  162.080521  149.818471   \n",
       "2439  235.529317  220.497198  210.910581  205.530370  162.294814  149.778755   \n",
       "2440  235.669179  220.493546  210.775845  205.365708  162.512681  149.737821   \n",
       "2441  235.811103  220.486423  210.640521  205.201545  162.733987  149.695575   \n",
       "2442  235.955342  220.476014  210.504516  205.038096  162.958676  149.651770   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     251.622217  256.867334  221.523587  234.912080  \n",
       "1     251.272735  256.729369  221.274181  234.445764  \n",
       "2     250.927682  256.590563  221.025559  233.983725  \n",
       "3     250.587079  256.450662  220.777616  233.526214  \n",
       "4     250.250877  256.309416  220.530344  233.073294  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  267.608044  262.881371  231.681003  226.686987  \n",
       "2439  267.622757  262.866899  231.663910  226.829957  \n",
       "2440  267.640554  262.851723  231.646888  226.975243  \n",
       "2441  267.661706  262.836162  231.629391  227.122726  \n",
       "2442  267.686305  262.820620  231.611029  227.272211  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 16ms/step - loss: 1073.9763 - val_loss: 813.5546\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 622.5889 - val_loss: 511.6420\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 434.7494 - val_loss: 391.6845\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 355.1763 - val_loss: 391.3338\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 286.2544 - val_loss: 270.7663\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 200.9646 - val_loss: 148.2646\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 92.3701 - val_loss: 62.7688\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 39.9440 - val_loss: 53.5165\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.1339 - val_loss: 12.8862\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 8.7850 - val_loss: 12.1762\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7958 - val_loss: 6.3802\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8083 - val_loss: 6.1275\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.1575 - val_loss: 3.5126\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 4.4207 - val_loss: 16.9906\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1012 - val_loss: 6.9828\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7809 - val_loss: 2.2095\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2043 - val_loss: 4.1181\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8980 - val_loss: 2.1331\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2255 - val_loss: 1.5719\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5091 - val_loss: 2.1603\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.5625 - val_loss: 61.1945\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.3297 - val_loss: 5.4699\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5126 - val_loss: 1.9603\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8196 - val_loss: 3.6556\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1621 - val_loss: 0.7493\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1919 - val_loss: 2.0592\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0973 - val_loss: 2.1803\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3663 - val_loss: 2.5030\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4986 - val_loss: 2.8178\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2552 - val_loss: 3.1751\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5406 - val_loss: 4.4511\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8635 - val_loss: 36.2164\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.1057 - val_loss: 0.8180\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8330 - val_loss: 1.3278\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9469 - val_loss: 1.1745\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8185 - val_loss: 0.8662\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4615 - val_loss: 2.4471\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9645 - val_loss: 0.5633\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7605 - val_loss: 2.3163\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6247 - val_loss: 1.9430\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8141 - val_loss: 0.7035\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0193 - val_loss: 4.2928\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1381 - val_loss: 2.5727\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5412 - val_loss: 0.6262\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7925 - val_loss: 1.4728\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5063 - val_loss: 0.4949\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2760 - val_loss: 6.6673\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.4166 - val_loss: 0.8509\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5921 - val_loss: 0.5742\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4383 - val_loss: 0.9193\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3999 - val_loss: 0.8578\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5206 - val_loss: 0.6381\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4427 - val_loss: 1.9282\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3540 - val_loss: 0.4832\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6220 - val_loss: 1.2447\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6811 - val_loss: 0.4987\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5447 - val_loss: 0.7693\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4325 - val_loss: 0.5725\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8576 - val_loss: 1.8730\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8025 - val_loss: 3.0900\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6034 - val_loss: 1.0602\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4645 - val_loss: 0.1354\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5140 - val_loss: 4.0497\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6182 - val_loss: 0.4751\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5938 - val_loss: 3.6126\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4650 - val_loss: 0.5592\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3078 - val_loss: 0.5065\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4552 - val_loss: 0.4217\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3498 - val_loss: 1.3288\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9324 - val_loss: 1.4997\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3684 - val_loss: 0.6063\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3179 - val_loss: 0.2766\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2250 - val_loss: 0.3016\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2998 - val_loss: 0.2581\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8086 - val_loss: 23.2240\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6272 - val_loss: 1.4799\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2501 - val_loss: 0.4791\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2520 - val_loss: 0.3413\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2522 - val_loss: 0.2262\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2494 - val_loss: 0.1748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2007 - val_loss: 0.2686\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2316 - val_loss: 0.2241\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2753 - val_loss: 0.5257\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4248 - val_loss: 0.4491\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3234 - val_loss: 0.4137\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2929 - val_loss: 12.5821\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8579 - val_loss: 0.2604\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2339 - val_loss: 0.2380\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2158 - val_loss: 0.7072\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2451 - val_loss: 0.1813\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1927 - val_loss: 0.6044\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1791 - val_loss: 0.1832\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.135305875388024\n",
      "Mean Absolute Error (MAE): 0.2857722808229147\n",
      "Root Mean Squared Error (RMSE): 0.3678394695896893\n",
      "Time taken: 290.41337871551514\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1102.5658 - val_loss: 815.8082\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 688.3784 - val_loss: 549.6024\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 472.0473 - val_loss: 410.7161\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 331.5428 - val_loss: 300.0655\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 227.1046 - val_loss: 224.8528\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 121.2990 - val_loss: 105.9114\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 49.1923 - val_loss: 34.6079\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 21.5803 - val_loss: 19.9266\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.5062 - val_loss: 8.6714\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.7596 - val_loss: 10.5638\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.3143 - val_loss: 7.2917\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.8897 - val_loss: 2.5542\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1315 - val_loss: 6.4155\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.9529 - val_loss: 1.8634\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.1019 - val_loss: 2.2555\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7318 - val_loss: 3.2362\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9309 - val_loss: 2.0990\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6828 - val_loss: 2.6857\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5920 - val_loss: 3.0770\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.9086 - val_loss: 3.5534\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1385 - val_loss: 1.2920\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.6306 - val_loss: 2.3038\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.1231 - val_loss: 2.8830\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7337 - val_loss: 0.9575\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6579 - val_loss: 7.1363\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4623 - val_loss: 1.3327\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9972 - val_loss: 0.8142\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6032 - val_loss: 2.0794\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2824 - val_loss: 3.5946\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9167 - val_loss: 0.9499\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6887 - val_loss: 1.0010\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6516 - val_loss: 0.5247\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6501 - val_loss: 2.9962\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0229 - val_loss: 2.6287\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1496 - val_loss: 1.9942\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6009 - val_loss: 0.9959\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5596 - val_loss: 17.5819\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8117 - val_loss: 1.4999\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3815 - val_loss: 0.6703\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4171 - val_loss: 5.3579\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5691 - val_loss: 0.4249\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5183 - val_loss: 30.5665\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2231 - val_loss: 0.4782\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4352 - val_loss: 1.1339\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2979 - val_loss: 0.3750\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8751 - val_loss: 0.3342\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4755 - val_loss: 0.8155\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.2307 - val_loss: 2.7652\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9927 - val_loss: 0.9931\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7597 - val_loss: 3.2894\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6059 - val_loss: 0.4581\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3823 - val_loss: 0.5394\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4021 - val_loss: 0.5740\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8750 - val_loss: 1.3159\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3519 - val_loss: 0.2834\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5610 - val_loss: 1.6636\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4395 - val_loss: 0.5293\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3564 - val_loss: 0.5608\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0016 - val_loss: 2.7933\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9673 - val_loss: 1.3367\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3710 - val_loss: 0.4589\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2840 - val_loss: 0.7221\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5839 - val_loss: 6.1614\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4877 - val_loss: 0.2643\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4120 - val_loss: 1.8152\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5076 - val_loss: 0.7112\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7131 - val_loss: 1.9031\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4897 - val_loss: 10.2622\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8896 - val_loss: 0.4161\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2792 - val_loss: 0.7904\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3668 - val_loss: 3.6360\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8415 - val_loss: 1.5739\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2455 - val_loss: 0.2150\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2700 - val_loss: 0.2690\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3281 - val_loss: 0.5786\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0658 - val_loss: 0.5502\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3129 - val_loss: 0.2466\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2725 - val_loss: 0.5097\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2285 - val_loss: 0.4930\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3015 - val_loss: 0.5162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3764 - val_loss: 2.9098\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4488 - val_loss: 0.5841\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2177 - val_loss: 0.2433\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1353 - val_loss: 0.1511\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2127 - val_loss: 0.1836\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1919 - val_loss: 0.1815\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7641 - val_loss: 0.7284\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2086 - val_loss: 0.2596\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1739 - val_loss: 1.5834\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3750 - val_loss: 0.6356\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2021 - val_loss: 0.6314\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1731 - val_loss: 0.0753\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1540 - val_loss: 0.4498\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1458 - val_loss: 0.5181\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5370 - val_loss: 13.4731\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8176 - val_loss: 0.4380\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1578 - val_loss: 0.0889\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1121 - val_loss: 0.1553\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1233 - val_loss: 0.0864\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1395 - val_loss: 0.1943\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1027 - val_loss: 0.2719\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1775 - val_loss: 0.0779\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1706 - val_loss: 0.1896\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7873 - val_loss: 1.1965\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1741 - val_loss: 0.1649\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0992 - val_loss: 0.0758\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1058 - val_loss: 0.2130\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0811 - val_loss: 0.2596\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1503 - val_loss: 0.5208\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2380 - val_loss: 0.3199\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1533 - val_loss: 0.1522\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1199 - val_loss: 0.1387\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2617 - val_loss: 0.3747\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2584 - val_loss: 0.3642\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2254 - val_loss: 0.3444\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2110 - val_loss: 0.3448\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4612 - val_loss: 3.7797\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8192 - val_loss: 0.3906\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1239 - val_loss: 0.1896\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1137 - val_loss: 0.0824\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1164 - val_loss: 0.3066\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1513 - val_loss: 0.1297\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.07532768443354199\n",
      "Mean Absolute Error (MAE): 0.20148132102514635\n",
      "Root Mean Squared Error (RMSE): 0.2744588938867567\n",
      "Time taken: 383.816118478775\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 13ms/step - loss: 1129.4886 - val_loss: 853.5308\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 687.9333 - val_loss: 535.1863\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 468.5273 - val_loss: 387.2410\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 357.4586 - val_loss: 342.2501\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 273.6815 - val_loss: 235.4559\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 173.4115 - val_loss: 180.3203\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 79.2746 - val_loss: 56.7044\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 31.7034 - val_loss: 27.6836\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.0478 - val_loss: 13.2208\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.3490 - val_loss: 5.5935\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.7204 - val_loss: 7.7114\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.1438 - val_loss: 19.6266\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2190 - val_loss: 2.7293\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4624 - val_loss: 4.4837\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4453 - val_loss: 1.8008\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.6710 - val_loss: 2.2387\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5201 - val_loss: 2.0043\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8920 - val_loss: 1.2429\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3815 - val_loss: 2.2922\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4755 - val_loss: 2.5370\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7851 - val_loss: 4.9443\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8047 - val_loss: 13.1549\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5313 - val_loss: 12.7219\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0784 - val_loss: 4.0318\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1829 - val_loss: 1.4391\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4969 - val_loss: 1.6127\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8963 - val_loss: 2.6414\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7326 - val_loss: 0.9940\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1478 - val_loss: 1.2263\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1118 - val_loss: 0.6563\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4240 - val_loss: 1.6423\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3450 - val_loss: 0.5954\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8509 - val_loss: 0.9280\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.5171 - val_loss: 1.5175\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7740 - val_loss: 3.8242\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6245 - val_loss: 0.4543\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5902 - val_loss: 0.7753\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6391 - val_loss: 0.8451\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7768 - val_loss: 3.4573\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3445 - val_loss: 0.7186\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6662 - val_loss: 0.3298\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8003 - val_loss: 0.5883\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6151 - val_loss: 0.7230\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8858 - val_loss: 1.6981\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.4495 - val_loss: 0.3877\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4985 - val_loss: 1.2499\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7783 - val_loss: 5.2733\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6009 - val_loss: 1.8636\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9084 - val_loss: 13.7201\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4865 - val_loss: 0.1775\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.9624 - val_loss: 49.9159\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6704 - val_loss: 0.7928\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2349 - val_loss: 0.4468\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5497 - val_loss: 0.4714\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4505 - val_loss: 0.7847\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4686 - val_loss: 0.5596\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5934 - val_loss: 0.9661\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5295 - val_loss: 5.9302\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6073 - val_loss: 0.3243\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5219 - val_loss: 0.9847\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9103 - val_loss: 0.6384\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6140 - val_loss: 1.0098\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4175 - val_loss: 0.3931\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6778 - val_loss: 2.5190\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3809 - val_loss: 0.6129\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4490 - val_loss: 2.1338\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9212 - val_loss: 0.8461\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4115 - val_loss: 0.5950\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3589 - val_loss: 2.3686\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4946 - val_loss: 0.3333\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3195 - val_loss: 2.7351\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7868 - val_loss: 1.3442\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2963 - val_loss: 0.5202\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3833 - val_loss: 0.3901\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3741 - val_loss: 6.3657\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4453 - val_loss: 0.4178\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7733 - val_loss: 0.4239\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2904 - val_loss: 0.5955\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3221 - val_loss: 1.8729\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5235 - val_loss: 0.6694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.17746767922309206\n",
      "Mean Absolute Error (MAE): 0.32485605484767627\n",
      "Root Mean Squared Error (RMSE): 0.42126912920731807\n",
      "Time taken: 237.39145827293396\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 11ms/step - loss: 1121.9458 - val_loss: 946.5211\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 804.6557 - val_loss: 731.6212\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 585.1012 - val_loss: 519.5142\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 418.5869 - val_loss: 420.7160\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 308.0389 - val_loss: 287.1309\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 219.6221 - val_loss: 169.8128\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 110.9685 - val_loss: 67.6212\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 46.2061 - val_loss: 34.1957\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 22.3907 - val_loss: 46.3152\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 11.8895 - val_loss: 6.3279\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.4508 - val_loss: 9.6979\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8252 - val_loss: 6.0870\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.4544 - val_loss: 2.3241\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.9588 - val_loss: 3.2360\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4523 - val_loss: 2.8482\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4116 - val_loss: 1.4828\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1944 - val_loss: 2.7958\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8737 - val_loss: 5.8910\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0194 - val_loss: 2.4786\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3302 - val_loss: 0.7473\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1928 - val_loss: 2.1852\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1321 - val_loss: 7.6965\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2134 - val_loss: 1.9625\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.6956 - val_loss: 13.5590\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8183 - val_loss: 1.2095\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0730 - val_loss: 0.9988\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9180 - val_loss: 1.4817\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9788 - val_loss: 3.1697\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8906 - val_loss: 0.3935\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7388 - val_loss: 0.9517\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7775 - val_loss: 2.6713\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3728 - val_loss: 0.7269\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0634 - val_loss: 0.9802\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5060 - val_loss: 0.6889\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5605 - val_loss: 0.5646\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7103 - val_loss: 1.9143\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2733 - val_loss: 3.8139\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5614 - val_loss: 0.4229\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0742 - val_loss: 2.5350\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7186 - val_loss: 0.7655\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4978 - val_loss: 0.3113\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9451 - val_loss: 1.8555\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7361 - val_loss: 0.2031\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5347 - val_loss: 0.4023\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4383 - val_loss: 0.9171\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4433 - val_loss: 1.3477\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5521 - val_loss: 0.4220\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7041 - val_loss: 1.7422\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7895 - val_loss: 1.6783\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6057 - val_loss: 2.8474\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0002 - val_loss: 114.2505\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9676 - val_loss: 1.5614\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4996 - val_loss: 1.2844\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3908 - val_loss: 0.4698\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3088 - val_loss: 0.5038\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3899 - val_loss: 0.5409\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3110 - val_loss: 0.6724\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3038 - val_loss: 0.1790\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2940 - val_loss: 0.3687\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5750 - val_loss: 1.6845\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3790 - val_loss: 1.1362\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4862 - val_loss: 0.7662\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5726 - val_loss: 1.6898\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8820 - val_loss: 1.0035\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3161 - val_loss: 5.4874\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4791 - val_loss: 0.3565\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3196 - val_loss: 0.7310\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4680 - val_loss: 1.6026\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6395 - val_loss: 2.4166\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2457 - val_loss: 0.2275\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2465 - val_loss: 0.6586\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3625 - val_loss: 1.9954\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5654 - val_loss: 0.8005\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4794 - val_loss: 0.6531\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3759 - val_loss: 0.4068\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2457 - val_loss: 0.6352\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2095 - val_loss: 0.1490\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1917 - val_loss: 0.1886\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1780 - val_loss: 0.3191\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2771 - val_loss: 0.1589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2709 - val_loss: 1.0856\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3257 - val_loss: 0.4325\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3571 - val_loss: 0.5556\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2560 - val_loss: 1.2328\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5393 - val_loss: 4.8311\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5115 - val_loss: 0.2566\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2175 - val_loss: 0.2917\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1988 - val_loss: 0.0945\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1254 - val_loss: 0.1810\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1906 - val_loss: 0.1627\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1832 - val_loss: 0.6117\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1951 - val_loss: 0.1679\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1809 - val_loss: 0.2330\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3668 - val_loss: 0.7466\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0437 - val_loss: 0.2671\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1780 - val_loss: 0.1325\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1371 - val_loss: 0.1518\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1910 - val_loss: 0.7149\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2442 - val_loss: 0.3993\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2554 - val_loss: 0.5427\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7367 - val_loss: 0.1795\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2490 - val_loss: 2.0562\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2666 - val_loss: 0.1520\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1804 - val_loss: 0.1472\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1617 - val_loss: 0.3394\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5008 - val_loss: 1.1497\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2918 - val_loss: 18.1832\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4146 - val_loss: 0.1805\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1643 - val_loss: 0.1543\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0982 - val_loss: 0.0993\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0877 - val_loss: 0.0753\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1291 - val_loss: 0.1385\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1131 - val_loss: 0.1781\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1090 - val_loss: 0.1811\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1868 - val_loss: 0.9827\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2273 - val_loss: 0.1632\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4648 - val_loss: 1.0448\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3487 - val_loss: 0.3975\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5491 - val_loss: 21.5322\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1717 - val_loss: 0.0982\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0967 - val_loss: 0.0769\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1006 - val_loss: 0.1237\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1480 - val_loss: 0.4628\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1730 - val_loss: 0.0676\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1002 - val_loss: 0.2310\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1221 - val_loss: 0.2330\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1146 - val_loss: 0.3219\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2139 - val_loss: 0.9453\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6474 - val_loss: 0.6454\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2673 - val_loss: 0.5670\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1251 - val_loss: 0.1413\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0907 - val_loss: 0.0643\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1593 - val_loss: 0.1554\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5528 - val_loss: 4.6873\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2127 - val_loss: 0.6776\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1372 - val_loss: 0.4239\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0854 - val_loss: 0.0814\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0657 - val_loss: 0.0959\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0776 - val_loss: 0.1496\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0791 - val_loss: 0.0785\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1225 - val_loss: 0.1274\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0972 - val_loss: 0.2597\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1261 - val_loss: 0.3806\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1989 - val_loss: 0.3552\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4791 - val_loss: 9.5585\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7734 - val_loss: 0.5598\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0936 - val_loss: 0.0770\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0644 - val_loss: 0.1956\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0573 - val_loss: 0.1065\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0564 - val_loss: 0.0463\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0506 - val_loss: 0.0792\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1164 - val_loss: 0.1494\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0931 - val_loss: 0.0979\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0894 - val_loss: 0.1301\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6199 - val_loss: 7.2463\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2011 - val_loss: 0.4039\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0928 - val_loss: 0.0578\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0565 - val_loss: 0.1409\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1102 - val_loss: 0.4615\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0704 - val_loss: 0.0920\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0875 - val_loss: 0.0749\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1590 - val_loss: 0.4033\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6423 - val_loss: 18.5531\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2553 - val_loss: 0.1157\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0944 - val_loss: 0.1403\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1066 - val_loss: 0.0801\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0756 - val_loss: 0.1101\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0679 - val_loss: 0.0677\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0748 - val_loss: 0.0677\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1006 - val_loss: 0.1267\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1210 - val_loss: 1.4839\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1667 - val_loss: 0.1238\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1087 - val_loss: 0.0940\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7806 - val_loss: 1.1222\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1314 - val_loss: 0.1665\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0625 - val_loss: 0.0927\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0761 - val_loss: 0.0927\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0893 - val_loss: 0.3405\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0930 - val_loss: 0.1891\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1077 - val_loss: 0.1836\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.04629084028256832\n",
      "Mean Absolute Error (MAE): 0.15593068647874808\n",
      "Root Mean Squared Error (RMSE): 0.2151530624522187\n",
      "Time taken: 546.3556451797485\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1105.8385 - val_loss: 884.6206\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 661.5095 - val_loss: 538.8972\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 451.6592 - val_loss: 419.5627\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 351.7854 - val_loss: 317.9568\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 270.1719 - val_loss: 276.5621\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 148.9963 - val_loss: 105.1572\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 59.4369 - val_loss: 40.2709\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 29.3656 - val_loss: 67.5313\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.5334 - val_loss: 57.2372\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0566 - val_loss: 7.7939\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2830 - val_loss: 5.0539\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8656 - val_loss: 2.6076\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7468 - val_loss: 5.4586\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4768 - val_loss: 12.3300\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6749 - val_loss: 2.3135\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9958 - val_loss: 1.0821\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6267 - val_loss: 13.8598\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6569 - val_loss: 1.7116\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2335 - val_loss: 3.2618\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6601 - val_loss: 6.5305\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6172 - val_loss: 1.0474\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5139 - val_loss: 4.7288\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2352 - val_loss: 10.5529\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.6203 - val_loss: 3.3147\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8043 - val_loss: 0.3660\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7877 - val_loss: 2.0366\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3063 - val_loss: 0.4622\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1680 - val_loss: 1.0773\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.1698 - val_loss: 1.6027\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0919 - val_loss: 1.0500\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8038 - val_loss: 1.5617\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4508 - val_loss: 24.5114\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1114 - val_loss: 0.5946\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7789 - val_loss: 1.5094\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7072 - val_loss: 1.5703\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9277 - val_loss: 0.8438\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4117 - val_loss: 6.8370\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3936 - val_loss: 1.0948\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5561 - val_loss: 3.0212\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5300 - val_loss: 1.6644\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5323 - val_loss: 3.1098\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3552 - val_loss: 1.8464\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7224 - val_loss: 1.9097\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3960 - val_loss: 0.7832\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6009 - val_loss: 0.4765\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5789 - val_loss: 0.5084\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7432 - val_loss: 1.9366\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8762 - val_loss: 1.1002\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0654 - val_loss: 0.5748\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.3074 - val_loss: 2.8434\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7272 - val_loss: 0.6309\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5145 - val_loss: 0.6765\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3914 - val_loss: 0.3538\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3749 - val_loss: 0.3506\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6924 - val_loss: 2.6689\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5574 - val_loss: 0.5372\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4089 - val_loss: 0.9115\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7055 - val_loss: 0.3676\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0573 - val_loss: 1.4995\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2462 - val_loss: 0.5109\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2994 - val_loss: 1.3693\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4866 - val_loss: 1.8924\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6112 - val_loss: 5.2054\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6282 - val_loss: 0.4872\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3574 - val_loss: 1.3639\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4009 - val_loss: 1.9314\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1257 - val_loss: 0.6487\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4230 - val_loss: 0.7231\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3255 - val_loss: 0.1963\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2725 - val_loss: 0.6289\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2906 - val_loss: 2.5345\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3260 - val_loss: 0.2590\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2457 - val_loss: 0.5183\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2420 - val_loss: 1.4377\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2281 - val_loss: 0.2445\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6235 - val_loss: 1.2900\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2813 - val_loss: 0.3067\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.6267 - val_loss: 0.7555\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2631 - val_loss: 0.3051\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1975 - val_loss: 0.1520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1880 - val_loss: 0.1916\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1971 - val_loss: 0.9761\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6540 - val_loss: 0.7651\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2731 - val_loss: 0.1716\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2193 - val_loss: 0.2267\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2116 - val_loss: 0.2141\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1969 - val_loss: 0.5851\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2650 - val_loss: 0.8594\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5328 - val_loss: 2.3619\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2122 - val_loss: 0.2450\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9784 - val_loss: 3.1427\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4815 - val_loss: 0.8726\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1554 - val_loss: 4.1520\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2807 - val_loss: 0.2496\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1714 - val_loss: 0.4861\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1710 - val_loss: 0.1355\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2044 - val_loss: 0.6642\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1626 - val_loss: 0.2563\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6027 - val_loss: 2.0980\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2784 - val_loss: 0.3245\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1534 - val_loss: 0.1459\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1286 - val_loss: 0.2460\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2007 - val_loss: 0.2587\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1384 - val_loss: 0.3424\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3216 - val_loss: 0.3849\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2040 - val_loss: 1.7618\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2585 - val_loss: 2.9681\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8854 - val_loss: 1.1035\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5435 - val_loss: 0.3026\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1472 - val_loss: 0.2572\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1037 - val_loss: 0.1202\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1044 - val_loss: 0.4212\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1248 - val_loss: 0.1578\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2688 - val_loss: 0.1717\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1399 - val_loss: 0.3661\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2096 - val_loss: 0.2630\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4792 - val_loss: 1.8359\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1726 - val_loss: 0.4807\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1555 - val_loss: 0.0787\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0863 - val_loss: 0.0700\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1502 - val_loss: 0.3217\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1113 - val_loss: 0.1389\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1343 - val_loss: 0.1217\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0999 - val_loss: 0.1429\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1713 - val_loss: 0.8551\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0470 - val_loss: 3.5523\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1563 - val_loss: 0.4009\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0865 - val_loss: 0.1385\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1059 - val_loss: 0.2160\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1014 - val_loss: 0.2235\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0953 - val_loss: 0.1959\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1585 - val_loss: 0.1645\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1524 - val_loss: 0.1791\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8011 - val_loss: 5.8537\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1684 - val_loss: 0.3293\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0912 - val_loss: 0.1699\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0799 - val_loss: 0.1311\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0895 - val_loss: 0.0974\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1330 - val_loss: 0.3907\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1741 - val_loss: 0.1388\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1435 - val_loss: 0.3242\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1318 - val_loss: 0.3153\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1404 - val_loss: 0.4252\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4768 - val_loss: 34.0382\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6260 - val_loss: 0.2291\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1010 - val_loss: 0.1239\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0934 - val_loss: 0.1570\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0720 - val_loss: 0.0641\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0775 - val_loss: 0.1789\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0641 - val_loss: 0.2899\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1614 - val_loss: 0.5536\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0779 - val_loss: 0.0857\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2039 - val_loss: 0.3195\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1284 - val_loss: 0.1217\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0881 - val_loss: 0.1017\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6578 - val_loss: 24.1793\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2947 - val_loss: 0.2745\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1036 - val_loss: 0.1584\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0783 - val_loss: 0.0958\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0656 - val_loss: 0.0756\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0642 - val_loss: 0.1183\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1455 - val_loss: 0.8347\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1128 - val_loss: 0.1191\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1104 - val_loss: 0.6073\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2075 - val_loss: 0.1153\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2579 - val_loss: 0.8665\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6532 - val_loss: 0.1265\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0834 - val_loss: 0.1824\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0719 - val_loss: 0.2827\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0823 - val_loss: 0.1877\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6553 - val_loss: 3.3469\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1139 - val_loss: 0.4460\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0842 - val_loss: 0.0823\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0593 - val_loss: 0.0858\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0450 - val_loss: 105.2999\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1480 - val_loss: 0.2513\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0723 - val_loss: 0.0544\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0723 - val_loss: 0.0792\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0943 - val_loss: 0.4021\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0715 - val_loss: 0.1341\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1102 - val_loss: 0.6616\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4552 - val_loss: 3.6641\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0777 - val_loss: 0.1068\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0595 - val_loss: 0.0476\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0527 - val_loss: 0.0402\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0517 - val_loss: 0.0753\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0526 - val_loss: 0.1564\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0894 - val_loss: 0.0795\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0665 - val_loss: 0.1125\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0839 - val_loss: 0.2778\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1215 - val_loss: 0.1825\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1430 - val_loss: 0.2123\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1238 - val_loss: 0.3578\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6830 - val_loss: 13.8155\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9562 - val_loss: 0.0943\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0776 - val_loss: 0.0608\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0547 - val_loss: 0.0468\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0418 - val_loss: 0.0363\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0435 - val_loss: 0.0360\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0436 - val_loss: 0.0388\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.038814719869727114\n",
      "Mean Absolute Error (MAE): 0.14837065573721528\n",
      "Root Mean Squared Error (RMSE): 0.19701451690098148\n",
      "Time taken: 611.823605298996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_19900\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.135306  0.285772  0.367839  290.413379\n",
      "1        2  0.075328  0.201481  0.274459  383.816118\n",
      "2        3  0.177468  0.324856  0.421269  237.391458\n",
      "3        4  0.046291  0.155931  0.215153  546.355645\n",
      "4        5  0.038815  0.148371  0.197015  611.823605\n",
      "5  Average  0.094641  0.223282  0.295147  413.960041\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f_obese.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_iReg_f_obese.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f_obese.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
