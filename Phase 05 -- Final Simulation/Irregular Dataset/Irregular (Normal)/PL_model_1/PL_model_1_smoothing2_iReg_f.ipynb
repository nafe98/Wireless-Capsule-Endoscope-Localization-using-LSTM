{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "              6           7           8           9   ...         38  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "              39         40         41          42          43          44  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "              45          46          47  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>...</td>\n",
       "      <td>80.511088</td>\n",
       "      <td>86.266577</td>\n",
       "      <td>91.324276</td>\n",
       "      <td>88.413835</td>\n",
       "      <td>118.079357</td>\n",
       "      <td>112.830599</td>\n",
       "      <td>101.581432</td>\n",
       "      <td>130.484864</td>\n",
       "      <td>65.849519</td>\n",
       "      <td>112.853971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>...</td>\n",
       "      <td>80.667880</td>\n",
       "      <td>86.301348</td>\n",
       "      <td>91.369136</td>\n",
       "      <td>88.468505</td>\n",
       "      <td>117.851056</td>\n",
       "      <td>112.732842</td>\n",
       "      <td>101.535137</td>\n",
       "      <td>130.422780</td>\n",
       "      <td>65.831506</td>\n",
       "      <td>112.816370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>...</td>\n",
       "      <td>80.823085</td>\n",
       "      <td>86.337129</td>\n",
       "      <td>91.414783</td>\n",
       "      <td>88.521309</td>\n",
       "      <td>117.621052</td>\n",
       "      <td>112.638386</td>\n",
       "      <td>101.491179</td>\n",
       "      <td>130.363766</td>\n",
       "      <td>65.812886</td>\n",
       "      <td>112.778795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>...</td>\n",
       "      <td>80.976531</td>\n",
       "      <td>86.373826</td>\n",
       "      <td>91.461416</td>\n",
       "      <td>88.572363</td>\n",
       "      <td>117.389684</td>\n",
       "      <td>112.547228</td>\n",
       "      <td>101.449812</td>\n",
       "      <td>130.307871</td>\n",
       "      <td>65.793756</td>\n",
       "      <td>112.741332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>...</td>\n",
       "      <td>81.128195</td>\n",
       "      <td>86.411394</td>\n",
       "      <td>91.509328</td>\n",
       "      <td>88.621720</td>\n",
       "      <td>117.157444</td>\n",
       "      <td>112.459148</td>\n",
       "      <td>101.411458</td>\n",
       "      <td>130.254975</td>\n",
       "      <td>65.774054</td>\n",
       "      <td>112.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>...</td>\n",
       "      <td>88.095891</td>\n",
       "      <td>110.712051</td>\n",
       "      <td>89.522396</td>\n",
       "      <td>71.221952</td>\n",
       "      <td>124.108295</td>\n",
       "      <td>113.383110</td>\n",
       "      <td>130.244963</td>\n",
       "      <td>114.622501</td>\n",
       "      <td>107.532818</td>\n",
       "      <td>76.703086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>...</td>\n",
       "      <td>88.034588</td>\n",
       "      <td>110.548388</td>\n",
       "      <td>89.310220</td>\n",
       "      <td>71.379769</td>\n",
       "      <td>124.099541</td>\n",
       "      <td>113.470836</td>\n",
       "      <td>130.083759</td>\n",
       "      <td>114.395532</td>\n",
       "      <td>107.742386</td>\n",
       "      <td>76.682848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>...</td>\n",
       "      <td>87.974760</td>\n",
       "      <td>110.380633</td>\n",
       "      <td>89.095824</td>\n",
       "      <td>71.538448</td>\n",
       "      <td>124.091471</td>\n",
       "      <td>113.557163</td>\n",
       "      <td>129.920154</td>\n",
       "      <td>114.165400</td>\n",
       "      <td>107.949968</td>\n",
       "      <td>76.663408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>...</td>\n",
       "      <td>87.916419</td>\n",
       "      <td>110.208561</td>\n",
       "      <td>88.879379</td>\n",
       "      <td>71.697850</td>\n",
       "      <td>124.084220</td>\n",
       "      <td>113.641886</td>\n",
       "      <td>129.753926</td>\n",
       "      <td>113.932214</td>\n",
       "      <td>108.155353</td>\n",
       "      <td>76.644759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>...</td>\n",
       "      <td>87.859482</td>\n",
       "      <td>110.032132</td>\n",
       "      <td>88.660934</td>\n",
       "      <td>71.857879</td>\n",
       "      <td>124.077867</td>\n",
       "      <td>113.725050</td>\n",
       "      <td>129.584939</td>\n",
       "      <td>113.696233</td>\n",
       "      <td>108.358487</td>\n",
       "      <td>76.626723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...   sensor39  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  ...  80.511088   \n",
       "1     105.788181  134.546280   90.431246  107.970290  ...  80.667880   \n",
       "2     105.613823  134.358052   90.503448  108.058270  ...  80.823085   \n",
       "3     105.437718  134.170555   90.574876  108.144722  ...  80.976531   \n",
       "4     105.260017  133.984101   90.645256  108.229792  ...  81.128195   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  ...  88.095891   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  ...  88.034588   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  ...  87.974760   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  ...  87.916419   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  ...  87.859482   \n",
       "\n",
       "        sensor40   sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      86.266577  91.324276  88.413835  118.079357  112.830599  101.581432   \n",
       "1      86.301348  91.369136  88.468505  117.851056  112.732842  101.535137   \n",
       "2      86.337129  91.414783  88.521309  117.621052  112.638386  101.491179   \n",
       "3      86.373826  91.461416  88.572363  117.389684  112.547228  101.449812   \n",
       "4      86.411394  91.509328  88.621720  117.157444  112.459148  101.411458   \n",
       "...          ...        ...        ...         ...         ...         ...   \n",
       "2438  110.712051  89.522396  71.221952  124.108295  113.383110  130.244963   \n",
       "2439  110.548388  89.310220  71.379769  124.099541  113.470836  130.083759   \n",
       "2440  110.380633  89.095824  71.538448  124.091471  113.557163  129.920154   \n",
       "2441  110.208561  88.879379  71.697850  124.084220  113.641886  129.753926   \n",
       "2442  110.032132  88.660934  71.857879  124.077867  113.725050  129.584939   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     130.484864   65.849519  112.853971  \n",
       "1     130.422780   65.831506  112.816370  \n",
       "2     130.363766   65.812886  112.778795  \n",
       "3     130.307871   65.793756  112.741332  \n",
       "4     130.254975   65.774054  112.703922  \n",
       "...          ...         ...         ...  \n",
       "2438  114.622501  107.532818   76.703086  \n",
       "2439  114.395532  107.742386   76.682848  \n",
       "2440  114.165400  107.949968   76.663408  \n",
       "2441  113.932214  108.155353   76.644759  \n",
       "2442  113.696233  108.358487   76.626723  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103.722102</td>\n",
       "      <td>133.549604</td>\n",
       "      <td>70.246654</td>\n",
       "      <td>114.393425</td>\n",
       "      <td>125.518042</td>\n",
       "      <td>148.490719</td>\n",
       "      <td>105.960735</td>\n",
       "      <td>134.734917</td>\n",
       "      <td>90.358364</td>\n",
       "      <td>107.880691</td>\n",
       "      <td>69.255084</td>\n",
       "      <td>97.482707</td>\n",
       "      <td>114.225516</td>\n",
       "      <td>129.327314</td>\n",
       "      <td>105.712779</td>\n",
       "      <td>121.945497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.741769</td>\n",
       "      <td>133.613840</td>\n",
       "      <td>70.333167</td>\n",
       "      <td>114.403077</td>\n",
       "      <td>125.425263</td>\n",
       "      <td>148.406322</td>\n",
       "      <td>105.788181</td>\n",
       "      <td>134.546280</td>\n",
       "      <td>90.431246</td>\n",
       "      <td>107.970290</td>\n",
       "      <td>69.394588</td>\n",
       "      <td>97.512139</td>\n",
       "      <td>114.149329</td>\n",
       "      <td>129.211959</td>\n",
       "      <td>105.436000</td>\n",
       "      <td>121.824863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.759517</td>\n",
       "      <td>133.678503</td>\n",
       "      <td>70.421559</td>\n",
       "      <td>114.410863</td>\n",
       "      <td>125.331731</td>\n",
       "      <td>148.320508</td>\n",
       "      <td>105.613823</td>\n",
       "      <td>134.358052</td>\n",
       "      <td>90.503448</td>\n",
       "      <td>108.058270</td>\n",
       "      <td>69.534232</td>\n",
       "      <td>97.543166</td>\n",
       "      <td>114.073031</td>\n",
       "      <td>129.094360</td>\n",
       "      <td>105.163506</td>\n",
       "      <td>121.700246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.775461</td>\n",
       "      <td>133.743510</td>\n",
       "      <td>70.511477</td>\n",
       "      <td>114.417015</td>\n",
       "      <td>125.237166</td>\n",
       "      <td>148.233207</td>\n",
       "      <td>105.437718</td>\n",
       "      <td>134.170555</td>\n",
       "      <td>90.574876</td>\n",
       "      <td>108.144722</td>\n",
       "      <td>69.674230</td>\n",
       "      <td>97.575632</td>\n",
       "      <td>113.996663</td>\n",
       "      <td>128.974681</td>\n",
       "      <td>104.895214</td>\n",
       "      <td>121.571460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.789800</td>\n",
       "      <td>133.808609</td>\n",
       "      <td>70.602435</td>\n",
       "      <td>114.421924</td>\n",
       "      <td>125.141318</td>\n",
       "      <td>148.144403</td>\n",
       "      <td>105.260017</td>\n",
       "      <td>133.984101</td>\n",
       "      <td>90.645256</td>\n",
       "      <td>108.229792</td>\n",
       "      <td>69.814823</td>\n",
       "      <td>97.609507</td>\n",
       "      <td>113.920433</td>\n",
       "      <td>128.853196</td>\n",
       "      <td>104.631075</td>\n",
       "      <td>121.438527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.394731</td>\n",
       "      <td>113.120062</td>\n",
       "      <td>102.722429</td>\n",
       "      <td>71.609424</td>\n",
       "      <td>151.352281</td>\n",
       "      <td>140.904922</td>\n",
       "      <td>128.827778</td>\n",
       "      <td>113.779812</td>\n",
       "      <td>107.558742</td>\n",
       "      <td>96.997103</td>\n",
       "      <td>82.209100</td>\n",
       "      <td>59.038592</td>\n",
       "      <td>133.164754</td>\n",
       "      <td>127.797332</td>\n",
       "      <td>119.305234</td>\n",
       "      <td>110.972701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>128.269154</td>\n",
       "      <td>112.982064</td>\n",
       "      <td>102.785866</td>\n",
       "      <td>71.581983</td>\n",
       "      <td>151.277725</td>\n",
       "      <td>140.892099</td>\n",
       "      <td>128.842679</td>\n",
       "      <td>113.832694</td>\n",
       "      <td>107.410332</td>\n",
       "      <td>96.815164</td>\n",
       "      <td>82.233775</td>\n",
       "      <td>59.061562</td>\n",
       "      <td>133.030608</td>\n",
       "      <td>127.851915</td>\n",
       "      <td>119.403156</td>\n",
       "      <td>111.115335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>128.143792</td>\n",
       "      <td>112.844766</td>\n",
       "      <td>102.852637</td>\n",
       "      <td>71.555953</td>\n",
       "      <td>151.200817</td>\n",
       "      <td>140.880374</td>\n",
       "      <td>128.857569</td>\n",
       "      <td>113.886728</td>\n",
       "      <td>107.258332</td>\n",
       "      <td>96.632920</td>\n",
       "      <td>82.256408</td>\n",
       "      <td>59.084139</td>\n",
       "      <td>132.896657</td>\n",
       "      <td>127.910104</td>\n",
       "      <td>119.503552</td>\n",
       "      <td>111.260795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>128.018936</td>\n",
       "      <td>112.708365</td>\n",
       "      <td>102.922962</td>\n",
       "      <td>71.531847</td>\n",
       "      <td>151.121227</td>\n",
       "      <td>140.870082</td>\n",
       "      <td>128.872267</td>\n",
       "      <td>113.942389</td>\n",
       "      <td>107.102486</td>\n",
       "      <td>96.450473</td>\n",
       "      <td>82.277304</td>\n",
       "      <td>59.106410</td>\n",
       "      <td>132.763045</td>\n",
       "      <td>127.971895</td>\n",
       "      <td>119.606420</td>\n",
       "      <td>111.408884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>127.894929</td>\n",
       "      <td>112.572888</td>\n",
       "      <td>102.996952</td>\n",
       "      <td>71.509933</td>\n",
       "      <td>151.038840</td>\n",
       "      <td>140.861436</td>\n",
       "      <td>128.886554</td>\n",
       "      <td>113.999895</td>\n",
       "      <td>106.942607</td>\n",
       "      <td>96.267932</td>\n",
       "      <td>82.296749</td>\n",
       "      <td>59.128432</td>\n",
       "      <td>132.629682</td>\n",
       "      <td>128.037204</td>\n",
       "      <td>119.711606</td>\n",
       "      <td>111.559314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     103.722102  133.549604   70.246654  114.393425  125.518042  148.490719   \n",
       "1     103.741769  133.613840   70.333167  114.403077  125.425263  148.406322   \n",
       "2     103.759517  133.678503   70.421559  114.410863  125.331731  148.320508   \n",
       "3     103.775461  133.743510   70.511477  114.417015  125.237166  148.233207   \n",
       "4     103.789800  133.808609   70.602435  114.421924  125.141318  148.144403   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.394731  113.120062  102.722429   71.609424  151.352281  140.904922   \n",
       "2439  128.269154  112.982064  102.785866   71.581983  151.277725  140.892099   \n",
       "2440  128.143792  112.844766  102.852637   71.555953  151.200817  140.880374   \n",
       "2441  128.018936  112.708365  102.922962   71.531847  151.121227  140.870082   \n",
       "2442  127.894929  112.572888  102.996952   71.509933  151.038840  140.861436   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10   sensor11   sensor12  \\\n",
       "0     105.960735  134.734917   90.358364  107.880691  69.255084  97.482707   \n",
       "1     105.788181  134.546280   90.431246  107.970290  69.394588  97.512139   \n",
       "2     105.613823  134.358052   90.503448  108.058270  69.534232  97.543166   \n",
       "3     105.437718  134.170555   90.574876  108.144722  69.674230  97.575632   \n",
       "4     105.260017  133.984101   90.645256  108.229792  69.814823  97.609507   \n",
       "...          ...         ...         ...         ...        ...        ...   \n",
       "2438  128.827778  113.779812  107.558742   96.997103  82.209100  59.038592   \n",
       "2439  128.842679  113.832694  107.410332   96.815164  82.233775  59.061562   \n",
       "2440  128.857569  113.886728  107.258332   96.632920  82.256408  59.084139   \n",
       "2441  128.872267  113.942389  107.102486   96.450473  82.277304  59.106410   \n",
       "2442  128.886554  113.999895  106.942607   96.267932  82.296749  59.128432   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     114.225516  129.327314  105.712779  121.945497  \n",
       "1     114.149329  129.211959  105.436000  121.824863  \n",
       "2     114.073031  129.094360  105.163506  121.700246  \n",
       "3     113.996663  128.974681  104.895214  121.571460  \n",
       "4     113.920433  128.853196  104.631075  121.438527  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  133.164754  127.797332  119.305234  110.972701  \n",
       "2439  133.030608  127.851915  119.403156  111.115335  \n",
       "2440  132.896657  127.910104  119.503552  111.260795  \n",
       "2441  132.763045  127.971895  119.606420  111.408884  \n",
       "2442  132.629682  128.037204  119.711606  111.559314  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 12s 16ms/step - loss: 1055.4326 - val_loss: 781.9579\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 608.0842 - val_loss: 505.6753\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 342.8273 - val_loss: 241.2360\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 156.2360 - val_loss: 97.6664\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 64.6762 - val_loss: 38.7662\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 28.5005 - val_loss: 15.2403\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 13.3218 - val_loss: 10.2515\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 8.3812 - val_loss: 3.8282\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.3904 - val_loss: 4.8382\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.1770 - val_loss: 2.5063\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8033 - val_loss: 3.5474\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3984 - val_loss: 1.7939\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9692 - val_loss: 2.0728\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4015 - val_loss: 1.4754\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2067 - val_loss: 3.5746\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5885 - val_loss: 2.5707\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2981 - val_loss: 2.9266\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7827 - val_loss: 2.0016\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6116 - val_loss: 1.0200\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9808 - val_loss: 1.3197\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0495 - val_loss: 1.3969\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7697 - val_loss: 0.7582\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5941 - val_loss: 4.4566\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4698 - val_loss: 0.8612\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7744 - val_loss: 1.0409\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8258 - val_loss: 0.5497\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9433 - val_loss: 1.5887\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9300 - val_loss: 3.4927\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.8563 - val_loss: 0.6129\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4449 - val_loss: 0.3284\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6573 - val_loss: 1.7212\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7859 - val_loss: 1.0341\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7813 - val_loss: 0.5523\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7105 - val_loss: 2.5167\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0381 - val_loss: 2.1449\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6849 - val_loss: 0.4712\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6669 - val_loss: 1.3443\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5491 - val_loss: 0.7429\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4302 - val_loss: 0.3886\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.8841 - val_loss: 0.5025\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3857 - val_loss: 0.3062\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3290 - val_loss: 0.4723\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4516 - val_loss: 0.6410\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3755 - val_loss: 1.1462\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6197 - val_loss: 0.5942\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5072 - val_loss: 0.5828\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4415 - val_loss: 0.3167\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.5188 - val_loss: 0.7243\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 10s 30ms/step - loss: 0.4986 - val_loss: 1.4034\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 10s 30ms/step - loss: 0.6565 - val_loss: 0.5003\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.5833 - val_loss: 1.1688\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.6928 - val_loss: 0.8105\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.6661 - val_loss: 0.7246\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.2957 - val_loss: 0.2387\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 10s 29ms/step - loss: 0.2850 - val_loss: 0.3763\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.3588 - val_loss: 0.3229\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3203 - val_loss: 0.3676\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6311 - val_loss: 1.6997\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5449 - val_loss: 0.7020\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3999 - val_loss: 0.4449\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6040 - val_loss: 0.3071\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3302 - val_loss: 0.2489\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2895 - val_loss: 0.2802\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 10s 30ms/step - loss: 0.3269 - val_loss: 0.4234\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.5205 - val_loss: 0.4679\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 0.2182 - val_loss: 0.1990\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2803 - val_loss: 0.9182\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 10s 30ms/step - loss: 0.5162 - val_loss: 0.2358\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 9s 29ms/step - loss: 2.9576 - val_loss: 37.9268\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 1.1054 - val_loss: 0.7642\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.2325 - val_loss: 0.1835\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 10s 31ms/step - loss: 0.1858 - val_loss: 0.1844\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 11s 35ms/step - loss: 0.1447 - val_loss: 0.2252\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1670 - val_loss: 0.5833\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2209 - val_loss: 0.1352\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2707 - val_loss: 0.5327\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3065 - val_loss: 0.3466\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2659 - val_loss: 0.3421\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2930 - val_loss: 0.4238\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2451 - val_loss: 0.1707\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3913 - val_loss: 0.2961\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5881 - val_loss: 1.3447\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3467 - val_loss: 0.3467\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2817 - val_loss: 0.4155\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1967 - val_loss: 0.3306\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3157 - val_loss: 0.3732\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2719 - val_loss: 0.4639\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1637 - val_loss: 0.5399\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4880 - val_loss: 1.7725\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1886 - val_loss: 0.1659\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3052 - val_loss: 0.4800\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3194 - val_loss: 0.3821\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1895 - val_loss: 0.2250\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2184 - val_loss: 0.2462\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2632 - val_loss: 1.6763\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3923 - val_loss: 0.5726\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5138 - val_loss: 0.5210\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1657 - val_loss: 0.0853\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1520 - val_loss: 0.1509\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1900 - val_loss: 0.9355\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2499 - val_loss: 0.2944\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2615 - val_loss: 0.5651\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2237 - val_loss: 0.1310\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4345 - val_loss: 0.4991\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1613 - val_loss: 0.1669\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2529 - val_loss: 0.2514\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1838 - val_loss: 0.1412\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1607 - val_loss: 0.1378\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1577 - val_loss: 0.1950\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4674 - val_loss: 0.2467\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1167 - val_loss: 0.2989\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0923 - val_loss: 0.0586\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0738 - val_loss: 0.1095\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0983 - val_loss: 0.0775\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1156 - val_loss: 0.4688\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1033 - val_loss: 0.2343\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1109 - val_loss: 0.2271\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 0.1528 - val_loss: 1.1978\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1651 - val_loss: 0.2424\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2292 - val_loss: 0.3534\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1612 - val_loss: 0.3027\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 9s 26ms/step - loss: 0.1243 - val_loss: 0.3456\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2248 - val_loss: 0.6931\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2450 - val_loss: 0.1073\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1095 - val_loss: 0.0911\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1908 - val_loss: 0.1557\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1073 - val_loss: 0.1005\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1040 - val_loss: 0.1409\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3222 - val_loss: 0.6462\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2035 - val_loss: 0.2167\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1649 - val_loss: 0.1237\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1711 - val_loss: 0.3251\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1522 - val_loss: 0.1918\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1958 - val_loss: 0.5293\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1432 - val_loss: 0.2602\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1484 - val_loss: 0.2842\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2381 - val_loss: 0.4782\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1155 - val_loss: 0.2015\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1125 - val_loss: 0.0604\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1761 - val_loss: 0.2785\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2102 - val_loss: 0.1191\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1102 - val_loss: 0.6669\n",
      "16/16 [==============================] - 1s 14ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.058589984952288586\n",
      "Mean Absolute Error (MAE): 0.17900302637066143\n",
      "Root Mean Squared Error (RMSE): 0.242053681963916\n",
      "Time taken: 1027.1482787132263\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 30ms/step - loss: 1026.5800 - val_loss: 761.4022\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 562.4113 - val_loss: 512.0412\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 383.4868 - val_loss: 384.8959\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 217.2104 - val_loss: 146.7585\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 93.4718 - val_loss: 75.3609\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 39.6667 - val_loss: 30.8852\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 19.5279 - val_loss: 13.0658\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 10.0888 - val_loss: 7.4143\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 7.3485 - val_loss: 6.3440\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 4.2694 - val_loss: 2.8473\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.9038 - val_loss: 3.0552\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.1802 - val_loss: 6.2636\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.0011 - val_loss: 5.4998\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.5318 - val_loss: 4.5836\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.4870 - val_loss: 4.9138\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.9012 - val_loss: 1.6813\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4558 - val_loss: 1.0295\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4375 - val_loss: 1.3094\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.6440 - val_loss: 2.0606\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.5168 - val_loss: 1.8893\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.5912 - val_loss: 2.0162\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.7538 - val_loss: 1.0063\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1014 - val_loss: 3.5554\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1686 - val_loss: 1.9766\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.2373 - val_loss: 2.2833\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9828 - val_loss: 0.8984\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7675 - val_loss: 1.3374\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9211 - val_loss: 1.6946\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7777 - val_loss: 1.5277\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 5.3750 - val_loss: 2.9124\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0172 - val_loss: 1.7029\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5488 - val_loss: 0.6099\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7358 - val_loss: 2.4530\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6544 - val_loss: 0.7809\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4373 - val_loss: 0.3943\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5788 - val_loss: 1.4816\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6894 - val_loss: 1.9688\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1077 - val_loss: 1.1798\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4937 - val_loss: 0.3140\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7837 - val_loss: 0.7894\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6646 - val_loss: 0.6388\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6689 - val_loss: 0.9804\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8883 - val_loss: 0.8573\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0743 - val_loss: 6.1287\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4981 - val_loss: 0.4741\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4652 - val_loss: 0.5075\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4239 - val_loss: 0.6707\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5447 - val_loss: 0.4364\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5973 - val_loss: 0.4364\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4271 - val_loss: 0.2870\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5218 - val_loss: 4.4970\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.5769 - val_loss: 1.1495\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8218 - val_loss: 0.9780\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3612 - val_loss: 0.6908\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2283 - val_loss: 0.5569\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3342 - val_loss: 0.6474\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3505 - val_loss: 0.6040\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6984 - val_loss: 0.9535\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2892 - val_loss: 0.3963\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3594 - val_loss: 0.8699\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7540 - val_loss: 0.3091\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3086 - val_loss: 0.2530\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3999 - val_loss: 1.5958\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4441 - val_loss: 1.4480\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3130 - val_loss: 0.1859\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1497 - val_loss: 0.2919\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4658 - val_loss: 0.3509\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2891 - val_loss: 1.2265\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3485 - val_loss: 0.4600\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2233 - val_loss: 0.9311\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2719 - val_loss: 0.1705\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3490 - val_loss: 0.1980\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4012 - val_loss: 0.4415\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4473 - val_loss: 0.8154\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2335 - val_loss: 0.4125\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2078 - val_loss: 0.3471\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2107 - val_loss: 0.4961\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.5114 - val_loss: 1.1856\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1342 - val_loss: 0.1094\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0937 - val_loss: 0.1260\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0989 - val_loss: 0.1465\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1439 - val_loss: 0.2752\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2257 - val_loss: 0.3444\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1247 - val_loss: 1.0209\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2833 - val_loss: 0.1965\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2381 - val_loss: 0.1949\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1571 - val_loss: 0.0687\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2564 - val_loss: 0.4515\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2663 - val_loss: 0.8058\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2510 - val_loss: 0.1667\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1594 - val_loss: 0.4919\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2653 - val_loss: 0.6497\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2613 - val_loss: 0.0862\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1836 - val_loss: 0.9599\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2252 - val_loss: 0.3639\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2747 - val_loss: 0.5132\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2028 - val_loss: 0.2160\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4044 - val_loss: 0.6678\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1623 - val_loss: 0.3047\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1307 - val_loss: 0.1381\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1994 - val_loss: 0.1431\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2273 - val_loss: 0.8680\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1375 - val_loss: 0.1952\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1607 - val_loss: 0.3152\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3279 - val_loss: 1.0931\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2470 - val_loss: 0.1759\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1202 - val_loss: 0.4656\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1609 - val_loss: 0.2971\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2747 - val_loss: 0.2509\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1598 - val_loss: 0.1375\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2268 - val_loss: 0.2531\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1711 - val_loss: 0.4803\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1742 - val_loss: 0.1756\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1865 - val_loss: 0.2666\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2172 - val_loss: 0.1382\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1110 - val_loss: 0.2396\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2960 - val_loss: 0.1871\n",
      "16/16 [==============================] - 1s 15ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.0687421719163868\n",
      "Mean Absolute Error (MAE): 0.19429974305919986\n",
      "Root Mean Squared Error (RMSE): 0.26218728404784775\n",
      "Time taken: 1027.5150108337402\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 30ms/step - loss: 997.8036 - val_loss: 649.5132\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 508.9756 - val_loss: 437.0154\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 337.6733 - val_loss: 271.0145\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 175.5118 - val_loss: 123.3195\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 76.6172 - val_loss: 47.5978\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 33.6884 - val_loss: 20.5635\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 17.1496 - val_loss: 10.4733\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 9.3882 - val_loss: 6.2481\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 6.9712 - val_loss: 9.2553\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 4.0529 - val_loss: 2.3125\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.9859 - val_loss: 3.0891\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.7039 - val_loss: 1.6679\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.2727 - val_loss: 1.4537\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.1826 - val_loss: 3.7939\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.8506 - val_loss: 1.3480\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.8188 - val_loss: 0.9086\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4249 - val_loss: 0.7324\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.7639 - val_loss: 1.5007\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.3901 - val_loss: 1.7450\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.2736 - val_loss: 1.0750\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.6072 - val_loss: 1.2630\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.6123 - val_loss: 0.5995\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0101 - val_loss: 0.6288\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4803 - val_loss: 1.1063\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1739 - val_loss: 1.0501\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7704 - val_loss: 1.0732\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8944 - val_loss: 1.0992\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7714 - val_loss: 1.2914\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8334 - val_loss: 1.1466\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8706 - val_loss: 3.4021\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8242 - val_loss: 0.6220\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6627 - val_loss: 0.5949\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7575 - val_loss: 0.9326\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9785 - val_loss: 1.0221\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9786 - val_loss: 1.5795\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7929 - val_loss: 0.5978\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4215 - val_loss: 0.5325\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5304 - val_loss: 0.5278\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5034 - val_loss: 0.8151\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 4.8789 - val_loss: 9.2006\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9941 - val_loss: 1.0697\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3153 - val_loss: 0.4920\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3452 - val_loss: 0.6053\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3473 - val_loss: 0.3088\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3663 - val_loss: 0.5246\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5610 - val_loss: 0.3901\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3232 - val_loss: 0.5432\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4835 - val_loss: 0.7283\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4770 - val_loss: 1.2064\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6072 - val_loss: 0.6516\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3924 - val_loss: 0.2569\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6166 - val_loss: 0.5408\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3260 - val_loss: 0.3444\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4032 - val_loss: 0.6262\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3000 - val_loss: 0.2923\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5478 - val_loss: 0.4796\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.7034 - val_loss: 0.5059\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3157 - val_loss: 0.8517\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2810 - val_loss: 0.2308\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3679 - val_loss: 0.7750\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5770 - val_loss: 0.3120\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2369 - val_loss: 0.3704\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3067 - val_loss: 0.8795\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2858 - val_loss: 0.8474\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7398 - val_loss: 0.6905\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3125 - val_loss: 0.4270\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2530 - val_loss: 0.4165\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3723 - val_loss: 0.3487\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2973 - val_loss: 0.7565\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3009 - val_loss: 0.1996\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3228 - val_loss: 0.1501\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2486 - val_loss: 0.8866\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3386 - val_loss: 0.3264\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3189 - val_loss: 0.7253\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3145 - val_loss: 0.7448\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4260 - val_loss: 0.6528\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2254 - val_loss: 0.2356\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2494 - val_loss: 0.2554\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3715 - val_loss: 0.2197\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2503 - val_loss: 0.1898\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2282 - val_loss: 1.0734\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2662 - val_loss: 0.9289\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3417 - val_loss: 0.2496\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2490 - val_loss: 0.4940\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2223 - val_loss: 0.1199\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1979 - val_loss: 0.9927\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2570 - val_loss: 0.1059\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.3444 - val_loss: 0.4920\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2056 - val_loss: 1.5682\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1759 - val_loss: 0.1752\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3215 - val_loss: 0.3492\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2965 - val_loss: 0.3215\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1737 - val_loss: 0.3017\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1924 - val_loss: 0.1384\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1388 - val_loss: 0.3007\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3261 - val_loss: 0.6499\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2192 - val_loss: 0.4324\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1807 - val_loss: 0.2023\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1235 - val_loss: 0.3437\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2573 - val_loss: 0.5282\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2115 - val_loss: 0.3253\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1855 - val_loss: 0.2422\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2036 - val_loss: 0.5189\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1913 - val_loss: 0.3022\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1398 - val_loss: 0.2884\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7260 - val_loss: 0.2491\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1148 - val_loss: 0.5912\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0843 - val_loss: 0.1080\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1044 - val_loss: 0.3199\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1756 - val_loss: 0.1695\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1175 - val_loss: 0.1420\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1570 - val_loss: 0.9822\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3856 - val_loss: 0.2899\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1173 - val_loss: 0.1300\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0832 - val_loss: 0.3158\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1245 - val_loss: 0.2366\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1651 - val_loss: 0.3631\n",
      "16/16 [==============================] - 1s 23ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.10588313209288903\n",
      "Mean Absolute Error (MAE): 0.24775494503103027\n",
      "Root Mean Squared Error (RMSE): 0.3253968839630906\n",
      "Time taken: 1027.7887456417084\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 13s 30ms/step - loss: 1104.5718 - val_loss: 820.5628\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 648.6977 - val_loss: 522.9218\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 413.1791 - val_loss: 311.9976\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 222.7375 - val_loss: 146.1971\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 100.8687 - val_loss: 60.8915\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 42.5373 - val_loss: 31.8059\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 19.1740 - val_loss: 11.6910\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 11.3187 - val_loss: 9.3442\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 7.5845 - val_loss: 7.1366\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 4.4934 - val_loss: 12.4707\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.0773 - val_loss: 1.8977\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.7434 - val_loss: 1.7247\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.3337 - val_loss: 10.5305\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.3033 - val_loss: 1.5414\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.9689 - val_loss: 1.0467\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.8382 - val_loss: 1.0108\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.2447 - val_loss: 1.6775\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0815 - val_loss: 1.8293\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.9029 - val_loss: 1.3198\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.5161 - val_loss: 2.8558\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1807 - val_loss: 0.5490\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6558 - val_loss: 1.5315\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0244 - val_loss: 0.7700\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8436 - val_loss: 2.1962\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1102 - val_loss: 0.5166\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9789 - val_loss: 2.2229\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6988 - val_loss: 1.2272\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.3083 - val_loss: 1.6581\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7651 - val_loss: 0.5076\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5229 - val_loss: 1.1421\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6104 - val_loss: 0.3441\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4941 - val_loss: 1.2213\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6218 - val_loss: 0.8345\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.1196 - val_loss: 0.5874\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6331 - val_loss: 0.5132\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7879 - val_loss: 0.6492\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4264 - val_loss: 0.4937\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0744 - val_loss: 13.9330\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7797 - val_loss: 0.2192\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3162 - val_loss: 0.4136\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3746 - val_loss: 0.4418\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8722 - val_loss: 0.4117\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7426 - val_loss: 0.4004\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4013 - val_loss: 0.4072\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3984 - val_loss: 1.2322\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4225 - val_loss: 0.3098\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3959 - val_loss: 1.6656\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9804 - val_loss: 1.5722\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4684 - val_loss: 0.4816\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3684 - val_loss: 0.7550\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3716 - val_loss: 0.3334\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4141 - val_loss: 0.2508\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3567 - val_loss: 0.7875\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3359 - val_loss: 0.2186\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.3085 - val_loss: 8.3334\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5792 - val_loss: 0.4150\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2614 - val_loss: 0.1344\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2008 - val_loss: 0.2969\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2323 - val_loss: 0.4247\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2710 - val_loss: 0.4228\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2512 - val_loss: 0.2274\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3723 - val_loss: 0.8497\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3036 - val_loss: 0.1163\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3690 - val_loss: 0.2528\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4276 - val_loss: 0.6316\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2524 - val_loss: 0.2294\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2772 - val_loss: 0.7707\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4290 - val_loss: 0.6472\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2777 - val_loss: 1.0144\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3141 - val_loss: 0.3121\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2539 - val_loss: 0.9949\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5129 - val_loss: 0.1675\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4210 - val_loss: 0.2919\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2679 - val_loss: 0.8509\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2510 - val_loss: 0.5401\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2910 - val_loss: 0.3807\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2566 - val_loss: 0.2987\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2569 - val_loss: 0.2846\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2733 - val_loss: 0.6374\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2719 - val_loss: 0.3170\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1804 - val_loss: 0.5608\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2288 - val_loss: 0.7668\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2833 - val_loss: 1.8411\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2818 - val_loss: 0.8354\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2779 - val_loss: 1.1808\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2692 - val_loss: 0.6454\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2387 - val_loss: 2.4226\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2696 - val_loss: 0.2787\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1778 - val_loss: 0.5154\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.3552 - val_loss: 1.2189\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2229 - val_loss: 0.1691\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0927 - val_loss: 0.1236\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1105 - val_loss: 0.1081\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1031 - val_loss: 0.2986\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1392 - val_loss: 0.2648\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1871 - val_loss: 0.1095\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1030 - val_loss: 0.2691\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1437 - val_loss: 0.2232\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3076 - val_loss: 0.4631\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1762 - val_loss: 0.3624\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1075 - val_loss: 0.2534\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3304 - val_loss: 0.6401\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2070 - val_loss: 0.2385\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1197 - val_loss: 0.1401\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1815 - val_loss: 0.5183\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1415 - val_loss: 0.5293\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2429 - val_loss: 0.5298\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2982 - val_loss: 0.3657\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2214 - val_loss: 0.0765\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1633 - val_loss: 0.8669\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2775 - val_loss: 0.1446\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1955 - val_loss: 0.0975\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0992 - val_loss: 0.1988\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1405 - val_loss: 0.1540\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1681 - val_loss: 0.2489\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4558 - val_loss: 0.4395\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1275 - val_loss: 0.0710\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0716 - val_loss: 0.1107\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1106 - val_loss: 0.2860\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1504 - val_loss: 0.2142\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2622 - val_loss: 2.1748\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2461 - val_loss: 0.1833\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0993 - val_loss: 0.1311\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1060 - val_loss: 0.1165\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1674 - val_loss: 0.1109\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0989 - val_loss: 0.6181\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1173 - val_loss: 0.1970\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1526 - val_loss: 0.5959\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1683 - val_loss: 0.2009\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1655 - val_loss: 0.2197\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1385 - val_loss: 0.2001\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1537 - val_loss: 0.5923\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1361 - val_loss: 0.2638\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.2081 - val_loss: 0.1504\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0826 - val_loss: 0.4308\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0833 - val_loss: 0.1531\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1794 - val_loss: 0.3905\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1626 - val_loss: 0.5719\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1887 - val_loss: 0.0874\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0960 - val_loss: 0.1164\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4532 - val_loss: 20.0753\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5366 - val_loss: 0.2135\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0796 - val_loss: 0.2303\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.0609 - val_loss: 0.0994\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0700 - val_loss: 0.0785\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1190 - val_loss: 0.1561\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.0929 - val_loss: 0.0753\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.0709631929517051\n",
      "Mean Absolute Error (MAE): 0.1900893502280929\n",
      "Root Mean Squared Error (RMSE): 0.26638917574050397\n",
      "Time taken: 1292.478069782257\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 14s 30ms/step - loss: 1025.2086 - val_loss: 762.2099\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 560.1056 - val_loss: 498.3395\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 390.6590 - val_loss: 362.0688\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 256.6887 - val_loss: 237.0400\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 140.6396 - val_loss: 102.5656\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 64.1083 - val_loss: 59.9952\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 28.7173 - val_loss: 37.4479\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 14.4766 - val_loss: 11.0667\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 10.2085 - val_loss: 29.5124\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 6.1862 - val_loss: 3.5993\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 4.4957 - val_loss: 6.7498\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 3.6741 - val_loss: 4.3004\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.3855 - val_loss: 2.0277\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.5787 - val_loss: 2.2474\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.1357 - val_loss: 2.3470\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4620 - val_loss: 2.7388\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 1.8436 - val_loss: 1.5914\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 1.6849 - val_loss: 2.3467\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.6204 - val_loss: 0.7507\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.4024 - val_loss: 0.7750\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.6577 - val_loss: 4.8463\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.9179 - val_loss: 0.7125\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0187 - val_loss: 1.0304\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7710 - val_loss: 0.6515\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6676 - val_loss: 1.6036\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8549 - val_loss: 0.6526\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9825 - val_loss: 1.8877\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0177 - val_loss: 1.0638\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0730 - val_loss: 2.6057\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.9411 - val_loss: 0.6646\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5580 - val_loss: 0.5231\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 2.2152 - val_loss: 1.2769\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5770 - val_loss: 0.3358\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4069 - val_loss: 0.6550\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.3345 - val_loss: 0.8426\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6887 - val_loss: 1.1933\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4001 - val_loss: 0.3800\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6093 - val_loss: 2.1390\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5639 - val_loss: 0.7514\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5532 - val_loss: 0.5735\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.2610 - val_loss: 1.1551\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5132 - val_loss: 0.5928\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4809 - val_loss: 0.4653\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6982 - val_loss: 2.9168\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8937 - val_loss: 0.4213\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.7692 - val_loss: 0.4825\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4008 - val_loss: 0.1861\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2894 - val_loss: 0.8079\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3449 - val_loss: 0.4376\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5062 - val_loss: 0.5259\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6561 - val_loss: 0.3376\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4372 - val_loss: 0.5053\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3048 - val_loss: 0.3725\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 1.0153 - val_loss: 0.6350\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4023 - val_loss: 0.8145\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5380 - val_loss: 2.0209\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5743 - val_loss: 0.2610\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2152 - val_loss: 0.3457\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2619 - val_loss: 0.2343\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3560 - val_loss: 0.3242\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.4898 - val_loss: 0.4630\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6165 - val_loss: 1.2295\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3962 - val_loss: 0.3402\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3080 - val_loss: 0.3533\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2377 - val_loss: 1.2187\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2368 - val_loss: 0.6910\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.6021 - val_loss: 0.3246\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3484 - val_loss: 0.1831\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2878 - val_loss: 0.2737\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5817 - val_loss: 0.9136\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2798 - val_loss: 0.1720\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2662 - val_loss: 0.3549\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2277 - val_loss: 0.2762\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5738 - val_loss: 0.2608\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1797 - val_loss: 0.3118\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2418 - val_loss: 0.3204\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2587 - val_loss: 0.3817\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.5084 - val_loss: 0.3939\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1953 - val_loss: 0.5737\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2216 - val_loss: 0.3481\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2270 - val_loss: 0.1623\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3239 - val_loss: 0.2261\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 9s 28ms/step - loss: 0.1930 - val_loss: 0.2278\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2412 - val_loss: 0.6479\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3349 - val_loss: 0.4196\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2055 - val_loss: 0.3644\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.8599 - val_loss: 0.9166\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2409 - val_loss: 0.1308\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1531 - val_loss: 0.1444\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1690 - val_loss: 0.6157\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1827 - val_loss: 0.2167\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2681 - val_loss: 0.4517\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2352 - val_loss: 0.3118\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2478 - val_loss: 0.0956\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1879 - val_loss: 1.2804\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1931 - val_loss: 0.7937\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1661 - val_loss: 0.3058\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3018 - val_loss: 0.2864\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1636 - val_loss: 0.1423\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1892 - val_loss: 0.3320\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2868 - val_loss: 0.8058\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2579 - val_loss: 0.2246\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1388 - val_loss: 0.2218\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1580 - val_loss: 0.4447\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.3021 - val_loss: 0.4135\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1819 - val_loss: 0.1595\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1454 - val_loss: 0.2501\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2278 - val_loss: 0.3837\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1768 - val_loss: 0.2423\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1209 - val_loss: 0.1721\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2011 - val_loss: 0.1125\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2637 - val_loss: 0.2307\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1178 - val_loss: 0.1714\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1817 - val_loss: 0.4318\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2160 - val_loss: 0.2142\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2370 - val_loss: 0.4950\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1720 - val_loss: 0.2576\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.2171 - val_loss: 0.1147\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1251 - val_loss: 0.4746\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1317 - val_loss: 0.2134\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1082 - val_loss: 0.1268\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1541 - val_loss: 0.2468\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1931 - val_loss: 0.4679\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 9s 27ms/step - loss: 0.1984 - val_loss: 0.1682\n",
      "16/16 [==============================] - 1s 15ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.09561586488448164\n",
      "Mean Absolute Error (MAE): 0.23127868788297376\n",
      "Root Mean Squared Error (RMSE): 0.30921815096219957\n",
      "Time taken: 1091.3348455429077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_3964\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE   Time taken\n",
      "0        1  0.058590  0.179003  0.242054  1027.148279\n",
      "1        2  0.068742  0.194300  0.262187  1027.515011\n",
      "2        3  0.105883  0.247755  0.325397  1027.788746\n",
      "3        4  0.070963  0.190089  0.266389  1292.478070\n",
      "4        5  0.095616  0.231279  0.309218  1091.334846\n",
      "5  Average  0.079959  0.208485  0.281049  1093.252990\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
