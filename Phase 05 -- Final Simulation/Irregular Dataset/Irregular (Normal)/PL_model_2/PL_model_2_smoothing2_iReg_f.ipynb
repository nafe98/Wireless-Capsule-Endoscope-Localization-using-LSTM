{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "             46         47  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>...</td>\n",
       "      <td>59.015999</td>\n",
       "      <td>62.518813</td>\n",
       "      <td>59.411256</td>\n",
       "      <td>60.758988</td>\n",
       "      <td>68.038102</td>\n",
       "      <td>72.988410</td>\n",
       "      <td>63.830242</td>\n",
       "      <td>75.252439</td>\n",
       "      <td>52.602491</td>\n",
       "      <td>67.851956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>...</td>\n",
       "      <td>59.062238</td>\n",
       "      <td>62.619356</td>\n",
       "      <td>59.705588</td>\n",
       "      <td>60.845566</td>\n",
       "      <td>67.996626</td>\n",
       "      <td>72.754005</td>\n",
       "      <td>63.917271</td>\n",
       "      <td>75.285079</td>\n",
       "      <td>52.570382</td>\n",
       "      <td>67.864368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>...</td>\n",
       "      <td>59.111119</td>\n",
       "      <td>62.720162</td>\n",
       "      <td>59.994576</td>\n",
       "      <td>60.937070</td>\n",
       "      <td>67.958247</td>\n",
       "      <td>72.523518</td>\n",
       "      <td>64.005781</td>\n",
       "      <td>75.315011</td>\n",
       "      <td>52.539130</td>\n",
       "      <td>67.878903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>...</td>\n",
       "      <td>59.162988</td>\n",
       "      <td>62.821366</td>\n",
       "      <td>60.277882</td>\n",
       "      <td>61.033224</td>\n",
       "      <td>67.922592</td>\n",
       "      <td>72.296890</td>\n",
       "      <td>64.095845</td>\n",
       "      <td>75.342087</td>\n",
       "      <td>52.508766</td>\n",
       "      <td>67.896058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>...</td>\n",
       "      <td>59.218087</td>\n",
       "      <td>62.922934</td>\n",
       "      <td>60.555414</td>\n",
       "      <td>61.133664</td>\n",
       "      <td>67.889440</td>\n",
       "      <td>72.073711</td>\n",
       "      <td>64.187436</td>\n",
       "      <td>75.366102</td>\n",
       "      <td>52.479200</td>\n",
       "      <td>67.916340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>66.743266</td>\n",
       "      <td>68.095627</td>\n",
       "      <td>59.788287</td>\n",
       "      <td>55.782259</td>\n",
       "      <td>73.302487</td>\n",
       "      <td>69.777199</td>\n",
       "      <td>70.634276</td>\n",
       "      <td>72.344860</td>\n",
       "      <td>66.105552</td>\n",
       "      <td>57.730447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>...</td>\n",
       "      <td>66.884332</td>\n",
       "      <td>68.147865</td>\n",
       "      <td>59.701153</td>\n",
       "      <td>55.875421</td>\n",
       "      <td>73.314650</td>\n",
       "      <td>69.681036</td>\n",
       "      <td>70.473344</td>\n",
       "      <td>72.306974</td>\n",
       "      <td>66.184077</td>\n",
       "      <td>57.778432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>...</td>\n",
       "      <td>67.027974</td>\n",
       "      <td>68.198538</td>\n",
       "      <td>59.614914</td>\n",
       "      <td>55.967148</td>\n",
       "      <td>73.321655</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>70.310319</td>\n",
       "      <td>72.267957</td>\n",
       "      <td>66.266954</td>\n",
       "      <td>57.829265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>...</td>\n",
       "      <td>67.173718</td>\n",
       "      <td>68.247988</td>\n",
       "      <td>59.530568</td>\n",
       "      <td>56.057411</td>\n",
       "      <td>73.323308</td>\n",
       "      <td>69.484466</td>\n",
       "      <td>70.145269</td>\n",
       "      <td>72.228032</td>\n",
       "      <td>66.353304</td>\n",
       "      <td>57.883165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>...</td>\n",
       "      <td>67.321086</td>\n",
       "      <td>68.296467</td>\n",
       "      <td>59.448892</td>\n",
       "      <td>56.146465</td>\n",
       "      <td>73.319703</td>\n",
       "      <td>69.384572</td>\n",
       "      <td>69.978262</td>\n",
       "      <td>72.187434</td>\n",
       "      <td>66.442219</td>\n",
       "      <td>57.940128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  ...  59.015999  62.518813   \n",
       "1     67.636949  77.055207  61.417464  68.656037  ...  59.062238  62.619356   \n",
       "2     67.468015  76.608876  61.529876  68.599884  ...  59.111119  62.720162   \n",
       "3     67.304084  76.171754  61.636534  68.548849  ...  59.162988  62.821366   \n",
       "4     67.145806  75.743710  61.738066  68.502746  ...  59.218087  62.922934   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  ...  66.743266  68.095627   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  ...  66.884332  68.147865   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  ...  67.027974  68.198538   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  ...  67.173718  68.247988   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  ...  67.321086  68.296467   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     59.411256  60.758988  68.038102  72.988410  63.830242  75.252439   \n",
       "1     59.705588  60.845566  67.996626  72.754005  63.917271  75.285079   \n",
       "2     59.994576  60.937070  67.958247  72.523518  64.005781  75.315011   \n",
       "3     60.277882  61.033224  67.922592  72.296890  64.095845  75.342087   \n",
       "4     60.555414  61.133664  67.889440  72.073711  64.187436  75.366102   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  59.788287  55.782259  73.302487  69.777199  70.634276  72.344860   \n",
       "2439  59.701153  55.875421  73.314650  69.681036  70.473344  72.306974   \n",
       "2440  59.614914  55.967148  73.321655  69.583333  70.310319  72.267957   \n",
       "2441  59.530568  56.057411  73.323308  69.484466  70.145269  72.228032   \n",
       "2442  59.448892  56.146465  73.319703  69.384572  69.978262  72.187434   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     52.602491  67.851956  \n",
       "1     52.570382  67.864368  \n",
       "2     52.539130  67.878903  \n",
       "3     52.508766  67.896058  \n",
       "4     52.479200  67.916340  \n",
       "...         ...        ...  \n",
       "2438  66.105552  57.730447  \n",
       "2439  66.184077  57.778432  \n",
       "2440  66.266954  57.829265  \n",
       "2441  66.353304  57.883165  \n",
       "2442  66.442219  57.940128  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.448072</td>\n",
       "      <td>71.866212</td>\n",
       "      <td>55.379099</td>\n",
       "      <td>67.169250</td>\n",
       "      <td>68.703894</td>\n",
       "      <td>73.546136</td>\n",
       "      <td>67.810228</td>\n",
       "      <td>77.510547</td>\n",
       "      <td>61.298868</td>\n",
       "      <td>68.717478</td>\n",
       "      <td>54.843534</td>\n",
       "      <td>67.566064</td>\n",
       "      <td>72.373162</td>\n",
       "      <td>72.463056</td>\n",
       "      <td>66.168056</td>\n",
       "      <td>69.098539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.418672</td>\n",
       "      <td>71.935271</td>\n",
       "      <td>55.344122</td>\n",
       "      <td>67.311666</td>\n",
       "      <td>68.862156</td>\n",
       "      <td>73.638498</td>\n",
       "      <td>67.636949</td>\n",
       "      <td>77.055207</td>\n",
       "      <td>61.417464</td>\n",
       "      <td>68.656037</td>\n",
       "      <td>54.631252</td>\n",
       "      <td>67.545145</td>\n",
       "      <td>72.247857</td>\n",
       "      <td>72.492057</td>\n",
       "      <td>66.099594</td>\n",
       "      <td>69.203420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.389637</td>\n",
       "      <td>72.007924</td>\n",
       "      <td>55.310861</td>\n",
       "      <td>67.452753</td>\n",
       "      <td>69.019299</td>\n",
       "      <td>73.733994</td>\n",
       "      <td>67.468015</td>\n",
       "      <td>76.608876</td>\n",
       "      <td>61.529876</td>\n",
       "      <td>68.599884</td>\n",
       "      <td>54.417764</td>\n",
       "      <td>67.520912</td>\n",
       "      <td>72.121019</td>\n",
       "      <td>72.520560</td>\n",
       "      <td>66.027539</td>\n",
       "      <td>69.304422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.360882</td>\n",
       "      <td>72.083804</td>\n",
       "      <td>55.280242</td>\n",
       "      <td>67.592834</td>\n",
       "      <td>69.175079</td>\n",
       "      <td>73.832377</td>\n",
       "      <td>67.304084</td>\n",
       "      <td>76.171754</td>\n",
       "      <td>61.636534</td>\n",
       "      <td>68.548849</td>\n",
       "      <td>54.202488</td>\n",
       "      <td>67.493660</td>\n",
       "      <td>71.992534</td>\n",
       "      <td>72.547992</td>\n",
       "      <td>65.951660</td>\n",
       "      <td>69.401670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.332575</td>\n",
       "      <td>72.162679</td>\n",
       "      <td>55.252883</td>\n",
       "      <td>67.732185</td>\n",
       "      <td>69.329214</td>\n",
       "      <td>73.933638</td>\n",
       "      <td>67.145806</td>\n",
       "      <td>75.743710</td>\n",
       "      <td>61.738066</td>\n",
       "      <td>68.502746</td>\n",
       "      <td>53.984751</td>\n",
       "      <td>67.463973</td>\n",
       "      <td>71.862293</td>\n",
       "      <td>72.573869</td>\n",
       "      <td>65.872039</td>\n",
       "      <td>69.495287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>71.350987</td>\n",
       "      <td>70.538258</td>\n",
       "      <td>68.544841</td>\n",
       "      <td>53.906213</td>\n",
       "      <td>73.763653</td>\n",
       "      <td>76.608353</td>\n",
       "      <td>69.085982</td>\n",
       "      <td>71.842437</td>\n",
       "      <td>70.823796</td>\n",
       "      <td>62.002797</td>\n",
       "      <td>60.229853</td>\n",
       "      <td>45.476810</td>\n",
       "      <td>75.157908</td>\n",
       "      <td>74.634189</td>\n",
       "      <td>71.548064</td>\n",
       "      <td>72.737987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>71.073659</td>\n",
       "      <td>70.368423</td>\n",
       "      <td>68.696703</td>\n",
       "      <td>53.930584</td>\n",
       "      <td>73.653961</td>\n",
       "      <td>76.505011</td>\n",
       "      <td>68.987498</td>\n",
       "      <td>71.977737</td>\n",
       "      <td>71.021247</td>\n",
       "      <td>61.899134</td>\n",
       "      <td>60.264440</td>\n",
       "      <td>45.468933</td>\n",
       "      <td>75.240427</td>\n",
       "      <td>74.640917</td>\n",
       "      <td>71.707523</td>\n",
       "      <td>72.865926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>70.796726</td>\n",
       "      <td>70.197133</td>\n",
       "      <td>68.850197</td>\n",
       "      <td>53.955863</td>\n",
       "      <td>73.544931</td>\n",
       "      <td>76.398826</td>\n",
       "      <td>68.891677</td>\n",
       "      <td>72.111296</td>\n",
       "      <td>71.217271</td>\n",
       "      <td>61.795862</td>\n",
       "      <td>60.300991</td>\n",
       "      <td>45.465837</td>\n",
       "      <td>75.326575</td>\n",
       "      <td>74.648854</td>\n",
       "      <td>71.873750</td>\n",
       "      <td>72.992255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>70.520438</td>\n",
       "      <td>70.024321</td>\n",
       "      <td>69.005341</td>\n",
       "      <td>53.982231</td>\n",
       "      <td>73.436429</td>\n",
       "      <td>76.289524</td>\n",
       "      <td>68.798594</td>\n",
       "      <td>72.242943</td>\n",
       "      <td>71.411729</td>\n",
       "      <td>61.693438</td>\n",
       "      <td>60.339244</td>\n",
       "      <td>45.467099</td>\n",
       "      <td>75.416016</td>\n",
       "      <td>74.658202</td>\n",
       "      <td>72.046393</td>\n",
       "      <td>73.117147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>70.245255</td>\n",
       "      <td>69.849882</td>\n",
       "      <td>69.162132</td>\n",
       "      <td>54.009588</td>\n",
       "      <td>73.328562</td>\n",
       "      <td>76.177104</td>\n",
       "      <td>68.708527</td>\n",
       "      <td>72.372422</td>\n",
       "      <td>71.604788</td>\n",
       "      <td>61.592348</td>\n",
       "      <td>60.378959</td>\n",
       "      <td>45.472050</td>\n",
       "      <td>75.508297</td>\n",
       "      <td>74.668878</td>\n",
       "      <td>72.224964</td>\n",
       "      <td>73.240962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     69.448072  71.866212  55.379099  67.169250  68.703894  73.546136   \n",
       "1     69.418672  71.935271  55.344122  67.311666  68.862156  73.638498   \n",
       "2     69.389637  72.007924  55.310861  67.452753  69.019299  73.733994   \n",
       "3     69.360882  72.083804  55.280242  67.592834  69.175079  73.832377   \n",
       "4     69.332575  72.162679  55.252883  67.732185  69.329214  73.933638   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  71.350987  70.538258  68.544841  53.906213  73.763653  76.608353   \n",
       "2439  71.073659  70.368423  68.696703  53.930584  73.653961  76.505011   \n",
       "2440  70.796726  70.197133  68.850197  53.955863  73.544931  76.398826   \n",
       "2441  70.520438  70.024321  69.005341  53.982231  73.436429  76.289524   \n",
       "2442  70.245255  69.849882  69.162132  54.009588  73.328562  76.177104   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     67.810228  77.510547  61.298868  68.717478  54.843534  67.566064   \n",
       "1     67.636949  77.055207  61.417464  68.656037  54.631252  67.545145   \n",
       "2     67.468015  76.608876  61.529876  68.599884  54.417764  67.520912   \n",
       "3     67.304084  76.171754  61.636534  68.548849  54.202488  67.493660   \n",
       "4     67.145806  75.743710  61.738066  68.502746  53.984751  67.463973   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  69.085982  71.842437  70.823796  62.002797  60.229853  45.476810   \n",
       "2439  68.987498  71.977737  71.021247  61.899134  60.264440  45.468933   \n",
       "2440  68.891677  72.111296  71.217271  61.795862  60.300991  45.465837   \n",
       "2441  68.798594  72.242943  71.411729  61.693438  60.339244  45.467099   \n",
       "2442  68.708527  72.372422  71.604788  61.592348  60.378959  45.472050   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     72.373162  72.463056  66.168056  69.098539  \n",
       "1     72.247857  72.492057  66.099594  69.203420  \n",
       "2     72.121019  72.520560  66.027539  69.304422  \n",
       "3     71.992534  72.547992  65.951660  69.401670  \n",
       "4     71.862293  72.573869  65.872039  69.495287  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  75.157908  74.634189  71.548064  72.737987  \n",
       "2439  75.240427  74.640917  71.707523  72.865926  \n",
       "2440  75.326575  74.648854  71.873750  72.992255  \n",
       "2441  75.416016  74.658202  72.046393  73.117147  \n",
       "2442  75.508297  74.668878  72.224964  73.240962  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1105.2192 - val_loss: 929.9451\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 685.9081 - val_loss: 560.1006\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 464.1588 - val_loss: 721.5952\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 344.0559 - val_loss: 430.7983\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 273.3934 - val_loss: 254.0827\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 170.8501 - val_loss: 178.8307\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 83.4073 - val_loss: 63.5084\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 46.0344 - val_loss: 62.4052\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 29.1684 - val_loss: 29.0643\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 18.5166 - val_loss: 37.4593\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.8087 - val_loss: 23.4844\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.7880 - val_loss: 40.1165\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.5232 - val_loss: 16.4362\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.1269 - val_loss: 20.9687\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3566 - val_loss: 8.7603\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 5.2891 - val_loss: 8.3111\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1529 - val_loss: 6.5616\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6916 - val_loss: 2.8223\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0432 - val_loss: 19.0778\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.4895 - val_loss: 8.2572\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1475 - val_loss: 7.6101\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5420 - val_loss: 9.9016\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.7407 - val_loss: 20.6548\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3136 - val_loss: 2.6241\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6818 - val_loss: 23.0023\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0471 - val_loss: 2.8265\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.5564 - val_loss: 1.2734\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2369 - val_loss: 13.9421\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7148 - val_loss: 1.4085\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1951 - val_loss: 2.5418\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.0411 - val_loss: 2.6500\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4290 - val_loss: 1.3414\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3727 - val_loss: 0.7268\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8210 - val_loss: 1.9743\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9219 - val_loss: 1.8205\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1245 - val_loss: 4.1875\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0986 - val_loss: 1.4652\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5808 - val_loss: 87.6899\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4367 - val_loss: 0.8272\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6493 - val_loss: 0.8593\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1033 - val_loss: 0.7482\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3863 - val_loss: 1.3496\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5949 - val_loss: 0.8650\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5869 - val_loss: 0.7466\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5848 - val_loss: 1.8775\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7493 - val_loss: 2.8550\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5531 - val_loss: 0.8544\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.3920 - val_loss: 2.6166\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6044 - val_loss: 0.5941\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3985 - val_loss: 0.4431\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3608 - val_loss: 0.5184\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4755 - val_loss: 17.5913\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4536 - val_loss: 0.6236\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4328 - val_loss: 0.7485\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8484 - val_loss: 1.2923\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5463 - val_loss: 0.5652\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5273 - val_loss: 0.8206\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4670 - val_loss: 0.7287\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4333 - val_loss: 0.7677\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6971 - val_loss: 2.5673\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5345 - val_loss: 4.7339\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6340 - val_loss: 45.0710\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1785 - val_loss: 0.8254\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4243 - val_loss: 1.5992\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3400 - val_loss: 0.2671\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3923 - val_loss: 0.7902\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3116 - val_loss: 0.6601\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2467 - val_loss: 0.3501\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7588 - val_loss: 0.6459\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3820 - val_loss: 0.6319\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3530 - val_loss: 1.8423\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5684 - val_loss: 42.5513\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7406 - val_loss: 0.4940\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2364 - val_loss: 0.2665\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3129 - val_loss: 1.1853\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1973 - val_loss: 0.2129\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2012 - val_loss: 0.2171\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1941 - val_loss: 0.2892\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2269 - val_loss: 0.9663\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3344 - val_loss: 0.7919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3515 - val_loss: 1.3202\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.6445 - val_loss: 13.0940\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7378 - val_loss: 0.3485\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2101 - val_loss: 0.2438\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1475 - val_loss: 0.1272\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1469 - val_loss: 0.1567\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2284 - val_loss: 0.3820\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1967 - val_loss: 0.2222\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2184 - val_loss: 0.4940\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2864 - val_loss: 0.4003\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2231 - val_loss: 0.8435\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3960 - val_loss: 0.8092\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3022 - val_loss: 0.4878\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2888 - val_loss: 0.3241\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3199 - val_loss: 0.4053\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2698 - val_loss: 0.4045\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2969 - val_loss: 1.1667\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3195 - val_loss: 0.3838\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1040 - val_loss: 3.0938\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4998 - val_loss: 0.2148\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1190 - val_loss: 0.2591\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1222 - val_loss: 0.1178\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1255 - val_loss: 0.1839\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1849 - val_loss: 0.1908\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1759 - val_loss: 0.1996\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2053 - val_loss: 0.5041\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1751 - val_loss: 0.3643\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2406 - val_loss: 0.4838\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2645 - val_loss: 1.2078\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4215 - val_loss: 0.9633\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2110 - val_loss: 0.2480\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2058 - val_loss: 0.3671\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2041 - val_loss: 0.4744\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8787 - val_loss: 0.1999\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1204 - val_loss: 0.1464\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0951 - val_loss: 0.2119\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0866 - val_loss: 0.0873\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1026 - val_loss: 0.4139\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1224 - val_loss: 0.1942\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2259 - val_loss: 3.7109\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2553 - val_loss: 0.3237\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1630 - val_loss: 0.6999\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1538 - val_loss: 0.2726\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1643 - val_loss: 0.5017\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2038 - val_loss: 0.5262\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2481 - val_loss: 0.4489\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2036 - val_loss: 0.6087\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2699 - val_loss: 0.2408\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1651 - val_loss: 0.5934\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1675 - val_loss: 0.6930\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3119 - val_loss: 0.2771\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1413 - val_loss: 0.2951\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1501 - val_loss: 0.7987\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2762 - val_loss: 0.3825\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1675 - val_loss: 0.9860\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1702 - val_loss: 0.4603\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1632 - val_loss: 0.7350\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2371 - val_loss: 1.2897\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1881 - val_loss: 0.3071\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1616 - val_loss: 0.5972\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1966 - val_loss: 0.6155\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1642 - val_loss: 0.1983\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1523 - val_loss: 0.2904\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1526 - val_loss: 0.1953\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2443 - val_loss: 0.4729\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1528 - val_loss: 0.2726\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1684 - val_loss: 0.2119\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.08731108298887875\n",
      "Mean Absolute Error (MAE): 0.2193069875771201\n",
      "Root Mean Squared Error (RMSE): 0.29548448857576054\n",
      "Time taken: 428.3897194862366\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1067.1251 - val_loss: 925.5745\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 707.2679 - val_loss: 629.9742\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 483.8291 - val_loss: 529.5242\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 333.2612 - val_loss: 427.2628\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 238.8364 - val_loss: 225.5214\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 138.0764 - val_loss: 179.5485\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 69.2618 - val_loss: 56.4768\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 38.6402 - val_loss: 40.1320\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.2426 - val_loss: 22.3351\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.8145 - val_loss: 39.1288\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.4650 - val_loss: 24.4754\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.7327 - val_loss: 16.0648\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0423 - val_loss: 9.6799\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.1892 - val_loss: 7.9210\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0041 - val_loss: 8.4842\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3699 - val_loss: 7.7134\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6009 - val_loss: 12.6974\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4419 - val_loss: 6.9654\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0465 - val_loss: 16.1559\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6655 - val_loss: 8.9730\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3506 - val_loss: 28.4708\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0864 - val_loss: 3.3870\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3758 - val_loss: 1.3138\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2744 - val_loss: 12.5519\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3713 - val_loss: 4.3957\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9886 - val_loss: 29.3548\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9912 - val_loss: 5.7220\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4280 - val_loss: 2.3875\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2145 - val_loss: 1.1546\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9421 - val_loss: 1.0417\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3006 - val_loss: 44.8312\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.9152 - val_loss: 1.1738\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1528 - val_loss: 1.1471\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4340 - val_loss: 7.1071\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2063 - val_loss: 0.6747\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7344 - val_loss: 0.8806\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6196 - val_loss: 1.5234\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9000 - val_loss: 0.6211\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1120 - val_loss: 0.6405\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8966 - val_loss: 1.5351\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3415 - val_loss: 1.3382\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6781 - val_loss: 0.7413\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9820 - val_loss: 7.1461\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0039 - val_loss: 2.4567\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8306 - val_loss: 0.6951\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6434 - val_loss: 1.0456\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6798 - val_loss: 4.7453\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.7425 - val_loss: 9.8706\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7818 - val_loss: 38.7292\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7537 - val_loss: 0.2798\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3824 - val_loss: 1.2269\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4247 - val_loss: 0.5425\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4083 - val_loss: 0.6188\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6235 - val_loss: 0.7440\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7020 - val_loss: 1.0207\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5482 - val_loss: 1.1682\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7500 - val_loss: 2.8346\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6632 - val_loss: 0.9221\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3636 - val_loss: 0.7434\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5316 - val_loss: 138.8802\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8067 - val_loss: 0.3414\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2774 - val_loss: 0.1547\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3061 - val_loss: 0.1549\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2733 - val_loss: 1.7949\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3253 - val_loss: 0.2545\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2602 - val_loss: 0.2707\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3950 - val_loss: 2.7488\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6762 - val_loss: 0.7996\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7195 - val_loss: 2.6898\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7197 - val_loss: 82.9998\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5234 - val_loss: 0.2714\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2594 - val_loss: 0.2148\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2436 - val_loss: 0.1883\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2389 - val_loss: 0.8798\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2104 - val_loss: 0.4719\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3411 - val_loss: 0.3525\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2611 - val_loss: 0.6859\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2939 - val_loss: 5.4262\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6433 - val_loss: 0.3037\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2091 - val_loss: 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2034 - val_loss: 0.3463\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2083 - val_loss: 0.4179\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2378 - val_loss: 0.2528\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2472 - val_loss: 0.6865\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5675 - val_loss: 0.5381\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2540 - val_loss: 0.4962\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3658 - val_loss: 0.4496\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2993 - val_loss: 0.6596\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5243 - val_loss: 181.7326\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3785 - val_loss: 0.1870\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1814 - val_loss: 0.2076\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1275 - val_loss: 0.1741\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.15465732502560134\n",
      "Mean Absolute Error (MAE): 0.30004701901027436\n",
      "Root Mean Squared Error (RMSE): 0.3932649552472243\n",
      "Time taken: 277.51717877388\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1083.7958 - val_loss: 874.1879\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 761.0789 - val_loss: 621.8890\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 524.9650 - val_loss: 459.4950\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 363.8624 - val_loss: 324.5111\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 289.1465 - val_loss: 299.7172\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 190.4539 - val_loss: 131.3616\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 99.3431 - val_loss: 105.0373\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 50.1733 - val_loss: 31.7595\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 31.3877 - val_loss: 26.6402\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 20.6415 - val_loss: 33.4390\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.8426 - val_loss: 16.3305\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 12.7805 - val_loss: 24.1830\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.7228 - val_loss: 7.1586\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.8256 - val_loss: 5.8958\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9862 - val_loss: 7.8020\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.1091 - val_loss: 8.5051\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.2493 - val_loss: 7.9579\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4582 - val_loss: 16.0623\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4082 - val_loss: 11.4725\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4969 - val_loss: 3.3272\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9853 - val_loss: 7.3262\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0075 - val_loss: 3.1975\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3014 - val_loss: 1.6435\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8734 - val_loss: 26.0149\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4277 - val_loss: 25.4069\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5381 - val_loss: 2.0891\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.7454 - val_loss: 13.8191\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9919 - val_loss: 1.4753\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5873 - val_loss: 2.2610\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2059 - val_loss: 20.4773\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3342 - val_loss: 1.7781\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1520 - val_loss: 2.3375\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1533 - val_loss: 4.4362\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7640 - val_loss: 1.3845\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3390 - val_loss: 3.5016\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0594 - val_loss: 1.2359\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7105 - val_loss: 0.7418\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8729 - val_loss: 1.6927\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8660 - val_loss: 1.5592\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1871 - val_loss: 31.1935\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8827 - val_loss: 1.3218\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6031 - val_loss: 1.3661\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6723 - val_loss: 1.1671\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7478 - val_loss: 1.2507\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8026 - val_loss: 1.0066\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7124 - val_loss: 1.2722\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1141 - val_loss: 40.9690\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9268 - val_loss: 2.1389\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7188 - val_loss: 0.3583\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4735 - val_loss: 0.2771\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4167 - val_loss: 0.7114\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5118 - val_loss: 0.7247\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5222 - val_loss: 1.2718\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5781 - val_loss: 1.8790\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4995 - val_loss: 3.5145\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7426 - val_loss: 2.7618\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.0474 - val_loss: 1.9941\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7637 - val_loss: 1.3147\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4270 - val_loss: 2.4994\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3484 - val_loss: 0.7434\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3146 - val_loss: 0.6195\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4396 - val_loss: 0.6614\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6004 - val_loss: 1.3120\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8066 - val_loss: 0.5563\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3955 - val_loss: 0.8060\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7068 - val_loss: 1.3330\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3697 - val_loss: 0.6163\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5273 - val_loss: 0.8355\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4887 - val_loss: 0.6384\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3702 - val_loss: 1.7288\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4321 - val_loss: 0.5702\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3076 - val_loss: 0.7660\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3615 - val_loss: 1.2478\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5595 - val_loss: 1.1345\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.7856 - val_loss: 3.1650\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3888 - val_loss: 0.2191\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2268 - val_loss: 0.7273\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2394 - val_loss: 0.4275\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1886 - val_loss: 0.1859\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1711 - val_loss: 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2372 - val_loss: 0.3023\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1950 - val_loss: 0.4443\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1918 - val_loss: 0.4315\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2737 - val_loss: 0.4967\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3848 - val_loss: 0.3226\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2563 - val_loss: 0.3133\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3916 - val_loss: 0.5827\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3732 - val_loss: 0.6198\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2238 - val_loss: 1.0745\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5548 - val_loss: 212.5645\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9034 - val_loss: 1.2542\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5152 - val_loss: 0.2612\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1630 - val_loss: 0.2271\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1505 - val_loss: 0.2018\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1928 - val_loss: 0.3125\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2007 - val_loss: 0.1739\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1632 - val_loss: 0.2250\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1923 - val_loss: 0.7119\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3448 - val_loss: 0.5095\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2379 - val_loss: 0.6463\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3459 - val_loss: 1.1886\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3774 - val_loss: 1.7677\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4483 - val_loss: 0.3099\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2354 - val_loss: 0.7478\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2440 - val_loss: 0.3476\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1935 - val_loss: 0.6163\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2876 - val_loss: 0.2830\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2056 - val_loss: 0.4949\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2842 - val_loss: 1.8505\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2375 - val_loss: 0.3103\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3520 - val_loss: 0.6576\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3430 - val_loss: 144.1392\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7587 - val_loss: 0.1373\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1442 - val_loss: 0.1382\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1255 - val_loss: 0.2038\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1002 - val_loss: 0.1205\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1205 - val_loss: 0.1496\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1556 - val_loss: 0.1654\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1276 - val_loss: 0.2156\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4124 - val_loss: 3.3265\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3260 - val_loss: 0.1981\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1179 - val_loss: 0.3009\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1595 - val_loss: 0.2234\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1559 - val_loss: 0.3237\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1959 - val_loss: 0.5968\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2284 - val_loss: 0.1915\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3320 - val_loss: 0.8584\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2017 - val_loss: 0.2556\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1543 - val_loss: 0.1368\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2088 - val_loss: 0.5596\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1857 - val_loss: 0.6557\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1707 - val_loss: 0.4003\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1824 - val_loss: 0.5553\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2392 - val_loss: 0.4213\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2839 - val_loss: 0.9702\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1508 - val_loss: 0.3573\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1790 - val_loss: 0.1969\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1624 - val_loss: 0.2237\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1517 - val_loss: 0.2513\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2322 - val_loss: 0.7421\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1804 - val_loss: 0.5862\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2345 - val_loss: 0.8374\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1495 - val_loss: 0.3738\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1656 - val_loss: 0.6696\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2004 - val_loss: 1.3330\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1485 - val_loss: 0.1588\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.12046059293809946\n",
      "Mean Absolute Error (MAE): 0.2629864214585969\n",
      "Root Mean Squared Error (RMSE): 0.34707433344760524\n",
      "Time taken: 436.1159255504608\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 15ms/step - loss: 1076.5623 - val_loss: 830.9523\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 604.8747 - val_loss: 495.9169\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 417.5901 - val_loss: 413.5314\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 319.2979 - val_loss: 315.3703\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 284.6122 - val_loss: 291.3010\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 236.8709 - val_loss: 258.8045\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 128.0781 - val_loss: 109.9954\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 61.7257 - val_loss: 51.7351\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 33.5829 - val_loss: 60.3609\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 20.6236 - val_loss: 16.6777\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.1646 - val_loss: 18.5304\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.4705 - val_loss: 44.8000\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4519 - val_loss: 12.1280\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.1426 - val_loss: 11.9619\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.8776 - val_loss: 11.9437\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.6746 - val_loss: 5.6163\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4331 - val_loss: 5.0720\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.1394 - val_loss: 4.4127\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4615 - val_loss: 2.6474\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5072 - val_loss: 9.1290\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.5114 - val_loss: 10.1321\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.8570 - val_loss: 6.4693\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9409 - val_loss: 2.4794\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5638 - val_loss: 9.7990\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7061 - val_loss: 10.7872\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7004 - val_loss: 3.4339\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7934 - val_loss: 1.6394\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2663 - val_loss: 2.4799\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2932 - val_loss: 1.8122\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2023 - val_loss: 5.7391\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7568 - val_loss: 3.4676\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8679 - val_loss: 24.2387\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8846 - val_loss: 2.1320\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1392 - val_loss: 1.1825\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8939 - val_loss: 1.8272\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9295 - val_loss: 2.3024\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2611 - val_loss: 2.8269\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8939 - val_loss: 1.1954\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9660 - val_loss: 39.7904\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8489 - val_loss: 0.6355\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6650 - val_loss: 1.0122\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6754 - val_loss: 1.0830\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8005 - val_loss: 1.3821\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5694 - val_loss: 0.9161\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1297 - val_loss: 1.3682\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7526 - val_loss: 0.4313\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6891 - val_loss: 1.2196\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5752 - val_loss: 2.4367\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5775 - val_loss: 1.6810\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6272 - val_loss: 0.8015\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6181 - val_loss: 0.6857\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8140 - val_loss: 1.6192\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6947 - val_loss: 2.7080\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6889 - val_loss: 0.4945\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5795 - val_loss: 1.4053\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8667 - val_loss: 0.9835\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4979 - val_loss: 0.5699\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6509 - val_loss: 42.8585\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5539 - val_loss: 1.0436\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4164 - val_loss: 0.7615\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3592 - val_loss: 1.5984\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3332 - val_loss: 0.2523\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3284 - val_loss: 0.9587\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6151 - val_loss: 0.4668\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3330 - val_loss: 0.5615\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3536 - val_loss: 0.2848\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3295 - val_loss: 0.8683\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3924 - val_loss: 0.3106\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7695 - val_loss: 0.8803\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4841 - val_loss: 0.3374\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2951 - val_loss: 0.6567\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2487 - val_loss: 0.4323\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4341 - val_loss: 3.7518\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9829 - val_loss: 0.3771\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2964 - val_loss: 0.3577\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5778 - val_loss: 0.8742\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3179 - val_loss: 0.4840\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2965 - val_loss: 0.3238\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.7522 - val_loss: 4.4703\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4801 - val_loss: 0.2371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2071 - val_loss: 0.2964\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1696 - val_loss: 0.2383\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1578 - val_loss: 0.1622\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1716 - val_loss: 0.2415\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2053 - val_loss: 0.3513\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2205 - val_loss: 0.6289\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7163 - val_loss: 7.1942\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5445 - val_loss: 0.2745\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1412 - val_loss: 0.1434\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1658 - val_loss: 0.3762\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2836 - val_loss: 0.4044\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2761 - val_loss: 0.2301\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4335 - val_loss: 9.3497\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2910 - val_loss: 0.3264\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2291 - val_loss: 0.1883\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1744 - val_loss: 0.2105\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1995 - val_loss: 0.1442\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1429 - val_loss: 0.1350\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1517 - val_loss: 0.1423\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1797 - val_loss: 0.2467\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2047 - val_loss: 0.7066\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3081 - val_loss: 0.2967\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2365 - val_loss: 0.3556\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2898 - val_loss: 0.4064\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2540 - val_loss: 0.3740\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2398 - val_loss: 0.6233\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2674 - val_loss: 0.1909\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1963 - val_loss: 1.2936\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3660 - val_loss: 2.1260\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3217 - val_loss: 0.2667\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2068 - val_loss: 0.3990\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3028 - val_loss: 0.4336\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2812 - val_loss: 1.3716\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3492 - val_loss: 8.8887\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3256 - val_loss: 0.8247\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1682 - val_loss: 0.2313\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2213 - val_loss: 0.6048\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2731 - val_loss: 0.4512\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1278 - val_loss: 0.1157\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0919 - val_loss: 0.2569\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1218 - val_loss: 0.1720\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1679 - val_loss: 0.2817\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2081 - val_loss: 0.2691\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1205 - val_loss: 0.1858\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5222 - val_loss: 2.5627\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2912 - val_loss: 0.5653\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1365 - val_loss: 0.2346\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1458 - val_loss: 0.2189\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1598 - val_loss: 0.3657\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1773 - val_loss: 0.2614\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2226 - val_loss: 0.4636\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2888 - val_loss: 0.3437\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1688 - val_loss: 0.3573\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1765 - val_loss: 0.6966\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1554 - val_loss: 0.7680\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2845 - val_loss: 0.4239\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1917 - val_loss: 0.2356\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1887 - val_loss: 0.6017\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2427 - val_loss: 0.7324\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1761 - val_loss: 0.2939\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2140 - val_loss: 0.1850\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1731 - val_loss: 0.6344\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1623 - val_loss: 0.3428\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0459 - val_loss: 5.2654\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3833 - val_loss: 0.0861\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0913 - val_loss: 0.0795\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0760 - val_loss: 0.0425\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0673 - val_loss: 0.0780\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0737 - val_loss: 0.0685\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0792 - val_loss: 0.0987\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1034 - val_loss: 0.1082\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0815 - val_loss: 0.1530\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1345 - val_loss: 0.5764\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1792 - val_loss: 0.1621\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1191 - val_loss: 0.3555\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2084 - val_loss: 0.6445\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1497 - val_loss: 0.3120\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1150 - val_loss: 0.1810\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1512 - val_loss: 0.2834\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1863 - val_loss: 0.3765\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1534 - val_loss: 1.9531\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1700 - val_loss: 0.3039\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1533 - val_loss: 0.2607\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2079 - val_loss: 0.2501\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1329 - val_loss: 0.3154\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1872 - val_loss: 1.5768\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1737 - val_loss: 0.8577\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1593 - val_loss: 0.4503\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1293 - val_loss: 0.1330\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1479 - val_loss: 0.1761\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1286 - val_loss: 0.4207\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1210 - val_loss: 0.5877\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 5s 16ms/step - loss: 0.2624 - val_loss: 0.8902\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 6s 17ms/step - loss: 0.1529 - val_loss: 0.1820\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1267 - val_loss: 0.4349\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1600 - val_loss: 0.2231\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 5s 17ms/step - loss: 0.1269 - val_loss: 0.2661\n",
      "16/16 [==============================] - 1s 8ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.04244904496393959\n",
      "Mean Absolute Error (MAE): 0.1557761002805669\n",
      "Root Mean Squared Error (RMSE): 0.2060316601009165\n",
      "Time taken: 558.0144493579865\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 9s 16ms/step - loss: 1089.6848 - val_loss: 803.5135\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 615.0992 - val_loss: 499.7967\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 424.5773 - val_loss: 342.0313\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 318.9599 - val_loss: 297.7652\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 245.6221 - val_loss: 211.3987\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 144.4740 - val_loss: 108.2286\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 82.0548 - val_loss: 55.5408\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 45.8136 - val_loss: 49.1924\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 28.2678 - val_loss: 24.4491\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 18.9857 - val_loss: 17.4435\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 13.7798 - val_loss: 12.0450\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 12.2633 - val_loss: 40.2890\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 9.6959 - val_loss: 6.7707\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.4466 - val_loss: 12.6673\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.4991 - val_loss: 21.3106\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.8165 - val_loss: 31.5005\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.2691 - val_loss: 20.7629\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.6235 - val_loss: 5.8461\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.3448 - val_loss: 5.2634\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.2900 - val_loss: 4.5391\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.6181 - val_loss: 6.4370\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.3352 - val_loss: 77.4987\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.9365 - val_loss: 2.6000\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.0873 - val_loss: 2.1748\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.0594 - val_loss: 4.8085\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6542 - val_loss: 2.4085\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.4634 - val_loss: 2.2081\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.3940 - val_loss: 137.7462\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7922 - val_loss: 1.9736\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4087 - val_loss: 2.7575\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4417 - val_loss: 1.8372\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.4424 - val_loss: 12.5293\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.7860 - val_loss: 14.7447\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.8946 - val_loss: 1.6157\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3121 - val_loss: 1.4556\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.5735 - val_loss: 7.0034\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9442 - val_loss: 0.7426\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7507 - val_loss: 0.9831\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8100 - val_loss: 0.8058\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7627 - val_loss: 1.3764\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8462 - val_loss: 3.9748\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8570 - val_loss: 0.8241\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7720 - val_loss: 0.7053\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9331 - val_loss: 1.1871\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6179 - val_loss: 0.6933\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.5113 - val_loss: 1.2646\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1642 - val_loss: 0.7751\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5274 - val_loss: 0.7991\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5061 - val_loss: 0.4826\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5237 - val_loss: 0.7287\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6557 - val_loss: 1.9485\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6884 - val_loss: 1.4719\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5174 - val_loss: 0.8066\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9944 - val_loss: 4.6871\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9224 - val_loss: 2.3132\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6218 - val_loss: 0.6729\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5380 - val_loss: 1.5611\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7928 - val_loss: 0.6707\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3887 - val_loss: 0.6671\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4610 - val_loss: 0.7903\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6254 - val_loss: 1.2283\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7076 - val_loss: 0.6291\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.8854 - val_loss: 39.8656\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.0387 - val_loss: 2.8743\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5305 - val_loss: 0.7166\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7931 - val_loss: 0.7216\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3584 - val_loss: 0.3743\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.2995 - val_loss: 0.4199\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2959 - val_loss: 0.3812\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2704 - val_loss: 0.5236\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3858 - val_loss: 0.3921\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3744 - val_loss: 0.7088\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4428 - val_loss: 1.2323\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4262 - val_loss: 2.3844\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.4868 - val_loss: 2.1314\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4217 - val_loss: 0.1982\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3353 - val_loss: 0.3761\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2440 - val_loss: 0.3078\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3086 - val_loss: 0.5945\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3097 - val_loss: 3.1974\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4872 - val_loss: 0.9612\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 2.6177 - val_loss: 5.4928\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5163 - val_loss: 0.2324\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4005 - val_loss: 0.2608\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1812 - val_loss: 0.2172\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2269 - val_loss: 0.3486\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2202 - val_loss: 0.3383\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2535 - val_loss: 0.2951\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2226 - val_loss: 0.7735\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2540 - val_loss: 0.5079\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3561 - val_loss: 0.6878\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3254 - val_loss: 0.5692\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2967 - val_loss: 0.8825\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3129 - val_loss: 0.6727\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3505 - val_loss: 0.5772\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3368 - val_loss: 0.9898\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2834 - val_loss: 0.6114\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 3.1968 - val_loss: 21.5388\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3419 - val_loss: 0.1821\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1548 - val_loss: 0.2106\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1301 - val_loss: 0.2061\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1379 - val_loss: 0.1739\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1385 - val_loss: 0.1990\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1859 - val_loss: 0.2955\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1650 - val_loss: 0.2945\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3258 - val_loss: 1.9467\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2972 - val_loss: 0.2079\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2544 - val_loss: 0.2348\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1941 - val_loss: 0.5742\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2010 - val_loss: 0.6768\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2776 - val_loss: 0.3561\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3992 - val_loss: 1.5947\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2319 - val_loss: 0.2040\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1149 - val_loss: 0.1216\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1220 - val_loss: 0.1465\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1252 - val_loss: 0.1312\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1125 - val_loss: 0.1476\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1553 - val_loss: 0.4031\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2036 - val_loss: 0.2827\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1470 - val_loss: 0.1633\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1847 - val_loss: 0.3996\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2224 - val_loss: 0.4265\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1824 - val_loss: 0.2559\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2065 - val_loss: 0.5702\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2090 - val_loss: 0.7264\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2281 - val_loss: 0.3789\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2965 - val_loss: 0.9854\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.2981 - val_loss: 0.2358\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1896 - val_loss: 0.2656\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.1427 - val_loss: 0.3247\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1792 - val_loss: 0.7841\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4587 - val_loss: 0.6252\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1662 - val_loss: 1.0296\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1528 - val_loss: 0.2686\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1367 - val_loss: 0.4015\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.2254 - val_loss: 0.2942\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1986 - val_loss: 0.2997\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1798 - val_loss: 0.1800\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1713 - val_loss: 0.4836\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1881 - val_loss: 0.5649\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1908 - val_loss: 0.3940\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.0786 - val_loss: 2.1383\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1589 - val_loss: 0.1109\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0803 - val_loss: 0.0698\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0676 - val_loss: 0.0856\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.0693 - val_loss: 0.0824\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0699 - val_loss: 0.1284\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0575 - val_loss: 0.1015\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1140 - val_loss: 0.1067\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0916 - val_loss: 0.1007\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1022 - val_loss: 0.3441\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1295 - val_loss: 0.1494\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.1716 - val_loss: 0.2636\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.1425 - val_loss: 0.4404\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1809 - val_loss: 0.5606\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1181 - val_loss: 0.2162\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1654 - val_loss: 0.3915\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2908 - val_loss: 0.6082\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1449 - val_loss: 0.3831\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1519 - val_loss: 0.4690\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1192 - val_loss: 0.1580\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.0954 - val_loss: 0.2019\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1500 - val_loss: 1.2829\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2606 - val_loss: 0.6089\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1791 - val_loss: 0.1782\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1094 - val_loss: 0.2518\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1623 - val_loss: 0.4291\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1437 - val_loss: 0.3271\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1317 - val_loss: 0.2475\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1077 - val_loss: 0.2036\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2047 - val_loss: 0.3385\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1141 - val_loss: 0.2725\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1591 - val_loss: 0.3715\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1474 - val_loss: 0.1914\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.06982729014570604\n",
      "Mean Absolute Error (MAE): 0.20082846036484067\n",
      "Root Mean Squared Error (RMSE): 0.26424853858764485\n",
      "Time taken: 741.2153413295746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_18696\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.087311  0.219307  0.295484  428.389719\n",
      "1        2  0.154657  0.300047  0.393265  277.517179\n",
      "2        3  0.120461  0.262986  0.347074  436.115926\n",
      "3        4  0.042449  0.155776  0.206032  558.014449\n",
      "4        5  0.069827  0.200828  0.264249  741.215341\n",
      "5  Average  0.094941  0.227789  0.301221  488.250523\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_iReg_f.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
