{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_iReg_f_over.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.843921</td>\n",
       "      <td>85.117574</td>\n",
       "      <td>66.199429</td>\n",
       "      <td>76.899627</td>\n",
       "      <td>87.788196</td>\n",
       "      <td>91.089447</td>\n",
       "      <td>75.743961</td>\n",
       "      <td>86.525597</td>\n",
       "      <td>83.432111</td>\n",
       "      <td>81.778161</td>\n",
       "      <td>...</td>\n",
       "      <td>72.865606</td>\n",
       "      <td>78.729236</td>\n",
       "      <td>75.034019</td>\n",
       "      <td>74.528333</td>\n",
       "      <td>85.553399</td>\n",
       "      <td>79.249138</td>\n",
       "      <td>81.185208</td>\n",
       "      <td>80.612998</td>\n",
       "      <td>59.175003</td>\n",
       "      <td>75.983292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.939019</td>\n",
       "      <td>85.132249</td>\n",
       "      <td>66.232727</td>\n",
       "      <td>76.899817</td>\n",
       "      <td>87.565384</td>\n",
       "      <td>90.878656</td>\n",
       "      <td>75.826668</td>\n",
       "      <td>86.203180</td>\n",
       "      <td>83.052069</td>\n",
       "      <td>81.563300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868093</td>\n",
       "      <td>78.454510</td>\n",
       "      <td>74.949960</td>\n",
       "      <td>74.650483</td>\n",
       "      <td>85.387279</td>\n",
       "      <td>79.340137</td>\n",
       "      <td>81.006439</td>\n",
       "      <td>80.654394</td>\n",
       "      <td>59.009440</td>\n",
       "      <td>75.998586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.033485</td>\n",
       "      <td>85.142924</td>\n",
       "      <td>66.267055</td>\n",
       "      <td>76.899800</td>\n",
       "      <td>87.342381</td>\n",
       "      <td>90.668924</td>\n",
       "      <td>75.914190</td>\n",
       "      <td>85.885511</td>\n",
       "      <td>82.676273</td>\n",
       "      <td>81.348898</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868292</td>\n",
       "      <td>78.182075</td>\n",
       "      <td>74.869758</td>\n",
       "      <td>74.768909</td>\n",
       "      <td>85.223174</td>\n",
       "      <td>79.432720</td>\n",
       "      <td>80.829889</td>\n",
       "      <td>80.697306</td>\n",
       "      <td>58.841687</td>\n",
       "      <td>76.015002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.127072</td>\n",
       "      <td>85.149434</td>\n",
       "      <td>66.302453</td>\n",
       "      <td>76.899253</td>\n",
       "      <td>87.119457</td>\n",
       "      <td>90.460149</td>\n",
       "      <td>76.006324</td>\n",
       "      <td>85.573394</td>\n",
       "      <td>82.304584</td>\n",
       "      <td>81.135511</td>\n",
       "      <td>...</td>\n",
       "      <td>72.866288</td>\n",
       "      <td>77.911985</td>\n",
       "      <td>74.793979</td>\n",
       "      <td>74.883125</td>\n",
       "      <td>85.061654</td>\n",
       "      <td>79.526583</td>\n",
       "      <td>80.656280</td>\n",
       "      <td>80.741616</td>\n",
       "      <td>58.672093</td>\n",
       "      <td>76.031822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.219727</td>\n",
       "      <td>85.151660</td>\n",
       "      <td>66.338765</td>\n",
       "      <td>76.897856</td>\n",
       "      <td>86.896708</td>\n",
       "      <td>90.252426</td>\n",
       "      <td>76.102647</td>\n",
       "      <td>85.267343</td>\n",
       "      <td>81.936555</td>\n",
       "      <td>80.923941</td>\n",
       "      <td>...</td>\n",
       "      <td>72.862138</td>\n",
       "      <td>77.644118</td>\n",
       "      <td>74.723442</td>\n",
       "      <td>74.992960</td>\n",
       "      <td>84.903421</td>\n",
       "      <td>79.621167</td>\n",
       "      <td>80.486152</td>\n",
       "      <td>80.787184</td>\n",
       "      <td>58.501165</td>\n",
       "      <td>76.048494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>83.327226</td>\n",
       "      <td>83.518546</td>\n",
       "      <td>72.426351</td>\n",
       "      <td>62.659755</td>\n",
       "      <td>88.956235</td>\n",
       "      <td>88.942345</td>\n",
       "      <td>82.268363</td>\n",
       "      <td>81.250052</td>\n",
       "      <td>82.160443</td>\n",
       "      <td>74.882290</td>\n",
       "      <td>...</td>\n",
       "      <td>76.776115</td>\n",
       "      <td>80.941986</td>\n",
       "      <td>74.631486</td>\n",
       "      <td>66.530311</td>\n",
       "      <td>85.707722</td>\n",
       "      <td>80.212064</td>\n",
       "      <td>85.543311</td>\n",
       "      <td>80.703155</td>\n",
       "      <td>73.788238</td>\n",
       "      <td>62.641024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>83.286758</td>\n",
       "      <td>83.560974</td>\n",
       "      <td>72.373649</td>\n",
       "      <td>62.609847</td>\n",
       "      <td>88.863257</td>\n",
       "      <td>88.942589</td>\n",
       "      <td>82.404968</td>\n",
       "      <td>81.322865</td>\n",
       "      <td>82.338203</td>\n",
       "      <td>74.808569</td>\n",
       "      <td>...</td>\n",
       "      <td>77.006760</td>\n",
       "      <td>80.864543</td>\n",
       "      <td>74.649267</td>\n",
       "      <td>66.347601</td>\n",
       "      <td>85.896138</td>\n",
       "      <td>80.159673</td>\n",
       "      <td>85.633353</td>\n",
       "      <td>80.670093</td>\n",
       "      <td>73.846209</td>\n",
       "      <td>62.712693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>83.246854</td>\n",
       "      <td>83.603087</td>\n",
       "      <td>72.324180</td>\n",
       "      <td>62.557213</td>\n",
       "      <td>88.768335</td>\n",
       "      <td>88.942075</td>\n",
       "      <td>82.543950</td>\n",
       "      <td>81.400108</td>\n",
       "      <td>82.515614</td>\n",
       "      <td>74.734698</td>\n",
       "      <td>...</td>\n",
       "      <td>77.236622</td>\n",
       "      <td>80.784010</td>\n",
       "      <td>74.664822</td>\n",
       "      <td>66.162730</td>\n",
       "      <td>86.087450</td>\n",
       "      <td>80.109937</td>\n",
       "      <td>85.722839</td>\n",
       "      <td>80.639276</td>\n",
       "      <td>73.899692</td>\n",
       "      <td>62.785040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>83.207049</td>\n",
       "      <td>83.645151</td>\n",
       "      <td>72.278027</td>\n",
       "      <td>62.501762</td>\n",
       "      <td>88.671491</td>\n",
       "      <td>88.940424</td>\n",
       "      <td>82.685667</td>\n",
       "      <td>81.482106</td>\n",
       "      <td>82.692290</td>\n",
       "      <td>74.661157</td>\n",
       "      <td>...</td>\n",
       "      <td>77.465150</td>\n",
       "      <td>80.700478</td>\n",
       "      <td>74.678199</td>\n",
       "      <td>65.976653</td>\n",
       "      <td>86.281472</td>\n",
       "      <td>80.062785</td>\n",
       "      <td>85.812210</td>\n",
       "      <td>80.610958</td>\n",
       "      <td>73.948257</td>\n",
       "      <td>62.858862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>83.166811</td>\n",
       "      <td>83.687639</td>\n",
       "      <td>72.234952</td>\n",
       "      <td>62.443628</td>\n",
       "      <td>88.572647</td>\n",
       "      <td>88.937398</td>\n",
       "      <td>82.830291</td>\n",
       "      <td>81.568810</td>\n",
       "      <td>82.868067</td>\n",
       "      <td>74.588255</td>\n",
       "      <td>...</td>\n",
       "      <td>77.691951</td>\n",
       "      <td>80.613928</td>\n",
       "      <td>74.689688</td>\n",
       "      <td>65.790084</td>\n",
       "      <td>86.478064</td>\n",
       "      <td>80.018261</td>\n",
       "      <td>85.901580</td>\n",
       "      <td>80.585607</td>\n",
       "      <td>73.991807</td>\n",
       "      <td>62.934768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     76.843921  85.117574  66.199429  76.899627  87.788196  91.089447   \n",
       "1     76.939019  85.132249  66.232727  76.899817  87.565384  90.878656   \n",
       "2     77.033485  85.142924  66.267055  76.899800  87.342381  90.668924   \n",
       "3     77.127072  85.149434  66.302453  76.899253  87.119457  90.460149   \n",
       "4     77.219727  85.151660  66.338765  76.897856  86.896708  90.252426   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  83.327226  83.518546  72.426351  62.659755  88.956235  88.942345   \n",
       "2439  83.286758  83.560974  72.373649  62.609847  88.863257  88.942589   \n",
       "2440  83.246854  83.603087  72.324180  62.557213  88.768335  88.942075   \n",
       "2441  83.207049  83.645151  72.278027  62.501762  88.671491  88.940424   \n",
       "2442  83.166811  83.687639  72.234952  62.443628  88.572647  88.937398   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     75.743961  86.525597  83.432111  81.778161  ...  72.865606  78.729236   \n",
       "1     75.826668  86.203180  83.052069  81.563300  ...  72.868093  78.454510   \n",
       "2     75.914190  85.885511  82.676273  81.348898  ...  72.868292  78.182075   \n",
       "3     76.006324  85.573394  82.304584  81.135511  ...  72.866288  77.911985   \n",
       "4     76.102647  85.267343  81.936555  80.923941  ...  72.862138  77.644118   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  82.268363  81.250052  82.160443  74.882290  ...  76.776115  80.941986   \n",
       "2439  82.404968  81.322865  82.338203  74.808569  ...  77.006760  80.864543   \n",
       "2440  82.543950  81.400108  82.515614  74.734698  ...  77.236622  80.784010   \n",
       "2441  82.685667  81.482106  82.692290  74.661157  ...  77.465150  80.700478   \n",
       "2442  82.830291  81.568810  82.868067  74.588255  ...  77.691951  80.613928   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     75.034019  74.528333  85.553399  79.249138  81.185208  80.612998   \n",
       "1     74.949960  74.650483  85.387279  79.340137  81.006439  80.654394   \n",
       "2     74.869758  74.768909  85.223174  79.432720  80.829889  80.697306   \n",
       "3     74.793979  74.883125  85.061654  79.526583  80.656280  80.741616   \n",
       "4     74.723442  74.992960  84.903421  79.621167  80.486152  80.787184   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  74.631486  66.530311  85.707722  80.212064  85.543311  80.703155   \n",
       "2439  74.649267  66.347601  85.896138  80.159673  85.633353  80.670093   \n",
       "2440  74.664822  66.162730  86.087450  80.109937  85.722839  80.639276   \n",
       "2441  74.678199  65.976653  86.281472  80.062785  85.812210  80.610958   \n",
       "2442  74.689688  65.790084  86.478064  80.018261  85.901580  80.585607   \n",
       "\n",
       "             46         47  \n",
       "0     59.175003  75.983292  \n",
       "1     59.009440  75.998586  \n",
       "2     58.841687  76.015002  \n",
       "3     58.672093  76.031822  \n",
       "4     58.501165  76.048494  \n",
       "...         ...        ...  \n",
       "2438  73.788238  62.641024  \n",
       "2439  73.846209  62.712693  \n",
       "2440  73.899692  62.785040  \n",
       "2441  73.948257  62.858862  \n",
       "2442  73.991807  62.934768  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.843921</td>\n",
       "      <td>85.117574</td>\n",
       "      <td>66.199429</td>\n",
       "      <td>76.899627</td>\n",
       "      <td>87.788196</td>\n",
       "      <td>91.089447</td>\n",
       "      <td>75.743961</td>\n",
       "      <td>86.525597</td>\n",
       "      <td>83.432111</td>\n",
       "      <td>81.778161</td>\n",
       "      <td>...</td>\n",
       "      <td>72.865606</td>\n",
       "      <td>78.729236</td>\n",
       "      <td>75.034019</td>\n",
       "      <td>74.528333</td>\n",
       "      <td>85.553399</td>\n",
       "      <td>79.249138</td>\n",
       "      <td>81.185208</td>\n",
       "      <td>80.612998</td>\n",
       "      <td>59.175003</td>\n",
       "      <td>75.983292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.939019</td>\n",
       "      <td>85.132249</td>\n",
       "      <td>66.232727</td>\n",
       "      <td>76.899817</td>\n",
       "      <td>87.565384</td>\n",
       "      <td>90.878656</td>\n",
       "      <td>75.826668</td>\n",
       "      <td>86.203180</td>\n",
       "      <td>83.052069</td>\n",
       "      <td>81.563300</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868093</td>\n",
       "      <td>78.454510</td>\n",
       "      <td>74.949960</td>\n",
       "      <td>74.650483</td>\n",
       "      <td>85.387279</td>\n",
       "      <td>79.340137</td>\n",
       "      <td>81.006439</td>\n",
       "      <td>80.654394</td>\n",
       "      <td>59.009440</td>\n",
       "      <td>75.998586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.033485</td>\n",
       "      <td>85.142924</td>\n",
       "      <td>66.267055</td>\n",
       "      <td>76.899800</td>\n",
       "      <td>87.342381</td>\n",
       "      <td>90.668924</td>\n",
       "      <td>75.914190</td>\n",
       "      <td>85.885511</td>\n",
       "      <td>82.676273</td>\n",
       "      <td>81.348898</td>\n",
       "      <td>...</td>\n",
       "      <td>72.868292</td>\n",
       "      <td>78.182075</td>\n",
       "      <td>74.869758</td>\n",
       "      <td>74.768909</td>\n",
       "      <td>85.223174</td>\n",
       "      <td>79.432720</td>\n",
       "      <td>80.829889</td>\n",
       "      <td>80.697306</td>\n",
       "      <td>58.841687</td>\n",
       "      <td>76.015002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.127072</td>\n",
       "      <td>85.149434</td>\n",
       "      <td>66.302453</td>\n",
       "      <td>76.899253</td>\n",
       "      <td>87.119457</td>\n",
       "      <td>90.460149</td>\n",
       "      <td>76.006324</td>\n",
       "      <td>85.573394</td>\n",
       "      <td>82.304584</td>\n",
       "      <td>81.135511</td>\n",
       "      <td>...</td>\n",
       "      <td>72.866288</td>\n",
       "      <td>77.911985</td>\n",
       "      <td>74.793979</td>\n",
       "      <td>74.883125</td>\n",
       "      <td>85.061654</td>\n",
       "      <td>79.526583</td>\n",
       "      <td>80.656280</td>\n",
       "      <td>80.741616</td>\n",
       "      <td>58.672093</td>\n",
       "      <td>76.031822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.219727</td>\n",
       "      <td>85.151660</td>\n",
       "      <td>66.338765</td>\n",
       "      <td>76.897856</td>\n",
       "      <td>86.896708</td>\n",
       "      <td>90.252426</td>\n",
       "      <td>76.102647</td>\n",
       "      <td>85.267343</td>\n",
       "      <td>81.936555</td>\n",
       "      <td>80.923941</td>\n",
       "      <td>...</td>\n",
       "      <td>72.862138</td>\n",
       "      <td>77.644118</td>\n",
       "      <td>74.723442</td>\n",
       "      <td>74.992960</td>\n",
       "      <td>84.903421</td>\n",
       "      <td>79.621167</td>\n",
       "      <td>80.486152</td>\n",
       "      <td>80.787184</td>\n",
       "      <td>58.501165</td>\n",
       "      <td>76.048494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>83.327226</td>\n",
       "      <td>83.518546</td>\n",
       "      <td>72.426351</td>\n",
       "      <td>62.659755</td>\n",
       "      <td>88.956235</td>\n",
       "      <td>88.942345</td>\n",
       "      <td>82.268363</td>\n",
       "      <td>81.250052</td>\n",
       "      <td>82.160443</td>\n",
       "      <td>74.882290</td>\n",
       "      <td>...</td>\n",
       "      <td>76.776115</td>\n",
       "      <td>80.941986</td>\n",
       "      <td>74.631486</td>\n",
       "      <td>66.530311</td>\n",
       "      <td>85.707722</td>\n",
       "      <td>80.212064</td>\n",
       "      <td>85.543311</td>\n",
       "      <td>80.703155</td>\n",
       "      <td>73.788238</td>\n",
       "      <td>62.641024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>83.286758</td>\n",
       "      <td>83.560974</td>\n",
       "      <td>72.373649</td>\n",
       "      <td>62.609847</td>\n",
       "      <td>88.863257</td>\n",
       "      <td>88.942589</td>\n",
       "      <td>82.404968</td>\n",
       "      <td>81.322865</td>\n",
       "      <td>82.338203</td>\n",
       "      <td>74.808569</td>\n",
       "      <td>...</td>\n",
       "      <td>77.006760</td>\n",
       "      <td>80.864543</td>\n",
       "      <td>74.649267</td>\n",
       "      <td>66.347601</td>\n",
       "      <td>85.896138</td>\n",
       "      <td>80.159673</td>\n",
       "      <td>85.633353</td>\n",
       "      <td>80.670093</td>\n",
       "      <td>73.846209</td>\n",
       "      <td>62.712693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>83.246854</td>\n",
       "      <td>83.603087</td>\n",
       "      <td>72.324180</td>\n",
       "      <td>62.557213</td>\n",
       "      <td>88.768335</td>\n",
       "      <td>88.942075</td>\n",
       "      <td>82.543950</td>\n",
       "      <td>81.400108</td>\n",
       "      <td>82.515614</td>\n",
       "      <td>74.734698</td>\n",
       "      <td>...</td>\n",
       "      <td>77.236622</td>\n",
       "      <td>80.784010</td>\n",
       "      <td>74.664822</td>\n",
       "      <td>66.162730</td>\n",
       "      <td>86.087450</td>\n",
       "      <td>80.109937</td>\n",
       "      <td>85.722839</td>\n",
       "      <td>80.639276</td>\n",
       "      <td>73.899692</td>\n",
       "      <td>62.785040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>83.207049</td>\n",
       "      <td>83.645151</td>\n",
       "      <td>72.278027</td>\n",
       "      <td>62.501762</td>\n",
       "      <td>88.671491</td>\n",
       "      <td>88.940424</td>\n",
       "      <td>82.685667</td>\n",
       "      <td>81.482106</td>\n",
       "      <td>82.692290</td>\n",
       "      <td>74.661157</td>\n",
       "      <td>...</td>\n",
       "      <td>77.465150</td>\n",
       "      <td>80.700478</td>\n",
       "      <td>74.678199</td>\n",
       "      <td>65.976653</td>\n",
       "      <td>86.281472</td>\n",
       "      <td>80.062785</td>\n",
       "      <td>85.812210</td>\n",
       "      <td>80.610958</td>\n",
       "      <td>73.948257</td>\n",
       "      <td>62.858862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>83.166811</td>\n",
       "      <td>83.687639</td>\n",
       "      <td>72.234952</td>\n",
       "      <td>62.443628</td>\n",
       "      <td>88.572647</td>\n",
       "      <td>88.937398</td>\n",
       "      <td>82.830291</td>\n",
       "      <td>81.568810</td>\n",
       "      <td>82.868067</td>\n",
       "      <td>74.588255</td>\n",
       "      <td>...</td>\n",
       "      <td>77.691951</td>\n",
       "      <td>80.613928</td>\n",
       "      <td>74.689688</td>\n",
       "      <td>65.790084</td>\n",
       "      <td>86.478064</td>\n",
       "      <td>80.018261</td>\n",
       "      <td>85.901580</td>\n",
       "      <td>80.585607</td>\n",
       "      <td>73.991807</td>\n",
       "      <td>62.934768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     76.843921  85.117574  66.199429  76.899627  87.788196  91.089447   \n",
       "1     76.939019  85.132249  66.232727  76.899817  87.565384  90.878656   \n",
       "2     77.033485  85.142924  66.267055  76.899800  87.342381  90.668924   \n",
       "3     77.127072  85.149434  66.302453  76.899253  87.119457  90.460149   \n",
       "4     77.219727  85.151660  66.338765  76.897856  86.896708  90.252426   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  83.327226  83.518546  72.426351  62.659755  88.956235  88.942345   \n",
       "2439  83.286758  83.560974  72.373649  62.609847  88.863257  88.942589   \n",
       "2440  83.246854  83.603087  72.324180  62.557213  88.768335  88.942075   \n",
       "2441  83.207049  83.645151  72.278027  62.501762  88.671491  88.940424   \n",
       "2442  83.166811  83.687639  72.234952  62.443628  88.572647  88.937398   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     75.743961  86.525597  83.432111  81.778161  ...  72.865606  78.729236   \n",
       "1     75.826668  86.203180  83.052069  81.563300  ...  72.868093  78.454510   \n",
       "2     75.914190  85.885511  82.676273  81.348898  ...  72.868292  78.182075   \n",
       "3     76.006324  85.573394  82.304584  81.135511  ...  72.866288  77.911985   \n",
       "4     76.102647  85.267343  81.936555  80.923941  ...  72.862138  77.644118   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  82.268363  81.250052  82.160443  74.882290  ...  76.776115  80.941986   \n",
       "2439  82.404968  81.322865  82.338203  74.808569  ...  77.006760  80.864543   \n",
       "2440  82.543950  81.400108  82.515614  74.734698  ...  77.236622  80.784010   \n",
       "2441  82.685667  81.482106  82.692290  74.661157  ...  77.465150  80.700478   \n",
       "2442  82.830291  81.568810  82.868067  74.588255  ...  77.691951  80.613928   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     75.034019  74.528333  85.553399  79.249138  81.185208  80.612998   \n",
       "1     74.949960  74.650483  85.387279  79.340137  81.006439  80.654394   \n",
       "2     74.869758  74.768909  85.223174  79.432720  80.829889  80.697306   \n",
       "3     74.793979  74.883125  85.061654  79.526583  80.656280  80.741616   \n",
       "4     74.723442  74.992960  84.903421  79.621167  80.486152  80.787184   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  74.631486  66.530311  85.707722  80.212064  85.543311  80.703155   \n",
       "2439  74.649267  66.347601  85.896138  80.159673  85.633353  80.670093   \n",
       "2440  74.664822  66.162730  86.087450  80.109937  85.722839  80.639276   \n",
       "2441  74.678199  65.976653  86.281472  80.062785  85.812210  80.610958   \n",
       "2442  74.689688  65.790084  86.478064  80.018261  85.901580  80.585607   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     59.175003  75.983292  \n",
       "1     59.009440  75.998586  \n",
       "2     58.841687  76.015002  \n",
       "3     58.672093  76.031822  \n",
       "4     58.501165  76.048494  \n",
       "...         ...        ...  \n",
       "2438  73.788238  62.641024  \n",
       "2439  73.846209  62.712693  \n",
       "2440  73.899692  62.785040  \n",
       "2441  73.948257  62.858862  \n",
       "2442  73.991807  62.934768  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.843921</td>\n",
       "      <td>85.117574</td>\n",
       "      <td>66.199429</td>\n",
       "      <td>76.899627</td>\n",
       "      <td>87.788196</td>\n",
       "      <td>91.089447</td>\n",
       "      <td>75.743961</td>\n",
       "      <td>86.525597</td>\n",
       "      <td>83.432111</td>\n",
       "      <td>81.778161</td>\n",
       "      <td>72.245540</td>\n",
       "      <td>70.271511</td>\n",
       "      <td>80.473125</td>\n",
       "      <td>86.415568</td>\n",
       "      <td>83.266282</td>\n",
       "      <td>84.760030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.939019</td>\n",
       "      <td>85.132249</td>\n",
       "      <td>66.232727</td>\n",
       "      <td>76.899817</td>\n",
       "      <td>87.565384</td>\n",
       "      <td>90.878656</td>\n",
       "      <td>75.826668</td>\n",
       "      <td>86.203180</td>\n",
       "      <td>83.052069</td>\n",
       "      <td>81.563300</td>\n",
       "      <td>72.119038</td>\n",
       "      <td>70.505218</td>\n",
       "      <td>80.538076</td>\n",
       "      <td>86.243503</td>\n",
       "      <td>83.119092</td>\n",
       "      <td>84.582257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.033485</td>\n",
       "      <td>85.142924</td>\n",
       "      <td>66.267055</td>\n",
       "      <td>76.899800</td>\n",
       "      <td>87.342381</td>\n",
       "      <td>90.668924</td>\n",
       "      <td>75.914190</td>\n",
       "      <td>85.885511</td>\n",
       "      <td>82.676273</td>\n",
       "      <td>81.348898</td>\n",
       "      <td>71.995856</td>\n",
       "      <td>70.734789</td>\n",
       "      <td>80.603094</td>\n",
       "      <td>86.074590</td>\n",
       "      <td>82.969073</td>\n",
       "      <td>84.402847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77.127072</td>\n",
       "      <td>85.149434</td>\n",
       "      <td>66.302453</td>\n",
       "      <td>76.899253</td>\n",
       "      <td>87.119457</td>\n",
       "      <td>90.460149</td>\n",
       "      <td>76.006324</td>\n",
       "      <td>85.573394</td>\n",
       "      <td>82.304584</td>\n",
       "      <td>81.135511</td>\n",
       "      <td>71.875677</td>\n",
       "      <td>70.960525</td>\n",
       "      <td>80.667661</td>\n",
       "      <td>85.908948</td>\n",
       "      <td>82.816662</td>\n",
       "      <td>84.221999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77.219727</td>\n",
       "      <td>85.151660</td>\n",
       "      <td>66.338765</td>\n",
       "      <td>76.897856</td>\n",
       "      <td>86.896708</td>\n",
       "      <td>90.252426</td>\n",
       "      <td>76.102647</td>\n",
       "      <td>85.267343</td>\n",
       "      <td>81.936555</td>\n",
       "      <td>80.923941</td>\n",
       "      <td>71.758509</td>\n",
       "      <td>71.182941</td>\n",
       "      <td>80.731396</td>\n",
       "      <td>85.746862</td>\n",
       "      <td>82.662509</td>\n",
       "      <td>84.040233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>83.327226</td>\n",
       "      <td>83.518546</td>\n",
       "      <td>72.426351</td>\n",
       "      <td>62.659755</td>\n",
       "      <td>88.956235</td>\n",
       "      <td>88.942345</td>\n",
       "      <td>82.268363</td>\n",
       "      <td>81.250052</td>\n",
       "      <td>82.160443</td>\n",
       "      <td>74.882290</td>\n",
       "      <td>68.654453</td>\n",
       "      <td>63.795613</td>\n",
       "      <td>86.729603</td>\n",
       "      <td>86.039017</td>\n",
       "      <td>85.607864</td>\n",
       "      <td>78.575978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>83.286758</td>\n",
       "      <td>83.560974</td>\n",
       "      <td>72.373649</td>\n",
       "      <td>62.609847</td>\n",
       "      <td>88.863257</td>\n",
       "      <td>88.942589</td>\n",
       "      <td>82.404968</td>\n",
       "      <td>81.322865</td>\n",
       "      <td>82.338203</td>\n",
       "      <td>74.808569</td>\n",
       "      <td>68.576428</td>\n",
       "      <td>63.834618</td>\n",
       "      <td>86.820346</td>\n",
       "      <td>86.031544</td>\n",
       "      <td>85.716625</td>\n",
       "      <td>78.492339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>83.246854</td>\n",
       "      <td>83.603087</td>\n",
       "      <td>72.324180</td>\n",
       "      <td>62.557213</td>\n",
       "      <td>88.768335</td>\n",
       "      <td>88.942075</td>\n",
       "      <td>82.543950</td>\n",
       "      <td>81.400108</td>\n",
       "      <td>82.515614</td>\n",
       "      <td>74.734698</td>\n",
       "      <td>68.494478</td>\n",
       "      <td>63.868756</td>\n",
       "      <td>86.910183</td>\n",
       "      <td>86.019243</td>\n",
       "      <td>85.823815</td>\n",
       "      <td>78.411219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>83.207049</td>\n",
       "      <td>83.645151</td>\n",
       "      <td>72.278027</td>\n",
       "      <td>62.501762</td>\n",
       "      <td>88.671491</td>\n",
       "      <td>88.940424</td>\n",
       "      <td>82.685667</td>\n",
       "      <td>81.482106</td>\n",
       "      <td>82.692290</td>\n",
       "      <td>74.661157</td>\n",
       "      <td>68.408324</td>\n",
       "      <td>63.897490</td>\n",
       "      <td>86.998348</td>\n",
       "      <td>86.002098</td>\n",
       "      <td>85.929297</td>\n",
       "      <td>78.332895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>83.166811</td>\n",
       "      <td>83.687639</td>\n",
       "      <td>72.234952</td>\n",
       "      <td>62.443628</td>\n",
       "      <td>88.572647</td>\n",
       "      <td>88.937398</td>\n",
       "      <td>82.830291</td>\n",
       "      <td>81.568810</td>\n",
       "      <td>82.868067</td>\n",
       "      <td>74.588255</td>\n",
       "      <td>68.318255</td>\n",
       "      <td>63.920363</td>\n",
       "      <td>87.084210</td>\n",
       "      <td>85.980034</td>\n",
       "      <td>86.033081</td>\n",
       "      <td>78.257335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     76.843921  85.117574  66.199429  76.899627  87.788196  91.089447   \n",
       "1     76.939019  85.132249  66.232727  76.899817  87.565384  90.878656   \n",
       "2     77.033485  85.142924  66.267055  76.899800  87.342381  90.668924   \n",
       "3     77.127072  85.149434  66.302453  76.899253  87.119457  90.460149   \n",
       "4     77.219727  85.151660  66.338765  76.897856  86.896708  90.252426   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  83.327226  83.518546  72.426351  62.659755  88.956235  88.942345   \n",
       "2439  83.286758  83.560974  72.373649  62.609847  88.863257  88.942589   \n",
       "2440  83.246854  83.603087  72.324180  62.557213  88.768335  88.942075   \n",
       "2441  83.207049  83.645151  72.278027  62.501762  88.671491  88.940424   \n",
       "2442  83.166811  83.687639  72.234952  62.443628  88.572647  88.937398   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     75.743961  86.525597  83.432111  81.778161  72.245540  70.271511   \n",
       "1     75.826668  86.203180  83.052069  81.563300  72.119038  70.505218   \n",
       "2     75.914190  85.885511  82.676273  81.348898  71.995856  70.734789   \n",
       "3     76.006324  85.573394  82.304584  81.135511  71.875677  70.960525   \n",
       "4     76.102647  85.267343  81.936555  80.923941  71.758509  71.182941   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  82.268363  81.250052  82.160443  74.882290  68.654453  63.795613   \n",
       "2439  82.404968  81.322865  82.338203  74.808569  68.576428  63.834618   \n",
       "2440  82.543950  81.400108  82.515614  74.734698  68.494478  63.868756   \n",
       "2441  82.685667  81.482106  82.692290  74.661157  68.408324  63.897490   \n",
       "2442  82.830291  81.568810  82.868067  74.588255  68.318255  63.920363   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     80.473125  86.415568  83.266282  84.760030  \n",
       "1     80.538076  86.243503  83.119092  84.582257  \n",
       "2     80.603094  86.074590  82.969073  84.402847  \n",
       "3     80.667661  85.908948  82.816662  84.221999  \n",
       "4     80.731396  85.746862  82.662509  84.040233  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  86.729603  86.039017  85.607864  78.575978  \n",
       "2439  86.820346  86.031544  85.716625  78.492339  \n",
       "2440  86.910183  86.019243  85.823815  78.411219  \n",
       "2441  86.998348  86.002098  85.929297  78.332895  \n",
       "2442  87.084210  85.980034  86.033081  78.257335  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 15ms/step - loss: 1123.4551 - val_loss: 854.3685\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 669.0803 - val_loss: 560.8049\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 459.2636 - val_loss: 411.9985\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 358.4537 - val_loss: 417.8754\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 315.1951 - val_loss: 330.5364\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 273.4678 - val_loss: 288.3798\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 192.0242 - val_loss: 162.1780\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 110.5716 - val_loss: 98.1596\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 63.2061 - val_loss: 91.6832\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 43.8501 - val_loss: 66.1548\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 27.5199 - val_loss: 83.8053\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.2782 - val_loss: 43.2618\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.2041 - val_loss: 20.2322\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.3340 - val_loss: 57.3054\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.7227 - val_loss: 23.3295\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.5650 - val_loss: 18.2901\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.3619 - val_loss: 6.0977\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.3413 - val_loss: 60.9780\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.6430 - val_loss: 24.8953\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.4390 - val_loss: 7.1340\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6428 - val_loss: 7.7255\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1613 - val_loss: 17.6748\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.7141 - val_loss: 57.3944\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0506 - val_loss: 3.8954\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6905 - val_loss: 73.6070\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6112 - val_loss: 3.8156\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2788 - val_loss: 10.8958\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0340 - val_loss: 4.0967\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7342 - val_loss: 3.1476\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6053 - val_loss: 13.9532\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8541 - val_loss: 7.5376\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.7331 - val_loss: 5.2547\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4860 - val_loss: 22.9929\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5189 - val_loss: 2.2860\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4317 - val_loss: 2.1476\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9006 - val_loss: 2.4906\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8913 - val_loss: 0.9899\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9382 - val_loss: 0.9503\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0158 - val_loss: 1.3581\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7401 - val_loss: 0.7472\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0874 - val_loss: 1.4853\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4501 - val_loss: 18.8001\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0824 - val_loss: 2.2421\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7201 - val_loss: 0.6370\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1076 - val_loss: 20.8131\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6015 - val_loss: 1.0098\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5231 - val_loss: 0.7426\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5037 - val_loss: 0.6523\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5957 - val_loss: 0.9468\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8626 - val_loss: 2.0524\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7324 - val_loss: 5.7070\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8320 - val_loss: 4.6694\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6883 - val_loss: 1.1369\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5337 - val_loss: 2.5066\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.2648 - val_loss: 4.0804\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5489 - val_loss: 0.8618\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3162 - val_loss: 0.4487\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9537 - val_loss: 0.7933\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3515 - val_loss: 0.4471\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2988 - val_loss: 0.5816\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3040 - val_loss: 0.3711\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3000 - val_loss: 0.6454\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1534 - val_loss: 0.4203\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2824 - val_loss: 0.4329\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2551 - val_loss: 0.2874\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2817 - val_loss: 0.5966\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4522 - val_loss: 0.7362\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7978 - val_loss: 0.8982\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3541 - val_loss: 1.3645\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3775 - val_loss: 0.7406\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7708 - val_loss: 94.7686\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3637 - val_loss: 1.0474\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2914 - val_loss: 0.2956\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2552 - val_loss: 0.3610\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2355 - val_loss: 0.3357\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2330 - val_loss: 0.1749\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2150 - val_loss: 0.8941\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2763 - val_loss: 0.2954\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3532 - val_loss: 0.9685\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8316 - val_loss: 0.3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2866 - val_loss: 0.7872\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2985 - val_loss: 1.0132\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4270 - val_loss: 0.5732\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2360 - val_loss: 2.1980\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3864 - val_loss: 0.5578\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9106 - val_loss: 4.6776\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2793 - val_loss: 0.1972\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9224 - val_loss: 0.7664\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2729 - val_loss: 0.2410\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1673 - val_loss: 0.2243\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1481 - val_loss: 0.3193\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1454 - val_loss: 0.4126\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1808 - val_loss: 0.2460\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1767 - val_loss: 0.4267\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1917 - val_loss: 1.8814\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3278 - val_loss: 0.4369\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2163 - val_loss: 0.1434\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1413 - val_loss: 0.1524\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1220 - val_loss: 0.1587\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1099 - val_loss: 0.1759\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1224 - val_loss: 0.1899\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1404 - val_loss: 0.1372\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1471 - val_loss: 0.2710\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1862 - val_loss: 0.2313\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2048 - val_loss: 0.2900\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2202 - val_loss: 0.4940\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2416 - val_loss: 0.5172\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8304 - val_loss: 44.3007\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3882 - val_loss: 0.1457\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1291 - val_loss: 0.1967\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1191 - val_loss: 0.4175\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1330 - val_loss: 0.1989\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1306 - val_loss: 0.2819\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1591 - val_loss: 0.3586\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1501 - val_loss: 0.4609\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3003 - val_loss: 0.8003\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1690 - val_loss: 0.1537\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5481 - val_loss: 18.2824\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4777 - val_loss: 0.1640\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1460 - val_loss: 0.1203\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1534 - val_loss: 0.6398\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1422 - val_loss: 0.3244\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1829 - val_loss: 0.9680\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2953 - val_loss: 0.3019\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1751 - val_loss: 0.2537\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2144 - val_loss: 0.2226\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1981 - val_loss: 3.6529\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2604 - val_loss: 0.6267\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2689 - val_loss: 0.3213\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1933 - val_loss: 1.0157\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2469 - val_loss: 0.8134\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2644 - val_loss: 0.3516\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1978 - val_loss: 1.2285\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2465 - val_loss: 0.8616\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1977 - val_loss: 0.3672\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3418 - val_loss: 1.8088\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3127 - val_loss: 0.6328\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1557 - val_loss: 0.2041\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1372 - val_loss: 2.2212\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1889 - val_loss: 1.5110\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2626 - val_loss: 0.5190\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2058 - val_loss: 0.5617\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1571 - val_loss: 0.2416\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2047 - val_loss: 0.5672\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2114 - val_loss: 0.3272\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1822 - val_loss: 1.0098\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1685 - val_loss: 0.4609\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1680 - val_loss: 0.4411\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1886 - val_loss: 3.8983\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1966 - val_loss: 0.2839\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.12034369469248846\n",
      "Mean Absolute Error (MAE): 0.2526373489409747\n",
      "Root Mean Squared Error (RMSE): 0.3469058873707514\n",
      "Time taken: 441.76898312568665\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 1092.5831 - val_loss: 856.3682\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 675.5577 - val_loss: 588.7722\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 464.8542 - val_loss: 554.4410\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 368.9576 - val_loss: 352.3471\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 326.3770 - val_loss: 321.9562\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 296.0900 - val_loss: 321.8080\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 258.0349 - val_loss: 330.8271\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 190.6851 - val_loss: 190.7543\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 106.9700 - val_loss: 128.5546\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 65.6477 - val_loss: 58.4391\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 41.7016 - val_loss: 60.3551\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.7624 - val_loss: 61.0691\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.1662 - val_loss: 34.7136\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.3119 - val_loss: 17.2673\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.3627 - val_loss: 66.9903\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.1857 - val_loss: 8.7978\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5388 - val_loss: 13.8868\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.8402 - val_loss: 8.1498\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 7.3677 - val_loss: 9.0437\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.8364 - val_loss: 16.7273\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.6588 - val_loss: 11.7062\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4015 - val_loss: 8.2879\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.3620 - val_loss: 4.2818\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.7893 - val_loss: 7.3599\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.7248 - val_loss: 6.4803\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4674 - val_loss: 6.0452\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.0601 - val_loss: 15.1546\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.7458 - val_loss: 14.7215\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1174 - val_loss: 6.8019\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0420 - val_loss: 7.7106\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5970 - val_loss: 8.8831\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5559 - val_loss: 12.7141\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8220 - val_loss: 2.8030\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4137 - val_loss: 2.7060\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8553 - val_loss: 111.6979\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.6030 - val_loss: 3.2888\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5666 - val_loss: 1.6307\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7736 - val_loss: 3.6694\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9617 - val_loss: 0.9773\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9063 - val_loss: 1.1293\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.8943 - val_loss: 1.4362\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8438 - val_loss: 1.2631\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7782 - val_loss: 4.3376\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2892 - val_loss: 2.0668\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7011 - val_loss: 1.1615\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6176 - val_loss: 1.0054\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9149 - val_loss: 1.0919\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8948 - val_loss: 3.6023\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6063 - val_loss: 2.5875\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2366 - val_loss: 5.7912\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6095 - val_loss: 0.9973\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7132 - val_loss: 1.7218\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6965 - val_loss: 3.1603\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8460 - val_loss: 8.3637\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.3541 - val_loss: 1.6316\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8115 - val_loss: 1.2300\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3930 - val_loss: 0.7949\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3400 - val_loss: 1.0044\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3882 - val_loss: 2.1070\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3506 - val_loss: 0.6353\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4279 - val_loss: 0.6845\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4973 - val_loss: 1.6006\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5916 - val_loss: 1.0626\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6490 - val_loss: 1.0024\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2785 - val_loss: 43.7233\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.1414 - val_loss: 0.4790\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3001 - val_loss: 0.2282\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2878 - val_loss: 5.3781\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3229 - val_loss: 0.5254\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2857 - val_loss: 0.5495\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7507 - val_loss: 0.4393\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3619 - val_loss: 0.3747\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2772 - val_loss: 0.5231\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2728 - val_loss: 0.3591\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2898 - val_loss: 0.4088\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3317 - val_loss: 3.0432\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2038 - val_loss: 0.2998\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2741 - val_loss: 0.5081\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4323 - val_loss: 1.0927\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4794 - val_loss: 0.4429\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3479 - val_loss: 0.3889\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4254 - val_loss: 0.9845\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3875 - val_loss: 163.5772\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8549 - val_loss: 0.2669\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2275 - val_loss: 0.1708\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2643 - val_loss: 0.2858\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1788 - val_loss: 0.1545\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1894 - val_loss: 0.3508\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2190 - val_loss: 0.8877\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1936 - val_loss: 0.2941\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6213 - val_loss: 0.7802\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2697 - val_loss: 0.2372\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2546 - val_loss: 0.3497\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2749 - val_loss: 0.4809\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4828 - val_loss: 27.4264\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.0608 - val_loss: 0.2529\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1835 - val_loss: 0.2222\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1762 - val_loss: 0.1961\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1627 - val_loss: 0.3300\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7418 - val_loss: 0.2219\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2033 - val_loss: 0.3535\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1728 - val_loss: 0.1892\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1779 - val_loss: 0.4609\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3838 - val_loss: 0.9022\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3122 - val_loss: 0.3621\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2189 - val_loss: 0.5848\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3374 - val_loss: 0.5745\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4151 - val_loss: 5.1886\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6782 - val_loss: 1.8025\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1973 - val_loss: 0.2400\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1287 - val_loss: 0.1421\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3672 - val_loss: 0.6200\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1953 - val_loss: 0.1430\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1300 - val_loss: 0.1194\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1214 - val_loss: 0.1156\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1298 - val_loss: 0.1319\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1454 - val_loss: 0.1031\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1229 - val_loss: 0.1821\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1457 - val_loss: 0.2743\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1644 - val_loss: 0.2955\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1727 - val_loss: 0.1595\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2111 - val_loss: 1.3769\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3169 - val_loss: 0.4570\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2017 - val_loss: 0.2540\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1776 - val_loss: 0.1711\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3046 - val_loss: 0.4612\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4718 - val_loss: 8.2011\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2860 - val_loss: 0.2925\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1679 - val_loss: 0.9531\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1889 - val_loss: 0.5135\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1704 - val_loss: 0.1775\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7395 - val_loss: 0.1523\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1182 - val_loss: 0.1054\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0909 - val_loss: 0.1525\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0947 - val_loss: 0.1809\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1010 - val_loss: 0.1972\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1089 - val_loss: 0.2007\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1572 - val_loss: 0.4458\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1523 - val_loss: 0.4887\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1368 - val_loss: 2.6836\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7165 - val_loss: 3.0074\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2486 - val_loss: 0.1784\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1173 - val_loss: 0.1678\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1168 - val_loss: 0.1679\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1258 - val_loss: 0.2289\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1456 - val_loss: 0.1509\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1550 - val_loss: 0.2914\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.10309348958131097\n",
      "Mean Absolute Error (MAE): 0.2366199163048912\n",
      "Root Mean Squared Error (RMSE): 0.32108174906293097\n",
      "Time taken: 432.3902051448822\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1090.1458 - val_loss: 877.5538\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 675.8121 - val_loss: 567.7314\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 455.9257 - val_loss: 385.9104\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 355.9538 - val_loss: 370.2531\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 317.7241 - val_loss: 321.7981\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 288.1918 - val_loss: 270.7763\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 240.7922 - val_loss: 366.2229\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 149.4932 - val_loss: 177.3272\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 79.5875 - val_loss: 72.6637\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 48.3106 - val_loss: 105.6802\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 35.9780 - val_loss: 160.0767\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 23.7094 - val_loss: 42.5264\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 17.8279 - val_loss: 24.2429\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.2268 - val_loss: 18.5680\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.5886 - val_loss: 9.8442\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.1082 - val_loss: 8.1639\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2913 - val_loss: 32.0427\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.6813 - val_loss: 24.9699\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.3317 - val_loss: 14.6778\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1243 - val_loss: 34.0644\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.2243 - val_loss: 109.2186\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4496 - val_loss: 5.5589\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6498 - val_loss: 3.1851\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0011 - val_loss: 15.8550\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7653 - val_loss: 7.9689\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2680 - val_loss: 11.5700\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3174 - val_loss: 55.9515\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0820 - val_loss: 72.4519\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4844 - val_loss: 14.8195\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3985 - val_loss: 2.4803\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5951 - val_loss: 8.3128\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4104 - val_loss: 2.9545\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8149 - val_loss: 6.4120\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7234 - val_loss: 61.3654\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.3759 - val_loss: 1.7964\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9259 - val_loss: 0.8368\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8667 - val_loss: 2.3715\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1445 - val_loss: 18.4343\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2350 - val_loss: 1.0151\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8131 - val_loss: 1.0883\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.8879 - val_loss: 12.3609\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9067 - val_loss: 1.4313\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7029 - val_loss: 1.1245\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5913 - val_loss: 1.5804\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6979 - val_loss: 0.6621\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6951 - val_loss: 0.9090\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6239 - val_loss: 9.5660\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6520 - val_loss: 0.9338\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7030 - val_loss: 2.3940\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7663 - val_loss: 16.4209\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.7357 - val_loss: 9.9016\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5248 - val_loss: 0.7132\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3822 - val_loss: 0.4560\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4429 - val_loss: 1.0566\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6393 - val_loss: 2.6656\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5569 - val_loss: 0.9359\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.2906 - val_loss: 5.9354\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6430 - val_loss: 0.5082\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5094 - val_loss: 0.7335\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4221 - val_loss: 0.4146\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3173 - val_loss: 0.6031\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9899 - val_loss: 3.6852\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4718 - val_loss: 0.3048\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3485 - val_loss: 0.8206\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3016 - val_loss: 0.4117\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3527 - val_loss: 0.3132\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4003 - val_loss: 1.1299\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4307 - val_loss: 0.6297\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4320 - val_loss: 1.4243\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5179 - val_loss: 0.7971\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.0585 - val_loss: 17.4845\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1797 - val_loss: 0.8847\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4529 - val_loss: 0.2703\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2524 - val_loss: 0.1997\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2521 - val_loss: 0.3207\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2639 - val_loss: 0.2181\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2419 - val_loss: 0.3053\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2743 - val_loss: 0.3829\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2974 - val_loss: 0.4988\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5122 - val_loss: 1.1208\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5016 - val_loss: 0.7497\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0111 - val_loss: 3.4731\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3944 - val_loss: 0.3923\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2436 - val_loss: 0.2555\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2131 - val_loss: 0.2656\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2511 - val_loss: 0.6032\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2768 - val_loss: 0.3150\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3078 - val_loss: 1.6834\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4224 - val_loss: 1.0211\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4017 - val_loss: 1.1475\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3514 - val_loss: 0.5852\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6607 - val_loss: 51.5748\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6545 - val_loss: 0.5134\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1862 - val_loss: 0.6808\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1761 - val_loss: 0.3950\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2271 - val_loss: 0.5545\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2013 - val_loss: 0.7424\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3320 - val_loss: 9.3849\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4751 - val_loss: 0.4728\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3559 - val_loss: 0.3427\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2395 - val_loss: 0.3277\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.3288 - val_loss: 0.5219\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2469 - val_loss: 0.2662\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3400 - val_loss: 3.4275\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.19969339028940145\n",
      "Mean Absolute Error (MAE): 0.3457615941384698\n",
      "Root Mean Squared Error (RMSE): 0.4468706639391329\n",
      "Time taken: 312.50624895095825\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1115.3494 - val_loss: 877.3229\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 800.0766 - val_loss: 659.9414\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 560.2663 - val_loss: 432.6193\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 403.4342 - val_loss: 380.5929\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 338.5548 - val_loss: 310.9182\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 300.3858 - val_loss: 283.8260\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 252.9770 - val_loss: 241.3064\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 170.7935 - val_loss: 161.4086\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 88.7582 - val_loss: 151.2591\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 57.3584 - val_loss: 73.8781\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 37.1945 - val_loss: 54.4078\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 25.6381 - val_loss: 37.9338\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.9724 - val_loss: 36.2673\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.4596 - val_loss: 14.8894\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 12.4496 - val_loss: 26.0965\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.4043 - val_loss: 19.7416\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.3839 - val_loss: 11.4361\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.4199 - val_loss: 18.7769\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.3933 - val_loss: 11.4317\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4293 - val_loss: 5.8427\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.6627 - val_loss: 7.3930\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.0383 - val_loss: 7.3270\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.2418 - val_loss: 14.5072\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6731 - val_loss: 15.1120\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.3513 - val_loss: 7.1475\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7058 - val_loss: 6.6898\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.1779 - val_loss: 37.4792\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2370 - val_loss: 3.7913\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9859 - val_loss: 2.9360\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8938 - val_loss: 3.7078\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9885 - val_loss: 7.4535\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5560 - val_loss: 5.8786\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4976 - val_loss: 2.6628\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3183 - val_loss: 1.6463\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6590 - val_loss: 8.4199\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7284 - val_loss: 12.7272\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.2584 - val_loss: 5.8566\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.9973 - val_loss: 5.6248\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9192 - val_loss: 1.8898\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5233 - val_loss: 34.1792\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0063 - val_loss: 4.6573\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2036 - val_loss: 2.6321\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0399 - val_loss: 3.9435\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9820 - val_loss: 0.9780\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9079 - val_loss: 1.0390\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7244 - val_loss: 1.0783\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8543 - val_loss: 1.0581\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9601 - val_loss: 1.6608\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8489 - val_loss: 1.0647\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8063 - val_loss: 1.9496\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.0647 - val_loss: 47.1662\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8726 - val_loss: 1.2611\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4517 - val_loss: 0.3936\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4512 - val_loss: 1.4494\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4804 - val_loss: 0.8132\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4551 - val_loss: 0.4944\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.2385 - val_loss: 2.9297\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5348 - val_loss: 0.3936\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5275 - val_loss: 0.5670\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4666 - val_loss: 0.5617\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3581 - val_loss: 0.8021\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3331 - val_loss: 0.4299\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3578 - val_loss: 0.5661\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4764 - val_loss: 1.0003\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8024 - val_loss: 16.7265\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8979 - val_loss: 0.8069\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5423 - val_loss: 2.1947\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5743 - val_loss: 0.8024\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4292 - val_loss: 1.1190\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5988 - val_loss: 0.5817\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7055 - val_loss: 0.6649\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2752 - val_loss: 0.2510\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2510 - val_loss: 0.7800\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5171 - val_loss: 0.8335\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2532 - val_loss: 0.2911\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2826 - val_loss: 0.4716\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4138 - val_loss: 1.4329\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4791 - val_loss: 2.2803\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5528 - val_loss: 4.3077\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0005 - val_loss: 0.7037\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2927 - val_loss: 0.9575\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3984 - val_loss: 2.2311\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4259 - val_loss: 5.9680\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9770 - val_loss: 0.4362\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2573 - val_loss: 0.3637\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1944 - val_loss: 0.2002\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1768 - val_loss: 0.3318\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1801 - val_loss: 0.3038\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1920 - val_loss: 0.2532\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2339 - val_loss: 0.3811\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2821 - val_loss: 0.7011\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3171 - val_loss: 0.6559\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3263 - val_loss: 0.6384\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5124 - val_loss: 23.4961\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4680 - val_loss: 0.8377\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4053 - val_loss: 0.9303\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2604 - val_loss: 1.0448\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3894 - val_loss: 3.4143\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4399 - val_loss: 3.8392\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1391 - val_loss: 86.3253\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9334 - val_loss: 0.2505\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1545 - val_loss: 0.4449\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1388 - val_loss: 0.2968\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1365 - val_loss: 0.3437\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1220 - val_loss: 0.1187\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1921 - val_loss: 1.9437\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1727 - val_loss: 0.2755\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2750 - val_loss: 0.2847\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2950 - val_loss: 2.9109\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3614 - val_loss: 1.3915\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2358 - val_loss: 0.2550\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1986 - val_loss: 0.5415\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3826 - val_loss: 0.3772\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3046 - val_loss: 0.7263\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2930 - val_loss: 1.2542\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1044 - val_loss: 2.6683\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2044 - val_loss: 0.1150\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1160 - val_loss: 0.1713\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1180 - val_loss: 0.1544\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1211 - val_loss: 0.1922\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1096 - val_loss: 0.1410\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1595 - val_loss: 0.2728\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1444 - val_loss: 0.4026\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1697 - val_loss: 0.2805\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1946 - val_loss: 0.3149\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1700 - val_loss: 0.4274\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2426 - val_loss: 1.7982\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4297 - val_loss: 0.6142\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1947 - val_loss: 1.4123\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3190 - val_loss: 0.6777\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1948 - val_loss: 0.2893\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1885 - val_loss: 0.5498\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2218 - val_loss: 1.4459\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3826 - val_loss: 2.5104\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2044 - val_loss: 0.5096\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1627 - val_loss: 4.7266\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3482 - val_loss: 1.0469\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2079 - val_loss: 0.3845\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2376 - val_loss: 6.5940\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2095 - val_loss: 0.8200\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1463 - val_loss: 0.2530\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2219 - val_loss: 0.4569\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4645 - val_loss: 344.7013\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6381 - val_loss: 0.2699\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1001 - val_loss: 0.1082\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0804 - val_loss: 0.0856\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0726 - val_loss: 0.0703\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0688 - val_loss: 0.1130\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0676 - val_loss: 0.0757\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0909 - val_loss: 0.1464\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1158 - val_loss: 0.2244\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1409 - val_loss: 0.3462\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1440 - val_loss: 0.2990\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1417 - val_loss: 0.2378\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1537 - val_loss: 0.8483\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2505 - val_loss: 0.6686\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1403 - val_loss: 0.3849\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1766 - val_loss: 1.1789\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1389 - val_loss: 0.3701\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1519 - val_loss: 0.4949\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1795 - val_loss: 0.9414\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3260 - val_loss: 0.1671\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0941 - val_loss: 0.1104\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0721 - val_loss: 0.0698\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0634 - val_loss: 0.0885\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0689 - val_loss: 0.1113\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0830 - val_loss: 0.1132\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1232 - val_loss: 0.2586\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1322 - val_loss: 0.4585\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1571 - val_loss: 0.3737\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1445 - val_loss: 0.1952\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1363 - val_loss: 0.4085\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1553 - val_loss: 0.2051\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1689 - val_loss: 0.2406\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1268 - val_loss: 0.4089\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1839 - val_loss: 1.0974\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2056 - val_loss: 0.6913\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2060 - val_loss: 0.4018\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1229 - val_loss: 0.2559\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1242 - val_loss: 0.1287\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0707 - val_loss: 0.0744\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0586 - val_loss: 0.1075\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0534 - val_loss: 0.0736\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0526 - val_loss: 0.0736\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0623 - val_loss: 0.1119\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0825 - val_loss: 0.1316\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0844 - val_loss: 0.2182\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0912 - val_loss: 0.1817\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0840 - val_loss: 0.2194\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1072 - val_loss: 0.1926\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1384 - val_loss: 0.4128\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1293 - val_loss: 0.2098\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1939 - val_loss: 0.6252\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2022 - val_loss: 0.3478\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.0697971676929618\n",
      "Mean Absolute Error (MAE): 0.1952417764928759\n",
      "Root Mean Squared Error (RMSE): 0.2641915359979608\n",
      "Time taken: 576.582836151123\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1069.1470 - val_loss: 867.2279\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 694.6701 - val_loss: 662.5438\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 482.5030 - val_loss: 418.0261\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 376.3655 - val_loss: 368.1574\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 325.5168 - val_loss: 577.1213\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 288.1807 - val_loss: 405.3416\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 226.7276 - val_loss: 199.2018\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 130.7972 - val_loss: 107.7826\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 73.0724 - val_loss: 83.3226\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 47.0388 - val_loss: 54.6631\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 36.0787 - val_loss: 49.1229\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 24.0924 - val_loss: 20.0513\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 18.8959 - val_loss: 69.9006\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 14.4627 - val_loss: 26.5000\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.8508 - val_loss: 24.8787\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.2265 - val_loss: 48.1046\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.0197 - val_loss: 29.8255\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2726 - val_loss: 9.4897\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.0104 - val_loss: 18.4299\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.3995 - val_loss: 14.1431\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4135 - val_loss: 5.2618\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6384 - val_loss: 4.9316\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2173 - val_loss: 11.9300\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8478 - val_loss: 14.4991\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6625 - val_loss: 5.3963\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1871 - val_loss: 2.7172\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7110 - val_loss: 6.5229\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7668 - val_loss: 1.9316\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2699 - val_loss: 23.2835\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6761 - val_loss: 3.5457\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6190 - val_loss: 6.0119\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0188 - val_loss: 14.9135\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3701 - val_loss: 1.6592\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2075 - val_loss: 6.0623\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1899 - val_loss: 7.7762\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3729 - val_loss: 8.6391\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.7965 - val_loss: 8.2416\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1960 - val_loss: 5.5886\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9995 - val_loss: 2.8114\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8836 - val_loss: 0.9076\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8183 - val_loss: 4.8245\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9975 - val_loss: 0.7595\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.9391 - val_loss: 51.3997\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6034 - val_loss: 1.2759\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5826 - val_loss: 1.9016\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5916 - val_loss: 1.3908\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5590 - val_loss: 1.3818\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6279 - val_loss: 0.4007\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7175 - val_loss: 1.3372\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.9150 - val_loss: 31.2278\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1828 - val_loss: 0.4834\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4842 - val_loss: 0.6736\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4175 - val_loss: 0.4843\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4199 - val_loss: 0.8139\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4080 - val_loss: 0.8692\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5666 - val_loss: 4.2181\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3782 - val_loss: 1.0468\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4143 - val_loss: 0.4614\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7454 - val_loss: 0.7354\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6820 - val_loss: 4.0760\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8613 - val_loss: 0.7944\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4132 - val_loss: 1.2904\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4385 - val_loss: 0.7366\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.6628 - val_loss: 9.7454\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8804 - val_loss: 0.4201\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3110 - val_loss: 0.3841\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2896 - val_loss: 0.3980\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2768 - val_loss: 0.4235\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2937 - val_loss: 1.0954\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3651 - val_loss: 1.3180\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3237 - val_loss: 0.6191\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4108 - val_loss: 0.8468\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5058 - val_loss: 1.0266\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4278 - val_loss: 24.8780\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.4070 - val_loss: 0.7154\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3154 - val_loss: 0.2105\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2096 - val_loss: 0.2268\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2334 - val_loss: 0.1998\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1956 - val_loss: 0.2254\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2053 - val_loss: 0.2947\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3174 - val_loss: 0.6241\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2996 - val_loss: 0.5154\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3809 - val_loss: 0.7223\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4001 - val_loss: 1.1275\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5620 - val_loss: 4.1044\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0112 - val_loss: 9.3418\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5958 - val_loss: 0.4267\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2005 - val_loss: 0.2044\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2110 - val_loss: 0.1683\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2002 - val_loss: 0.6584\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2293 - val_loss: 0.4430\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2524 - val_loss: 0.3358\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3472 - val_loss: 2.3568\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4420 - val_loss: 0.5123\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2932 - val_loss: 1.7778\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4559 - val_loss: 1.8688\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3669 - val_loss: 1.7385\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3060 - val_loss: 22.1634\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.1580 - val_loss: 131.4594\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6239 - val_loss: 0.3424\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1860 - val_loss: 0.2465\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3089 - val_loss: 0.4128\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1451 - val_loss: 0.2170\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1468 - val_loss: 0.6460\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1663 - val_loss: 0.2073\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1981 - val_loss: 0.4409\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2110 - val_loss: 0.5371\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2884 - val_loss: 1.5197\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3393 - val_loss: 0.5504\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2394 - val_loss: 0.3456\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3195 - val_loss: 0.7694\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0197 - val_loss: 22.8865\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3918 - val_loss: 0.3424\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1256 - val_loss: 0.1737\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1232 - val_loss: 0.2368\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1387 - val_loss: 0.3316\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3239 - val_loss: 0.3009\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1888 - val_loss: 0.2507\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1971 - val_loss: 0.4884\n",
      "16/16 [==============================] - 1s 23ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.16833217452632454\n",
      "Mean Absolute Error (MAE): 0.30539936218092567\n",
      "Root Mean Squared Error (RMSE): 0.4102830419677671\n",
      "Time taken: 350.2068409919739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_3420\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.120344  0.252637  0.346906  441.768983\n",
      "1        2  0.103093  0.236620  0.321082  432.390205\n",
      "2        3  0.199693  0.345762  0.446871  312.506249\n",
      "3        4  0.069797  0.195242  0.264192  576.582836\n",
      "4        5  0.168332  0.305399  0.410283  350.206841\n",
      "5  Average  0.132252  0.267132  0.357867  422.691023\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f_over.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_iReg_f_over.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_iReg_f_over.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
