{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_iReg_f_over.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.476682</td>\n",
       "      <td>193.330550</td>\n",
       "      <td>96.162106</td>\n",
       "      <td>142.324408</td>\n",
       "      <td>204.069137</td>\n",
       "      <td>226.487294</td>\n",
       "      <td>159.507560</td>\n",
       "      <td>188.621279</td>\n",
       "      <td>145.304518</td>\n",
       "      <td>162.856305</td>\n",
       "      <td>...</td>\n",
       "      <td>123.466814</td>\n",
       "      <td>137.342275</td>\n",
       "      <td>136.770446</td>\n",
       "      <td>131.359859</td>\n",
       "      <td>186.077649</td>\n",
       "      <td>181.399217</td>\n",
       "      <td>154.185528</td>\n",
       "      <td>189.427030</td>\n",
       "      <td>78.635342</td>\n",
       "      <td>137.662134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.640865</td>\n",
       "      <td>193.421432</td>\n",
       "      <td>96.473737</td>\n",
       "      <td>142.538333</td>\n",
       "      <td>203.821631</td>\n",
       "      <td>226.279209</td>\n",
       "      <td>159.292625</td>\n",
       "      <td>188.367463</td>\n",
       "      <td>145.516914</td>\n",
       "      <td>163.023106</td>\n",
       "      <td>...</td>\n",
       "      <td>123.548958</td>\n",
       "      <td>137.168631</td>\n",
       "      <td>137.034249</td>\n",
       "      <td>131.572686</td>\n",
       "      <td>185.877640</td>\n",
       "      <td>181.117865</td>\n",
       "      <td>154.137553</td>\n",
       "      <td>189.195777</td>\n",
       "      <td>78.208883</td>\n",
       "      <td>137.446677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.804876</td>\n",
       "      <td>193.513963</td>\n",
       "      <td>96.785755</td>\n",
       "      <td>142.754701</td>\n",
       "      <td>203.574562</td>\n",
       "      <td>226.072615</td>\n",
       "      <td>159.074802</td>\n",
       "      <td>188.115384</td>\n",
       "      <td>145.723935</td>\n",
       "      <td>163.189982</td>\n",
       "      <td>...</td>\n",
       "      <td>123.632229</td>\n",
       "      <td>136.999740</td>\n",
       "      <td>137.298953</td>\n",
       "      <td>131.785763</td>\n",
       "      <td>185.675631</td>\n",
       "      <td>180.833507</td>\n",
       "      <td>154.094457</td>\n",
       "      <td>188.969105</td>\n",
       "      <td>77.784021</td>\n",
       "      <td>137.231304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.969023</td>\n",
       "      <td>193.608218</td>\n",
       "      <td>97.098013</td>\n",
       "      <td>142.973684</td>\n",
       "      <td>203.327923</td>\n",
       "      <td>225.867702</td>\n",
       "      <td>158.854006</td>\n",
       "      <td>187.865118</td>\n",
       "      <td>145.925562</td>\n",
       "      <td>163.356997</td>\n",
       "      <td>...</td>\n",
       "      <td>123.716187</td>\n",
       "      <td>136.835852</td>\n",
       "      <td>137.564418</td>\n",
       "      <td>131.999238</td>\n",
       "      <td>185.471432</td>\n",
       "      <td>180.546277</td>\n",
       "      <td>154.056215</td>\n",
       "      <td>188.747092</td>\n",
       "      <td>77.360992</td>\n",
       "      <td>137.016240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.133541</td>\n",
       "      <td>193.704239</td>\n",
       "      <td>97.410161</td>\n",
       "      <td>143.195258</td>\n",
       "      <td>203.081681</td>\n",
       "      <td>225.664618</td>\n",
       "      <td>158.630091</td>\n",
       "      <td>187.616890</td>\n",
       "      <td>146.121871</td>\n",
       "      <td>163.524314</td>\n",
       "      <td>...</td>\n",
       "      <td>123.800229</td>\n",
       "      <td>136.677121</td>\n",
       "      <td>137.830676</td>\n",
       "      <td>132.213338</td>\n",
       "      <td>185.265040</td>\n",
       "      <td>180.256474</td>\n",
       "      <td>154.022577</td>\n",
       "      <td>188.529873</td>\n",
       "      <td>76.940078</td>\n",
       "      <td>136.801560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>186.890133</td>\n",
       "      <td>167.136498</td>\n",
       "      <td>124.028245</td>\n",
       "      <td>85.043260</td>\n",
       "      <td>231.545990</td>\n",
       "      <td>216.365295</td>\n",
       "      <td>189.006691</td>\n",
       "      <td>169.742748</td>\n",
       "      <td>155.697636</td>\n",
       "      <td>143.211613</td>\n",
       "      <td>...</td>\n",
       "      <td>122.724440</td>\n",
       "      <td>157.900833</td>\n",
       "      <td>134.673921</td>\n",
       "      <td>111.367070</td>\n",
       "      <td>196.520345</td>\n",
       "      <td>182.752655</td>\n",
       "      <td>187.431706</td>\n",
       "      <td>171.551963</td>\n",
       "      <td>125.922063</td>\n",
       "      <td>87.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>186.702775</td>\n",
       "      <td>166.895061</td>\n",
       "      <td>123.939206</td>\n",
       "      <td>85.009440</td>\n",
       "      <td>231.401355</td>\n",
       "      <td>216.246492</td>\n",
       "      <td>189.010895</td>\n",
       "      <td>169.791443</td>\n",
       "      <td>155.558933</td>\n",
       "      <td>142.925403</td>\n",
       "      <td>...</td>\n",
       "      <td>122.509028</td>\n",
       "      <td>157.845609</td>\n",
       "      <td>134.558118</td>\n",
       "      <td>111.428789</td>\n",
       "      <td>196.649728</td>\n",
       "      <td>182.907850</td>\n",
       "      <td>187.353642</td>\n",
       "      <td>171.458225</td>\n",
       "      <td>126.062032</td>\n",
       "      <td>87.723378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>186.514267</td>\n",
       "      <td>166.652393</td>\n",
       "      <td>123.846834</td>\n",
       "      <td>84.976688</td>\n",
       "      <td>231.254031</td>\n",
       "      <td>216.126424</td>\n",
       "      <td>189.009812</td>\n",
       "      <td>169.838965</td>\n",
       "      <td>155.421963</td>\n",
       "      <td>142.636550</td>\n",
       "      <td>...</td>\n",
       "      <td>122.292300</td>\n",
       "      <td>157.786215</td>\n",
       "      <td>134.441261</td>\n",
       "      <td>111.491544</td>\n",
       "      <td>196.781185</td>\n",
       "      <td>183.064300</td>\n",
       "      <td>187.272978</td>\n",
       "      <td>171.364782</td>\n",
       "      <td>126.203656</td>\n",
       "      <td>87.886959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>186.323815</td>\n",
       "      <td>166.408256</td>\n",
       "      <td>123.751067</td>\n",
       "      <td>84.945188</td>\n",
       "      <td>231.103987</td>\n",
       "      <td>216.005000</td>\n",
       "      <td>189.003431</td>\n",
       "      <td>169.885537</td>\n",
       "      <td>155.286893</td>\n",
       "      <td>142.345518</td>\n",
       "      <td>...</td>\n",
       "      <td>122.074546</td>\n",
       "      <td>157.723039</td>\n",
       "      <td>134.323109</td>\n",
       "      <td>111.555569</td>\n",
       "      <td>196.914248</td>\n",
       "      <td>183.222101</td>\n",
       "      <td>187.189564</td>\n",
       "      <td>171.271735</td>\n",
       "      <td>126.346672</td>\n",
       "      <td>88.051739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>186.130811</td>\n",
       "      <td>166.162262</td>\n",
       "      <td>123.652019</td>\n",
       "      <td>84.915006</td>\n",
       "      <td>230.951197</td>\n",
       "      <td>215.882337</td>\n",
       "      <td>188.991762</td>\n",
       "      <td>169.931555</td>\n",
       "      <td>155.153987</td>\n",
       "      <td>142.052720</td>\n",
       "      <td>...</td>\n",
       "      <td>121.855963</td>\n",
       "      <td>157.656580</td>\n",
       "      <td>134.203592</td>\n",
       "      <td>111.621060</td>\n",
       "      <td>197.048575</td>\n",
       "      <td>183.381282</td>\n",
       "      <td>187.103513</td>\n",
       "      <td>171.179020</td>\n",
       "      <td>126.490913</td>\n",
       "      <td>88.217522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     165.476682  193.330550   96.162106  142.324408  204.069137  226.487294   \n",
       "1     165.640865  193.421432   96.473737  142.538333  203.821631  226.279209   \n",
       "2     165.804876  193.513963   96.785755  142.754701  203.574562  226.072615   \n",
       "3     165.969023  193.608218   97.098013  142.973684  203.327923  225.867702   \n",
       "4     166.133541  193.704239   97.410161  143.195258  203.081681  225.664618   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  186.890133  167.136498  124.028245   85.043260  231.545990  216.365295   \n",
       "2439  186.702775  166.895061  123.939206   85.009440  231.401355  216.246492   \n",
       "2440  186.514267  166.652393  123.846834   84.976688  231.254031  216.126424   \n",
       "2441  186.323815  166.408256  123.751067   84.945188  231.103987  216.005000   \n",
       "2442  186.130811  166.162262  123.652019   84.915006  230.951197  215.882337   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     159.507560  188.621279  145.304518  162.856305  ...  123.466814   \n",
       "1     159.292625  188.367463  145.516914  163.023106  ...  123.548958   \n",
       "2     159.074802  188.115384  145.723935  163.189982  ...  123.632229   \n",
       "3     158.854006  187.865118  145.925562  163.356997  ...  123.716187   \n",
       "4     158.630091  187.616890  146.121871  163.524314  ...  123.800229   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  189.006691  169.742748  155.697636  143.211613  ...  122.724440   \n",
       "2439  189.010895  169.791443  155.558933  142.925403  ...  122.509028   \n",
       "2440  189.009812  169.838965  155.421963  142.636550  ...  122.292300   \n",
       "2441  189.003431  169.885537  155.286893  142.345518  ...  122.074546   \n",
       "2442  188.991762  169.931555  155.153987  142.052720  ...  121.855963   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     137.342275  136.770446  131.359859  186.077649  181.399217  154.185528   \n",
       "1     137.168631  137.034249  131.572686  185.877640  181.117865  154.137553   \n",
       "2     136.999740  137.298953  131.785763  185.675631  180.833507  154.094457   \n",
       "3     136.835852  137.564418  131.999238  185.471432  180.546277  154.056215   \n",
       "4     136.677121  137.830676  132.213338  185.265040  180.256474  154.022577   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  157.900833  134.673921  111.367070  196.520345  182.752655  187.431706   \n",
       "2439  157.845609  134.558118  111.428789  196.649728  182.907850  187.353642   \n",
       "2440  157.786215  134.441261  111.491544  196.781185  183.064300  187.272978   \n",
       "2441  157.723039  134.323109  111.555569  196.914248  183.222101  187.189564   \n",
       "2442  157.656580  134.203592  111.621060  197.048575  183.381282  187.103513   \n",
       "\n",
       "              45          46          47  \n",
       "0     189.427030   78.635342  137.662134  \n",
       "1     189.195777   78.208883  137.446677  \n",
       "2     188.969105   77.784021  137.231304  \n",
       "3     188.747092   77.360992  137.016240  \n",
       "4     188.529873   76.940078  136.801560  \n",
       "...          ...         ...         ...  \n",
       "2438  171.551963  125.922063   87.561227  \n",
       "2439  171.458225  126.062032   87.723378  \n",
       "2440  171.364782  126.203656   87.886959  \n",
       "2441  171.271735  126.346672   88.051739  \n",
       "2442  171.179020  126.490913   88.217522  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_iReg_f.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.476682</td>\n",
       "      <td>193.330550</td>\n",
       "      <td>96.162106</td>\n",
       "      <td>142.324408</td>\n",
       "      <td>204.069137</td>\n",
       "      <td>226.487294</td>\n",
       "      <td>159.507560</td>\n",
       "      <td>188.621279</td>\n",
       "      <td>145.304518</td>\n",
       "      <td>162.856305</td>\n",
       "      <td>...</td>\n",
       "      <td>123.466814</td>\n",
       "      <td>137.342275</td>\n",
       "      <td>136.770446</td>\n",
       "      <td>131.359859</td>\n",
       "      <td>186.077649</td>\n",
       "      <td>181.399217</td>\n",
       "      <td>154.185528</td>\n",
       "      <td>189.427030</td>\n",
       "      <td>78.635342</td>\n",
       "      <td>137.662134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.640865</td>\n",
       "      <td>193.421432</td>\n",
       "      <td>96.473737</td>\n",
       "      <td>142.538333</td>\n",
       "      <td>203.821631</td>\n",
       "      <td>226.279209</td>\n",
       "      <td>159.292625</td>\n",
       "      <td>188.367463</td>\n",
       "      <td>145.516914</td>\n",
       "      <td>163.023106</td>\n",
       "      <td>...</td>\n",
       "      <td>123.548958</td>\n",
       "      <td>137.168631</td>\n",
       "      <td>137.034249</td>\n",
       "      <td>131.572686</td>\n",
       "      <td>185.877640</td>\n",
       "      <td>181.117865</td>\n",
       "      <td>154.137553</td>\n",
       "      <td>189.195777</td>\n",
       "      <td>78.208883</td>\n",
       "      <td>137.446677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.804876</td>\n",
       "      <td>193.513963</td>\n",
       "      <td>96.785755</td>\n",
       "      <td>142.754701</td>\n",
       "      <td>203.574562</td>\n",
       "      <td>226.072615</td>\n",
       "      <td>159.074802</td>\n",
       "      <td>188.115384</td>\n",
       "      <td>145.723935</td>\n",
       "      <td>163.189982</td>\n",
       "      <td>...</td>\n",
       "      <td>123.632229</td>\n",
       "      <td>136.999740</td>\n",
       "      <td>137.298953</td>\n",
       "      <td>131.785763</td>\n",
       "      <td>185.675631</td>\n",
       "      <td>180.833507</td>\n",
       "      <td>154.094457</td>\n",
       "      <td>188.969105</td>\n",
       "      <td>77.784021</td>\n",
       "      <td>137.231304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.969023</td>\n",
       "      <td>193.608218</td>\n",
       "      <td>97.098013</td>\n",
       "      <td>142.973684</td>\n",
       "      <td>203.327923</td>\n",
       "      <td>225.867702</td>\n",
       "      <td>158.854006</td>\n",
       "      <td>187.865118</td>\n",
       "      <td>145.925562</td>\n",
       "      <td>163.356997</td>\n",
       "      <td>...</td>\n",
       "      <td>123.716187</td>\n",
       "      <td>136.835852</td>\n",
       "      <td>137.564418</td>\n",
       "      <td>131.999238</td>\n",
       "      <td>185.471432</td>\n",
       "      <td>180.546277</td>\n",
       "      <td>154.056215</td>\n",
       "      <td>188.747092</td>\n",
       "      <td>77.360992</td>\n",
       "      <td>137.016240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.133541</td>\n",
       "      <td>193.704239</td>\n",
       "      <td>97.410161</td>\n",
       "      <td>143.195258</td>\n",
       "      <td>203.081681</td>\n",
       "      <td>225.664618</td>\n",
       "      <td>158.630091</td>\n",
       "      <td>187.616890</td>\n",
       "      <td>146.121871</td>\n",
       "      <td>163.524314</td>\n",
       "      <td>...</td>\n",
       "      <td>123.800229</td>\n",
       "      <td>136.677121</td>\n",
       "      <td>137.830676</td>\n",
       "      <td>132.213338</td>\n",
       "      <td>185.265040</td>\n",
       "      <td>180.256474</td>\n",
       "      <td>154.022577</td>\n",
       "      <td>188.529873</td>\n",
       "      <td>76.940078</td>\n",
       "      <td>136.801560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>186.890133</td>\n",
       "      <td>167.136498</td>\n",
       "      <td>124.028245</td>\n",
       "      <td>85.043260</td>\n",
       "      <td>231.545990</td>\n",
       "      <td>216.365295</td>\n",
       "      <td>189.006691</td>\n",
       "      <td>169.742748</td>\n",
       "      <td>155.697636</td>\n",
       "      <td>143.211613</td>\n",
       "      <td>...</td>\n",
       "      <td>122.724440</td>\n",
       "      <td>157.900833</td>\n",
       "      <td>134.673921</td>\n",
       "      <td>111.367070</td>\n",
       "      <td>196.520345</td>\n",
       "      <td>182.752655</td>\n",
       "      <td>187.431706</td>\n",
       "      <td>171.551963</td>\n",
       "      <td>125.922063</td>\n",
       "      <td>87.561227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>186.702775</td>\n",
       "      <td>166.895061</td>\n",
       "      <td>123.939206</td>\n",
       "      <td>85.009440</td>\n",
       "      <td>231.401355</td>\n",
       "      <td>216.246492</td>\n",
       "      <td>189.010895</td>\n",
       "      <td>169.791443</td>\n",
       "      <td>155.558933</td>\n",
       "      <td>142.925403</td>\n",
       "      <td>...</td>\n",
       "      <td>122.509028</td>\n",
       "      <td>157.845609</td>\n",
       "      <td>134.558118</td>\n",
       "      <td>111.428789</td>\n",
       "      <td>196.649728</td>\n",
       "      <td>182.907850</td>\n",
       "      <td>187.353642</td>\n",
       "      <td>171.458225</td>\n",
       "      <td>126.062032</td>\n",
       "      <td>87.723378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>186.514267</td>\n",
       "      <td>166.652393</td>\n",
       "      <td>123.846834</td>\n",
       "      <td>84.976688</td>\n",
       "      <td>231.254031</td>\n",
       "      <td>216.126424</td>\n",
       "      <td>189.009812</td>\n",
       "      <td>169.838965</td>\n",
       "      <td>155.421963</td>\n",
       "      <td>142.636550</td>\n",
       "      <td>...</td>\n",
       "      <td>122.292300</td>\n",
       "      <td>157.786215</td>\n",
       "      <td>134.441261</td>\n",
       "      <td>111.491544</td>\n",
       "      <td>196.781185</td>\n",
       "      <td>183.064300</td>\n",
       "      <td>187.272978</td>\n",
       "      <td>171.364782</td>\n",
       "      <td>126.203656</td>\n",
       "      <td>87.886959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>186.323815</td>\n",
       "      <td>166.408256</td>\n",
       "      <td>123.751067</td>\n",
       "      <td>84.945188</td>\n",
       "      <td>231.103987</td>\n",
       "      <td>216.005000</td>\n",
       "      <td>189.003431</td>\n",
       "      <td>169.885537</td>\n",
       "      <td>155.286893</td>\n",
       "      <td>142.345518</td>\n",
       "      <td>...</td>\n",
       "      <td>122.074546</td>\n",
       "      <td>157.723039</td>\n",
       "      <td>134.323109</td>\n",
       "      <td>111.555569</td>\n",
       "      <td>196.914248</td>\n",
       "      <td>183.222101</td>\n",
       "      <td>187.189564</td>\n",
       "      <td>171.271735</td>\n",
       "      <td>126.346672</td>\n",
       "      <td>88.051739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>186.130811</td>\n",
       "      <td>166.162262</td>\n",
       "      <td>123.652019</td>\n",
       "      <td>84.915006</td>\n",
       "      <td>230.951197</td>\n",
       "      <td>215.882337</td>\n",
       "      <td>188.991762</td>\n",
       "      <td>169.931555</td>\n",
       "      <td>155.153987</td>\n",
       "      <td>142.052720</td>\n",
       "      <td>...</td>\n",
       "      <td>121.855963</td>\n",
       "      <td>157.656580</td>\n",
       "      <td>134.203592</td>\n",
       "      <td>111.621060</td>\n",
       "      <td>197.048575</td>\n",
       "      <td>183.381282</td>\n",
       "      <td>187.103513</td>\n",
       "      <td>171.179020</td>\n",
       "      <td>126.490913</td>\n",
       "      <td>88.217522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.476682  193.330550   96.162106  142.324408  204.069137  226.487294   \n",
       "1     165.640865  193.421432   96.473737  142.538333  203.821631  226.279209   \n",
       "2     165.804876  193.513963   96.785755  142.754701  203.574562  226.072615   \n",
       "3     165.969023  193.608218   97.098013  142.973684  203.327923  225.867702   \n",
       "4     166.133541  193.704239   97.410161  143.195258  203.081681  225.664618   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  186.890133  167.136498  124.028245   85.043260  231.545990  216.365295   \n",
       "2439  186.702775  166.895061  123.939206   85.009440  231.401355  216.246492   \n",
       "2440  186.514267  166.652393  123.846834   84.976688  231.254031  216.126424   \n",
       "2441  186.323815  166.408256  123.751067   84.945188  231.103987  216.005000   \n",
       "2442  186.130811  166.162262  123.652019   84.915006  230.951197  215.882337   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     159.507560  188.621279  145.304518  162.856305  ...  123.466814   \n",
       "1     159.292625  188.367463  145.516914  163.023106  ...  123.548958   \n",
       "2     159.074802  188.115384  145.723935  163.189982  ...  123.632229   \n",
       "3     158.854006  187.865118  145.925562  163.356997  ...  123.716187   \n",
       "4     158.630091  187.616890  146.121871  163.524314  ...  123.800229   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  189.006691  169.742748  155.697636  143.211613  ...  122.724440   \n",
       "2439  189.010895  169.791443  155.558933  142.925403  ...  122.509028   \n",
       "2440  189.009812  169.838965  155.421963  142.636550  ...  122.292300   \n",
       "2441  189.003431  169.885537  155.286893  142.345518  ...  122.074546   \n",
       "2442  188.991762  169.931555  155.153987  142.052720  ...  121.855963   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     137.342275  136.770446  131.359859  186.077649  181.399217  154.185528   \n",
       "1     137.168631  137.034249  131.572686  185.877640  181.117865  154.137553   \n",
       "2     136.999740  137.298953  131.785763  185.675631  180.833507  154.094457   \n",
       "3     136.835852  137.564418  131.999238  185.471432  180.546277  154.056215   \n",
       "4     136.677121  137.830676  132.213338  185.265040  180.256474  154.022577   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  157.900833  134.673921  111.367070  196.520345  182.752655  187.431706   \n",
       "2439  157.845609  134.558118  111.428789  196.649728  182.907850  187.353642   \n",
       "2440  157.786215  134.441261  111.491544  196.781185  183.064300  187.272978   \n",
       "2441  157.723039  134.323109  111.555569  196.914248  183.222101  187.189564   \n",
       "2442  157.656580  134.203592  111.621060  197.048575  183.381282  187.103513   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     189.427030   78.635342  137.662134  \n",
       "1     189.195777   78.208883  137.446677  \n",
       "2     188.969105   77.784021  137.231304  \n",
       "3     188.747092   77.360992  137.016240  \n",
       "4     188.529873   76.940078  136.801560  \n",
       "...          ...         ...         ...  \n",
       "2438  171.551963  125.922063   87.561227  \n",
       "2439  171.458225  126.062032   87.723378  \n",
       "2440  171.364782  126.203656   87.886959  \n",
       "2441  171.271735  126.346672   88.051739  \n",
       "2442  171.179020  126.490913   88.217522  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.476682</td>\n",
       "      <td>193.330550</td>\n",
       "      <td>96.162106</td>\n",
       "      <td>142.324408</td>\n",
       "      <td>204.069137</td>\n",
       "      <td>226.487294</td>\n",
       "      <td>159.507560</td>\n",
       "      <td>188.621279</td>\n",
       "      <td>145.304518</td>\n",
       "      <td>162.856305</td>\n",
       "      <td>107.596247</td>\n",
       "      <td>134.096562</td>\n",
       "      <td>188.493278</td>\n",
       "      <td>200.317079</td>\n",
       "      <td>169.804415</td>\n",
       "      <td>180.304011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.640865</td>\n",
       "      <td>193.421432</td>\n",
       "      <td>96.473737</td>\n",
       "      <td>142.538333</td>\n",
       "      <td>203.821631</td>\n",
       "      <td>226.279209</td>\n",
       "      <td>159.292625</td>\n",
       "      <td>188.367463</td>\n",
       "      <td>145.516914</td>\n",
       "      <td>163.023106</td>\n",
       "      <td>107.914383</td>\n",
       "      <td>134.231446</td>\n",
       "      <td>188.307685</td>\n",
       "      <td>200.204129</td>\n",
       "      <td>169.354158</td>\n",
       "      <td>179.973971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.804876</td>\n",
       "      <td>193.513963</td>\n",
       "      <td>96.785755</td>\n",
       "      <td>142.754701</td>\n",
       "      <td>203.574562</td>\n",
       "      <td>226.072615</td>\n",
       "      <td>159.074802</td>\n",
       "      <td>188.115384</td>\n",
       "      <td>145.723935</td>\n",
       "      <td>163.189982</td>\n",
       "      <td>108.228351</td>\n",
       "      <td>134.364754</td>\n",
       "      <td>188.119757</td>\n",
       "      <td>200.090371</td>\n",
       "      <td>168.906354</td>\n",
       "      <td>179.643260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.969023</td>\n",
       "      <td>193.608218</td>\n",
       "      <td>97.098013</td>\n",
       "      <td>142.973684</td>\n",
       "      <td>203.327923</td>\n",
       "      <td>225.867702</td>\n",
       "      <td>158.854006</td>\n",
       "      <td>187.865118</td>\n",
       "      <td>145.925562</td>\n",
       "      <td>163.356997</td>\n",
       "      <td>108.538275</td>\n",
       "      <td>134.496496</td>\n",
       "      <td>187.929448</td>\n",
       "      <td>199.975818</td>\n",
       "      <td>168.461286</td>\n",
       "      <td>179.311859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.133541</td>\n",
       "      <td>193.704239</td>\n",
       "      <td>97.410161</td>\n",
       "      <td>143.195258</td>\n",
       "      <td>203.081681</td>\n",
       "      <td>225.664618</td>\n",
       "      <td>158.630091</td>\n",
       "      <td>187.616890</td>\n",
       "      <td>146.121871</td>\n",
       "      <td>163.524314</td>\n",
       "      <td>108.844455</td>\n",
       "      <td>134.626579</td>\n",
       "      <td>187.736829</td>\n",
       "      <td>199.860346</td>\n",
       "      <td>168.019134</td>\n",
       "      <td>178.979892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>186.890133</td>\n",
       "      <td>167.136498</td>\n",
       "      <td>124.028245</td>\n",
       "      <td>85.043260</td>\n",
       "      <td>231.545990</td>\n",
       "      <td>216.365295</td>\n",
       "      <td>189.006691</td>\n",
       "      <td>169.742748</td>\n",
       "      <td>155.697636</td>\n",
       "      <td>143.211613</td>\n",
       "      <td>108.076708</td>\n",
       "      <td>90.668429</td>\n",
       "      <td>208.205398</td>\n",
       "      <td>200.971157</td>\n",
       "      <td>182.710224</td>\n",
       "      <td>173.339021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>186.702775</td>\n",
       "      <td>166.895061</td>\n",
       "      <td>123.939206</td>\n",
       "      <td>85.009440</td>\n",
       "      <td>231.401355</td>\n",
       "      <td>216.246492</td>\n",
       "      <td>189.010895</td>\n",
       "      <td>169.791443</td>\n",
       "      <td>155.558933</td>\n",
       "      <td>142.925403</td>\n",
       "      <td>107.977577</td>\n",
       "      <td>90.699818</td>\n",
       "      <td>208.136708</td>\n",
       "      <td>200.910375</td>\n",
       "      <td>182.821499</td>\n",
       "      <td>173.492392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>186.514267</td>\n",
       "      <td>166.652393</td>\n",
       "      <td>123.846834</td>\n",
       "      <td>84.976688</td>\n",
       "      <td>231.254031</td>\n",
       "      <td>216.126424</td>\n",
       "      <td>189.009812</td>\n",
       "      <td>169.838965</td>\n",
       "      <td>155.421963</td>\n",
       "      <td>142.636550</td>\n",
       "      <td>107.878952</td>\n",
       "      <td>90.730368</td>\n",
       "      <td>208.068298</td>\n",
       "      <td>200.851833</td>\n",
       "      <td>182.931557</td>\n",
       "      <td>173.643298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>186.323815</td>\n",
       "      <td>166.408256</td>\n",
       "      <td>123.751067</td>\n",
       "      <td>84.945188</td>\n",
       "      <td>231.103987</td>\n",
       "      <td>216.005000</td>\n",
       "      <td>189.003431</td>\n",
       "      <td>169.885537</td>\n",
       "      <td>155.286893</td>\n",
       "      <td>142.345518</td>\n",
       "      <td>107.780761</td>\n",
       "      <td>90.759883</td>\n",
       "      <td>208.000456</td>\n",
       "      <td>200.795929</td>\n",
       "      <td>183.040288</td>\n",
       "      <td>173.791279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>186.130811</td>\n",
       "      <td>166.162262</td>\n",
       "      <td>123.652019</td>\n",
       "      <td>84.915006</td>\n",
       "      <td>230.951197</td>\n",
       "      <td>215.882337</td>\n",
       "      <td>188.991762</td>\n",
       "      <td>169.931555</td>\n",
       "      <td>155.153987</td>\n",
       "      <td>142.052720</td>\n",
       "      <td>107.682861</td>\n",
       "      <td>90.788209</td>\n",
       "      <td>207.933469</td>\n",
       "      <td>200.742832</td>\n",
       "      <td>183.147499</td>\n",
       "      <td>173.936032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.476682  193.330550   96.162106  142.324408  204.069137  226.487294   \n",
       "1     165.640865  193.421432   96.473737  142.538333  203.821631  226.279209   \n",
       "2     165.804876  193.513963   96.785755  142.754701  203.574562  226.072615   \n",
       "3     165.969023  193.608218   97.098013  142.973684  203.327923  225.867702   \n",
       "4     166.133541  193.704239   97.410161  143.195258  203.081681  225.664618   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  186.890133  167.136498  124.028245   85.043260  231.545990  216.365295   \n",
       "2439  186.702775  166.895061  123.939206   85.009440  231.401355  216.246492   \n",
       "2440  186.514267  166.652393  123.846834   84.976688  231.254031  216.126424   \n",
       "2441  186.323815  166.408256  123.751067   84.945188  231.103987  216.005000   \n",
       "2442  186.130811  166.162262  123.652019   84.915006  230.951197  215.882337   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10    sensor11    sensor12  \\\n",
       "0     159.507560  188.621279  145.304518  162.856305  107.596247  134.096562   \n",
       "1     159.292625  188.367463  145.516914  163.023106  107.914383  134.231446   \n",
       "2     159.074802  188.115384  145.723935  163.189982  108.228351  134.364754   \n",
       "3     158.854006  187.865118  145.925562  163.356997  108.538275  134.496496   \n",
       "4     158.630091  187.616890  146.121871  163.524314  108.844455  134.626579   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  189.006691  169.742748  155.697636  143.211613  108.076708   90.668429   \n",
       "2439  189.010895  169.791443  155.558933  142.925403  107.977577   90.699818   \n",
       "2440  189.009812  169.838965  155.421963  142.636550  107.878952   90.730368   \n",
       "2441  189.003431  169.885537  155.286893  142.345518  107.780761   90.759883   \n",
       "2442  188.991762  169.931555  155.153987  142.052720  107.682861   90.788209   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     188.493278  200.317079  169.804415  180.304011  \n",
       "1     188.307685  200.204129  169.354158  179.973971  \n",
       "2     188.119757  200.090371  168.906354  179.643260  \n",
       "3     187.929448  199.975818  168.461286  179.311859  \n",
       "4     187.736829  199.860346  168.019134  178.979892  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  208.205398  200.971157  182.710224  173.339021  \n",
       "2439  208.136708  200.910375  182.821499  173.492392  \n",
       "2440  208.068298  200.851833  182.931557  173.643298  \n",
       "2441  208.000456  200.795929  183.040288  173.791279  \n",
       "2442  207.933469  200.742832  183.147499  173.936032  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 14s 11ms/step - loss: 1105.8287 - val_loss: 804.9569\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 632.2764 - val_loss: 490.3082\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 386.2763 - val_loss: 265.8195\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 220.3156 - val_loss: 151.1045\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 107.9809 - val_loss: 83.0146\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 42.6799 - val_loss: 24.1284\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.7436 - val_loss: 14.2249\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.1826 - val_loss: 4.7727\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.2153 - val_loss: 5.7910\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0485 - val_loss: 9.8004\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6875 - val_loss: 2.7831\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6522 - val_loss: 5.4239\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.9922 - val_loss: 2.3233\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.1633 - val_loss: 5.2710\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0294 - val_loss: 7.4642\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7932 - val_loss: 3.5770\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5888 - val_loss: 1.3680\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6671 - val_loss: 2.8429\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8351 - val_loss: 3.3683\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7137 - val_loss: 1.1061\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4071 - val_loss: 3.5585\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5194 - val_loss: 7.3860\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5887 - val_loss: 0.8174\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1382 - val_loss: 1.6281\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0434 - val_loss: 1.4967\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1227 - val_loss: 2.7844\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2972 - val_loss: 1.1845\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7797 - val_loss: 10.5657\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7756 - val_loss: 1.4424\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8404 - val_loss: 1.3035\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0879 - val_loss: 0.8343\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6721 - val_loss: 0.4705\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2976 - val_loss: 1.1292\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9688 - val_loss: 2.1131\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7274 - val_loss: 2.2729\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5977 - val_loss: 0.7244\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8870 - val_loss: 2.0987\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7590 - val_loss: 0.4557\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1201 - val_loss: 0.4361\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7564 - val_loss: 0.6605\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9915 - val_loss: 0.6805\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0186 - val_loss: 0.4563\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4695 - val_loss: 0.2343\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8222 - val_loss: 1.5195\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6017 - val_loss: 0.2984\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5476 - val_loss: 0.3294\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1556 - val_loss: 1.9614\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6989 - val_loss: 0.9240\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8128 - val_loss: 1.5024\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6030 - val_loss: 0.2202\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4223 - val_loss: 0.7967\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3973 - val_loss: 0.3639\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3754 - val_loss: 0.2128\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6871 - val_loss: 1.6959\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2878 - val_loss: 0.7959\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3209 - val_loss: 0.3146\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2013 - val_loss: 0.1307\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2869 - val_loss: 0.4565\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3342 - val_loss: 0.2781\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3243 - val_loss: 0.5897\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5305 - val_loss: 0.9143\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3006 - val_loss: 0.9428\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1226 - val_loss: 16.8248\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4837 - val_loss: 0.2724\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2590 - val_loss: 0.4453\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2268 - val_loss: 0.4391\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6037 - val_loss: 0.2886\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2095 - val_loss: 0.2472\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2571 - val_loss: 0.5512\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3593 - val_loss: 0.2586\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3965 - val_loss: 0.4139\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2685 - val_loss: 0.2911\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7985 - val_loss: 6.8124\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5102 - val_loss: 0.2719\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2405 - val_loss: 0.3449\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1893 - val_loss: 0.2560\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1822 - val_loss: 0.1535\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1606 - val_loss: 0.4050\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2780 - val_loss: 2.9689\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4755 - val_loss: 0.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1592 - val_loss: 0.2100\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1717 - val_loss: 0.1425\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1339 - val_loss: 0.1798\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1348 - val_loss: 0.1683\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1844 - val_loss: 0.3970\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3055 - val_loss: 0.4583\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1413 - val_loss: 1.5366\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.1306691252647545\n",
      "Mean Absolute Error (MAE): 0.2697989802792979\n",
      "Root Mean Squared Error (RMSE): 0.36148184638340347\n",
      "Time taken: 269.95443415641785\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 1044.4451 - val_loss: 696.9604\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 559.8731 - val_loss: 439.2410\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 376.2683 - val_loss: 301.2016\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 265.9848 - val_loss: 325.2503\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 181.0969 - val_loss: 155.7677\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 84.1451 - val_loss: 81.0889\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 39.4517 - val_loss: 41.3117\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 16.7592 - val_loss: 8.9914\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 9.3732 - val_loss: 10.6579\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 5.8126 - val_loss: 3.8336\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.2767 - val_loss: 1.7647\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.6918 - val_loss: 4.3284\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8240 - val_loss: 11.0255\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8168 - val_loss: 8.5020\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1558 - val_loss: 1.2379\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2079 - val_loss: 3.7322\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9464 - val_loss: 4.8766\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9668 - val_loss: 4.2704\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7221 - val_loss: 1.0780\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3686 - val_loss: 9.7356\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6726 - val_loss: 1.6165\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6279 - val_loss: 2.9320\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4595 - val_loss: 2.8146\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0760 - val_loss: 0.6744\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0526 - val_loss: 2.5570\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.5767 - val_loss: 6.6134\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.5112 - val_loss: 1.2635\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9708 - val_loss: 0.5132\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7118 - val_loss: 0.8252\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6573 - val_loss: 0.3559\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8304 - val_loss: 2.0033\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9523 - val_loss: 0.5359\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5886 - val_loss: 0.8535\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6400 - val_loss: 0.9176\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6284 - val_loss: 1.2175\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5752 - val_loss: 2.5228\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3753 - val_loss: 0.4161\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5058 - val_loss: 1.6395\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5961 - val_loss: 2.1455\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6489 - val_loss: 1.3487\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8062 - val_loss: 4.1120\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0182 - val_loss: 0.5056\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5559 - val_loss: 1.3532\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4689 - val_loss: 1.2287\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8845 - val_loss: 0.6062\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5143 - val_loss: 2.4941\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3908 - val_loss: 0.2955\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4370 - val_loss: 0.9654\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4392 - val_loss: 0.4540\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4757 - val_loss: 1.6807\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5997 - val_loss: 1.2686\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9153 - val_loss: 2.2478\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4690 - val_loss: 0.4439\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4751 - val_loss: 0.6304\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5602 - val_loss: 0.8990\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4069 - val_loss: 0.2756\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.0355 - val_loss: 8.8579\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4830 - val_loss: 0.2587\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3330 - val_loss: 0.2318\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2370 - val_loss: 0.1120\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1642 - val_loss: 0.1149\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1893 - val_loss: 0.3957\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2886 - val_loss: 0.2206\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2414 - val_loss: 1.0613\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3630 - val_loss: 0.2978\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4069 - val_loss: 0.6491\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3507 - val_loss: 3.4278\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5037 - val_loss: 0.6610\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1851 - val_loss: 0.2739\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2547 - val_loss: 0.6790\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3277 - val_loss: 0.3072\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2008 - val_loss: 0.4166\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2037 - val_loss: 1.1384\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4074 - val_loss: 0.3637\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2394 - val_loss: 0.1608\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2392 - val_loss: 0.1733\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2792 - val_loss: 0.1692\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1894 - val_loss: 0.3162\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5024 - val_loss: 1.0150\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6001 - val_loss: 2.8068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3269 - val_loss: 0.4178\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1489 - val_loss: 0.2614\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1933 - val_loss: 1.3854\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4392 - val_loss: 0.2098\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1881 - val_loss: 0.5202\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1219 - val_loss: 0.1167\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1531 - val_loss: 0.2009\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3011 - val_loss: 0.5584\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2249 - val_loss: 1.3267\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1873 - val_loss: 0.5134\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.11203081612928086\n",
      "Mean Absolute Error (MAE): 0.2419457506523022\n",
      "Root Mean Squared Error (RMSE): 0.3347100478463126\n",
      "Time taken: 291.0508863925934\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1080.7184 - val_loss: 815.2533\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 652.3237 - val_loss: 555.6414\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 426.8646 - val_loss: 357.5860\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 270.5137 - val_loss: 227.3569\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 155.7381 - val_loss: 132.9689\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 71.5736 - val_loss: 49.2061\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 29.9562 - val_loss: 21.0435\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 16.0848 - val_loss: 11.2548\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.2669 - val_loss: 14.7735\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 6.0339 - val_loss: 3.8293\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.0661 - val_loss: 4.5559\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.4305 - val_loss: 2.9807\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6063 - val_loss: 1.7936\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0643 - val_loss: 4.6120\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0402 - val_loss: 1.3476\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2391 - val_loss: 1.3776\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7818 - val_loss: 1.1155\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5395 - val_loss: 2.3490\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.2742 - val_loss: 2.6551\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.7665 - val_loss: 0.8351\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.3394 - val_loss: 0.7650\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1927 - val_loss: 0.8545\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6795 - val_loss: 2.4952\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.8517 - val_loss: 5.2531\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 1.4804 - val_loss: 2.7138\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.1251 - val_loss: 1.4377\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9860 - val_loss: 0.8703\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8034 - val_loss: 0.7662\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0353 - val_loss: 1.2557\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0614 - val_loss: 8.2703\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7027 - val_loss: 2.5840\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7916 - val_loss: 0.6138\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6829 - val_loss: 0.8388\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6576 - val_loss: 0.6877\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8952 - val_loss: 4.6276\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7700 - val_loss: 0.5484\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3860 - val_loss: 36.6325\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.7824 - val_loss: 0.4946\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5550 - val_loss: 0.3293\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3912 - val_loss: 0.4120\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4555 - val_loss: 0.4379\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4315 - val_loss: 0.2659\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4919 - val_loss: 0.4695\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9783 - val_loss: 3.0041\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0088 - val_loss: 0.6383\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3452 - val_loss: 0.4747\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5742 - val_loss: 0.3717\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6352 - val_loss: 1.2524\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5268 - val_loss: 0.7094\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4821 - val_loss: 0.4348\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7266 - val_loss: 4.3016\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1087 - val_loss: 3.3337\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2006 - val_loss: 0.4219\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3531 - val_loss: 1.4286\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2576 - val_loss: 0.2593\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5550 - val_loss: 0.7549\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5184 - val_loss: 1.0534\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6796 - val_loss: 0.5463\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6487 - val_loss: 0.4722\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3882 - val_loss: 0.3174\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3061 - val_loss: 0.4635\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2368 - val_loss: 0.2435\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2289 - val_loss: 0.4380\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2346 - val_loss: 0.2963\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2022 - val_loss: 0.1562\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4051 - val_loss: 0.1334\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2833 - val_loss: 0.9189\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4025 - val_loss: 0.3132\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9173 - val_loss: 0.9731\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3853 - val_loss: 0.3754\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1988 - val_loss: 0.3134\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5231 - val_loss: 1.7310\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7431 - val_loss: 0.4328\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3631 - val_loss: 0.6548\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2818 - val_loss: 0.3982\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2902 - val_loss: 0.1970\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1833 - val_loss: 0.2364\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4018 - val_loss: 2.6743\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9303 - val_loss: 1.4947\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2541 - val_loss: 0.2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5108 - val_loss: 107.5617\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.2361 - val_loss: 0.5928\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2637 - val_loss: 0.1019\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2061 - val_loss: 0.2895\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1805 - val_loss: 0.1040\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1279 - val_loss: 0.1285\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2162 - val_loss: 0.2769\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2143 - val_loss: 0.2845\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4490 - val_loss: 0.3020\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2069 - val_loss: 0.5997\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2053 - val_loss: 0.1322\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1619 - val_loss: 0.1975\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1987 - val_loss: 0.1856\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2452 - val_loss: 0.2163\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3460 - val_loss: 0.2790\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2320 - val_loss: 0.3611\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1548 - val_loss: 0.1617\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2128 - val_loss: 0.4254\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3640 - val_loss: 0.5269\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2526 - val_loss: 0.1911\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0629 - val_loss: 0.2656\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2157 - val_loss: 0.0904\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1356 - val_loss: 0.2012\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1098 - val_loss: 0.0642\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1140 - val_loss: 0.1302\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1345 - val_loss: 0.5426\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2724 - val_loss: 0.6491\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2350 - val_loss: 0.4366\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1902 - val_loss: 0.0916\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1459 - val_loss: 0.1379\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1730 - val_loss: 0.1939\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5929 - val_loss: 28.8629\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4104 - val_loss: 0.1257\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1612 - val_loss: 0.0970\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1542 - val_loss: 0.0921\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0937 - val_loss: 0.0769\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1092 - val_loss: 0.1703\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1129 - val_loss: 0.1076\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1016 - val_loss: 0.1558\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0950 - val_loss: 0.1276\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7259 - val_loss: 37.4044\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6621 - val_loss: 0.2071\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1027 - val_loss: 0.1628\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0929 - val_loss: 0.0997\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0853 - val_loss: 0.0831\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0791 - val_loss: 0.1094\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1413 - val_loss: 0.1923\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1576 - val_loss: 0.3214\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1466 - val_loss: 0.3295\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1910 - val_loss: 0.2019\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1731 - val_loss: 0.2063\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7582 - val_loss: 0.6043\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2462 - val_loss: 0.1571\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1241 - val_loss: 0.0616\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0858 - val_loss: 0.0499\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0722 - val_loss: 0.0927\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0549 - val_loss: 0.0653\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0769 - val_loss: 0.1112\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0738 - val_loss: 0.1114\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0922 - val_loss: 0.1082\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0835 - val_loss: 0.2216\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1633 - val_loss: 0.1481\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2547 - val_loss: 0.1462\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1120 - val_loss: 0.1086\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1942 - val_loss: 0.3223\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1284 - val_loss: 0.2117\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2546 - val_loss: 0.3749\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1002 - val_loss: 0.2370\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0880 - val_loss: 0.0865\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0687 - val_loss: 0.1166\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1036 - val_loss: 0.1799\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0898 - val_loss: 0.0808\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1075 - val_loss: 0.3748\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2631 - val_loss: 0.5359\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2487 - val_loss: 0.1656\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1505 - val_loss: 0.0748\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0838 - val_loss: 0.0451\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0563 - val_loss: 0.0627\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0716 - val_loss: 0.0570\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0597 - val_loss: 0.0483\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0589 - val_loss: 0.0811\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0917 - val_loss: 0.1039\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0998 - val_loss: 0.0858\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1159 - val_loss: 0.2947\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1373 - val_loss: 0.1448\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2541 - val_loss: 0.1668\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0920 - val_loss: 0.3114\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1572 - val_loss: 0.1184\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7988 - val_loss: 0.1052\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0801 - val_loss: 0.0724\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0652 - val_loss: 0.0581\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0773 - val_loss: 0.0993\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1164 - val_loss: 0.0705\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0842 - val_loss: 0.0579\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1921 - val_loss: 0.4859\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0992 - val_loss: 0.0846\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0896 - val_loss: 0.1166\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1433 - val_loss: 0.2768\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0178 - val_loss: 0.4057\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0913 - val_loss: 0.0928\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0534 - val_loss: 0.0617\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0441 - val_loss: 0.0659\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0493 - val_loss: 0.0780\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0690 - val_loss: 0.0964\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0737 - val_loss: 0.1376\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0929 - val_loss: 0.2941\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8273 - val_loss: 0.3222\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.045060973078207424\n",
      "Mean Absolute Error (MAE): 0.15683449103442806\n",
      "Root Mean Squared Error (RMSE): 0.21227570063058895\n",
      "Time taken: 580.9634239673615\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1057.9562 - val_loss: 786.8021\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 617.6901 - val_loss: 519.6912\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 402.5506 - val_loss: 311.3132\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 240.1657 - val_loss: 202.0202\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 127.8792 - val_loss: 154.8165\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 58.3527 - val_loss: 174.0468\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 24.9474 - val_loss: 24.8842\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.5690 - val_loss: 10.3128\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.2718 - val_loss: 11.1502\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7381 - val_loss: 3.6612\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.6335 - val_loss: 8.1451\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3011 - val_loss: 2.5851\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1993 - val_loss: 2.9441\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.9935 - val_loss: 30.6390\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7493 - val_loss: 2.1081\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5073 - val_loss: 2.5011\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0622 - val_loss: 11.0832\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7981 - val_loss: 1.4108\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6424 - val_loss: 2.9023\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3128 - val_loss: 12.7009\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6576 - val_loss: 2.5499\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8758 - val_loss: 0.7392\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3050 - val_loss: 0.9901\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5158 - val_loss: 1.3063\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5223 - val_loss: 1.1171\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8205 - val_loss: 1.5445\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1300 - val_loss: 5.0546\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0327 - val_loss: 4.8660\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8660 - val_loss: 0.8143\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0087 - val_loss: 8.0341\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3819 - val_loss: 1.9431\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6040 - val_loss: 0.9331\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6219 - val_loss: 1.4251\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6442 - val_loss: 1.7007\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9645 - val_loss: 1.4628\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8638 - val_loss: 0.6944\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6660 - val_loss: 1.0100\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8314 - val_loss: 0.9910\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2055 - val_loss: 2.7444\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9177 - val_loss: 1.0755\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5817 - val_loss: 1.9117\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4633 - val_loss: 1.6543\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8748 - val_loss: 3.5093\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7354 - val_loss: 0.7899\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4575 - val_loss: 0.5146\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6106 - val_loss: 7.5808\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7875 - val_loss: 0.8242\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4300 - val_loss: 0.7522\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1413 - val_loss: 1.5551\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5354 - val_loss: 0.7209\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3840 - val_loss: 1.0460\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3264 - val_loss: 0.4328\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7688 - val_loss: 12.8918\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8946 - val_loss: 0.3446\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2590 - val_loss: 0.2504\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2469 - val_loss: 0.1550\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2440 - val_loss: 0.3085\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3159 - val_loss: 0.4433\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5962 - val_loss: 1.1612\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4875 - val_loss: 1.2644\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2545 - val_loss: 0.6429\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4636 - val_loss: 0.9783\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0610 - val_loss: 0.6282\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2550 - val_loss: 0.3836\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1707 - val_loss: 0.2623\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2483 - val_loss: 0.4445\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2018 - val_loss: 0.1737\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1804 - val_loss: 0.1722\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2124 - val_loss: 0.2271\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2157 - val_loss: 0.4394\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4360 - val_loss: 8.2495\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2752 - val_loss: 0.2492\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2583 - val_loss: 0.5413\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1851 - val_loss: 0.2694\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2804 - val_loss: 0.4690\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3204 - val_loss: 0.5397\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2726 - val_loss: 0.5413\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0590 - val_loss: 7.2484\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2214 - val_loss: 0.3210\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1664 - val_loss: 0.1398\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3758 - val_loss: 0.8768\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2053 - val_loss: 0.1919\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2616 - val_loss: 1.4021\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3231 - val_loss: 0.4111\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2361 - val_loss: 0.7057\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7256 - val_loss: 24.8139\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7746 - val_loss: 1.5145\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2427 - val_loss: 0.2431\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1287 - val_loss: 0.2796\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1452 - val_loss: 0.5266\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2686 - val_loss: 0.8208\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4593 - val_loss: 0.2637\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7771 - val_loss: 0.5401\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3472 - val_loss: 0.2222\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1822 - val_loss: 0.1122\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1237 - val_loss: 0.0748\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1037 - val_loss: 0.1784\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1465 - val_loss: 0.2661\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2180 - val_loss: 0.1680\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1493 - val_loss: 0.1553\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1652 - val_loss: 0.3161\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2869 - val_loss: 0.5716\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3846 - val_loss: 0.2525\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1324 - val_loss: 0.1746\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1885 - val_loss: 2.2200\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2067 - val_loss: 0.3597\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1069 - val_loss: 0.1085\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1280 - val_loss: 0.0603\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1417 - val_loss: 0.1338\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1729 - val_loss: 0.1519\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1427 - val_loss: 0.1595\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1814 - val_loss: 0.4373\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2061 - val_loss: 0.1555\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1943 - val_loss: 0.2748\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1945 - val_loss: 0.2218\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2045 - val_loss: 0.1862\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7827 - val_loss: 23.1817\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9592 - val_loss: 0.2057\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0949 - val_loss: 0.0362\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0765 - val_loss: 0.0883\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1002 - val_loss: 0.4530\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1257 - val_loss: 0.1735\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0951 - val_loss: 0.2074\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1449 - val_loss: 0.0753\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2014 - val_loss: 0.8698\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1602 - val_loss: 0.1054\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1918 - val_loss: 0.6892\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1707 - val_loss: 1.7363\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1874 - val_loss: 0.1034\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1030 - val_loss: 0.0910\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1111 - val_loss: 0.1108\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0844 - val_loss: 0.0622\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1302 - val_loss: 0.2812\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1398 - val_loss: 0.0843\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0867 - val_loss: 0.2309\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1640 - val_loss: 0.4908\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5689 - val_loss: 1.1111\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2967 - val_loss: 0.2355\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2239 - val_loss: 0.2795\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0942 - val_loss: 0.1173\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0880 - val_loss: 0.1115\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1372 - val_loss: 0.5150\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2081 - val_loss: 0.1642\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1884 - val_loss: 0.3388\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4893 - val_loss: 4.4895\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1603 - val_loss: 0.4807\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1174 - val_loss: 0.3568\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1172 - val_loss: 0.1238\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1250 - val_loss: 0.1237\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.03619658926803981\n",
      "Mean Absolute Error (MAE): 0.13793761309023225\n",
      "Root Mean Squared Error (RMSE): 0.19025401248867213\n",
      "Time taken: 449.0828812122345\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 12ms/step - loss: 1005.0970 - val_loss: 717.6319\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 549.6553 - val_loss: 454.2360\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 380.4352 - val_loss: 348.3840\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 278.3474 - val_loss: 297.4018\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 187.2742 - val_loss: 152.1642\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 87.3059 - val_loss: 57.7908\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 38.3389 - val_loss: 23.8194\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.6893 - val_loss: 16.0613\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.8947 - val_loss: 7.4010\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.1750 - val_loss: 2.6113\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.0005 - val_loss: 3.1492\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6952 - val_loss: 2.4839\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6873 - val_loss: 1.4897\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4427 - val_loss: 1.6846\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5817 - val_loss: 1.4160\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8599 - val_loss: 1.0377\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2636 - val_loss: 8.6878\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2660 - val_loss: 2.5816\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4364 - val_loss: 1.0172\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2316 - val_loss: 2.2753\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9375 - val_loss: 2.3741\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3803 - val_loss: 22.4899\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0316 - val_loss: 1.9652\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1130 - val_loss: 4.2592\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4253 - val_loss: 1.8140\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.0934 - val_loss: 6.4034\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1044 - val_loss: 0.6629\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8802 - val_loss: 0.9176\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8556 - val_loss: 1.4663\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.9013 - val_loss: 10.8369\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1790 - val_loss: 1.8433\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7033 - val_loss: 0.7859\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6759 - val_loss: 0.9614\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5385 - val_loss: 0.3309\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5679 - val_loss: 1.4764\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1405 - val_loss: 7.5818\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5915 - val_loss: 0.2727\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4262 - val_loss: 0.4601\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7873 - val_loss: 0.8394\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8271 - val_loss: 1.2220\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9390 - val_loss: 1.3707\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6236 - val_loss: 0.5474\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5367 - val_loss: 0.5598\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6280 - val_loss: 2.2015\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9617 - val_loss: 1.2662\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7254 - val_loss: 0.4978\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4499 - val_loss: 0.4208\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3866 - val_loss: 0.4242\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9189 - val_loss: 0.5590\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3278 - val_loss: 0.9186\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3521 - val_loss: 0.6098\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3290 - val_loss: 0.4189\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3357 - val_loss: 0.4313\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3666 - val_loss: 1.4130\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7308 - val_loss: 0.4754\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3868 - val_loss: 0.6820\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3212 - val_loss: 0.3037\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3491 - val_loss: 0.5820\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8241 - val_loss: 43.7577\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2044 - val_loss: 0.6550\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3963 - val_loss: 0.2311\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2931 - val_loss: 0.1984\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2793 - val_loss: 1.3178\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2893 - val_loss: 0.3402\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2311 - val_loss: 0.5914\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3405 - val_loss: 0.3097\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3269 - val_loss: 1.3293\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4359 - val_loss: 0.9338\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2103 - val_loss: 0.6926\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4817 - val_loss: 0.3534\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3526 - val_loss: 0.9151\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1459 - val_loss: 0.7956\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2448 - val_loss: 1.4062\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2457 - val_loss: 0.2913\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2048 - val_loss: 0.3000\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5047 - val_loss: 0.2513\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2027 - val_loss: 0.4399\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1477 - val_loss: 0.1893\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2634 - val_loss: 1.0048\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2998 - val_loss: 0.2043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1740 - val_loss: 0.2649\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5923 - val_loss: 1.1949\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4365 - val_loss: 0.2188\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1814 - val_loss: 0.1243\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1736 - val_loss: 0.2997\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1574 - val_loss: 0.1669\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1612 - val_loss: 0.2276\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1239 - val_loss: 0.1032\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2510 - val_loss: 0.4414\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2349 - val_loss: 0.1946\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2083 - val_loss: 0.5370\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2470 - val_loss: 0.2397\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8474 - val_loss: 5.8730\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6313 - val_loss: 0.1709\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1254 - val_loss: 0.1431\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1051 - val_loss: 0.2770\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1398 - val_loss: 0.2292\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1356 - val_loss: 0.1811\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1294 - val_loss: 0.3255\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2100 - val_loss: 0.3960\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2067 - val_loss: 0.2086\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3147 - val_loss: 2.1620\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3686 - val_loss: 0.1949\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1134 - val_loss: 0.3773\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1050 - val_loss: 0.3283\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0949 - val_loss: 0.0829\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1530 - val_loss: 0.4688\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0920 - val_loss: 0.1982\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1609 - val_loss: 0.2498\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1630 - val_loss: 0.1884\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2745 - val_loss: 0.2424\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2208 - val_loss: 0.3528\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.8703 - val_loss: 0.3330\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1769 - val_loss: 0.1853\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1258 - val_loss: 0.1436\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1004 - val_loss: 0.0923\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1015 - val_loss: 0.0726\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0918 - val_loss: 0.1207\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0922 - val_loss: 0.3042\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1198 - val_loss: 0.2088\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1649 - val_loss: 0.4062\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1705 - val_loss: 0.1850\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2396 - val_loss: 1.5870\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2542 - val_loss: 0.5843\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1563 - val_loss: 0.1750\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1465 - val_loss: 0.4429\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2221 - val_loss: 0.1329\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5106 - val_loss: 0.2541\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2238 - val_loss: 0.4942\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0959 - val_loss: 0.1084\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0821 - val_loss: 0.1194\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0732 - val_loss: 0.0606\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0883 - val_loss: 0.2126\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1036 - val_loss: 0.0929\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0994 - val_loss: 0.1067\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2007 - val_loss: 0.8746\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2594 - val_loss: 0.0937\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1141 - val_loss: 0.2377\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2087 - val_loss: 0.1536\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1615 - val_loss: 0.5240\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2903 - val_loss: 0.6240\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1008 - val_loss: 0.0566\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0640 - val_loss: 0.0643\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0623 - val_loss: 0.1233\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1096 - val_loss: 0.2489\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1227 - val_loss: 0.1196\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0771 - val_loss: 0.1630\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2610 - val_loss: 0.2382\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0696 - val_loss: 0.1305\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1038 - val_loss: 0.2444\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1381 - val_loss: 0.1116\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1669 - val_loss: 0.1983\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1392 - val_loss: 0.3080\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2214 - val_loss: 0.1021\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7209 - val_loss: 13.1131\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1997 - val_loss: 0.3888\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0736 - val_loss: 0.0538\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0728 - val_loss: 0.0558\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0649 - val_loss: 0.0948\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0806 - val_loss: 0.1694\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0587 - val_loss: 0.0911\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0668 - val_loss: 0.0767\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0926 - val_loss: 0.1671\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1181 - val_loss: 0.1078\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1422 - val_loss: 0.2188\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1563 - val_loss: 0.9805\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1111 - val_loss: 0.0908\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5270 - val_loss: 30.3941\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2876 - val_loss: 0.2316\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0958 - val_loss: 0.1008\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0561 - val_loss: 0.0382\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0466 - val_loss: 0.0327\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0419 - val_loss: 0.0370\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0691 - val_loss: 0.0799\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0661 - val_loss: 0.2339\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0661 - val_loss: 0.0646\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0985 - val_loss: 0.8139\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1667 - val_loss: 0.2680\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1775 - val_loss: 0.1741\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1427 - val_loss: 0.1867\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0702 - val_loss: 0.0418\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0455 - val_loss: 0.0424\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0480 - val_loss: 0.0585\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0485 - val_loss: 0.0856\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0957 - val_loss: 0.1240\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0843 - val_loss: 0.0336\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.0669 - val_loss: 0.1681\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1842 - val_loss: 0.1954\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0982 - val_loss: 0.0699\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1063 - val_loss: 0.1648\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7530 - val_loss: 31.7203\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7786 - val_loss: 0.1178\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0644 - val_loss: 0.0448\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0466 - val_loss: 0.0645\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0381 - val_loss: 0.0214\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0382 - val_loss: 0.0227\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0451 - val_loss: 0.0794\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0477 - val_loss: 0.0571\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1104 - val_loss: 0.0578\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0776 - val_loss: 0.0522\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.05223519080166902\n",
      "Mean Absolute Error (MAE): 0.16014997014433527\n",
      "Root Mean Squared Error (RMSE): 0.22855019317792977\n",
      "Time taken: 605.5793058872223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_20000\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.130669  0.269799  0.361482  269.954434\n",
      "1        2  0.112031  0.241946  0.334710  291.050886\n",
      "2        3  0.045061  0.156834  0.212276  580.963424\n",
      "3        4  0.036197  0.137938  0.190254  449.082881\n",
      "4        5  0.052235  0.160150  0.228550  605.579306\n",
      "5  Average  0.075239  0.193333  0.265454  439.326186\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f_over.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_iReg_f_over.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_iReg_f_over.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
