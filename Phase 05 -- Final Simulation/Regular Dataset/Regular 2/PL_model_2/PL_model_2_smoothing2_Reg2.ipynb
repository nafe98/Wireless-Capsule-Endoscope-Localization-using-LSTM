{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_Reg2.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.632971</td>\n",
       "      <td>81.399266</td>\n",
       "      <td>77.250888</td>\n",
       "      <td>77.881737</td>\n",
       "      <td>87.606491</td>\n",
       "      <td>90.878011</td>\n",
       "      <td>83.683674</td>\n",
       "      <td>76.939886</td>\n",
       "      <td>77.811940</td>\n",
       "      <td>79.216417</td>\n",
       "      <td>...</td>\n",
       "      <td>80.122692</td>\n",
       "      <td>77.439776</td>\n",
       "      <td>80.094806</td>\n",
       "      <td>69.037217</td>\n",
       "      <td>92.465980</td>\n",
       "      <td>80.227891</td>\n",
       "      <td>82.639231</td>\n",
       "      <td>81.956382</td>\n",
       "      <td>68.796643</td>\n",
       "      <td>79.465122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.894162</td>\n",
       "      <td>81.456455</td>\n",
       "      <td>76.972121</td>\n",
       "      <td>77.906493</td>\n",
       "      <td>87.566765</td>\n",
       "      <td>90.759398</td>\n",
       "      <td>83.613337</td>\n",
       "      <td>77.200645</td>\n",
       "      <td>77.859044</td>\n",
       "      <td>79.324864</td>\n",
       "      <td>...</td>\n",
       "      <td>80.102728</td>\n",
       "      <td>77.443129</td>\n",
       "      <td>80.208063</td>\n",
       "      <td>69.342133</td>\n",
       "      <td>92.073577</td>\n",
       "      <td>80.326154</td>\n",
       "      <td>82.357046</td>\n",
       "      <td>81.884584</td>\n",
       "      <td>68.953000</td>\n",
       "      <td>79.397995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.147814</td>\n",
       "      <td>81.518846</td>\n",
       "      <td>76.695525</td>\n",
       "      <td>77.930667</td>\n",
       "      <td>87.527735</td>\n",
       "      <td>90.642030</td>\n",
       "      <td>83.540562</td>\n",
       "      <td>77.458909</td>\n",
       "      <td>77.911848</td>\n",
       "      <td>79.433134</td>\n",
       "      <td>...</td>\n",
       "      <td>80.082263</td>\n",
       "      <td>77.453403</td>\n",
       "      <td>80.322355</td>\n",
       "      <td>69.643368</td>\n",
       "      <td>91.684261</td>\n",
       "      <td>80.420395</td>\n",
       "      <td>82.077674</td>\n",
       "      <td>81.811890</td>\n",
       "      <td>69.101216</td>\n",
       "      <td>79.332075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.394387</td>\n",
       "      <td>81.586233</td>\n",
       "      <td>76.420343</td>\n",
       "      <td>77.954146</td>\n",
       "      <td>87.489211</td>\n",
       "      <td>90.526372</td>\n",
       "      <td>83.465142</td>\n",
       "      <td>77.714454</td>\n",
       "      <td>77.970389</td>\n",
       "      <td>79.541525</td>\n",
       "      <td>...</td>\n",
       "      <td>80.061302</td>\n",
       "      <td>77.470195</td>\n",
       "      <td>80.437383</td>\n",
       "      <td>69.941393</td>\n",
       "      <td>91.298301</td>\n",
       "      <td>80.510691</td>\n",
       "      <td>81.801362</td>\n",
       "      <td>81.738656</td>\n",
       "      <td>69.241014</td>\n",
       "      <td>79.267396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.634173</td>\n",
       "      <td>81.658761</td>\n",
       "      <td>76.145646</td>\n",
       "      <td>77.977021</td>\n",
       "      <td>87.451288</td>\n",
       "      <td>90.412748</td>\n",
       "      <td>83.387176</td>\n",
       "      <td>77.966851</td>\n",
       "      <td>78.034360</td>\n",
       "      <td>79.650200</td>\n",
       "      <td>...</td>\n",
       "      <td>80.040164</td>\n",
       "      <td>77.492911</td>\n",
       "      <td>80.552828</td>\n",
       "      <td>70.236678</td>\n",
       "      <td>90.915933</td>\n",
       "      <td>80.597446</td>\n",
       "      <td>81.528380</td>\n",
       "      <td>81.665320</td>\n",
       "      <td>69.371942</td>\n",
       "      <td>79.203816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>81.085500</td>\n",
       "      <td>83.269269</td>\n",
       "      <td>77.997856</td>\n",
       "      <td>67.669686</td>\n",
       "      <td>87.586293</td>\n",
       "      <td>86.693977</td>\n",
       "      <td>79.712469</td>\n",
       "      <td>83.950738</td>\n",
       "      <td>84.346201</td>\n",
       "      <td>78.132821</td>\n",
       "      <td>...</td>\n",
       "      <td>79.811065</td>\n",
       "      <td>81.004474</td>\n",
       "      <td>78.294866</td>\n",
       "      <td>67.316514</td>\n",
       "      <td>82.950869</td>\n",
       "      <td>80.483248</td>\n",
       "      <td>83.071120</td>\n",
       "      <td>81.965866</td>\n",
       "      <td>75.374322</td>\n",
       "      <td>65.513829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>80.871869</td>\n",
       "      <td>83.367393</td>\n",
       "      <td>78.059860</td>\n",
       "      <td>67.583630</td>\n",
       "      <td>87.631434</td>\n",
       "      <td>86.606911</td>\n",
       "      <td>79.477164</td>\n",
       "      <td>84.132178</td>\n",
       "      <td>84.549588</td>\n",
       "      <td>78.050669</td>\n",
       "      <td>...</td>\n",
       "      <td>79.786175</td>\n",
       "      <td>80.986456</td>\n",
       "      <td>78.193919</td>\n",
       "      <td>67.210842</td>\n",
       "      <td>82.792298</td>\n",
       "      <td>80.468536</td>\n",
       "      <td>83.056954</td>\n",
       "      <td>82.122991</td>\n",
       "      <td>75.464144</td>\n",
       "      <td>65.102826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>80.656301</td>\n",
       "      <td>83.466033</td>\n",
       "      <td>78.125031</td>\n",
       "      <td>67.499484</td>\n",
       "      <td>87.677524</td>\n",
       "      <td>86.518893</td>\n",
       "      <td>79.242501</td>\n",
       "      <td>84.317854</td>\n",
       "      <td>84.758155</td>\n",
       "      <td>77.968463</td>\n",
       "      <td>...</td>\n",
       "      <td>79.761260</td>\n",
       "      <td>80.965988</td>\n",
       "      <td>78.090956</td>\n",
       "      <td>67.102590</td>\n",
       "      <td>82.633703</td>\n",
       "      <td>80.454453</td>\n",
       "      <td>83.036478</td>\n",
       "      <td>82.281210</td>\n",
       "      <td>75.548762</td>\n",
       "      <td>64.685126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>80.439400</td>\n",
       "      <td>83.564900</td>\n",
       "      <td>78.192290</td>\n",
       "      <td>67.417331</td>\n",
       "      <td>87.723150</td>\n",
       "      <td>86.430248</td>\n",
       "      <td>79.008253</td>\n",
       "      <td>84.507677</td>\n",
       "      <td>84.972099</td>\n",
       "      <td>77.886868</td>\n",
       "      <td>...</td>\n",
       "      <td>79.737010</td>\n",
       "      <td>80.943514</td>\n",
       "      <td>77.985717</td>\n",
       "      <td>66.991973</td>\n",
       "      <td>82.475239</td>\n",
       "      <td>80.441972</td>\n",
       "      <td>83.009817</td>\n",
       "      <td>82.440347</td>\n",
       "      <td>75.628233</td>\n",
       "      <td>64.260938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>80.221609</td>\n",
       "      <td>83.663892</td>\n",
       "      <td>78.260620</td>\n",
       "      <td>67.337267</td>\n",
       "      <td>87.767240</td>\n",
       "      <td>86.341214</td>\n",
       "      <td>78.774122</td>\n",
       "      <td>84.701836</td>\n",
       "      <td>85.191428</td>\n",
       "      <td>77.806510</td>\n",
       "      <td>...</td>\n",
       "      <td>79.714033</td>\n",
       "      <td>80.919808</td>\n",
       "      <td>77.878151</td>\n",
       "      <td>66.879278</td>\n",
       "      <td>82.317245</td>\n",
       "      <td>80.431920</td>\n",
       "      <td>82.977138</td>\n",
       "      <td>82.600111</td>\n",
       "      <td>75.702741</td>\n",
       "      <td>63.830729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     78.632971  81.399266  77.250888  77.881737  87.606491  90.878011   \n",
       "1     78.894162  81.456455  76.972121  77.906493  87.566765  90.759398   \n",
       "2     79.147814  81.518846  76.695525  77.930667  87.527735  90.642030   \n",
       "3     79.394387  81.586233  76.420343  77.954146  87.489211  90.526372   \n",
       "4     79.634173  81.658761  76.145646  77.977021  87.451288  90.412748   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  81.085500  83.269269  77.997856  67.669686  87.586293  86.693977   \n",
       "2439  80.871869  83.367393  78.059860  67.583630  87.631434  86.606911   \n",
       "2440  80.656301  83.466033  78.125031  67.499484  87.677524  86.518893   \n",
       "2441  80.439400  83.564900  78.192290  67.417331  87.723150  86.430248   \n",
       "2442  80.221609  83.663892  78.260620  67.337267  87.767240  86.341214   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     83.683674  76.939886  77.811940  79.216417  ...  80.122692  77.439776   \n",
       "1     83.613337  77.200645  77.859044  79.324864  ...  80.102728  77.443129   \n",
       "2     83.540562  77.458909  77.911848  79.433134  ...  80.082263  77.453403   \n",
       "3     83.465142  77.714454  77.970389  79.541525  ...  80.061302  77.470195   \n",
       "4     83.387176  77.966851  78.034360  79.650200  ...  80.040164  77.492911   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  79.712469  83.950738  84.346201  78.132821  ...  79.811065  81.004474   \n",
       "2439  79.477164  84.132178  84.549588  78.050669  ...  79.786175  80.986456   \n",
       "2440  79.242501  84.317854  84.758155  77.968463  ...  79.761260  80.965988   \n",
       "2441  79.008253  84.507677  84.972099  77.886868  ...  79.737010  80.943514   \n",
       "2442  78.774122  84.701836  85.191428  77.806510  ...  79.714033  80.919808   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     80.094806  69.037217  92.465980  80.227891  82.639231  81.956382   \n",
       "1     80.208063  69.342133  92.073577  80.326154  82.357046  81.884584   \n",
       "2     80.322355  69.643368  91.684261  80.420395  82.077674  81.811890   \n",
       "3     80.437383  69.941393  91.298301  80.510691  81.801362  81.738656   \n",
       "4     80.552828  70.236678  90.915933  80.597446  81.528380  81.665320   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  78.294866  67.316514  82.950869  80.483248  83.071120  81.965866   \n",
       "2439  78.193919  67.210842  82.792298  80.468536  83.056954  82.122991   \n",
       "2440  78.090956  67.102590  82.633703  80.454453  83.036478  82.281210   \n",
       "2441  77.985717  66.991973  82.475239  80.441972  83.009817  82.440347   \n",
       "2442  77.878151  66.879278  82.317245  80.431920  82.977138  82.600111   \n",
       "\n",
       "             46         47  \n",
       "0     68.796643  79.465122  \n",
       "1     68.953000  79.397995  \n",
       "2     69.101216  79.332075  \n",
       "3     69.241014  79.267396  \n",
       "4     69.371942  79.203816  \n",
       "...         ...        ...  \n",
       "2438  75.374322  65.513829  \n",
       "2439  75.464144  65.102826  \n",
       "2440  75.548762  64.685126  \n",
       "2441  75.628233  64.260938  \n",
       "2442  75.702741  63.830729  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.632971</td>\n",
       "      <td>81.399266</td>\n",
       "      <td>77.250888</td>\n",
       "      <td>77.881737</td>\n",
       "      <td>87.606491</td>\n",
       "      <td>90.878011</td>\n",
       "      <td>83.683674</td>\n",
       "      <td>76.939886</td>\n",
       "      <td>77.811940</td>\n",
       "      <td>79.216417</td>\n",
       "      <td>...</td>\n",
       "      <td>80.122692</td>\n",
       "      <td>77.439776</td>\n",
       "      <td>80.094806</td>\n",
       "      <td>69.037217</td>\n",
       "      <td>92.465980</td>\n",
       "      <td>80.227891</td>\n",
       "      <td>82.639231</td>\n",
       "      <td>81.956382</td>\n",
       "      <td>68.796643</td>\n",
       "      <td>79.465122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.894162</td>\n",
       "      <td>81.456455</td>\n",
       "      <td>76.972121</td>\n",
       "      <td>77.906493</td>\n",
       "      <td>87.566765</td>\n",
       "      <td>90.759398</td>\n",
       "      <td>83.613337</td>\n",
       "      <td>77.200645</td>\n",
       "      <td>77.859044</td>\n",
       "      <td>79.324864</td>\n",
       "      <td>...</td>\n",
       "      <td>80.102728</td>\n",
       "      <td>77.443129</td>\n",
       "      <td>80.208063</td>\n",
       "      <td>69.342133</td>\n",
       "      <td>92.073577</td>\n",
       "      <td>80.326154</td>\n",
       "      <td>82.357046</td>\n",
       "      <td>81.884584</td>\n",
       "      <td>68.953000</td>\n",
       "      <td>79.397995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.147814</td>\n",
       "      <td>81.518846</td>\n",
       "      <td>76.695525</td>\n",
       "      <td>77.930667</td>\n",
       "      <td>87.527735</td>\n",
       "      <td>90.642030</td>\n",
       "      <td>83.540562</td>\n",
       "      <td>77.458909</td>\n",
       "      <td>77.911848</td>\n",
       "      <td>79.433134</td>\n",
       "      <td>...</td>\n",
       "      <td>80.082263</td>\n",
       "      <td>77.453403</td>\n",
       "      <td>80.322355</td>\n",
       "      <td>69.643368</td>\n",
       "      <td>91.684261</td>\n",
       "      <td>80.420395</td>\n",
       "      <td>82.077674</td>\n",
       "      <td>81.811890</td>\n",
       "      <td>69.101216</td>\n",
       "      <td>79.332075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.394387</td>\n",
       "      <td>81.586233</td>\n",
       "      <td>76.420343</td>\n",
       "      <td>77.954146</td>\n",
       "      <td>87.489211</td>\n",
       "      <td>90.526372</td>\n",
       "      <td>83.465142</td>\n",
       "      <td>77.714454</td>\n",
       "      <td>77.970389</td>\n",
       "      <td>79.541525</td>\n",
       "      <td>...</td>\n",
       "      <td>80.061302</td>\n",
       "      <td>77.470195</td>\n",
       "      <td>80.437383</td>\n",
       "      <td>69.941393</td>\n",
       "      <td>91.298301</td>\n",
       "      <td>80.510691</td>\n",
       "      <td>81.801362</td>\n",
       "      <td>81.738656</td>\n",
       "      <td>69.241014</td>\n",
       "      <td>79.267396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.634173</td>\n",
       "      <td>81.658761</td>\n",
       "      <td>76.145646</td>\n",
       "      <td>77.977021</td>\n",
       "      <td>87.451288</td>\n",
       "      <td>90.412748</td>\n",
       "      <td>83.387176</td>\n",
       "      <td>77.966851</td>\n",
       "      <td>78.034360</td>\n",
       "      <td>79.650200</td>\n",
       "      <td>...</td>\n",
       "      <td>80.040164</td>\n",
       "      <td>77.492911</td>\n",
       "      <td>80.552828</td>\n",
       "      <td>70.236678</td>\n",
       "      <td>90.915933</td>\n",
       "      <td>80.597446</td>\n",
       "      <td>81.528380</td>\n",
       "      <td>81.665320</td>\n",
       "      <td>69.371942</td>\n",
       "      <td>79.203816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>81.085500</td>\n",
       "      <td>83.269269</td>\n",
       "      <td>77.997856</td>\n",
       "      <td>67.669686</td>\n",
       "      <td>87.586293</td>\n",
       "      <td>86.693977</td>\n",
       "      <td>79.712469</td>\n",
       "      <td>83.950738</td>\n",
       "      <td>84.346201</td>\n",
       "      <td>78.132821</td>\n",
       "      <td>...</td>\n",
       "      <td>79.811065</td>\n",
       "      <td>81.004474</td>\n",
       "      <td>78.294866</td>\n",
       "      <td>67.316514</td>\n",
       "      <td>82.950869</td>\n",
       "      <td>80.483248</td>\n",
       "      <td>83.071120</td>\n",
       "      <td>81.965866</td>\n",
       "      <td>75.374322</td>\n",
       "      <td>65.513829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>80.871869</td>\n",
       "      <td>83.367393</td>\n",
       "      <td>78.059860</td>\n",
       "      <td>67.583630</td>\n",
       "      <td>87.631434</td>\n",
       "      <td>86.606911</td>\n",
       "      <td>79.477164</td>\n",
       "      <td>84.132178</td>\n",
       "      <td>84.549588</td>\n",
       "      <td>78.050669</td>\n",
       "      <td>...</td>\n",
       "      <td>79.786175</td>\n",
       "      <td>80.986456</td>\n",
       "      <td>78.193919</td>\n",
       "      <td>67.210842</td>\n",
       "      <td>82.792298</td>\n",
       "      <td>80.468536</td>\n",
       "      <td>83.056954</td>\n",
       "      <td>82.122991</td>\n",
       "      <td>75.464144</td>\n",
       "      <td>65.102826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>80.656301</td>\n",
       "      <td>83.466033</td>\n",
       "      <td>78.125031</td>\n",
       "      <td>67.499484</td>\n",
       "      <td>87.677524</td>\n",
       "      <td>86.518893</td>\n",
       "      <td>79.242501</td>\n",
       "      <td>84.317854</td>\n",
       "      <td>84.758155</td>\n",
       "      <td>77.968463</td>\n",
       "      <td>...</td>\n",
       "      <td>79.761260</td>\n",
       "      <td>80.965988</td>\n",
       "      <td>78.090956</td>\n",
       "      <td>67.102590</td>\n",
       "      <td>82.633703</td>\n",
       "      <td>80.454453</td>\n",
       "      <td>83.036478</td>\n",
       "      <td>82.281210</td>\n",
       "      <td>75.548762</td>\n",
       "      <td>64.685126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>80.439400</td>\n",
       "      <td>83.564900</td>\n",
       "      <td>78.192290</td>\n",
       "      <td>67.417331</td>\n",
       "      <td>87.723150</td>\n",
       "      <td>86.430248</td>\n",
       "      <td>79.008253</td>\n",
       "      <td>84.507677</td>\n",
       "      <td>84.972099</td>\n",
       "      <td>77.886868</td>\n",
       "      <td>...</td>\n",
       "      <td>79.737010</td>\n",
       "      <td>80.943514</td>\n",
       "      <td>77.985717</td>\n",
       "      <td>66.991973</td>\n",
       "      <td>82.475239</td>\n",
       "      <td>80.441972</td>\n",
       "      <td>83.009817</td>\n",
       "      <td>82.440347</td>\n",
       "      <td>75.628233</td>\n",
       "      <td>64.260938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>80.221609</td>\n",
       "      <td>83.663892</td>\n",
       "      <td>78.260620</td>\n",
       "      <td>67.337267</td>\n",
       "      <td>87.767240</td>\n",
       "      <td>86.341214</td>\n",
       "      <td>78.774122</td>\n",
       "      <td>84.701836</td>\n",
       "      <td>85.191428</td>\n",
       "      <td>77.806510</td>\n",
       "      <td>...</td>\n",
       "      <td>79.714033</td>\n",
       "      <td>80.919808</td>\n",
       "      <td>77.878151</td>\n",
       "      <td>66.879278</td>\n",
       "      <td>82.317245</td>\n",
       "      <td>80.431920</td>\n",
       "      <td>82.977138</td>\n",
       "      <td>82.600111</td>\n",
       "      <td>75.702741</td>\n",
       "      <td>63.830729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     78.632971  81.399266  77.250888  77.881737  87.606491  90.878011   \n",
       "1     78.894162  81.456455  76.972121  77.906493  87.566765  90.759398   \n",
       "2     79.147814  81.518846  76.695525  77.930667  87.527735  90.642030   \n",
       "3     79.394387  81.586233  76.420343  77.954146  87.489211  90.526372   \n",
       "4     79.634173  81.658761  76.145646  77.977021  87.451288  90.412748   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  81.085500  83.269269  77.997856  67.669686  87.586293  86.693977   \n",
       "2439  80.871869  83.367393  78.059860  67.583630  87.631434  86.606911   \n",
       "2440  80.656301  83.466033  78.125031  67.499484  87.677524  86.518893   \n",
       "2441  80.439400  83.564900  78.192290  67.417331  87.723150  86.430248   \n",
       "2442  80.221609  83.663892  78.260620  67.337267  87.767240  86.341214   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     83.683674  76.939886  77.811940  79.216417  ...  80.122692  77.439776   \n",
       "1     83.613337  77.200645  77.859044  79.324864  ...  80.102728  77.443129   \n",
       "2     83.540562  77.458909  77.911848  79.433134  ...  80.082263  77.453403   \n",
       "3     83.465142  77.714454  77.970389  79.541525  ...  80.061302  77.470195   \n",
       "4     83.387176  77.966851  78.034360  79.650200  ...  80.040164  77.492911   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  79.712469  83.950738  84.346201  78.132821  ...  79.811065  81.004474   \n",
       "2439  79.477164  84.132178  84.549588  78.050669  ...  79.786175  80.986456   \n",
       "2440  79.242501  84.317854  84.758155  77.968463  ...  79.761260  80.965988   \n",
       "2441  79.008253  84.507677  84.972099  77.886868  ...  79.737010  80.943514   \n",
       "2442  78.774122  84.701836  85.191428  77.806510  ...  79.714033  80.919808   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     80.094806  69.037217  92.465980  80.227891  82.639231  81.956382   \n",
       "1     80.208063  69.342133  92.073577  80.326154  82.357046  81.884584   \n",
       "2     80.322355  69.643368  91.684261  80.420395  82.077674  81.811890   \n",
       "3     80.437383  69.941393  91.298301  80.510691  81.801362  81.738656   \n",
       "4     80.552828  70.236678  90.915933  80.597446  81.528380  81.665320   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  78.294866  67.316514  82.950869  80.483248  83.071120  81.965866   \n",
       "2439  78.193919  67.210842  82.792298  80.468536  83.056954  82.122991   \n",
       "2440  78.090956  67.102590  82.633703  80.454453  83.036478  82.281210   \n",
       "2441  77.985717  66.991973  82.475239  80.441972  83.009817  82.440347   \n",
       "2442  77.878151  66.879278  82.317245  80.431920  82.977138  82.600111   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     68.796643  79.465122  \n",
       "1     68.953000  79.397995  \n",
       "2     69.101216  79.332075  \n",
       "3     69.241014  79.267396  \n",
       "4     69.371942  79.203816  \n",
       "...         ...        ...  \n",
       "2438  75.374322  65.513829  \n",
       "2439  75.464144  65.102826  \n",
       "2440  75.548762  64.685126  \n",
       "2441  75.628233  64.260938  \n",
       "2442  75.702741  63.830729  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.632971</td>\n",
       "      <td>81.399266</td>\n",
       "      <td>77.250888</td>\n",
       "      <td>77.881737</td>\n",
       "      <td>87.606491</td>\n",
       "      <td>90.878011</td>\n",
       "      <td>83.683674</td>\n",
       "      <td>76.939886</td>\n",
       "      <td>77.811940</td>\n",
       "      <td>79.216417</td>\n",
       "      <td>72.285113</td>\n",
       "      <td>76.054070</td>\n",
       "      <td>88.799427</td>\n",
       "      <td>85.541577</td>\n",
       "      <td>79.554588</td>\n",
       "      <td>77.376026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.894162</td>\n",
       "      <td>81.456455</td>\n",
       "      <td>76.972121</td>\n",
       "      <td>77.906493</td>\n",
       "      <td>87.566765</td>\n",
       "      <td>90.759398</td>\n",
       "      <td>83.613337</td>\n",
       "      <td>77.200645</td>\n",
       "      <td>77.859044</td>\n",
       "      <td>79.324864</td>\n",
       "      <td>72.327813</td>\n",
       "      <td>76.184820</td>\n",
       "      <td>88.616788</td>\n",
       "      <td>85.655757</td>\n",
       "      <td>79.542472</td>\n",
       "      <td>77.521796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.147814</td>\n",
       "      <td>81.518846</td>\n",
       "      <td>76.695525</td>\n",
       "      <td>77.930667</td>\n",
       "      <td>87.527735</td>\n",
       "      <td>90.642030</td>\n",
       "      <td>83.540562</td>\n",
       "      <td>77.458909</td>\n",
       "      <td>77.911848</td>\n",
       "      <td>79.433134</td>\n",
       "      <td>72.372197</td>\n",
       "      <td>76.309628</td>\n",
       "      <td>88.435512</td>\n",
       "      <td>85.768770</td>\n",
       "      <td>79.525247</td>\n",
       "      <td>77.668052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.394387</td>\n",
       "      <td>81.586233</td>\n",
       "      <td>76.420343</td>\n",
       "      <td>77.954146</td>\n",
       "      <td>87.489211</td>\n",
       "      <td>90.526372</td>\n",
       "      <td>83.465142</td>\n",
       "      <td>77.714454</td>\n",
       "      <td>77.970389</td>\n",
       "      <td>79.541525</td>\n",
       "      <td>72.418332</td>\n",
       "      <td>76.428593</td>\n",
       "      <td>88.255934</td>\n",
       "      <td>85.880019</td>\n",
       "      <td>79.503741</td>\n",
       "      <td>77.814502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.634173</td>\n",
       "      <td>81.658761</td>\n",
       "      <td>76.145646</td>\n",
       "      <td>77.977021</td>\n",
       "      <td>87.451288</td>\n",
       "      <td>90.412748</td>\n",
       "      <td>83.387176</td>\n",
       "      <td>77.966851</td>\n",
       "      <td>78.034360</td>\n",
       "      <td>79.650200</td>\n",
       "      <td>72.466226</td>\n",
       "      <td>76.542027</td>\n",
       "      <td>88.078211</td>\n",
       "      <td>85.988787</td>\n",
       "      <td>79.479003</td>\n",
       "      <td>77.960657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>81.085500</td>\n",
       "      <td>83.269269</td>\n",
       "      <td>77.997856</td>\n",
       "      <td>67.669686</td>\n",
       "      <td>87.586293</td>\n",
       "      <td>86.693977</td>\n",
       "      <td>79.712469</td>\n",
       "      <td>83.950738</td>\n",
       "      <td>84.346201</td>\n",
       "      <td>78.132821</td>\n",
       "      <td>74.015121</td>\n",
       "      <td>69.104199</td>\n",
       "      <td>87.535874</td>\n",
       "      <td>84.526447</td>\n",
       "      <td>78.901255</td>\n",
       "      <td>83.241964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>80.871869</td>\n",
       "      <td>83.367393</td>\n",
       "      <td>78.059860</td>\n",
       "      <td>67.583630</td>\n",
       "      <td>87.631434</td>\n",
       "      <td>86.606911</td>\n",
       "      <td>79.477164</td>\n",
       "      <td>84.132178</td>\n",
       "      <td>84.549588</td>\n",
       "      <td>78.050669</td>\n",
       "      <td>74.120358</td>\n",
       "      <td>69.204665</td>\n",
       "      <td>87.559580</td>\n",
       "      <td>84.388064</td>\n",
       "      <td>78.818552</td>\n",
       "      <td>83.426164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>80.656301</td>\n",
       "      <td>83.466033</td>\n",
       "      <td>78.125031</td>\n",
       "      <td>67.499484</td>\n",
       "      <td>87.677524</td>\n",
       "      <td>86.518893</td>\n",
       "      <td>79.242501</td>\n",
       "      <td>84.317854</td>\n",
       "      <td>84.758155</td>\n",
       "      <td>77.968463</td>\n",
       "      <td>74.226409</td>\n",
       "      <td>69.306722</td>\n",
       "      <td>87.585359</td>\n",
       "      <td>84.245057</td>\n",
       "      <td>78.738473</td>\n",
       "      <td>83.611219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>80.439400</td>\n",
       "      <td>83.564900</td>\n",
       "      <td>78.192290</td>\n",
       "      <td>67.417331</td>\n",
       "      <td>87.723150</td>\n",
       "      <td>86.430248</td>\n",
       "      <td>79.008253</td>\n",
       "      <td>84.507677</td>\n",
       "      <td>84.972099</td>\n",
       "      <td>77.886868</td>\n",
       "      <td>74.333323</td>\n",
       "      <td>69.410655</td>\n",
       "      <td>87.612699</td>\n",
       "      <td>84.097218</td>\n",
       "      <td>78.660728</td>\n",
       "      <td>83.797359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>80.221609</td>\n",
       "      <td>83.663892</td>\n",
       "      <td>78.260620</td>\n",
       "      <td>67.337267</td>\n",
       "      <td>87.767240</td>\n",
       "      <td>86.341214</td>\n",
       "      <td>78.774122</td>\n",
       "      <td>84.701836</td>\n",
       "      <td>85.191428</td>\n",
       "      <td>77.806510</td>\n",
       "      <td>74.440980</td>\n",
       "      <td>69.516619</td>\n",
       "      <td>87.641164</td>\n",
       "      <td>83.944627</td>\n",
       "      <td>78.585017</td>\n",
       "      <td>83.984588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     78.632971  81.399266  77.250888  77.881737  87.606491  90.878011   \n",
       "1     78.894162  81.456455  76.972121  77.906493  87.566765  90.759398   \n",
       "2     79.147814  81.518846  76.695525  77.930667  87.527735  90.642030   \n",
       "3     79.394387  81.586233  76.420343  77.954146  87.489211  90.526372   \n",
       "4     79.634173  81.658761  76.145646  77.977021  87.451288  90.412748   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  81.085500  83.269269  77.997856  67.669686  87.586293  86.693977   \n",
       "2439  80.871869  83.367393  78.059860  67.583630  87.631434  86.606911   \n",
       "2440  80.656301  83.466033  78.125031  67.499484  87.677524  86.518893   \n",
       "2441  80.439400  83.564900  78.192290  67.417331  87.723150  86.430248   \n",
       "2442  80.221609  83.663892  78.260620  67.337267  87.767240  86.341214   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     83.683674  76.939886  77.811940  79.216417  72.285113  76.054070   \n",
       "1     83.613337  77.200645  77.859044  79.324864  72.327813  76.184820   \n",
       "2     83.540562  77.458909  77.911848  79.433134  72.372197  76.309628   \n",
       "3     83.465142  77.714454  77.970389  79.541525  72.418332  76.428593   \n",
       "4     83.387176  77.966851  78.034360  79.650200  72.466226  76.542027   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  79.712469  83.950738  84.346201  78.132821  74.015121  69.104199   \n",
       "2439  79.477164  84.132178  84.549588  78.050669  74.120358  69.204665   \n",
       "2440  79.242501  84.317854  84.758155  77.968463  74.226409  69.306722   \n",
       "2441  79.008253  84.507677  84.972099  77.886868  74.333323  69.410655   \n",
       "2442  78.774122  84.701836  85.191428  77.806510  74.440980  69.516619   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     88.799427  85.541577  79.554588  77.376026  \n",
       "1     88.616788  85.655757  79.542472  77.521796  \n",
       "2     88.435512  85.768770  79.525247  77.668052  \n",
       "3     88.255934  85.880019  79.503741  77.814502  \n",
       "4     88.078211  85.988787  79.479003  77.960657  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  87.535874  84.526447  78.901255  83.241964  \n",
       "2439  87.559580  84.388064  78.818552  83.426164  \n",
       "2440  87.585359  84.245057  78.738473  83.611219  \n",
       "2441  87.612699  84.097218  78.660728  83.797359  \n",
       "2442  87.641164  83.944627  78.585017  83.984588  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 11s 16ms/step - loss: 3561.8611 - val_loss: 2729.1416\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2466.5237 - val_loss: 2330.0032\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1940.8668 - val_loss: 1795.5514\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1485.3848 - val_loss: 1328.9391\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1174.8040 - val_loss: 1026.3557\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1017.9231 - val_loss: 972.6082\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 946.1671 - val_loss: 942.1558\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 903.7617 - val_loss: 871.4171\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 846.7199 - val_loss: 1331.8624\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 657.5988 - val_loss: 536.2695\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 447.7671 - val_loss: 428.4352\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 271.7536 - val_loss: 218.5084\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 181.4642 - val_loss: 357.6628\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 130.9921 - val_loss: 174.1569\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 96.6489 - val_loss: 104.7762\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 64.9536 - val_loss: 115.8838\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 51.9059 - val_loss: 140.0495\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 38.6799 - val_loss: 51.5093\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 35.7646 - val_loss: 148.6255\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 26.2497 - val_loss: 106.6509\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 25.9018 - val_loss: 23.6627\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 18.7150 - val_loss: 50.0356\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.2827 - val_loss: 19.4944\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.5634 - val_loss: 42.2703\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.0420 - val_loss: 21.7314\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.9180 - val_loss: 22.9623\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.5033 - val_loss: 21.9628\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.3373 - val_loss: 99.8096\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.0300 - val_loss: 11.8660\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.0888 - val_loss: 59.9341\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.6610 - val_loss: 41.9385\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.5434 - val_loss: 7.1723\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8700 - val_loss: 206.0110\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9228 - val_loss: 6.5040\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.6024 - val_loss: 7.8584\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.0703 - val_loss: 14.1392\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.2943 - val_loss: 282.6820\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 9.4417 - val_loss: 4.9717\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.9853 - val_loss: 10.8061\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1475 - val_loss: 2.5012\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3727 - val_loss: 5.5279\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8875 - val_loss: 3.1185\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6929 - val_loss: 15.0514\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5634 - val_loss: 5.4257\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.5014 - val_loss: 38.8674\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2834 - val_loss: 3.9608\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5677 - val_loss: 1.8953\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.4706 - val_loss: 2.9187\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3633 - val_loss: 2.3601\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7398 - val_loss: 6.2057\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5584 - val_loss: 6.1398\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8821 - val_loss: 2.5002\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.8403 - val_loss: 6.0380\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 20.7857 - val_loss: 322.0343\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.6728 - val_loss: 4.3198\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2884 - val_loss: 1.3485\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0217 - val_loss: 3.4148\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9662 - val_loss: 1.7256\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0683 - val_loss: 1.6924\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1150 - val_loss: 2.2179\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0380 - val_loss: 12.9324\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3693 - val_loss: 2.6211\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1602 - val_loss: 3.8081\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6471 - val_loss: 3.3471\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4792 - val_loss: 7.8053\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2438 - val_loss: 7.3290\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1532 - val_loss: 2.5969\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4373 - val_loss: 5.2649\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4457 - val_loss: 1.4269\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 21.6522 - val_loss: 447.5686\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.5587 - val_loss: 3.2919\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2105 - val_loss: 1.6144\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8630 - val_loss: 0.8676\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7996 - val_loss: 0.6730\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7212 - val_loss: 2.2060\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7691 - val_loss: 1.0083\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6868 - val_loss: 2.1103\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9292 - val_loss: 3.1992\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8927 - val_loss: 4.6515\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9390 - val_loss: 1.8832\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7780 - val_loss: 1.3192\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0163 - val_loss: 1.9068\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2293 - val_loss: 1.2576\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.6722 - val_loss: 6.2345\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7600 - val_loss: 0.5306\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4407 - val_loss: 0.7521\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4965 - val_loss: 0.7082\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5599 - val_loss: 0.6521\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5623 - val_loss: 1.0828\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5529 - val_loss: 1.0974\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5210 - val_loss: 1.1134\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6608 - val_loss: 3.0952\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8543 - val_loss: 1.7636\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8238 - val_loss: 1.5026\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8994 - val_loss: 2.4969\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.4550 - val_loss: 0.7349\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5394 - val_loss: 0.6162\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4028 - val_loss: 0.5131\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1570 - val_loss: 199.7755\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1891 - val_loss: 0.6277\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3632 - val_loss: 0.6796\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3385 - val_loss: 0.5712\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5097 - val_loss: 0.9990\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3633 - val_loss: 1.3506\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6441 - val_loss: 0.6265\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4900 - val_loss: 1.2637\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6122 - val_loss: 2.0223\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8501 - val_loss: 3.7452\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6435 - val_loss: 1.2494\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9689 - val_loss: 9.4325\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8992 - val_loss: 0.9140\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5518 - val_loss: 2.9098\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8682 - val_loss: 2.6756\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4954 - val_loss: 0.9646\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6665 - val_loss: 4.7953\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8475 - val_loss: 4.5833\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6518 - val_loss: 2.5597\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6594 - val_loss: 2.5033\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8386 - val_loss: 5.4631\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7197 - val_loss: 2.4258\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5446 - val_loss: 2.9607\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6959 - val_loss: 3.2330\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5104 - val_loss: 1.2214\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6809 - val_loss: 2.3997\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5723 - val_loss: 1.0181\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3678 - val_loss: 1.0801\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7379 - val_loss: 15.0192\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5408 - val_loss: 2.3497\n",
      "16/16 [==============================] - 1s 21ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.5130886310280384\n",
      "Mean Absolute Error (MAE): 0.5361700309048654\n",
      "Root Mean Squared Error (RMSE): 0.7163020529274213\n",
      "Time taken: 366.1504738330841\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 3374.2676 - val_loss: 2765.1763\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2547.5156 - val_loss: 2509.0225\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2328.0229 - val_loss: 2198.7563\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1927.9205 - val_loss: 1722.0298\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1480.9025 - val_loss: 1326.5250\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1157.9298 - val_loss: 1215.0865\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 969.7627 - val_loss: 1266.3086\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 855.4317 - val_loss: 855.4112\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 734.2559 - val_loss: 688.0831\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 540.4395 - val_loss: 576.7797\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 341.3654 - val_loss: 393.1746\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 217.2495 - val_loss: 264.4452\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 149.9156 - val_loss: 129.5957\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 107.1718 - val_loss: 251.7387\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 78.9642 - val_loss: 104.1119\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 52.5314 - val_loss: 60.9560\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 34.5647 - val_loss: 47.6701\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 28.4386 - val_loss: 120.2299\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 25.4655 - val_loss: 27.2283\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 23.8485 - val_loss: 24.8609\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 15.4569 - val_loss: 22.1238\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.0705 - val_loss: 59.7676\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.0330 - val_loss: 11.1369\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 13.7456 - val_loss: 330.3282\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.4248 - val_loss: 39.3072\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.5312 - val_loss: 23.9180\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.6152 - val_loss: 10.5734\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.9605 - val_loss: 8.7339\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3271 - val_loss: 14.4541\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.9314 - val_loss: 3.9899\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5356 - val_loss: 4.9613\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0531 - val_loss: 5.7408\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.4463 - val_loss: 72.3850\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.3352 - val_loss: 5.6565\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4477 - val_loss: 5.1207\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9869 - val_loss: 2.4110\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.2510 - val_loss: 3.0134\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0742 - val_loss: 9.4731\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 5.5272 - val_loss: 273.5992\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.1343 - val_loss: 2.2337\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6654 - val_loss: 3.9100\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4578 - val_loss: 1.8436\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5702 - val_loss: 5.5662\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7176 - val_loss: 8.7990\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 16.0073 - val_loss: 6.9717\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3480 - val_loss: 1.0835\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4475 - val_loss: 1.7643\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3033 - val_loss: 1.8591\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1566 - val_loss: 1.7590\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1191 - val_loss: 3.7907\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9475 - val_loss: 16.6025\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7028 - val_loss: 3.0775\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2084 - val_loss: 1.6667\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3117 - val_loss: 3.1853\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4217 - val_loss: 1.9379\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.0463 - val_loss: 2.8857\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.4477 - val_loss: 4.9238\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.0063 - val_loss: 0.6215\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7642 - val_loss: 0.9098\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7330 - val_loss: 1.3372\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7592 - val_loss: 1.2703\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9264 - val_loss: 1.3183\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9653 - val_loss: 2.1647\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8805 - val_loss: 0.9983\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1711 - val_loss: 2.6137\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0126 - val_loss: 2.5654\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.9002 - val_loss: 240.2533\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2834 - val_loss: 1.3226\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8257 - val_loss: 1.6664\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6003 - val_loss: 1.5362\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6415 - val_loss: 0.5850\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6592 - val_loss: 1.4344\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5920 - val_loss: 1.2555\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7274 - val_loss: 1.8129\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7099 - val_loss: 3.1263\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1292 - val_loss: 2.2832\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0757 - val_loss: 73.5831\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 19.3874 - val_loss: 1.5020\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7738 - val_loss: 0.8087\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5469 - val_loss: 0.6745\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4457 - val_loss: 0.5489\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4812 - val_loss: 0.4840\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4523 - val_loss: 0.6021\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4339 - val_loss: 0.7077\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5335 - val_loss: 0.4098\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4901 - val_loss: 2.5549\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5446 - val_loss: 0.7661\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5973 - val_loss: 1.1331\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5226 - val_loss: 1.3393\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7027 - val_loss: 1.4520\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9040 - val_loss: 2.4998\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9876 - val_loss: 4.0799\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 12.0544 - val_loss: 53.1175\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2481 - val_loss: 0.7565\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3949 - val_loss: 0.4410\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3610 - val_loss: 0.5931\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3119 - val_loss: 0.3206\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3444 - val_loss: 0.6065\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3980 - val_loss: 0.8863\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3772 - val_loss: 1.0663\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4012 - val_loss: 0.5330\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4177 - val_loss: 0.5543\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0456 - val_loss: 3.2788\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5511 - val_loss: 0.6454\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6631 - val_loss: 3.2749\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.0896 - val_loss: 192.2662\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 17.5662 - val_loss: 2.5953\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9926 - val_loss: 0.6235\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5524 - val_loss: 0.4610\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3748 - val_loss: 0.3927\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3531 - val_loss: 0.5637\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3723 - val_loss: 0.4620\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3723 - val_loss: 1.0712\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3638 - val_loss: 0.5349\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3849 - val_loss: 0.6416\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5068 - val_loss: 1.3376\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7020 - val_loss: 4.7770\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7628 - val_loss: 1.6565\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5477 - val_loss: 0.7574\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6923 - val_loss: 1.1589\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5288 - val_loss: 0.6875\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5559 - val_loss: 0.7553\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5164 - val_loss: 1.0589\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4613 - val_loss: 1.7128\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6869 - val_loss: 2.0472\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6750 - val_loss: 1.6985\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7832 - val_loss: 0.6324\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.3204735156918393\n",
      "Mean Absolute Error (MAE): 0.4103996341229161\n",
      "Root Mean Squared Error (RMSE): 0.5661038029300274\n",
      "Time taken: 350.6985945701599\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 3389.4353 - val_loss: 2630.8293\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2449.6968 - val_loss: 2173.8313\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2003.2880 - val_loss: 2309.1836\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1520.6309 - val_loss: 1377.9550\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1195.3734 - val_loss: 1046.5614\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1035.3682 - val_loss: 1036.2893\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 956.5190 - val_loss: 1075.7085\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 913.7978 - val_loss: 929.2245\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 883.7857 - val_loss: 862.8309\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 823.7635 - val_loss: 790.4689\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 666.1166 - val_loss: 600.7098\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 455.4156 - val_loss: 427.3506\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 282.1932 - val_loss: 233.9119\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 182.2347 - val_loss: 279.7364\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 127.6848 - val_loss: 128.1437\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 92.0412 - val_loss: 115.3980\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 73.7078 - val_loss: 90.4787\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 46.7718 - val_loss: 63.4449\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 30.7243 - val_loss: 58.3597\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 31.5511 - val_loss: 102.9548\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 24.5572 - val_loss: 26.2329\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 22.9100 - val_loss: 21.4008\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 15.2067 - val_loss: 17.2857\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.2787 - val_loss: 107.1802\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.0506 - val_loss: 35.5455\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.0487 - val_loss: 532.1025\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.9368 - val_loss: 101.7725\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.2840 - val_loss: 15.0174\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8116 - val_loss: 24.9638\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.1922 - val_loss: 87.1283\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.4013 - val_loss: 10.7136\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.9152 - val_loss: 23.7683\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.1373 - val_loss: 6.2472\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8383 - val_loss: 21.4249\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.8827 - val_loss: 11.0967\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8503 - val_loss: 3.0856\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7387 - val_loss: 3.3107\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1162 - val_loss: 14.1268\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0178 - val_loss: 25.3621\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8266 - val_loss: 3.2864\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.4398 - val_loss: 319.6885\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.6388 - val_loss: 3.6358\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7935 - val_loss: 1.7896\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5724 - val_loss: 2.4941\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5681 - val_loss: 2.3907\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0020 - val_loss: 3.2221\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5840 - val_loss: 6.5628\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8245 - val_loss: 4.1899\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1716 - val_loss: 5.6381\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8326 - val_loss: 9.8826\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 31.1711 - val_loss: 119.6045\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9800 - val_loss: 7.7135\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8976 - val_loss: 4.6295\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4539 - val_loss: 1.5516\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4120 - val_loss: 1.2193\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5295 - val_loss: 2.3061\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2461 - val_loss: 1.5994\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3468 - val_loss: 1.1289\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3797 - val_loss: 139.7046\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.7790 - val_loss: 1.8149\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3659 - val_loss: 1.8289\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0165 - val_loss: 3.7591\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0735 - val_loss: 1.4409\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2083 - val_loss: 6.8634\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2764 - val_loss: 8.5507\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 9.4980 - val_loss: 1558.1655\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 23.5452 - val_loss: 20.5599\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.3841 - val_loss: 3.0366\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8892 - val_loss: 0.8692\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8123 - val_loss: 0.7146\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6525 - val_loss: 0.9638\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8071 - val_loss: 1.1026\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7326 - val_loss: 1.1009\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8512 - val_loss: 1.7554\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9437 - val_loss: 1.8441\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7127 - val_loss: 2.8988\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2467 - val_loss: 4.1900\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1575 - val_loss: 4.5510\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0783 - val_loss: 2.9545\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0166 - val_loss: 1.5341\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8463 - val_loss: 1.7950\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 18.1920 - val_loss: 5.6959\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1539 - val_loss: 0.9480\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6220 - val_loss: 0.6147\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6177 - val_loss: 0.7614\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4580 - val_loss: 1.1346\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5349 - val_loss: 0.9238\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5481 - val_loss: 0.7510\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5285 - val_loss: 0.5741\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7046 - val_loss: 1.7386\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0464 - val_loss: 2.1410\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6527 - val_loss: 2.4162\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9340 - val_loss: 3.2528\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9079 - val_loss: 2.1094\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9655 - val_loss: 3.6466\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9087 - val_loss: 4.6240\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9139 - val_loss: 1.7436\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5918 - val_loss: 0.8671\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 21.8805 - val_loss: 4.6202\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7909 - val_loss: 0.7190\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4421 - val_loss: 0.6133\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3924 - val_loss: 0.5073\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3424 - val_loss: 0.3567\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3365 - val_loss: 0.6662\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4600 - val_loss: 0.4390\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3741 - val_loss: 0.8248\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3875 - val_loss: 0.6810\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5553 - val_loss: 1.0188\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5741 - val_loss: 1.2708\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6014 - val_loss: 2.7214\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6885 - val_loss: 0.9932\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6602 - val_loss: 4.6623\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.3516 - val_loss: 2.4580\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5047 - val_loss: 0.2633\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3074 - val_loss: 0.4889\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2694 - val_loss: 0.3372\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2880 - val_loss: 0.4880\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2907 - val_loss: 0.3755\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3334 - val_loss: 1.1652\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3742 - val_loss: 1.0274\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7305 - val_loss: 1.6362\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5692 - val_loss: 1.3034\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5332 - val_loss: 0.6447\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5596 - val_loss: 2.5957\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5387 - val_loss: 1.5197\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6000 - val_loss: 5.0959\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0146 - val_loss: 1.2085\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6698 - val_loss: 1.3172\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6227 - val_loss: 1.6659\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5455 - val_loss: 0.9045\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7066 - val_loss: 5.0139\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7863 - val_loss: 0.4863\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.7724 - val_loss: 1.0039\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4078 - val_loss: 0.3301\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2728 - val_loss: 0.2424\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2202 - val_loss: 0.1917\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2106 - val_loss: 0.2124\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1984 - val_loss: 0.2315\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2088 - val_loss: 0.2765\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2248 - val_loss: 0.3631\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2042 - val_loss: 0.2315\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4218 - val_loss: 0.5050\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2815 - val_loss: 0.3510\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3821 - val_loss: 0.3779\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3529 - val_loss: 0.3976\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4240 - val_loss: 1.0691\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3992 - val_loss: 1.3665\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6178 - val_loss: 1.0159\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2733 - val_loss: 1.0184\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5239 - val_loss: 1.1645\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6698 - val_loss: 1.8711\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4899 - val_loss: 0.6397\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4220 - val_loss: 3.4745\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3853 - val_loss: 0.6600\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6182 - val_loss: 1.4230\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4561 - val_loss: 2.8542\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4509 - val_loss: 0.9791\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7627 - val_loss: 84.3699\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 6.2525 - val_loss: 0.3784\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2532 - val_loss: 0.1921\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1620 - val_loss: 0.1537\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1525 - val_loss: 0.1500\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1628 - val_loss: 0.2165\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1352 - val_loss: 0.2612\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1588 - val_loss: 0.2995\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1995 - val_loss: 0.5536\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2088 - val_loss: 0.7974\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3016 - val_loss: 1.9473\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4417 - val_loss: 1.3529\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3335 - val_loss: 0.8098\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3346 - val_loss: 1.1645\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3027 - val_loss: 0.8277\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5130 - val_loss: 2.5423\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4531 - val_loss: 0.4980\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2832 - val_loss: 0.8124\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3263 - val_loss: 1.1466\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3668 - val_loss: 2.9533\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5595 - val_loss: 0.7613\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2451 - val_loss: 0.4031\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2417 - val_loss: 2.6325\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3538 - val_loss: 1.3712\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5142 - val_loss: 0.9697\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6681 - val_loss: 4.6142\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5283 - val_loss: 0.4889\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1759 - val_loss: 0.5625\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2552 - val_loss: 2.8621\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3778 - val_loss: 2.2857\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4639 - val_loss: 3.9399\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3982 - val_loss: 19.3394\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4076 - val_loss: 0.9991\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2495 - val_loss: 0.7771\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2843 - val_loss: 0.4716\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.1499945415237845\n",
      "Mean Absolute Error (MAE): 0.2824696932600452\n",
      "Root Mean Squared Error (RMSE): 0.3872912876941392\n",
      "Time taken: 533.554358959198\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 3378.0586 - val_loss: 2778.3430\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2552.7488 - val_loss: 2582.4084\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2492.6091 - val_loss: 2575.0483\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2392.2156 - val_loss: 2372.7339\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2051.4409 - val_loss: 1926.5747\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1626.7441 - val_loss: 1512.8422\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1268.0989 - val_loss: 1223.2666\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1052.2408 - val_loss: 1083.6461\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 936.2448 - val_loss: 1098.7424\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 868.6904 - val_loss: 948.5213\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 821.5315 - val_loss: 902.3499\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 725.0407 - val_loss: 755.3038\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 541.9103 - val_loss: 513.4783\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 346.3358 - val_loss: 674.8632\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 212.9258 - val_loss: 194.0489\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 142.6810 - val_loss: 138.7541\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 96.1052 - val_loss: 115.6884\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 73.7573 - val_loss: 155.1887\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 50.1468 - val_loss: 53.6143\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 39.4051 - val_loss: 85.0056\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 30.1606 - val_loss: 107.5211\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 27.2374 - val_loss: 82.8319\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.7762 - val_loss: 67.7389\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 18.5765 - val_loss: 15.9840\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.1085 - val_loss: 126.3235\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.4050 - val_loss: 10.3881\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.1932 - val_loss: 27.5091\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.1254 - val_loss: 7.3470\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.9192 - val_loss: 23.6953\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.7037 - val_loss: 10.0068\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.0441 - val_loss: 9.3618\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.7578 - val_loss: 6.3903\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.9852 - val_loss: 10.2079\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.2977 - val_loss: 58.6812\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.6413 - val_loss: 7.2059\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4936 - val_loss: 54.0309\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 23.4184 - val_loss: 6.7756\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6334 - val_loss: 10.5932\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9535 - val_loss: 3.5347\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5138 - val_loss: 3.2002\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.0943 - val_loss: 4.4336\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3044 - val_loss: 3.6628\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 16.1016 - val_loss: 95.4763\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4619 - val_loss: 2.6184\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9142 - val_loss: 5.2153\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9258 - val_loss: 3.8776\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6142 - val_loss: 3.0729\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9005 - val_loss: 5.5486\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7042 - val_loss: 6.3686\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9777 - val_loss: 2.5308\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3946 - val_loss: 3.5793\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.1180 - val_loss: 6.7226\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.5943 - val_loss: 2.6386\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3097 - val_loss: 1.8768\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2783 - val_loss: 5.9006\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4180 - val_loss: 7.8315\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8895 - val_loss: 2.5282\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2379 - val_loss: 1.3199\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4666 - val_loss: 5.0928\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5769 - val_loss: 33.3858\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2559 - val_loss: 3.7736\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5724 - val_loss: 2.4547\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.5037 - val_loss: 132.0838\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3685 - val_loss: 1.4057\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 1.0098 - val_loss: 1.1798\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8230 - val_loss: 1.2475\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8145 - val_loss: 1.1605\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9077 - val_loss: 2.1142\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9623 - val_loss: 2.1465\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8391 - val_loss: 2.0028\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1241 - val_loss: 1.5170\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9450 - val_loss: 2.5817\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2514 - val_loss: 3.2176\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0492 - val_loss: 2.3802\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2005 - val_loss: 2.0329\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4944 - val_loss: 1.9294\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.3858 - val_loss: 2.9489\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9836 - val_loss: 1.4439\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6532 - val_loss: 0.7418\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6259 - val_loss: 0.9418\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5681 - val_loss: 0.8961\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6172 - val_loss: 0.5461\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5848 - val_loss: 1.1806\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5955 - val_loss: 3.3752\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9612 - val_loss: 5.3243\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.2845 - val_loss: 1.4444\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6811 - val_loss: 1.3815\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7987 - val_loss: 2.5584\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2060 - val_loss: 5.1280\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.4878 - val_loss: 123.7137\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6737 - val_loss: 1.0563\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5660 - val_loss: 0.4025\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4628 - val_loss: 0.9416\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4547 - val_loss: 0.4618\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4717 - val_loss: 0.5493\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4233 - val_loss: 0.7201\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4430 - val_loss: 0.5952\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5207 - val_loss: 0.5028\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5345 - val_loss: 2.0971\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6952 - val_loss: 0.6325\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9791 - val_loss: 1.8155\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7788 - val_loss: 1.1082\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6382 - val_loss: 0.7096\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2804 - val_loss: 5.9102\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.5836 - val_loss: 0.8116\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4643 - val_loss: 0.3616\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3640 - val_loss: 0.4634\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3070 - val_loss: 0.3277\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3125 - val_loss: 0.2518\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3172 - val_loss: 0.3159\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3365 - val_loss: 0.2329\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3758 - val_loss: 0.4823\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4013 - val_loss: 0.7215\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4779 - val_loss: 0.5985\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4456 - val_loss: 0.8129\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9237 - val_loss: 1.3871\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4757 - val_loss: 1.7108\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4266 - val_loss: 0.9546\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5568 - val_loss: 1.2477\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.1644 - val_loss: 0.6812\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4571 - val_loss: 0.5137\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3173 - val_loss: 0.2656\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2709 - val_loss: 0.3571\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2800 - val_loss: 0.2968\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2414 - val_loss: 0.3732\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3139 - val_loss: 0.3918\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3582 - val_loss: 0.4762\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3039 - val_loss: 0.5092\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4253 - val_loss: 4.7593\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4583 - val_loss: 0.7846\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4377 - val_loss: 2.1275\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5428 - val_loss: 10.5813\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7832 - val_loss: 0.9979\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5675 - val_loss: 2.6494\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7243 - val_loss: 1.4537\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4764 - val_loss: 0.7354\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4222 - val_loss: 1.3333\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7892 - val_loss: 2.8394\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5093 - val_loss: 1.1696\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3919 - val_loss: 0.3600\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5198 - val_loss: 1.4642\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.232915723647354\n",
      "Mean Absolute Error (MAE): 0.36237934044822073\n",
      "Root Mean Squared Error (RMSE): 0.48261343086092623\n",
      "Time taken: 394.60920572280884\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 3349.3848 - val_loss: 2738.4958\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2553.7292 - val_loss: 2561.6863\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2497.9619 - val_loss: 2553.1880\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2419.6248 - val_loss: 2323.2146\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2039.9945 - val_loss: 1994.7789\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1584.9760 - val_loss: 1410.8804\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1228.5688 - val_loss: 1173.3065\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1023.1594 - val_loss: 1005.3390\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 923.1097 - val_loss: 932.6561\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 863.1992 - val_loss: 871.6526\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 790.1307 - val_loss: 871.5739\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 637.2154 - val_loss: 567.5159\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 429.8065 - val_loss: 408.5247\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 283.5216 - val_loss: 348.3964\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 193.4206 - val_loss: 177.4268\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 120.9619 - val_loss: 148.3112\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 87.8480 - val_loss: 180.2321\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 67.5805 - val_loss: 121.5054\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 46.2484 - val_loss: 78.9092\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 40.3880 - val_loss: 64.8593\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 25.0765 - val_loss: 31.4228\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.3320 - val_loss: 63.1001\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 17.4900 - val_loss: 15.3432\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 18.8094 - val_loss: 65.5962\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 14.9425 - val_loss: 24.5984\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.3306 - val_loss: 41.7046\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 11.7689 - val_loss: 65.0225\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.4996 - val_loss: 8.2893\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.4908 - val_loss: 20.7861\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.6924 - val_loss: 8.7145\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.7816 - val_loss: 11.9860\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.1676 - val_loss: 10.0636\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.9298 - val_loss: 5.9929\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8039 - val_loss: 3.5068\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.2355 - val_loss: 89.1639\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 13.8624 - val_loss: 65.7557\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.3249 - val_loss: 3.2133\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.1357 - val_loss: 60.8053\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 12.3406 - val_loss: 2.5802\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9230 - val_loss: 1.7281\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.8068 - val_loss: 2.6471\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5893 - val_loss: 4.9246\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5891 - val_loss: 4.0107\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9127 - val_loss: 1.8697\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.6421 - val_loss: 3.2792\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2723 - val_loss: 5.5666\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2861 - val_loss: 6.7310\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6903 - val_loss: 2.9811\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 26.3007 - val_loss: 27.2144\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6938 - val_loss: 1.6902\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2551 - val_loss: 0.9115\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0580 - val_loss: 10.0116\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1919 - val_loss: 1.2451\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.2583 - val_loss: 1.0006\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0854 - val_loss: 1.7781\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4622 - val_loss: 4.1316\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1644 - val_loss: 1.2611\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3411 - val_loss: 3.4581\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 21.4757 - val_loss: 82.1953\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.5889 - val_loss: 4.3890\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9600 - val_loss: 0.9181\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8307 - val_loss: 1.3345\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7410 - val_loss: 0.9619\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7570 - val_loss: 0.8617\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6741 - val_loss: 1.0152\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9929 - val_loss: 1.6729\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9574 - val_loss: 1.3095\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1531 - val_loss: 2.8273\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.0315 - val_loss: 1.7447\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2583 - val_loss: 5.8096\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0049 - val_loss: 1.0523\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1089 - val_loss: 3.1166\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2976 - val_loss: 5.1649\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.4039 - val_loss: 172.2108\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3314 - val_loss: 0.9172\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6186 - val_loss: 0.6099\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5121 - val_loss: 0.8750\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5160 - val_loss: 0.6188\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6045 - val_loss: 0.6473\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5654 - val_loss: 0.7748\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5982 - val_loss: 0.5232\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5789 - val_loss: 2.2315\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8533 - val_loss: 3.0188\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.4255 - val_loss: 0.9120\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6890 - val_loss: 0.7324\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4882 - val_loss: 0.5396\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4502 - val_loss: 1.1267\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4755 - val_loss: 5.3604\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6226 - val_loss: 0.6143\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5269 - val_loss: 0.6283\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8705 - val_loss: 2.8343\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7701 - val_loss: 0.8487\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8030 - val_loss: 1.5601\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6931 - val_loss: 1.0617\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5461 - val_loss: 0.7706\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 11.7165 - val_loss: 19.0277\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8399 - val_loss: 0.9545\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4340 - val_loss: 0.3464\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3440 - val_loss: 0.4687\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3277 - val_loss: 0.3182\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4031 - val_loss: 0.3553\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3246 - val_loss: 1.7116\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3583 - val_loss: 0.3085\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4991 - val_loss: 0.7192\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4773 - val_loss: 1.0413\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5458 - val_loss: 0.9956\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7412 - val_loss: 2.2953\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7112 - val_loss: 1.0916\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7155 - val_loss: 1.2934\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.9670 - val_loss: 2.2649\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5210 - val_loss: 3.3487\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.0078 - val_loss: 1.5468\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5046 - val_loss: 0.5018\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3358 - val_loss: 0.3153\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2891 - val_loss: 0.3323\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2861 - val_loss: 0.5571\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3046 - val_loss: 0.2319\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2862 - val_loss: 0.4938\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3181 - val_loss: 0.5154\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3316 - val_loss: 0.5353\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3348 - val_loss: 0.5777\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4801 - val_loss: 1.7159\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6132 - val_loss: 1.5783\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7187 - val_loss: 3.2949\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7014 - val_loss: 0.7186\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5896 - val_loss: 2.6774\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6364 - val_loss: 1.2558\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4744 - val_loss: 1.3679\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6783 - val_loss: 2.2623\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5435 - val_loss: 1.7751\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5177 - val_loss: 1.1595\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6344 - val_loss: 3.2643\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6783 - val_loss: 1.2284\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.0284 - val_loss: 1.7111\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5034 - val_loss: 0.5202\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3176 - val_loss: 0.4052\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2507 - val_loss: 0.2264\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2212 - val_loss: 0.2634\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2251 - val_loss: 0.2367\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2283 - val_loss: 0.3079\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2337 - val_loss: 0.2413\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2688 - val_loss: 0.6092\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3569 - val_loss: 0.3023\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2950 - val_loss: 0.4453\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4359 - val_loss: 0.6167\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4163 - val_loss: 1.2517\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5537 - val_loss: 0.4750\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4864 - val_loss: 1.2044\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3738 - val_loss: 0.7437\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5899 - val_loss: 1.9923\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6448 - val_loss: 1.6669\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5538 - val_loss: 1.0421\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5577 - val_loss: 0.7673\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2929 - val_loss: 0.4297\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5451 - val_loss: 2.1392\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4340 - val_loss: 0.7656\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5256 - val_loss: 1.1136\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4395 - val_loss: 0.5284\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5926 - val_loss: 0.8761\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 4.0494 - val_loss: 0.4931\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1903 - val_loss: 0.2012\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1679 - val_loss: 0.1546\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1548 - val_loss: 0.2324\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1463 - val_loss: 0.1393\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1528 - val_loss: 0.1505\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1522 - val_loss: 0.2569\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2099 - val_loss: 0.2682\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2033 - val_loss: 0.4933\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5880 - val_loss: 4.1578\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3810 - val_loss: 0.7549\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1907 - val_loss: 0.4323\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2340 - val_loss: 0.2494\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2428 - val_loss: 0.2833\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4165 - val_loss: 1.2453\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3275 - val_loss: 0.3483\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3958 - val_loss: 2.3486\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4809 - val_loss: 1.0530\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3822 - val_loss: 1.0507\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3437 - val_loss: 1.1242\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4370 - val_loss: 0.3761\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3239 - val_loss: 0.6294\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4201 - val_loss: 0.4970\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3493 - val_loss: 0.9648\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4522 - val_loss: 0.4312\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5139 - val_loss: 2.4352\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3232 - val_loss: 1.5699\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3019 - val_loss: 0.9779\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3399 - val_loss: 1.0751\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3846 - val_loss: 1.1357\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4151 - val_loss: 8.3131\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4004 - val_loss: 0.7720\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2434 - val_loss: 1.2427\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2889 - val_loss: 1.0793\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4115 - val_loss: 1.3502\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.13930060634549507\n",
      "Mean Absolute Error (MAE): 0.2743933961978579\n",
      "Root Mean Squared Error (RMSE): 0.37322996442608286\n",
      "Time taken: 534.1905326843262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_18756\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.513089  0.536170  0.716302  366.150474\n",
      "1        2  0.320474  0.410400  0.566104  350.698595\n",
      "2        3  0.149995  0.282470  0.387291  533.554359\n",
      "3        4  0.232916  0.362379  0.482613  394.609206\n",
      "4        5  0.139301  0.274393  0.373230  534.190533\n",
      "5  Average  0.271155  0.373162  0.505108  435.840633\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_Reg2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_Reg2.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_Reg2.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
