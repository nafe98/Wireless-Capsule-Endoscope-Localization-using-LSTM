{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg2.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.243675</td>\n",
       "      <td>191.472930</td>\n",
       "      <td>120.844514</td>\n",
       "      <td>161.973862</td>\n",
       "      <td>203.309157</td>\n",
       "      <td>223.942286</td>\n",
       "      <td>172.272042</td>\n",
       "      <td>201.715548</td>\n",
       "      <td>165.343468</td>\n",
       "      <td>181.189056</td>\n",
       "      <td>...</td>\n",
       "      <td>162.676355</td>\n",
       "      <td>160.308657</td>\n",
       "      <td>171.779385</td>\n",
       "      <td>134.252465</td>\n",
       "      <td>210.414208</td>\n",
       "      <td>182.083076</td>\n",
       "      <td>155.812278</td>\n",
       "      <td>184.634597</td>\n",
       "      <td>110.600911</td>\n",
       "      <td>152.888042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.270370</td>\n",
       "      <td>191.619220</td>\n",
       "      <td>121.106482</td>\n",
       "      <td>162.092142</td>\n",
       "      <td>203.169182</td>\n",
       "      <td>223.871564</td>\n",
       "      <td>172.093343</td>\n",
       "      <td>201.375005</td>\n",
       "      <td>165.538999</td>\n",
       "      <td>181.421935</td>\n",
       "      <td>...</td>\n",
       "      <td>162.704305</td>\n",
       "      <td>160.165693</td>\n",
       "      <td>172.022959</td>\n",
       "      <td>134.354939</td>\n",
       "      <td>210.195337</td>\n",
       "      <td>181.743372</td>\n",
       "      <td>155.731854</td>\n",
       "      <td>184.679094</td>\n",
       "      <td>110.362424</td>\n",
       "      <td>152.731892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.302643</td>\n",
       "      <td>191.768979</td>\n",
       "      <td>121.366443</td>\n",
       "      <td>162.209259</td>\n",
       "      <td>203.026693</td>\n",
       "      <td>223.798630</td>\n",
       "      <td>171.910986</td>\n",
       "      <td>201.032445</td>\n",
       "      <td>165.733919</td>\n",
       "      <td>181.652777</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732617</td>\n",
       "      <td>160.022876</td>\n",
       "      <td>172.263765</td>\n",
       "      <td>134.456503</td>\n",
       "      <td>209.974871</td>\n",
       "      <td>181.403982</td>\n",
       "      <td>155.654399</td>\n",
       "      <td>184.723051</td>\n",
       "      <td>110.124378</td>\n",
       "      <td>152.574211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.340422</td>\n",
       "      <td>191.921973</td>\n",
       "      <td>121.624512</td>\n",
       "      <td>162.325496</td>\n",
       "      <td>202.881402</td>\n",
       "      <td>223.723243</td>\n",
       "      <td>171.725116</td>\n",
       "      <td>200.688059</td>\n",
       "      <td>165.928499</td>\n",
       "      <td>181.881396</td>\n",
       "      <td>...</td>\n",
       "      <td>162.761213</td>\n",
       "      <td>159.880347</td>\n",
       "      <td>172.501652</td>\n",
       "      <td>134.557184</td>\n",
       "      <td>209.752425</td>\n",
       "      <td>181.064774</td>\n",
       "      <td>155.579700</td>\n",
       "      <td>184.766445</td>\n",
       "      <td>109.886510</td>\n",
       "      <td>152.415073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.383411</td>\n",
       "      <td>192.077960</td>\n",
       "      <td>121.880971</td>\n",
       "      <td>162.441055</td>\n",
       "      <td>202.733207</td>\n",
       "      <td>223.645400</td>\n",
       "      <td>171.535999</td>\n",
       "      <td>200.342203</td>\n",
       "      <td>166.123080</td>\n",
       "      <td>182.107430</td>\n",
       "      <td>...</td>\n",
       "      <td>162.790045</td>\n",
       "      <td>159.738426</td>\n",
       "      <td>172.736318</td>\n",
       "      <td>134.657218</td>\n",
       "      <td>209.527727</td>\n",
       "      <td>180.725519</td>\n",
       "      <td>155.507653</td>\n",
       "      <td>184.809417</td>\n",
       "      <td>109.648511</td>\n",
       "      <td>152.254752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>183.851947</td>\n",
       "      <td>169.897148</td>\n",
       "      <td>138.281160</td>\n",
       "      <td>108.937324</td>\n",
       "      <td>232.694714</td>\n",
       "      <td>217.587301</td>\n",
       "      <td>196.776191</td>\n",
       "      <td>181.842620</td>\n",
       "      <td>177.813604</td>\n",
       "      <td>170.610492</td>\n",
       "      <td>...</td>\n",
       "      <td>162.979941</td>\n",
       "      <td>179.132099</td>\n",
       "      <td>171.526947</td>\n",
       "      <td>112.014481</td>\n",
       "      <td>220.481556</td>\n",
       "      <td>181.341623</td>\n",
       "      <td>187.811124</td>\n",
       "      <td>171.976998</td>\n",
       "      <td>136.395307</td>\n",
       "      <td>108.910087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>183.511357</td>\n",
       "      <td>169.679639</td>\n",
       "      <td>138.390898</td>\n",
       "      <td>109.054093</td>\n",
       "      <td>232.650925</td>\n",
       "      <td>217.531290</td>\n",
       "      <td>196.876168</td>\n",
       "      <td>182.050297</td>\n",
       "      <td>177.707430</td>\n",
       "      <td>170.562320</td>\n",
       "      <td>...</td>\n",
       "      <td>162.718275</td>\n",
       "      <td>179.004554</td>\n",
       "      <td>171.365925</td>\n",
       "      <td>112.141669</td>\n",
       "      <td>220.444739</td>\n",
       "      <td>181.449889</td>\n",
       "      <td>187.662169</td>\n",
       "      <td>171.959291</td>\n",
       "      <td>136.412017</td>\n",
       "      <td>109.095598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>183.169072</td>\n",
       "      <td>169.462522</td>\n",
       "      <td>138.500958</td>\n",
       "      <td>109.170887</td>\n",
       "      <td>232.606076</td>\n",
       "      <td>217.473657</td>\n",
       "      <td>196.976591</td>\n",
       "      <td>182.254254</td>\n",
       "      <td>177.601171</td>\n",
       "      <td>170.514881</td>\n",
       "      <td>...</td>\n",
       "      <td>162.454129</td>\n",
       "      <td>178.877790</td>\n",
       "      <td>171.205267</td>\n",
       "      <td>112.271075</td>\n",
       "      <td>220.409626</td>\n",
       "      <td>181.556351</td>\n",
       "      <td>187.512505</td>\n",
       "      <td>171.945252</td>\n",
       "      <td>136.426329</td>\n",
       "      <td>109.279941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>182.825475</td>\n",
       "      <td>169.246118</td>\n",
       "      <td>138.611468</td>\n",
       "      <td>109.287583</td>\n",
       "      <td>232.560118</td>\n",
       "      <td>217.414344</td>\n",
       "      <td>197.077582</td>\n",
       "      <td>182.454750</td>\n",
       "      <td>177.494501</td>\n",
       "      <td>170.468468</td>\n",
       "      <td>...</td>\n",
       "      <td>162.187125</td>\n",
       "      <td>178.751987</td>\n",
       "      <td>171.044821</td>\n",
       "      <td>112.402489</td>\n",
       "      <td>220.376130</td>\n",
       "      <td>181.660586</td>\n",
       "      <td>187.362046</td>\n",
       "      <td>171.934720</td>\n",
       "      <td>136.438444</td>\n",
       "      <td>109.462795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>182.480920</td>\n",
       "      <td>169.030592</td>\n",
       "      <td>138.722488</td>\n",
       "      <td>109.404024</td>\n",
       "      <td>232.513155</td>\n",
       "      <td>217.353402</td>\n",
       "      <td>197.179353</td>\n",
       "      <td>182.652230</td>\n",
       "      <td>177.387208</td>\n",
       "      <td>170.423237</td>\n",
       "      <td>...</td>\n",
       "      <td>161.917079</td>\n",
       "      <td>178.627367</td>\n",
       "      <td>170.884336</td>\n",
       "      <td>112.535554</td>\n",
       "      <td>220.344095</td>\n",
       "      <td>181.762446</td>\n",
       "      <td>187.210616</td>\n",
       "      <td>171.927396</td>\n",
       "      <td>136.448449</td>\n",
       "      <td>109.643939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     165.243675  191.472930  120.844514  161.973862  203.309157  223.942286   \n",
       "1     165.270370  191.619220  121.106482  162.092142  203.169182  223.871564   \n",
       "2     165.302643  191.768979  121.366443  162.209259  203.026693  223.798630   \n",
       "3     165.340422  191.921973  121.624512  162.325496  202.881402  223.723243   \n",
       "4     165.383411  192.077960  121.880971  162.441055  202.733207  223.645400   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  183.851947  169.897148  138.281160  108.937324  232.694714  217.587301   \n",
       "2439  183.511357  169.679639  138.390898  109.054093  232.650925  217.531290   \n",
       "2440  183.169072  169.462522  138.500958  109.170887  232.606076  217.473657   \n",
       "2441  182.825475  169.246118  138.611468  109.287583  232.560118  217.414344   \n",
       "2442  182.480920  169.030592  138.722488  109.404024  232.513155  217.353402   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     172.272042  201.715548  165.343468  181.189056  ...  162.676355   \n",
       "1     172.093343  201.375005  165.538999  181.421935  ...  162.704305   \n",
       "2     171.910986  201.032445  165.733919  181.652777  ...  162.732617   \n",
       "3     171.725116  200.688059  165.928499  181.881396  ...  162.761213   \n",
       "4     171.535999  200.342203  166.123080  182.107430  ...  162.790045   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.776191  181.842620  177.813604  170.610492  ...  162.979941   \n",
       "2439  196.876168  182.050297  177.707430  170.562320  ...  162.718275   \n",
       "2440  196.976591  182.254254  177.601171  170.514881  ...  162.454129   \n",
       "2441  197.077582  182.454750  177.494501  170.468468  ...  162.187125   \n",
       "2442  197.179353  182.652230  177.387208  170.423237  ...  161.917079   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     160.308657  171.779385  134.252465  210.414208  182.083076  155.812278   \n",
       "1     160.165693  172.022959  134.354939  210.195337  181.743372  155.731854   \n",
       "2     160.022876  172.263765  134.456503  209.974871  181.403982  155.654399   \n",
       "3     159.880347  172.501652  134.557184  209.752425  181.064774  155.579700   \n",
       "4     159.738426  172.736318  134.657218  209.527727  180.725519  155.507653   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  179.132099  171.526947  112.014481  220.481556  181.341623  187.811124   \n",
       "2439  179.004554  171.365925  112.141669  220.444739  181.449889  187.662169   \n",
       "2440  178.877790  171.205267  112.271075  220.409626  181.556351  187.512505   \n",
       "2441  178.751987  171.044821  112.402489  220.376130  181.660586  187.362046   \n",
       "2442  178.627367  170.884336  112.535554  220.344095  181.762446  187.210616   \n",
       "\n",
       "              45          46          47  \n",
       "0     184.634597  110.600911  152.888042  \n",
       "1     184.679094  110.362424  152.731892  \n",
       "2     184.723051  110.124378  152.574211  \n",
       "3     184.766445  109.886510  152.415073  \n",
       "4     184.809417  109.648511  152.254752  \n",
       "...          ...         ...         ...  \n",
       "2438  171.976998  136.395307  108.910087  \n",
       "2439  171.959291  136.412017  109.095598  \n",
       "2440  171.945252  136.426329  109.279941  \n",
       "2441  171.934720  136.438444  109.462795  \n",
       "2442  171.927396  136.448449  109.643939  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.243675</td>\n",
       "      <td>191.472930</td>\n",
       "      <td>120.844514</td>\n",
       "      <td>161.973862</td>\n",
       "      <td>203.309157</td>\n",
       "      <td>223.942286</td>\n",
       "      <td>172.272042</td>\n",
       "      <td>201.715548</td>\n",
       "      <td>165.343468</td>\n",
       "      <td>181.189056</td>\n",
       "      <td>...</td>\n",
       "      <td>162.676355</td>\n",
       "      <td>160.308657</td>\n",
       "      <td>171.779385</td>\n",
       "      <td>134.252465</td>\n",
       "      <td>210.414208</td>\n",
       "      <td>182.083076</td>\n",
       "      <td>155.812278</td>\n",
       "      <td>184.634597</td>\n",
       "      <td>110.600911</td>\n",
       "      <td>152.888042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.270370</td>\n",
       "      <td>191.619220</td>\n",
       "      <td>121.106482</td>\n",
       "      <td>162.092142</td>\n",
       "      <td>203.169182</td>\n",
       "      <td>223.871564</td>\n",
       "      <td>172.093343</td>\n",
       "      <td>201.375005</td>\n",
       "      <td>165.538999</td>\n",
       "      <td>181.421935</td>\n",
       "      <td>...</td>\n",
       "      <td>162.704305</td>\n",
       "      <td>160.165693</td>\n",
       "      <td>172.022959</td>\n",
       "      <td>134.354939</td>\n",
       "      <td>210.195337</td>\n",
       "      <td>181.743372</td>\n",
       "      <td>155.731854</td>\n",
       "      <td>184.679094</td>\n",
       "      <td>110.362424</td>\n",
       "      <td>152.731892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.302643</td>\n",
       "      <td>191.768979</td>\n",
       "      <td>121.366443</td>\n",
       "      <td>162.209259</td>\n",
       "      <td>203.026693</td>\n",
       "      <td>223.798630</td>\n",
       "      <td>171.910986</td>\n",
       "      <td>201.032445</td>\n",
       "      <td>165.733919</td>\n",
       "      <td>181.652777</td>\n",
       "      <td>...</td>\n",
       "      <td>162.732617</td>\n",
       "      <td>160.022876</td>\n",
       "      <td>172.263765</td>\n",
       "      <td>134.456503</td>\n",
       "      <td>209.974871</td>\n",
       "      <td>181.403982</td>\n",
       "      <td>155.654399</td>\n",
       "      <td>184.723051</td>\n",
       "      <td>110.124378</td>\n",
       "      <td>152.574211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.340422</td>\n",
       "      <td>191.921973</td>\n",
       "      <td>121.624512</td>\n",
       "      <td>162.325496</td>\n",
       "      <td>202.881402</td>\n",
       "      <td>223.723243</td>\n",
       "      <td>171.725116</td>\n",
       "      <td>200.688059</td>\n",
       "      <td>165.928499</td>\n",
       "      <td>181.881396</td>\n",
       "      <td>...</td>\n",
       "      <td>162.761213</td>\n",
       "      <td>159.880347</td>\n",
       "      <td>172.501652</td>\n",
       "      <td>134.557184</td>\n",
       "      <td>209.752425</td>\n",
       "      <td>181.064774</td>\n",
       "      <td>155.579700</td>\n",
       "      <td>184.766445</td>\n",
       "      <td>109.886510</td>\n",
       "      <td>152.415073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.383411</td>\n",
       "      <td>192.077960</td>\n",
       "      <td>121.880971</td>\n",
       "      <td>162.441055</td>\n",
       "      <td>202.733207</td>\n",
       "      <td>223.645400</td>\n",
       "      <td>171.535999</td>\n",
       "      <td>200.342203</td>\n",
       "      <td>166.123080</td>\n",
       "      <td>182.107430</td>\n",
       "      <td>...</td>\n",
       "      <td>162.790045</td>\n",
       "      <td>159.738426</td>\n",
       "      <td>172.736318</td>\n",
       "      <td>134.657218</td>\n",
       "      <td>209.527727</td>\n",
       "      <td>180.725519</td>\n",
       "      <td>155.507653</td>\n",
       "      <td>184.809417</td>\n",
       "      <td>109.648511</td>\n",
       "      <td>152.254752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>183.851947</td>\n",
       "      <td>169.897148</td>\n",
       "      <td>138.281160</td>\n",
       "      <td>108.937324</td>\n",
       "      <td>232.694714</td>\n",
       "      <td>217.587301</td>\n",
       "      <td>196.776191</td>\n",
       "      <td>181.842620</td>\n",
       "      <td>177.813604</td>\n",
       "      <td>170.610492</td>\n",
       "      <td>...</td>\n",
       "      <td>162.979941</td>\n",
       "      <td>179.132099</td>\n",
       "      <td>171.526947</td>\n",
       "      <td>112.014481</td>\n",
       "      <td>220.481556</td>\n",
       "      <td>181.341623</td>\n",
       "      <td>187.811124</td>\n",
       "      <td>171.976998</td>\n",
       "      <td>136.395307</td>\n",
       "      <td>108.910087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>183.511357</td>\n",
       "      <td>169.679639</td>\n",
       "      <td>138.390898</td>\n",
       "      <td>109.054093</td>\n",
       "      <td>232.650925</td>\n",
       "      <td>217.531290</td>\n",
       "      <td>196.876168</td>\n",
       "      <td>182.050297</td>\n",
       "      <td>177.707430</td>\n",
       "      <td>170.562320</td>\n",
       "      <td>...</td>\n",
       "      <td>162.718275</td>\n",
       "      <td>179.004554</td>\n",
       "      <td>171.365925</td>\n",
       "      <td>112.141669</td>\n",
       "      <td>220.444739</td>\n",
       "      <td>181.449889</td>\n",
       "      <td>187.662169</td>\n",
       "      <td>171.959291</td>\n",
       "      <td>136.412017</td>\n",
       "      <td>109.095598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>183.169072</td>\n",
       "      <td>169.462522</td>\n",
       "      <td>138.500958</td>\n",
       "      <td>109.170887</td>\n",
       "      <td>232.606076</td>\n",
       "      <td>217.473657</td>\n",
       "      <td>196.976591</td>\n",
       "      <td>182.254254</td>\n",
       "      <td>177.601171</td>\n",
       "      <td>170.514881</td>\n",
       "      <td>...</td>\n",
       "      <td>162.454129</td>\n",
       "      <td>178.877790</td>\n",
       "      <td>171.205267</td>\n",
       "      <td>112.271075</td>\n",
       "      <td>220.409626</td>\n",
       "      <td>181.556351</td>\n",
       "      <td>187.512505</td>\n",
       "      <td>171.945252</td>\n",
       "      <td>136.426329</td>\n",
       "      <td>109.279941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>182.825475</td>\n",
       "      <td>169.246118</td>\n",
       "      <td>138.611468</td>\n",
       "      <td>109.287583</td>\n",
       "      <td>232.560118</td>\n",
       "      <td>217.414344</td>\n",
       "      <td>197.077582</td>\n",
       "      <td>182.454750</td>\n",
       "      <td>177.494501</td>\n",
       "      <td>170.468468</td>\n",
       "      <td>...</td>\n",
       "      <td>162.187125</td>\n",
       "      <td>178.751987</td>\n",
       "      <td>171.044821</td>\n",
       "      <td>112.402489</td>\n",
       "      <td>220.376130</td>\n",
       "      <td>181.660586</td>\n",
       "      <td>187.362046</td>\n",
       "      <td>171.934720</td>\n",
       "      <td>136.438444</td>\n",
       "      <td>109.462795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>182.480920</td>\n",
       "      <td>169.030592</td>\n",
       "      <td>138.722488</td>\n",
       "      <td>109.404024</td>\n",
       "      <td>232.513155</td>\n",
       "      <td>217.353402</td>\n",
       "      <td>197.179353</td>\n",
       "      <td>182.652230</td>\n",
       "      <td>177.387208</td>\n",
       "      <td>170.423237</td>\n",
       "      <td>...</td>\n",
       "      <td>161.917079</td>\n",
       "      <td>178.627367</td>\n",
       "      <td>170.884336</td>\n",
       "      <td>112.535554</td>\n",
       "      <td>220.344095</td>\n",
       "      <td>181.762446</td>\n",
       "      <td>187.210616</td>\n",
       "      <td>171.927396</td>\n",
       "      <td>136.448449</td>\n",
       "      <td>109.643939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.243675  191.472930  120.844514  161.973862  203.309157  223.942286   \n",
       "1     165.270370  191.619220  121.106482  162.092142  203.169182  223.871564   \n",
       "2     165.302643  191.768979  121.366443  162.209259  203.026693  223.798630   \n",
       "3     165.340422  191.921973  121.624512  162.325496  202.881402  223.723243   \n",
       "4     165.383411  192.077960  121.880971  162.441055  202.733207  223.645400   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  183.851947  169.897148  138.281160  108.937324  232.694714  217.587301   \n",
       "2439  183.511357  169.679639  138.390898  109.054093  232.650925  217.531290   \n",
       "2440  183.169072  169.462522  138.500958  109.170887  232.606076  217.473657   \n",
       "2441  182.825475  169.246118  138.611468  109.287583  232.560118  217.414344   \n",
       "2442  182.480920  169.030592  138.722488  109.404024  232.513155  217.353402   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     172.272042  201.715548  165.343468  181.189056  ...  162.676355   \n",
       "1     172.093343  201.375005  165.538999  181.421935  ...  162.704305   \n",
       "2     171.910986  201.032445  165.733919  181.652777  ...  162.732617   \n",
       "3     171.725116  200.688059  165.928499  181.881396  ...  162.761213   \n",
       "4     171.535999  200.342203  166.123080  182.107430  ...  162.790045   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  196.776191  181.842620  177.813604  170.610492  ...  162.979941   \n",
       "2439  196.876168  182.050297  177.707430  170.562320  ...  162.718275   \n",
       "2440  196.976591  182.254254  177.601171  170.514881  ...  162.454129   \n",
       "2441  197.077582  182.454750  177.494501  170.468468  ...  162.187125   \n",
       "2442  197.179353  182.652230  177.387208  170.423237  ...  161.917079   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     160.308657  171.779385  134.252465  210.414208  182.083076  155.812278   \n",
       "1     160.165693  172.022959  134.354939  210.195337  181.743372  155.731854   \n",
       "2     160.022876  172.263765  134.456503  209.974871  181.403982  155.654399   \n",
       "3     159.880347  172.501652  134.557184  209.752425  181.064774  155.579700   \n",
       "4     159.738426  172.736318  134.657218  209.527727  180.725519  155.507653   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  179.132099  171.526947  112.014481  220.481556  181.341623  187.811124   \n",
       "2439  179.004554  171.365925  112.141669  220.444739  181.449889  187.662169   \n",
       "2440  178.877790  171.205267  112.271075  220.409626  181.556351  187.512505   \n",
       "2441  178.751987  171.044821  112.402489  220.376130  181.660586  187.362046   \n",
       "2442  178.627367  170.884336  112.535554  220.344095  181.762446  187.210616   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     184.634597  110.600911  152.888042  \n",
       "1     184.679094  110.362424  152.731892  \n",
       "2     184.723051  110.124378  152.574211  \n",
       "3     184.766445  109.886510  152.415073  \n",
       "4     184.809417  109.648511  152.254752  \n",
       "...          ...         ...         ...  \n",
       "2438  171.976998  136.395307  108.910087  \n",
       "2439  171.959291  136.412017  109.095598  \n",
       "2440  171.945252  136.426329  109.279941  \n",
       "2441  171.934720  136.438444  109.462795  \n",
       "2442  171.927396  136.448449  109.643939  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165.243675</td>\n",
       "      <td>191.472930</td>\n",
       "      <td>120.844514</td>\n",
       "      <td>161.973862</td>\n",
       "      <td>203.309157</td>\n",
       "      <td>223.942286</td>\n",
       "      <td>172.272042</td>\n",
       "      <td>201.715548</td>\n",
       "      <td>165.343468</td>\n",
       "      <td>181.189056</td>\n",
       "      <td>124.501448</td>\n",
       "      <td>146.250305</td>\n",
       "      <td>205.312542</td>\n",
       "      <td>216.888091</td>\n",
       "      <td>178.501477</td>\n",
       "      <td>188.909294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.270370</td>\n",
       "      <td>191.619220</td>\n",
       "      <td>121.106482</td>\n",
       "      <td>162.092142</td>\n",
       "      <td>203.169182</td>\n",
       "      <td>223.871564</td>\n",
       "      <td>172.093343</td>\n",
       "      <td>201.375005</td>\n",
       "      <td>165.538999</td>\n",
       "      <td>181.421935</td>\n",
       "      <td>124.634918</td>\n",
       "      <td>146.301862</td>\n",
       "      <td>205.096722</td>\n",
       "      <td>216.735410</td>\n",
       "      <td>178.085018</td>\n",
       "      <td>188.734022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.302643</td>\n",
       "      <td>191.768979</td>\n",
       "      <td>121.366443</td>\n",
       "      <td>162.209259</td>\n",
       "      <td>203.026693</td>\n",
       "      <td>223.798630</td>\n",
       "      <td>171.910986</td>\n",
       "      <td>201.032445</td>\n",
       "      <td>165.733919</td>\n",
       "      <td>181.652777</td>\n",
       "      <td>124.769863</td>\n",
       "      <td>146.351700</td>\n",
       "      <td>204.882481</td>\n",
       "      <td>216.581892</td>\n",
       "      <td>177.671452</td>\n",
       "      <td>188.556327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165.340422</td>\n",
       "      <td>191.921973</td>\n",
       "      <td>121.624512</td>\n",
       "      <td>162.325496</td>\n",
       "      <td>202.881402</td>\n",
       "      <td>223.723243</td>\n",
       "      <td>171.725116</td>\n",
       "      <td>200.688059</td>\n",
       "      <td>165.928499</td>\n",
       "      <td>181.881396</td>\n",
       "      <td>124.906493</td>\n",
       "      <td>146.399853</td>\n",
       "      <td>204.670000</td>\n",
       "      <td>216.427796</td>\n",
       "      <td>177.260448</td>\n",
       "      <td>188.376345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165.383411</td>\n",
       "      <td>192.077960</td>\n",
       "      <td>121.880971</td>\n",
       "      <td>162.441055</td>\n",
       "      <td>202.733207</td>\n",
       "      <td>223.645400</td>\n",
       "      <td>171.535999</td>\n",
       "      <td>200.342203</td>\n",
       "      <td>166.123080</td>\n",
       "      <td>182.107430</td>\n",
       "      <td>125.044908</td>\n",
       "      <td>146.446366</td>\n",
       "      <td>204.459598</td>\n",
       "      <td>216.273418</td>\n",
       "      <td>176.851475</td>\n",
       "      <td>188.194345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>183.851947</td>\n",
       "      <td>169.897148</td>\n",
       "      <td>138.281160</td>\n",
       "      <td>108.937324</td>\n",
       "      <td>232.694714</td>\n",
       "      <td>217.587301</td>\n",
       "      <td>196.776191</td>\n",
       "      <td>181.842620</td>\n",
       "      <td>177.813604</td>\n",
       "      <td>170.610492</td>\n",
       "      <td>125.499410</td>\n",
       "      <td>103.320179</td>\n",
       "      <td>224.263225</td>\n",
       "      <td>217.890769</td>\n",
       "      <td>190.244386</td>\n",
       "      <td>177.895705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>183.511357</td>\n",
       "      <td>169.679639</td>\n",
       "      <td>138.390898</td>\n",
       "      <td>109.054093</td>\n",
       "      <td>232.650925</td>\n",
       "      <td>217.531290</td>\n",
       "      <td>196.876168</td>\n",
       "      <td>182.050297</td>\n",
       "      <td>177.707430</td>\n",
       "      <td>170.562320</td>\n",
       "      <td>125.661896</td>\n",
       "      <td>103.185077</td>\n",
       "      <td>224.236689</td>\n",
       "      <td>217.781971</td>\n",
       "      <td>190.472532</td>\n",
       "      <td>178.015678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>183.169072</td>\n",
       "      <td>169.462522</td>\n",
       "      <td>138.500958</td>\n",
       "      <td>109.170887</td>\n",
       "      <td>232.606076</td>\n",
       "      <td>217.473657</td>\n",
       "      <td>196.976591</td>\n",
       "      <td>182.254254</td>\n",
       "      <td>177.601171</td>\n",
       "      <td>170.514881</td>\n",
       "      <td>125.827358</td>\n",
       "      <td>103.047311</td>\n",
       "      <td>224.209917</td>\n",
       "      <td>217.673901</td>\n",
       "      <td>190.701634</td>\n",
       "      <td>178.135872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>182.825475</td>\n",
       "      <td>169.246118</td>\n",
       "      <td>138.611468</td>\n",
       "      <td>109.287583</td>\n",
       "      <td>232.560118</td>\n",
       "      <td>217.414344</td>\n",
       "      <td>197.077582</td>\n",
       "      <td>182.454750</td>\n",
       "      <td>177.494501</td>\n",
       "      <td>170.468468</td>\n",
       "      <td>125.995980</td>\n",
       "      <td>102.906697</td>\n",
       "      <td>224.183037</td>\n",
       "      <td>217.566461</td>\n",
       "      <td>190.931821</td>\n",
       "      <td>178.255972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>182.480920</td>\n",
       "      <td>169.030592</td>\n",
       "      <td>138.722488</td>\n",
       "      <td>109.404024</td>\n",
       "      <td>232.513155</td>\n",
       "      <td>217.353402</td>\n",
       "      <td>197.179353</td>\n",
       "      <td>182.652230</td>\n",
       "      <td>177.387208</td>\n",
       "      <td>170.423237</td>\n",
       "      <td>126.167780</td>\n",
       "      <td>102.763120</td>\n",
       "      <td>224.156198</td>\n",
       "      <td>217.459584</td>\n",
       "      <td>191.163324</td>\n",
       "      <td>178.375786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     165.243675  191.472930  120.844514  161.973862  203.309157  223.942286   \n",
       "1     165.270370  191.619220  121.106482  162.092142  203.169182  223.871564   \n",
       "2     165.302643  191.768979  121.366443  162.209259  203.026693  223.798630   \n",
       "3     165.340422  191.921973  121.624512  162.325496  202.881402  223.723243   \n",
       "4     165.383411  192.077960  121.880971  162.441055  202.733207  223.645400   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  183.851947  169.897148  138.281160  108.937324  232.694714  217.587301   \n",
       "2439  183.511357  169.679639  138.390898  109.054093  232.650925  217.531290   \n",
       "2440  183.169072  169.462522  138.500958  109.170887  232.606076  217.473657   \n",
       "2441  182.825475  169.246118  138.611468  109.287583  232.560118  217.414344   \n",
       "2442  182.480920  169.030592  138.722488  109.404024  232.513155  217.353402   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10    sensor11    sensor12  \\\n",
       "0     172.272042  201.715548  165.343468  181.189056  124.501448  146.250305   \n",
       "1     172.093343  201.375005  165.538999  181.421935  124.634918  146.301862   \n",
       "2     171.910986  201.032445  165.733919  181.652777  124.769863  146.351700   \n",
       "3     171.725116  200.688059  165.928499  181.881396  124.906493  146.399853   \n",
       "4     171.535999  200.342203  166.123080  182.107430  125.044908  146.446366   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  196.776191  181.842620  177.813604  170.610492  125.499410  103.320179   \n",
       "2439  196.876168  182.050297  177.707430  170.562320  125.661896  103.185077   \n",
       "2440  196.976591  182.254254  177.601171  170.514881  125.827358  103.047311   \n",
       "2441  197.077582  182.454750  177.494501  170.468468  125.995980  102.906697   \n",
       "2442  197.179353  182.652230  177.387208  170.423237  126.167780  102.763120   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     205.312542  216.888091  178.501477  188.909294  \n",
       "1     205.096722  216.735410  178.085018  188.734022  \n",
       "2     204.882481  216.581892  177.671452  188.556327  \n",
       "3     204.670000  216.427796  177.260448  188.376345  \n",
       "4     204.459598  216.273418  176.851475  188.194345  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  224.263225  217.890769  190.244386  177.895705  \n",
       "2439  224.236689  217.781971  190.472532  178.015678  \n",
       "2440  224.209917  217.673901  190.701634  178.135872  \n",
       "2441  224.183037  217.566461  190.931821  178.255972  \n",
       "2442  224.156198  217.459584  191.163324  178.375786  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 15s 12ms/step - loss: 3307.1646 - val_loss: 2495.8726\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1980.1881 - val_loss: 1687.8156\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1443.5504 - val_loss: 1247.4161\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1095.7101 - val_loss: 971.6105\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 816.1617 - val_loss: 672.3507\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 568.4372 - val_loss: 466.6648\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 336.9753 - val_loss: 232.8880\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 172.3276 - val_loss: 120.5307\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 81.9227 - val_loss: 48.9641\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 40.5566 - val_loss: 27.9034\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.6272 - val_loss: 22.2182\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.5182 - val_loss: 14.3987\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8712 - val_loss: 9.2349\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.0020 - val_loss: 7.2525\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.5799 - val_loss: 3.9578\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.3893 - val_loss: 4.5533\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3206 - val_loss: 7.6272\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5698 - val_loss: 2.2984\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.3662 - val_loss: 20.8174\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0577 - val_loss: 2.5226\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1264 - val_loss: 5.2781\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1560 - val_loss: 1.4657\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9106 - val_loss: 2.1627\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9633 - val_loss: 4.3953\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4772 - val_loss: 1.4906\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1305 - val_loss: 3.6324\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8238 - val_loss: 1.5264\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7153 - val_loss: 3.0709\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6529 - val_loss: 2.0875\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9085 - val_loss: 2.7896\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.6206 - val_loss: 1.7892\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4043 - val_loss: 2.6153\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3812 - val_loss: 1.3200\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4041 - val_loss: 2.8804\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7003 - val_loss: 2.6328\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.7144 - val_loss: 3.0594\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.1934 - val_loss: 1.1595\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6501 - val_loss: 2.9832\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4994 - val_loss: 2.8252\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5271 - val_loss: 7.0118\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0621 - val_loss: 3.5133\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4835 - val_loss: 1.1870\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2215 - val_loss: 1.3492\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.0577 - val_loss: 1.5324\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.2533 - val_loss: 6.9128\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0386 - val_loss: 0.7611\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7503 - val_loss: 0.7391\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.7150 - val_loss: 2.7054\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9120 - val_loss: 1.2818\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.3117 - val_loss: 1.1268\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6204 - val_loss: 0.6640\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0535 - val_loss: 0.9072\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2183 - val_loss: 2.2276\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3309 - val_loss: 38.6659\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.5645 - val_loss: 0.9350\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4118 - val_loss: 0.2679\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4451 - val_loss: 0.7186\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4725 - val_loss: 0.4433\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4623 - val_loss: 0.9838\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5157 - val_loss: 0.6012\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2397 - val_loss: 91.4212\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.3901 - val_loss: 1.6300\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5229 - val_loss: 0.2932\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4029 - val_loss: 2.1225\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5294 - val_loss: 1.1135\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5603 - val_loss: 0.2624\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5687 - val_loss: 2.1233\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6386 - val_loss: 0.7931\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5343 - val_loss: 3.7911\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.6803 - val_loss: 0.8298\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5655 - val_loss: 0.4852\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4171 - val_loss: 0.9094\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5441 - val_loss: 0.3841\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5359 - val_loss: 1.0168\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7344 - val_loss: 2.8770\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9930 - val_loss: 0.5901\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3585 - val_loss: 0.7797\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0478 - val_loss: 0.9731\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3072 - val_loss: 0.1652\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1312 - val_loss: 0.8788\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4541 - val_loss: 0.9584\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2649 - val_loss: 0.3353\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2394 - val_loss: 0.6542\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3981 - val_loss: 0.4765\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5820 - val_loss: 0.3268\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4118 - val_loss: 0.7693\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4217 - val_loss: 0.2596\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3762 - val_loss: 0.2379\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4597 - val_loss: 0.8120\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6161 - val_loss: 1.2518\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.9415 - val_loss: 5.9441\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6766 - val_loss: 0.3347\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2698 - val_loss: 0.4330\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2421 - val_loss: 0.7418\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2440 - val_loss: 0.1964\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2455 - val_loss: 0.3548\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3083 - val_loss: 0.3933\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4707 - val_loss: 0.2394\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4419 - val_loss: 0.3591\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3193 - val_loss: 0.3945\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5092 - val_loss: 2.3407\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3576 - val_loss: 0.4800\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2222 - val_loss: 0.1997\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1413 - val_loss: 0.3364\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2092 - val_loss: 0.2009\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5411 - val_loss: 1.8700\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3902 - val_loss: 0.5099\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2852 - val_loss: 1.0499\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3427 - val_loss: 0.7230\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.16521215362330874\n",
      "Mean Absolute Error (MAE): 0.29713676446098825\n",
      "Root Mean Squared Error (RMSE): 0.4064629794007183\n",
      "Time taken: 357.5149943828583\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 3304.6838 - val_loss: 2467.5703\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2169.3928 - val_loss: 1855.1104\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1670.0320 - val_loss: 1416.9994\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1233.5072 - val_loss: 1148.1067\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 881.2229 - val_loss: 720.4924\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 625.0475 - val_loss: 517.5911\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 406.1875 - val_loss: 290.4814\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 220.4250 - val_loss: 155.4549\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 107.0560 - val_loss: 84.0077\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 50.5705 - val_loss: 34.3971\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.3064 - val_loss: 35.8903\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 15.7943 - val_loss: 13.6618\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.7592 - val_loss: 13.4038\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8222 - val_loss: 14.0398\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9045 - val_loss: 3.3484\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.4133 - val_loss: 2.9846\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6102 - val_loss: 3.0860\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6546 - val_loss: 3.7239\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8662 - val_loss: 3.1728\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2105 - val_loss: 4.0459\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8560 - val_loss: 4.3002\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3912 - val_loss: 2.5415\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3554 - val_loss: 3.5549\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0260 - val_loss: 2.6320\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4744 - val_loss: 1.7615\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3307 - val_loss: 1.2799\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2400 - val_loss: 12.3388\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2849 - val_loss: 1.4010\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5860 - val_loss: 1.7256\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4382 - val_loss: 1.0801\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2723 - val_loss: 3.6300\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4431 - val_loss: 1.1012\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4363 - val_loss: 1.5799\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6777 - val_loss: 0.7998\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8425 - val_loss: 1.8897\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6251 - val_loss: 1.3032\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4453 - val_loss: 2.2010\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.9805 - val_loss: 6.2929\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.6127 - val_loss: 1.3542\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6855 - val_loss: 0.5477\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6683 - val_loss: 0.6572\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9433 - val_loss: 0.9643\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7684 - val_loss: 1.0442\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3634 - val_loss: 2.8673\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3599 - val_loss: 1.2852\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7335 - val_loss: 2.0622\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3247 - val_loss: 1.2076\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6331 - val_loss: 3.3763\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9908 - val_loss: 0.4349\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4883 - val_loss: 0.6821\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6378 - val_loss: 0.4404\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6767 - val_loss: 0.7866\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7306 - val_loss: 0.7083\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7164 - val_loss: 1.6757\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9531 - val_loss: 0.9488\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2807 - val_loss: 0.8317\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6845 - val_loss: 1.3410\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.0240 - val_loss: 3.3601\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8799 - val_loss: 0.6540\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4461 - val_loss: 0.8065\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1.5022 - val_loss: 1.6198\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.9623 - val_loss: 1.7614\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 1.0087 - val_loss: 1.0928\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.8168 - val_loss: 1.0971\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 0.9282 - val_loss: 1.1033\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.5237 - val_loss: 0.8133\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.7221 - val_loss: 0.8671\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 5s 15ms/step - loss: 0.9584 - val_loss: 0.4421\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4872 - val_loss: 0.4618\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.7951 - val_loss: 1.0909\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 3.1569 - val_loss: 0.6310\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 5s 14ms/step - loss: 0.4463 - val_loss: 0.3066\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4015 - val_loss: 1.0428\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5082 - val_loss: 0.4964\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3858 - val_loss: 0.4022\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4395 - val_loss: 1.8705\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8572 - val_loss: 0.3547\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5908 - val_loss: 0.5482\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5402 - val_loss: 4.9562\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 12ms/step - loss: 0.8605 - val_loss: 0.6968\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 8s 25ms/step - loss: 0.2840 - val_loss: 0.4110\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3938 - val_loss: 0.3906\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3294 - val_loss: 0.8114\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8744 - val_loss: 1.5033\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6272 - val_loss: 1.1477\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4848 - val_loss: 0.4653\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3938 - val_loss: 0.2489\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 4.6536 - val_loss: 1.2999\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2725 - val_loss: 0.1401\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1979 - val_loss: 0.1461\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1719 - val_loss: 0.1797\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1885 - val_loss: 0.2997\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2728 - val_loss: 0.3168\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3356 - val_loss: 0.7392\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3257 - val_loss: 0.1726\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2890 - val_loss: 0.8286\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.6159 - val_loss: 0.7758\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5018 - val_loss: 0.5393\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4160 - val_loss: 0.6845\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4340 - val_loss: 0.4262\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4308 - val_loss: 0.2929\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3701 - val_loss: 0.8533\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7592 - val_loss: 1.1634\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5249 - val_loss: 0.6558\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3725 - val_loss: 0.4331\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2851 - val_loss: 1.2545\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4072 - val_loss: 0.4292\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.3954 - val_loss: 8.6646\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5288 - val_loss: 0.3627\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2732 - val_loss: 0.1093\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1634 - val_loss: 0.1191\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1555 - val_loss: 0.1390\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1630 - val_loss: 0.1951\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3296 - val_loss: 0.5358\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2578 - val_loss: 0.3846\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5919 - val_loss: 0.3624\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2217 - val_loss: 0.4127\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3409 - val_loss: 0.7358\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3467 - val_loss: 0.3925\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4202 - val_loss: 0.3738\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3686 - val_loss: 0.8066\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.4427 - val_loss: 0.5064\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6134 - val_loss: 0.6774\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.7549 - val_loss: 0.7191\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2802 - val_loss: 0.3772\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1512 - val_loss: 0.1456\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2061 - val_loss: 0.3686\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2193 - val_loss: 0.7388\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2226 - val_loss: 0.1725\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.1996 - val_loss: 0.1527\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2991 - val_loss: 0.7669\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3904 - val_loss: 0.6190\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3789 - val_loss: 1.5969\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.5605 - val_loss: 2.7086\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3814 - val_loss: 0.4366\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2628 - val_loss: 0.3967\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3414 - val_loss: 2.4087\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4441 - val_loss: 0.7441\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.2614 - val_loss: 0.2385\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.3190 - val_loss: 0.4216\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.10929231625854792\n",
      "Mean Absolute Error (MAE): 0.2532699515142584\n",
      "Root Mean Squared Error (RMSE): 0.3305938841820095\n",
      "Time taken: 529.3427472114563\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 10s 17ms/step - loss: 3388.6980 - val_loss: 2594.3140\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2315.6919 - val_loss: 2046.1447\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1832.4872 - val_loss: 1579.8531\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 1343.5000 - val_loss: 1218.7871\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 865.4461 - val_loss: 670.9290\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 518.5297 - val_loss: 370.5787\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 270.3062 - val_loss: 182.8560\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 141.6378 - val_loss: 115.7151\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 66.8175 - val_loss: 54.8679\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 34.8488 - val_loss: 21.7478\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 17.6159 - val_loss: 22.9238\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 13.2090 - val_loss: 6.9521\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.0389 - val_loss: 9.0367\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 6.2281 - val_loss: 3.9570\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 8.5436 - val_loss: 5.0605\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 4s 14ms/step - loss: 4.6363 - val_loss: 5.6967\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.5845 - val_loss: 4.4107\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 4.6524 - val_loss: 4.7743\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 3.2813 - val_loss: 6.6390\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 2.7817 - val_loss: 3.7874\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8202 - val_loss: 2.6255\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.7857 - val_loss: 6.3060\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 3.4143 - val_loss: 5.9196\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.5627 - val_loss: 4.3970\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.7127 - val_loss: 62.3203\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 5.9578 - val_loss: 1.2228\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5412 - val_loss: 3.1191\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.3937 - val_loss: 3.0907\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4387 - val_loss: 2.0209\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0725 - val_loss: 1.8286\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7302 - val_loss: 2.2581\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9639 - val_loss: 4.5994\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8174 - val_loss: 1.4665\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1174 - val_loss: 1.1508\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0272 - val_loss: 1.8713\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0996 - val_loss: 1.5766\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4913 - val_loss: 2.4508\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.8592 - val_loss: 2.8270\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7096 - val_loss: 5.9716\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.5676 - val_loss: 1.9491\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2478 - val_loss: 1.5517\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7848 - val_loss: 0.8208\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8851 - val_loss: 0.6105\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9833 - val_loss: 1.2016\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9345 - val_loss: 3.4156\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4444 - val_loss: 4.0842\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5391 - val_loss: 8.6733\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3737 - val_loss: 0.6121\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0822 - val_loss: 1.1976\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2422 - val_loss: 0.8580\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.6822 - val_loss: 1.0833\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9162 - val_loss: 1.2793\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6283 - val_loss: 0.7960\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5090 - val_loss: 0.4350\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8291 - val_loss: 0.9066\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8176 - val_loss: 2.0604\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6777 - val_loss: 0.6473\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8847 - val_loss: 4.1759\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7335 - val_loss: 2.5209\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2094 - val_loss: 0.3713\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6050 - val_loss: 1.0133\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5976 - val_loss: 0.5481\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6133 - val_loss: 0.9252\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2861 - val_loss: 2.8040\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9259 - val_loss: 1.6336\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9019 - val_loss: 1.6913\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7645 - val_loss: 15.2194\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6569 - val_loss: 0.4178\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3946 - val_loss: 0.7589\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5217 - val_loss: 0.8189\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3496 - val_loss: 0.5304\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7097 - val_loss: 1.1711\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7454 - val_loss: 1.8889\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6668 - val_loss: 16.1417\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3344 - val_loss: 1.9335\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8021 - val_loss: 0.5448\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5807 - val_loss: 1.3554\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4704 - val_loss: 1.0850\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8763 - val_loss: 0.7849\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9294 - val_loss: 2.5848\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1035 - val_loss: 1.3844\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5658 - val_loss: 1.1459\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8638 - val_loss: 2.8189\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3998 - val_loss: 0.8895\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4543 - val_loss: 0.3987\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6909 - val_loss: 2.0204\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0789 - val_loss: 0.7034\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4571 - val_loss: 1.5719\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6147 - val_loss: 0.6024\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4470 - val_loss: 0.8378\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.3713180241105048\n",
      "Mean Absolute Error (MAE): 0.4501394247211607\n",
      "Root Mean Squared Error (RMSE): 0.6093586990521304\n",
      "Time taken: 308.1484422683716\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 3311.5566 - val_loss: 2622.3584\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2207.9177 - val_loss: 2033.3601\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1684.1996 - val_loss: 1567.8632\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1143.5964 - val_loss: 986.4841\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 757.6325 - val_loss: 643.2817\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 489.1949 - val_loss: 397.7220\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 281.3943 - val_loss: 220.5537\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 146.2671 - val_loss: 110.0801\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.0371 - val_loss: 49.8362\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 36.6877 - val_loss: 27.2197\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 21.0455 - val_loss: 14.4644\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 19.3277 - val_loss: 24.8362\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.1957 - val_loss: 12.7609\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9412 - val_loss: 8.4090\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8217 - val_loss: 5.0429\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4658 - val_loss: 5.1102\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1893 - val_loss: 5.0353\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4343 - val_loss: 3.1082\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1434 - val_loss: 2.5246\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8983 - val_loss: 2.5460\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7251 - val_loss: 2.5834\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7086 - val_loss: 3.5436\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0342 - val_loss: 4.5940\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8588 - val_loss: 13.4161\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7549 - val_loss: 5.3357\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8653 - val_loss: 1.7059\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6949 - val_loss: 2.8056\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7630 - val_loss: 3.1395\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4518 - val_loss: 3.0686\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3323 - val_loss: 2.8091\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6909 - val_loss: 1.2088\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7641 - val_loss: 2.3284\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3051 - val_loss: 1.0017\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6795 - val_loss: 3.6579\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5093 - val_loss: 2.6238\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4080 - val_loss: 1.8442\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7817 - val_loss: 2.3092\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.0824 - val_loss: 1.6161\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.7779 - val_loss: 1.8661\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4298 - val_loss: 1.0852\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.2800 - val_loss: 3.3187\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1945 - val_loss: 1.5514\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9866 - val_loss: 0.4117\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4375 - val_loss: 1.7291\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1546 - val_loss: 1.8997\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0206 - val_loss: 1.9746\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0471 - val_loss: 1.5007\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8490 - val_loss: 0.6473\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6968 - val_loss: 3.8404\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1013 - val_loss: 2.3193\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2361 - val_loss: 2.2115\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6685 - val_loss: 0.3289\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0638 - val_loss: 1.5499\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0737 - val_loss: 0.5697\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.2052 - val_loss: 2.8468\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9477 - val_loss: 0.4708\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4768 - val_loss: 0.3943\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4387 - val_loss: 0.6586\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5106 - val_loss: 1.3486\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4952 - val_loss: 0.6974\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5119 - val_loss: 1.0233\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5734 - val_loss: 0.3321\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4975 - val_loss: 0.5798\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9492 - val_loss: 1.5057\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0348 - val_loss: 0.8532\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6205 - val_loss: 0.6151\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4815 - val_loss: 2.6291\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2843 - val_loss: 1.0184\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5826 - val_loss: 0.6828\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7746 - val_loss: 2.9821\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6317 - val_loss: 1.0534\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6644 - val_loss: 1.7038\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8454 - val_loss: 0.8308\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9375 - val_loss: 0.8886\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5270 - val_loss: 0.6972\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5838 - val_loss: 1.0668\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0813 - val_loss: 1.8761\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5070 - val_loss: 1.3403\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5569 - val_loss: 0.6706\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4522 - val_loss: 0.8638\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7952 - val_loss: 0.6976\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.5171 - val_loss: 2.1740\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.3287985019580727\n",
      "Mean Absolute Error (MAE): 0.43076008591922954\n",
      "Root Mean Squared Error (RMSE): 0.5734095412164614\n",
      "Time taken: 254.134033203125\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 3304.0200 - val_loss: 2481.8625\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2167.9824 - val_loss: 1851.4009\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1665.8025 - val_loss: 1395.9503\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1199.3021 - val_loss: 956.4041\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 831.5261 - val_loss: 679.5446\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 551.1605 - val_loss: 427.1610\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 324.8988 - val_loss: 231.9807\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 168.4426 - val_loss: 112.3383\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 83.6198 - val_loss: 55.2920\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 40.1757 - val_loss: 24.8454\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.6331 - val_loss: 24.5058\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.2056 - val_loss: 16.9878\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.6151 - val_loss: 7.4637\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7533 - val_loss: 8.8834\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.5152 - val_loss: 7.0812\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1878 - val_loss: 5.2427\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6990 - val_loss: 8.1129\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.4541 - val_loss: 8.3273\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8832 - val_loss: 4.1003\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5552 - val_loss: 2.6896\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4130 - val_loss: 2.6562\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3723 - val_loss: 6.6390\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4580 - val_loss: 4.1224\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.3425 - val_loss: 12.0657\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9055 - val_loss: 2.9295\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3830 - val_loss: 3.5477\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6416 - val_loss: 1.0769\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8480 - val_loss: 3.3856\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6248 - val_loss: 3.8808\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4902 - val_loss: 3.4663\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1351 - val_loss: 0.6461\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8614 - val_loss: 4.5897\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4776 - val_loss: 1.3672\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6065 - val_loss: 4.2599\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3282 - val_loss: 1.6089\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3564 - val_loss: 1.7396\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.0040 - val_loss: 3.6284\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3153 - val_loss: 5.2166\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0178 - val_loss: 0.9648\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7417 - val_loss: 0.8480\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7357 - val_loss: 1.1541\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1882 - val_loss: 3.1054\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9903 - val_loss: 1.6569\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0701 - val_loss: 1.1719\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5706 - val_loss: 2.2571\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4993 - val_loss: 1.8889\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1639 - val_loss: 2.2090\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3814 - val_loss: 1.0124\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8866 - val_loss: 2.4825\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8628 - val_loss: 1.2761\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1221 - val_loss: 2.0301\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2062 - val_loss: 0.7052\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4023 - val_loss: 6.1106\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6213 - val_loss: 0.7698\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5220 - val_loss: 0.2388\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5989 - val_loss: 2.0665\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5803 - val_loss: 0.8839\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6939 - val_loss: 1.7243\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6012 - val_loss: 0.5526\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1579 - val_loss: 5.4027\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2107 - val_loss: 1.8493\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5221 - val_loss: 0.5054\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9054 - val_loss: 0.5813\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5728 - val_loss: 0.4312\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.6264 - val_loss: 62.7768\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0817 - val_loss: 0.7223\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4771 - val_loss: 0.3689\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4464 - val_loss: 0.2757\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3747 - val_loss: 0.3149\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5338 - val_loss: 0.9217\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3607 - val_loss: 0.6324\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4993 - val_loss: 0.5810\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5438 - val_loss: 0.6615\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6576 - val_loss: 0.4705\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9244 - val_loss: 2.3718\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5116 - val_loss: 0.9864\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4907 - val_loss: 0.4976\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5728 - val_loss: 1.4928\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5290 - val_loss: 2.4487\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8423 - val_loss: 1.2280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6483 - val_loss: 0.5003\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4665 - val_loss: 0.4417\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3406 - val_loss: 0.3579\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6501 - val_loss: 2.7180\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0031 - val_loss: 0.8525\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.2388438013732539\n",
      "Mean Absolute Error (MAE): 0.35534742730555074\n",
      "Root Mean Squared Error (RMSE): 0.48871648363161835\n",
      "Time taken: 265.3746929168701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_19244\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.165212  0.297137  0.406463  357.514994\n",
      "1        2  0.109292  0.253270  0.330594  529.342747\n",
      "2        3  0.371318  0.450139  0.609359  308.148442\n",
      "3        4  0.328799  0.430760  0.573410  254.134033\n",
      "4        5  0.238844  0.355347  0.488716  265.374693\n",
      "5  Average  0.242693  0.357331  0.481708  342.902982\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_Reg2.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_Reg2.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_Reg2.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACal0lEQVR4nOzdeXwU9f0/8NfMbHaT3U02QMhFAgRIOBQUURQPipWKSD0q9aoVtVorBVq0HvXr8RV7UK/a1rOtrdh+tVX7qyco4oEnKqAoIkIIgRBIAiFkQzbJXjO/PyY72c1BJsl+spnN6/l4RDazk93PZ/eVmHc+x0iapmkgIiIiIiLqAznRDSAiIiIiIutjYUFERERERH3GwoKIiIiIiPqMhQUREREREfUZCwsiIiIiIuozFhZERERERNRnLCyIiIiIiKjPWFgQEREREVGfsbAgIiIiIqI+Y2FBRERERER9xsKCiGgQWrFiBSRJwoYNGxLdFFM2bdqEH/7whygsLITD4cDQoUMxe/ZsPPnkkwiHw4luHhERAbAlugFERERH8sQTT+C6665DTk4OLr/8chQXF+Pw4cN46623cPXVV6Oqqgr/8z//k+hmEhENeiwsiIhowPr4449x3XXXYcaMGVi1ahXS09ON+5YuXYoNGzbgq6++istz+Xw+uFyuuDwWEdFgxKlQRETUpc8//xxz585FRkYG3G43zjjjDHz88ccx5wSDQSxbtgzFxcVITU3FsGHDcOqpp2LNmjXGOdXV1bjqqqtQUFAAh8OBvLw8nHfeedi1a9cRn3/ZsmWQJAlPP/10TFERcfzxx+PKK68EAKxduxaSJGHt2rUx5+zatQuSJGHFihXGsSuvvBJutxtlZWU4++yzkZ6ejssuuwyLFy+G2+1GU1NTh+e69NJLkZubGzP16rXXXsNpp50Gl8uF9PR0zJs3D1u2bDlin4iIkhULCyIi6tSWLVtw2mmn4YsvvsDNN9+MO+64A+Xl5Zg1axY++eQT47y77roLy5Ytw+mnn46HH34Yt912G0aOHInPPvvMOGf+/Pl44YUXcNVVV+HRRx/Fz372Mxw+fBgVFRVdPn9TUxPeeustzJw5EyNHjox7/0KhEObMmYPs7Gzcf//9mD9/Pi6++GL4fD6sXLmyQ1teeeUVfP/734eiKACAf/7zn5g3bx7cbjfuuece3HHHHfj6669x6qmndlswERElI06FIiKiTt1+++0IBoP44IMPMGbMGADAggULMH78eNx888149913AQArV67E2Wefjb/85S+dPk59fT0++ugj3HfffbjxxhuN47feeusRn3/Hjh0IBoOYPHlynHoUy+/348ILL8Ty5cuNY5qmYcSIEXj22Wdx4YUXGsdXrlwJn8+Hiy++GADQ2NiIn/3sZ7jmmmti+n3FFVdg/Pjx+O1vf9vl60FElKw4YkFERB2Ew2G88cYbOP/8842iAgDy8vLwgx/8AB988AEaGhoAAJmZmdiyZQtKS0s7fay0tDTY7XasXbsWhw4dMt2GyON3NgUqXhYuXBjzuSRJuPDCC7Fq1So0NjYax5999lmMGDECp556KgBgzZo1qK+vx6WXXora2lrjQ1EUnHjiiXjnnXeEtZmIaKBiYUFERB0cOHAATU1NGD9+fIf7Jk6cCFVVsWfPHgDA3Xffjfr6epSUlGDy5Mm46aab8OWXXxrnOxwO3HPPPXjttdeQk5ODmTNn4t5770V1dfUR25CRkQEAOHz4cBx71sZms6GgoKDD8YsvvhjNzc14+eWXAeijE6tWrcKFF14ISZIAwCiivv3tb2P48OExH2+88Qb2798vpM1ERAMZCwsiIuqTmTNnoqysDH//+99x9NFH44knnsBxxx2HJ554wjhn6dKl2L59O5YvX47U1FTccccdmDhxIj7//PMuH3fcuHGw2WzYvHmzqXZEfulvr6vrXDgcDshyx/8NnnTSSRg9ejSee+45AMArr7yC5uZmYxoUAKiqCkBfZ7FmzZoOHy+99JKpNhMRJRMWFkRE1MHw4cPhdDqxbdu2Dvd98803kGUZhYWFxrGhQ4fiqquuwr/+9S/s2bMHU6ZMwV133RXzdWPHjsUvfvELvPHGG/jqq68QCATwwAMPdNkGp9OJb3/723jvvfeM0ZEjGTJkCAB9TUe03bt3d/u17V100UV4/fXX0dDQgGeffRajR4/GSSedFNMXAMjOzsbs2bM7fMyaNavHz0lEZHUsLIiIqANFUXDmmWfipZdeitnhqKamBs888wxOPfVUY6rSwYMHY77W7XZj3Lhx8Pv9APQdlVpaWmLOGTt2LNLT041zuvK///u/0DQNl19+ecyah4iNGzfiqaeeAgCMGjUKiqLgvffeiznn0UcfNdfpKBdffDH8fj+eeuopvP7667joooti7p8zZw4yMjLw29/+FsFgsMPXHzhwoMfPSURkddwViohoEPv73/+O119/vcPxn//85/j1r3+NNWvW4NRTT8VPf/pT2Gw2/PnPf4bf78e9995rnDtp0iTMmjUL06ZNw9ChQ7Fhwwb85z//weLFiwEA27dvxxlnnIGLLroIkyZNgs1mwwsvvICamhpccsklR2zfySefjEceeQQ//elPMWHChJgrb69duxYvv/wyfv3rXwMAPB4PLrzwQjz00EOQJAljx47Fq6++2qv1DscddxzGjRuH2267DX6/P2YaFKCv/3jsscdw+eWX47jjjsMll1yC4cOHo6KiAitXrsQpp5yChx9+uMfPS0RkaRoREQ06Tz75pAagy489e/ZomqZpn332mTZnzhzN7XZrTqdTO/3007WPPvoo5rF+/etfa9OnT9cyMzO1tLQ0bcKECdpvfvMbLRAIaJqmabW1tdqiRYu0CRMmaC6XS/N4PNqJJ56oPffcc6bbu3HjRu0HP/iBlp+fr6WkpGhDhgzRzjjjDO2pp57SwuGwcd6BAwe0+fPna06nUxsyZIj2k5/8RPvqq680ANqTTz5pnHfFFVdoLpfriM952223aQC0cePGdXnOO++8o82ZM0fzeDxaamqqNnbsWO3KK6/UNmzYYLpvRETJQtI0TUtYVUNEREREREmBayyIiIiIiKjPWFgQEREREVGfsbAgIiIiIqI+Y2FBRERERER9xsKCiIiIiIj6jIUFERERERH1GS+QZ4Kqqti3bx/S09MhSVKim0NERERE1C80TcPhw4eRn58PWT7ymAQLCxP27duHwsLCRDeDiIiIiCgh9uzZg4KCgiOew8LChPT0dAD6C5qRkdHvzx8Oh1FWVoaxY8dCUZR+f35KfswYicaMkWjMGIk0mPPV0NCAwsJC4/fhI2FhYUJk+lNGRkbCCgu3242MjIxBF2bqH8wYicaMkWjMGInEfMHUcgAu3iYiIiIioj5jYWER3S2WIeorZoxEY8ZINGaMRGK+uidpmqYluhEDXUNDAzweD7xeb0KmQhERERERJUJPfg/mGgsL0DQNPp8PLpeL292SEMwYicaMkWiDIWOqqiIQCCS6GYOSpmloamqC0+lMunylpKTEbd0ICwsLUFUVlZWVKC4uHrQLhkgsZoxEY8ZItGTPWCAQQHl5OVRVTXRTBiVN0xAKhWCz2ZKusACAzMxM5Obm9rlvLCyIiIiIBjBN01BVVQVFUVBYWMi5/gmgaRr8fj8cDkdSFRaRkZj9+/cDAPLy8vr0eCwsiIiIiAawUCiEpqYm5Ofnw+l0Jro5g1JkSXJqampSFRYAkJaWBgDYv38/srOz+zTix5LXAiRJgt1uT7og08DBjJFozBiJlswZC4fDAAC73Z7glgxuyTxSFClYg8Fgnx6HIxYWIMsyxowZk+hmUBJjxkg0ZoxEGwwZS8aiySokSYLD4Uh0M4SJV7aSt/RKIpqmob6+HtwZmERhxkg0ZoxEY8ZIpMjibebryFhYWICqqqiuruZOECQMM0aiMWMkGjM2OIwePRp/+MMfTJ+/du1aSJKE+vr6Pj93X6cJDQYsLIiIiIgoriRJOuLHXXfd1avHXb9+Pa699lrT55988smoqqqCx+Pp1fOZFc8Cxsq4xoKIiIiI4qqqqsq4/eyzz+LOO+/Etm3bjGNut9u4rWkawuEwbLbufy0dPnx4j9pht9uRm5vbo6+h3uOIhQVIkpTUVxKlxGPGSDRmjERjxgaW3Nxc48Pj8UCSJOPzb775Bunp6Xjttdcwbdo0OBwOfPDBBygrK8N5552HnJwcuN1unHDCCXjzzTdjHrf9VChJkvDEE0/ge9/7HpxOJ4qLi/Hyyy8b97cfSVixYgUyMzOxevVqTJw4EW63G2eddVZMIRQKhfCzn/0MmZmZGDZsGG655RZceeWVuPjii3v9ehw6dAgLFizAkCFD4HQ6MXfuXJSWlhr37969G+eccw6GDBkCl8uFo446CqtWrTK+9rLLLsPw4cORlpaG4uJiPPnkk71ui0gsLCxAlmVeEIeEYsZINGaMRGPGrOeXv/wlfve732Hr1q2YMmUKGhsbcfbZZ+Ott97C559/jrPOOgvnnHMOKioqjvg4y5Ytw0UXXYQvv/wSZ599Ni677DLU1dV1eX5TUxPuv/9+/POf/8R7772HiooK3Hjjjcb999xzD55++mk8+eST+PDDD9HQ0IAXX3wRsiz3unC98sorsWHDBrz88stYt24dNE3D2WefbazbWLRoEfx+P9577z1s3rwZ99xzjzGqc8cdd+Drr7/Ga6+9hq1bt+Kxxx5DVlZWr9ohGqdCWYCqqqirq8PQoUP5A5OEYMZINGaMRBtsGTvnoQ9w4LC/3593eLoDryw5NS6Pdffdd+M73/mO8fnQoUNxzDHHGJ//6le/wgsvvICXX34Zixcv7vJxrrzySlx66aUAgN/+9rf405/+hE8//RRnnXVWp+cHg0E8/vjjGDt2LABg8eLFuPvuu437H3roIdx666343ve+BwB4+OGHsWrVKqiqCk3TelxclJaW4uWXX8aHH36Ik08+GQDw9NNPo7CwEC+++CIuvPBCVFRUYP78+Zg8eTIAxGydXFFRgalTp+L4448HoI/aDFQsLCxA0zTU1tZiyJAhiW4KJSlmjERjxki0wZaxA4f9qG5oSXQz+iTyi3JEY2Mj7rrrLqxcuRJVVVUIhUJobm7udsRiypQpxm2Xy4WMjAzs37+/y/OdTqdRVABAXl6ecb7X60VNTQ2mT59u3K8oCqZNm9brXaG2bt0Km82GE0880Tg2bNgwjB8/Hlu3bgUA/OxnP8PChQvxxhtvYPbs2Zg/f77Rr4ULF2L+/Pn47LPPcOaZZ+L88883CpSBhoUFERERkcUMT0/Mxdri+bwulyvm8xtvvBFr1qzB/fffj3HjxiEtLQ3f//73EQgEjvg4KSkpMZ9LknTEbYc7Oz/R16e45pprMGfOHKxcuRJvvPEGli9fjgceeABLlizB3LlzsXv3bqxatQpr1qzBGWecgUWLFuH+++9PaJs7w8LCAmob/ag6HISt1odxORmJbg4RERElWLymIw0kH374Ia688kpjClJjYyN27drVr23weDzIycnB+vXrMXPmTABAOBzGZ599ZkxT6qmJEyciFArhk08+MUYaDh48iG3btmHSpEnGeYWFhbjuuutw3XXX4dZbb8Vf//pXLFmyBIC+G9YVV1yBK664AqeddhpuuukmFhbUO7Pufw/NwTBKsuvwxg3fSnRzKAlJkmTs2kEkAjNGojFj1ldcXIz//ve/OOeccyBJEu64446EXPBwyZIlWL58OcaNG4cJEybgoYcewqFDh0yt3dm8eTPS09ONzyVJwjHHHIPzzjsPP/7xj/HnP/8Z6enp+OUvf4kRI0bgvPPOAwAsXboUc+fORUlJCQ4dOoR33nkHEydOBADceeedmDZtGo466ij4/X68+uqrxn0DDQsLC3A5bGgOhuELhBPdFEpSsiwjLy8v0c2gJMaMkWjMmPX9/ve/x49+9COcfPLJyMrKwi233IKGhoZ+b8ctt9yC6upqLFiwAIqi4Nprr8WcOXOgKEq3hWtklCNCURSEQiE8+eST+PnPf47vfve7CAQCmDlzJlatWmVMywqHw1i0aBEqKyuRkZGBs846Cw8++CAA/Voct956K3bt2oW0tDScdtpp+Pe//y2m830kaYmeVGYBDQ0N8Hg88Hq9yMjo/6lI37r3Heyua0JmWgo2/e+Z/f78lPxUVUVNTQ1ycnIGxW4q1P+YMRItmTPW0tKC8vJyFBUVITU1NdHNGXRUVcXEiRMxf/58/OY3v0nKUbEjZawnvwcn13deknLaFQBAUyCU4JZQstI0DV6vN+GL1yh5MWMkGjNG8bJ792789a9/xfbt27F582YsXLgQ5eXluPDCCxPdtAGPhYUFuBz6jLVAWEMg1P9zDYmIiIgGC1mWsWLFCpxwwgk45ZRTsHnzZqxZswYTJkxIdNMGPK6xsIDIiAWgj1rYbfYEtoaIiIgoeRUWFuLDDz+MOaZpGlparH3dkP7AEQsLcDva6r9GP6dDUfxJkoSsrKyknDdKAwMzRqIxYySazca/x3eHr5AFuKIKiybuDEUCyLKMrKysRDeDkhgzRqIxYySSJEkdLqxHHXHEwgKip0L5OGJBAqiqij179iRkv3AaHJgxEo0ZI5E0TUMgEODmAN1gYWEBsYUFRywo/jRNg8/n4w9MEoYZI9GYMRItHObvYN1hYWEBLkdUYcEtZ4mIiIhoAGJhYQEue9saC06FIiIiIqKBiIWFBbhT2xYL+bh4mwSQZRm5ublJd7VaGjiYMRKNGUtOs2bNwtKlS43PR48ejT/84Q9H/BpJkvDiiy/2+bnbPw4Xb3eP330WELMrFEcsSABJkpCZmcltGkkYZoxEY8YGlnPOOQdnnXVWp/e9//77kCQJX375ZY8fd/369bj22mv72rwYd911F4499tgOx6uqqjB37lwAer5sNlvc87VixQpkZmbG9TETiYWFBThT2t4mToUiEVRVxc6dO7mbCgnDjJFozNjAcvXVV2PNmjWorKzscN+TTz6J448/HlOmTOnx4w4fPhxOpzMeTexWbm4uHA4HAH1zAL/fz80BusHCwgLSoneF4lQoEoDb6JFozBiJxowNLN/97ncxfPhwrFixIuZ4Y2Mjnn/+eVx99dU4ePAgLr30UowYMQJOpxOTJ0/Gv/71ryM+bvupUKWlpZg5cyZSU1MxadIkrFmzpsPX3HLLLSgpKYHT6cSYMWNwxx13IBgMAtBHDJYtW4YvvvgCkiRBkiSjze2nQn355Zc444wzkJaWhmHDhuHaa69FY2Ojcf+VV16J888/H/fffz/y8vIwbNgwLFq0yHiu3qioqMB5550Ht9uNjIwMXHTRRaipqTHu/+KLL3D66acjPT0dGRkZmDZtGjZs2AAA2L17N8455xwMGTIELpcLRx11FFatWtXrtpjBC+RZQPSVtzliQURERAOdzWbDggULsGLFCtx2223GFKLnn38e4XAYl156KRobGzFt2jTccsstyMjIwMqVK3H55Zdj7NixmD59erfPoaoqLrjgAuTk5OCTTz6B1+uNWY8RkZ6ejhUrViA/Px+bN2/Gj3/8Y6Snp+Pmm2/GxRdfjK+++gqvv/463nzzTQCAx+Pp8Bg+nw/nnnsuTj75ZKxfvx779+/HNddcg8WLF8cUT++88w7y8vLwzjvvYMeOHbj44otx7LHH4sc//nGPX0NVVY2i4t1330UoFMKiRYtw8cUXY+3atQCAyy67DFOnTsVjjz0GRVGwadMmYy3IokWLEAgE8N5778HlcuHrr7+G2+3ucTt6goWFBTg5YkFERETR/vwtoHF//z+vOxv4ybumTv3Rj36E++67D++++y5mzZoFQJ8GNX/+fHg8Hng8Htx4443G+UuWLMHq1avx3HPPmSos3nzzTXzzzTdYvXo18vPzAQC//e1vjXUREbfffrtxe/To0bjxxhvx73//GzfffDPS0tLgdrths9mQm5vb5XM988wz8Pv9eOqpp4xfzh9++GGcc845uOeee5CTkwMAGDJkCB5++GEoioIJEyZg3rx5eOutt3pVWLz11lvYvHkzysvLUVhYCAD4xz/+gaOOOgrr16/HCSecgIqKCtx0002YMGECAKC4uNj4+oqKCsyfPx+TJ08GAIwZM6bHbegpFhYWEL0rFBdvkwiyLKOgoIC7qZAwzBiJNugy1rgfOLwv0a04ogkTJuDkk0/G3//+d8yaNQs7duzA+++/j7vvvhuAfsG53/72t3juueewd+9eBAIB+P1+02sotm7disLCQqOoAIAZM2Z0OO/ZZ5/Fn/70J5SVlaGxsRGhUAgZGRk96svWrVsxZcoUuFwu49gpp5wCVVWxbds2o7A46qijoChtfxDOy8vD5s2be/Rc0c9ZWFhoFBUAMGnSJGRmZmLr1q044YQTcMMNN+Caa67BP//5T8yePRsXXnghxo4dCwD42c9+hoULF+KNN97A7NmzMX/+/F6ta+mJQfLdZ23pUYVFIwsLEkCSJLjdbu6mQsIwYyTaoMuYOxtIz+//D3d2j5p59dVX4//9v/+Hw4cP48knn8TYsWPxrW99CwBw33334Y9//CNuueUWvPPOO9i0aRPmzJmDQCAQt5dp3bp1uOyyy3D22Wfj1Vdfxeeff47bbrutx88RWX/RXb7ab0krSZLQDQXuuusubNmyBfPmzcPbb7+NSZMm4YUXXgAAXHPNNdi5cycuv/xybN68GccffzweeughYW0BOGJhCTZJgywBqgY0cSoUCRAOh1FWVoaxY8fG/KWFKF6YMRJt0GXM5HSkRLvooovw85//HM888wz+8Y9/YOHChcYv5x9++CHOO+88/PCHPwSgrynYvn07Jk2aZOqxJ06ciD179qCqqgp5eXkAgI8//jjmnI8++gijRo3CbbfdZhzbvXt3zDl2ux3h8JF/v5owYQJWrFiBxsZGYyrUhx9+CFmWMX78eFPt7alI//bs2WOMWnz99deor6+PeY1KSkpQUlKC66+/HpdeeimefPJJfO973wMAFBYW4rrrrsN1112HW2+9FX/961+xZMkSIe0FOGJhCZIkIc2mv1VcvE2icItGEo0ZI9GYsYHH7Xbj4osvxq233oqqqipceeWVxn3FxcVYs2YNPvroI2zduhU/+clPYnY86s7s2bNRUlKCK664Al988QXef//9mAIi8hwVFRX497//jbKyMvzpT38y/qIfMXr0aJSXl2PTpk2ora2F3+/v8FyXXXYZHA4HrrzySnz11Vd45513sGTJElx++eXGNKjeCofD2LRpU8zH1q1bMXv2bEyePBmXXXYZPvvsM3z66adYsGABvvWtb+H4449Hc3MzFi9ejLVr12L37t348MMPsX79ekycOBEAsHTpUqxevRrl5eX47LPP8M477xj3icLCwiLSUvTq3hdgYUFERETWcfXVV+PQoUOYM2dOzHqI22+/HccddxzmzJmDWbNmITc3F+eff77px5VlGS+88AKam5sxffp0XHPNNfjNb34Tc865556L66+/HosXL8axxx6Ljz76CHfccUfMOfPnz8dZZ52F008/HcOHD+90y1un04mXX34ZdXV1OOGEE/D9738fZ5xxBh5++OGevRidaGxsxNSpU2M+zjnnHEiShJdeeglDhgzBzJkzMXv2bIwZMwbPPvssAEBRFBw8eBALFixASUkJLrroIsydOxfLli0DoBcsixYtwsSJE3HWWWehpKQEjz76aJ/beySSxg2fu9XQ0ACPxwOv19vjxT7xEA6H8a173kJlQxDpDhs2L5vT722g5BYOh1FaWori4uLBMYWA+h0zRqIlc8ZaWlpQXl6OoqIipKamJro5g5KmaWhpaUFqampSruM5UsZ68nswRywsQJZlZLrTAOgjFqwFKd5kWUZRUdHg2U2F+h0zRqIxYyRa5Crc1DV+91mEq/UieaoGtAQ5h5Tiz2bjXg4kFjNGojFjJFIyjlTEGwsLC1BVFQi1GJ9zy1mKN1VVUVpayoWPJAwzRqIxYyRaS0tL9ycNciwsLCKyKxQANHEBNxERERENMCwsLCKyKxQA+Py8lgURERERDSwsLCwiLaXtreKWs0RERIMPN28hUeI1hZCrnCxAlmWMyM4CtngB8CJ5FH+yLKO4uJi7qZAwzBiJlswZS0lJgSRJOHDgAIYPH85FxAkQKepaWlqS6vXXNA2BQAAHDhyALMuw2+19ejwWFhYRM2LBqVAkQCgU6vMPFKIjYcZItGTNmKIoKCgoQGVlJXbt2pXo5gxamqYlVVERzel0YuTIkX0uzFlYWICqqmhuOGR8zqlQFG+qqqK8vDwpLyxFAwMzRqIle8bcbjeKi4sRDAYT3ZRBKRwOY/fu3Rg5cmTS5UtRFNhstrgUTSwsLCI1asSiiVOhiIiIBh1FUZLul1qrCIfDkGUZqampfA+OIPkmIiYpZ8zibU6FIiIiIqKBhYWFRTjtbdUxF2+TCMm44JEGFmaMRGPGSCTmq3ucCmUBiqJg/NjRAPYCYGFB8acoCkpKShLdDEpizBiJxoyRSMyXOSy9LEDTNEghv/E5p0JRvGmahsbGRu6RTsIwYyQaM0YiMV/msLCwAFVV4T243/i8ibtCUZypqorKysq4XSCHqD1mjERjxkgk5suchBYWjz32GKZMmYKMjAxkZGRgxowZeO2114z7W1pasGjRIgwbNgxutxvz589HTU1NzGNUVFRg3rx5cDqdyM7Oxk033YRQKPYX77Vr1+K4446Dw+HAuHHjsGLFiv7oXlxFX8eikdexICIiIqIBJqGFRUFBAX73u99h48aN2LBhA7797W/jvPPOw5YtWwAA119/PV555RU8//zzePfdd7Fv3z5ccMEFxteHw2HMmzcPgUAAH330EZ566imsWLECd955p3FOeXk55s2bh9NPPx2bNm3C0qVLcc0112D16tX93t++SON2s0REREQ0gEnaAJssNnToUNx33334/ve/j+HDh+OZZ57B97//fQDAN998g4kTJ2LdunU46aST8Nprr+G73/0u9u3bh5ycHADA448/jltuuQUHDhyA3W7HLbfcgpUrV+Krr74ynuOSSy5BfX09Xn/9dVNtamhogMfjgdfrRUZGRvw73Q1VVbFr1y6c9fdtCIRUTMzLwGs/P63f20HJK5Kx0aNHc9cLEoIZI9GYMRJpMOerJ78HD5hdocLhMJ5//nn4fD7MmDEDGzduRDAYxOzZs41zJkyYgJEjRxqFxbp16zB58mSjqACAOXPmYOHChdiyZQumTp2KdevWxTxG5JylS5d22Ra/3w+/v22xdENDg9HGcFifhiRJEmRZhqqqMQt5ujouyzIkSeryeORxo48DMObyjRo1Ci77DgRCKnz+UIfzFUWBpmkxc/8ibenquNm2i+pTd8fZp/7t06hRo6BpmnFOMvSpu7azT/3bp+iMJUuf2h9nnxLbp/YZS4Y+tW87+5SYPgFt/58Mh8NJ0Sezx3syBpHwwmLz5s2YMWMGWlpa4Ha78cILL2DSpEnYtGkT7HY7MjMzY87PyclBdXU1AKC6ujqmqIjcH7nvSOc0NDSgubkZaWlpHdq0fPlyLFu2rMPxsrIyuN1uAIDH40FeXh5qamrg9XqNc7KyspCVlYW9e/fC5/MZx3Nzc5GZmYldu3YhEAgYxwsKCuB2u1FWVhYThqKiIthsNpSWlkLTNPj9fqRI+hvr84dQWlpqnCvLMkpKSuDz+VBZWWkct9vtGDNmDLxer/F6AIDL5UJhYSHq6upQW1trHO/PPkUrLi5GKBRCeXk5+5SgPoXDYfj9fjgcDowZMyYp+pSM75OV+xT5g43D4UBhYWFS9CkZ3ycr96mxsdHIWF5eXlL0KRnfJyv3qaamBg6HA5IkJU2fzLxPTqcTZiV8KlQgEEBFRQW8Xi/+85//4IknnsC7776LTZs24aqrrooZOQCA6dOn4/TTT8c999yDa6+9Frt3745ZL9HU1ASXy4VVq1Zh7ty5KCkpwVVXXYVbb73VOGfVqlWYN28empqaOi0sOhuxiLwxkSGg/qzKw+EwduzYgaWvH8D2/Y1ITZGx5a4zY87nXxrYp770KZKxcePGISUlJSn6ZKbt7FP/9SkUCsVkLBn6lIzvk5X71FnGrN6nZHyfrNqnYDCI0tJSjBs3DoqiJEWfzB5vbGxEZmamNaZC2e12jBs3DgAwbdo0rF+/Hn/84x9x8cUXIxAIoL6+PmbUoqamBrm5uQD0qvDTTz+NebzIrlHR57TfSaqmpgYZGRmdFhUA4HA44HA4OhxXFAWKosQci7zx7fX0ePvHbX9clmW4HPrtlqAKSDIUWYo5V5KkTh+nq+Pxantv+2TmOPvUf32SZdn4YdlV27s6PlD71Jfj7FN8+6QoSp8y1tVxvk/sU+R4Zxmzep96cpx9Et+nSL6iv87qfTJzPPL9ZMaAW32iqir8fj+mTZuGlJQUvPXWW8Z927ZtQ0VFBWbMmAEAmDFjBjZv3oz9+9uu8bBmzRpkZGRg0qRJxjnRjxE5J/IYVuJytNWBvJYFEREREQ0kCR2xuPXWWzF37lyMHDkShw8fxjPPPIO1a9di9erV8Hg8uPrqq3HDDTdg6NChyMjIwJIlSzBjxgycdNJJAIAzzzwTkyZNwuWXX457770X1dXVuP3227Fo0SJjxOG6667Dww8/jJtvvhk/+tGP8Pbbb+O5557DypUrE9n1HpEkCS6XC057o3HM5w8jPTUlga2iZBLJWE/+KkHUE8wYicaMkUjMlzkJLSz279+PBQsWoKqqCh6PB1OmTMHq1avxne98BwDw4IMPQpZlzJ8/H36/H3PmzMGjjz5qfL2iKHj11VexcOFCzJgxAy6XC1dccQXuvvtu45yioiKsXLkS119/Pf74xz+ioKAATzzxBObMmdPv/e0tWZb1xY6Og8YxH0csKI4iGSMShRkj0ZgxEon5Mifhi7etYCBcx6Kurg5/eL8K//dJBQDg5cWnYEpBZr+3hZJTJGNDhw7tcs4lUV8wYyQaM0YiDeZ89eT34MH1yliUpmmora2F0962IMfnDx/hK4h6JpIx/p2BRGHGSDRmjERivsxhYWEhXLxNRERERAMVCwsLcUWNWDT6WVgQERER0cDBwsICJEmCx+NpN2LBqVAUP5GMcbcLEoUZI9GYMRKJ+TKHhYUFyLKMvLw8uKO2l/VxxILiKJKxwbYgjfoPM0aiMWMkEvNlDl8dC1BVFVVVVUhLaXu7uHib4imSMVVVE90USlLMGInGjJFIzJc5LCwsQNM0eL3emDUWXLxN8RTJGHe7IFGYMRKNGSORmC9zWFhYiJOLt4mIiIhogGJhYSFcvE1EREREA5Wt+1Mo0SRJQlZWFkK2treLIxYUT5GMcbcLEoUZI9GYMRKJ+TKHhYUFyLKMrKysmGKCaywoniIZIxKFGSPRmDESifkyh1OhLEBVVezZswepSluVzF2hKJ4iGeNuFyQKM0aiMWMkEvNlDgsLC9A0DT6fD5LUtoCb17GgeIpkjLtdkCjMGInGjJFIzJc5LCwsxmnXZ69x8TYRERERDSQsLCzG7dBHLLh4m4iIiIgGEhYWFiDLMnJzcyHLctSIBQsLip/ojBGJwIyRaMwYicR8mcNdoSxAkiRkZmYCANyt17IIhjUEQirsNgac+i46Y0QiMGMkGjNGIjFf5vC3UgtQVRU7d+6EqqpwOtquvs0F3BQv0RkjEoEZI9GYMRKJ+TKHhYUFaJqGQCAATdPgsrcNMvk4HYriJDpjRCIwYyQaM0YiMV/msLCwGFfMiAV3hiIiIiKigYGFhcU4OWJBRERERAMQCwsLkGUZBQUFkGXZWLwNAE0csaA4ic4YkQjMGInGjJFIzJc53BXKAiRJgtvtBoCYxdu8lgXFS3TGiERgxkg0ZoxEYr7MYdllAeFwGNu3b0c4HI5ZvM1rWVC8RGeMSARmjERjxkgk5sscFhYWEdnezBU1FYrbzVI8cQs9Eo0ZI9GYMRKJ+eoeCwuLcdmjdoUKsGomIiIiooGBhYXFuGIWb3PEgoiIiIgGBhYWFiDLMoqKiiDLcsx1LBq5KxTFSXTGiERgxkg0ZoxEYr7M4atjETabPlLh5OJtEiSSMSJRmDESjRkjkZiv7rGwsABVVVFaWgpVVWOuY8E1FhQv0RkjEoEZI9GYMRKJ+TKHhYXFOKMXb3ONBRERERENECwsLIbbzRIRERHRQMTCwmIcNhmKLAEAfFxjQUREREQDBAsLC5BlGcXFxZBlGZIkGdOhmrgrFMVJdMaIRGDGSDRmjERivszhq2MRoVDb6ERkATdHLCieojNGJAIzRqIxYyQS89U9FhYWoKoqysvLjZ0IIiMWPo5YUJy0zxhRvDFjJBozRiIxX+awsLCg6BELTdMS3BoiIiIiIhYWlhS5SJ6mAc1BjloQERERUeKxsLCI6MVCsVvOsrCg+OCCNBKNGSPRmDESifnqHq9NbgGKoqCkpMT43OVou0heUyAEwJGAVlEyaZ8xonhjxkg0ZoxEYr7MYellAZqmobGx0VhPEZkKBQCNvEgexUH7jBHFGzNGojFjJBLzZQ4LCwtQVRWVlZXGTgTumBELToWivmufMaJ4Y8ZINGaMRGK+zGFhYUEcsSAiIiKigYaFhQW5oxZv8+rbRERERDQQsLCwAEmSYLfbIUkSAMAZNRWKV9+meGifMaJ4Y8ZINGaMRGK+zOGuUBYgyzLGjBljfO6yR283y8KC+q59xojijRkj0ZgxEon5MocjFhagaRrq6+uNnQiir2PBxdsUD+0zRhRvzBiJxoyRSMyXOSwsLEBVVVRXVxs7EbjsbVOhuHib4qF9xojijRkj0ZgxEon5MoeFhQXFjFiwsCAiIiKiAYCFhQW5YhZvcyoUERERESUeCwsLkCQJLperbVcoLt6mOGufMaJ4Y8ZINGaMRGK+zOGuUBYgyzIKCwuNz6OnQnHEguKhfcaI4o0ZI9GYMRKJ+TKHIxYWoKoqamtrO128zRELiof2GSOKN2aMRGPGSCTmyxwWFhagaRpqa2uNLc5sigyHTX/rWFhQPLTPGFG8MWMkGjNGIjFf5rCwsKjIdChex4KIiIiIBgIWFhblbJ0OxRELIiIiIhoIWFhYgCRJ8Hg8MTsRuFtHLHwBFhbUd51ljCiemDESjRkjkZgvc7grlAXIsoy8vLyYY5ERi5agilBYhU1hjUi911nGiOKJGSPRmDESifkyJ6G/jS5fvhwnnHAC0tPTkZ2djfPPPx/btm2LOWfWrFmQJCnm47rrros5p6KiAvPmzYPT6UR2djZuuukmhEKxf8lfu3YtjjvuODgcDowbNw4rVqwQ3b24UVUVVVVVMTsRxFx9O8h1FtQ3nWWMKJ6YMRKNGSORmC9zElpYvPvuu1i0aBE+/vhjrFmzBsFgEGeeeSZ8Pl/MeT/+8Y9RVVVlfNx7773GfeFwGPPmzUMgEMBHH32Ep556CitWrMCdd95pnFNeXo558+bh9NNPx6ZNm7B06VJcc801WL16db/1tS80TYPX643ZicAVdZG8Jj8LC+qbzjJGFE/MGInGjJFIzJc5CZ0K9frrr8d8vmLFCmRnZ2Pjxo2YOXOmcdzpdCI3N7fTx3jjjTfw9ddf480330ROTg6OPfZY/OpXv8Itt9yCu+66C3a7HY8//jiKiorwwAMPAAAmTpyIDz74AA8++CDmzJkjroMCOR1t17Jo5AJuIiIiIkqwATUx3+v1AgCGDh0ac/zpp59GVlYWjj76aNx6661oamoy7lu3bh0mT56MnJwc49icOXPQ0NCALVu2GOfMnj075jHnzJmDdevWieqKcO7oqVBcwE1ERERECTZgFm+rqoqlS5filFNOwdFHH20c/8EPfoBRo0YhPz8fX375JW655RZs27YN//3vfwEA1dXVMUUFAOPz6urqI57T0NCA5uZmpKWlxdzn9/vh9/uNzxsaGgDo067CYX3akSRJkGUZqqrGDIt1dVyWZUiS1OXxyONGH4+8LqqqYujQoVBV1TieltJWEzY0BwDow3TRc/8ibenquNm2i+iTmeOKorBP/dSnzjJm9T6ZaTv71H99ap+xZOhTMr5PVu5TZxmzep+S8X2yap80TTPylSx9Mnu8J9O/BkxhsWjRInz11Vf44IMPYo5fe+21xu3JkycjLy8PZ5xxBsrKyjB27FghbVm+fDmWLVvW4XhZWRncbjcAwOPxIC8vDzU1NcZICwBkZWUhKysLe/fujVkrkpubi8zMTOzatQuBQMA4XlBQALfbjbKyspgwFBUVwWazobS0FAAghf3w7ivDmMnTEQqF0HK47TnLdlfi1OJs+Hw+VFZWGsftdjvGjBkDr9drFFkA4HK5UFhYiLq6OtTW1hrH+7tPEcXFxQiFQigvLzeOybKMkpIS9qmf+1RXV5d0fQKS732ycp/q6uqSrk9A8r1PVu5TXV1d0vUJSL73yWp9OnDgALxeL+rq6pKmT2bfJ6fTCbMkbQCsQlm8eDFeeuklvPfeeygqKjriuT6fD263G6+//jrmzJmDO++8Ey+//DI2bdpknFNeXo4xY8bgs88+w9SpUzFz5kwcd9xx+MMf/mCc8+STT2Lp0qUxL2hEZyMWkTcmIyMDQD9W5eEQ5PvHQmrxQs07FtK1awEAf/tgJ3698hsAwIMXTcH3jivkXxrYpz6NWOzbtw/5+fmw2WxJ0SczbWef+q9P4XA4JmPJ0KdkfJ+s3KfOMmb1PiXj+2TVPoVCIezduxf5+flG+6zeJ7PHGxsbkZmZCa/Xa/we3JWEjlhomoYlS5bghRdewNq1a7stKgAYBURkL+EZM2bgN7/5Dfbv34/s7GwAwJo1a5CRkYFJkyYZ56xatSrmcdasWYMZM2Z0+hwOhwMOh6PDcUVRoChKzLHIG99eT4+3f1zjuC0FmmIHAEhNtZAk/cIs6akpxjnNQf3NlySp08fp6ni82t7jPvXgOPvUf31qbm42fvB11faujg/UPvXlOPsU3z5pmtanjHV1nO8T+xQ53lnGrN6nnhxnn8T2SZIkI1/RX2flPpk9Hvl+MiOhi7cXLVqE//u//8MzzzyD9PR0VFdXo7q6Gs3NzQD0qUe/+tWvsHHjRuzatQsvv/wyFixYgJkzZ2LKlCkAgDPPPBOTJk3C5Zdfji+++AKrV6/G7bffjkWLFhnFwXXXXYedO3fi5ptvxjfffINHH30Uzz33HK6//vqE9b1HXMP1fxsPAK0VpIuLt4mIiIhoAEloYfHYY4/B6/Vi1qxZyMvLMz6effZZAPqcsTfffBNnnnkmJkyYgF/84heYP38+XnnlFeMxFEXBq6++CkVRMGPGDPzwhz/EggULcPfddxvnFBUVYeXKlVizZg2OOeYYPPDAA3jiiSess9Vsa2Ehhf2A/7B+KOo6Fj5ex4KIiIiIEizhU6GOpLCwEO+++263jzNq1KgOU53amzVrFj7//PMetW/AiIxYAIDvAJCaETNi4eOIBfWRLMvIzc3tcliUqK+YMRKNGSORmC9z+OpYgOTObvvEdwAA4LS3zaHz8QJ51EeSJCEzM7NH8yiJeoIZI9GYMRKJ+TKHhYUFqM5hbZ+0FhYxIxYsLKiPVFXFzp07O+w+QRQvzBiJxoyRSMyXOSwsrKD9VCgALkfUiEWAayyobzRNQyAQ6NFFcIh6ghkj0ZgxEon5MoeFhQVozqjCorG1sLBzVygiIiIiGjhYWFiBK6vtduuIRVqKgsg0v0buCkVERERECcbCwgLk9Jy2T1oLC1mW4EzRp0M1cY0F9ZEsyygoKOBuFyQMM0aiMWMkEvNlDl8dC5A6WWMBAM7WBdxcvE19JUkS3G43d7sgYZgxEo0ZI5GYL3NYWFhAWHFAtTn1T6IKC3eksODibeqjcDiM7du3IxxmlkgMZoxEY8ZIJObLHBYWFhFKHarfiB6xaL2WBRdvUzxwCz0SjRkj0ZgxEon56h4LC4sIOYboN5oPAeEggLZrWQTDGvwhVtBERERElDgsLCwinDqk7RNfLQDAFXX17SbuDEVERERECcTCwgJkWYZz+Ki2A63ToZxRV99u5AJu6gNZllFUVMTdLkgYZoxEY8ZIJObLHL46FiG5s9s+8e0HALhjLpLHEQvqG5vN1v1JRH3AjJFozBiJxHx1j4WFBaiqigNNUQdap0I5HW1ToXxcwE19oKoqSktLuTCNhGHGSDRmjERivsxhYWERxq5QgDEVyh01FYrXsiAiIiKiRGJhYRFhR9Ti7UZ9KpTTHl1YcCoUERERESUOCwuLCHW2K1T0VCiOWBARERFRArGwsABZljFq0gltB1qnQrliFm+zsKDek2UZxcXF3O2ChGHGSDRmjERivszhq2MRoRQ3NKl1hKJ1V6iYEQvuCkV9FAqxOCWxmDESjRkjkZiv7rGwsABVVVG+azfgytIPGFOhuHib4kNVVZSXl3O3CxKGGSPRmDESifkyh4WFlbiG6//6DgCaxsXbRERERDRgsLCwksiIRTgAtHi5eJuIiIiIBgwWFhYhyzI05/C2A77amMXbvEAe9RUXpJFozBiJxoyRSMxX9/gKWYCiKCgpKYGcntN20HcgZo1FExdvUx9EMqYoSvcnE/UCM0aiMWMkEvNlDgsLC9A0DY2NjdCcWW0HfftjpkI1cioU9YGRMU1LdFMoSTFjJBozRiIxX+awsLAAVVVRWVkJzTms7aDvAOyKDJssAeB1LKhvIhnjbhckCjNGojFjJBLzZQ4LCwvRXNltn/hqIUkSnHZ91IK7QhERERFRIrGwsBJX1FSoRv0iee7WdRbcFYqIiIiIEomFhQVIkgS73Q7JHb0r1AEAgLO1sODibeoLI2OSlOimUJJixkg0ZoxEYr7MsXV/CiWaLMsYM2YMEGxpO9ju6tu+QAiapjHw1CtGxogEYcZINGaMRGK+zOGIhQVomob6+npoNgfgyNAP+vSpUK7WNRaaBjQHOWpBvWNkjLtdkCDMGInGjJFIzJc5LCwsQFVVVFdX6zsRRNZZRKZCRV0kj1vOUm/FZIxIAGaMRGPGSCTmyxwWFlYT2RmqxQuEAnBHXcuiiTtDEREREVGCsLCwmuidoXwHjMXbgL7OgoiIiIgoEVhYWIAkSXC5XPrCbFfszlDu6MKCIxbUSzEZIxKAGSPRmDESifkyh7tCWYAsyygsLNQ/ccdeJM9pH9n2KUcsqJdiMkYkADNGojFjJBLzZQ5HLCxAVVXU1ta2Lt6OHrHYD1fU4m2usaDeiskYkQDMGInGjJFIzJc5LCwsQNM01NbW6luctVtj4YqZCsURC+qdmIwRCcCMkWjMGInEfJnDwsJqXNFToQ7AFbUrFKdCEREREVGisLCwmuipUI0HYqZCccSCiIiIiBKFhYUFSJIEj8fTuitU7FSoIa4U49PaxkACWkfJICZjRAIwYyQaM0YiMV/mcFcoC5BlGXl5efonaUMA2QaoIcB3ALmeNOO8Km9zglpIVheTMSIBmDESjRkjkZgvczhiYQGqqqKqqkrfiSD6Wha+A8hOdyBSPFd7WxLXSLK0mIwRCcCMkWjMGInEfJnDwsICNE2D1+tt24kgMh3KdwApsoThbgcAoIqFBfVSh4wRxRkzRqIxYyQS82UOCwsriuwMpYaAlnrkeVIBAAca/QiGWUkTERERUf9jYWFF7XaGymtdZ6FpwP7D/gQ1ioiIiIgGMxYWFiBJErKystp2Imi3M1Ru64gFAFRzATf1QoeMEcUZM0aiMWMkEvNlDneFsgBZlpGVFVVMuGMvkpfnyTU+5ToL6o0OGSOKM2aMRGPGSCTmyxyOWFiAqqrYs2dP204E0VOhOoxYsLCgnuuQMaI4Y8ZINGaMRGK+zGFhYQGapsHn80XtChVbWOTFXMuChQX1XIeMEcUZM0aiMWMkEvNlDgsLK+pQWHDEgoiIiIgSi4WFFcXsCrUf2RkO41NefZuIiIiIEoGFhQXIsozc3FzIcuvbFbMrVC0cNgVZbjsAjlhQ73TIGFGcMWMkGjNGIjFf5vDVsQBJkpCZmdm2xZnNAaR69Nu+AwBgLOCuOexHWOX8P+qZDhkjijNmjERjxkgk5sscFhYWoKoqdu7cGbsTQWQ6VKSwyNAXcIdVDbWNvEge9UynGSOKI2aMRGPGSCTmyxwWFhagaRoCgUDsTgSRwsLfAARbYhZwc2co6qlOM0YUR8wYicaMkUjMlzksLKwqegF3Uy2vvk1ERERECcXCwqra7QzFEQsiIiIiSiQWFhYgyzIKCgpidyKIuZZF+xELFhbUM51mjCiOmDESjRkjkZgvcxL66ixfvhwnnHAC0tPTkZ2djfPPPx/btm2LOaelpQWLFi3CsGHD4Ha7MX/+fNTU1MScU1FRgXnz5sHpdCI7Oxs33XQTQqFQzDlr167FcccdB4fDgXHjxmHFihWiuxc3kiTB7XbH7kTgji4s9sdcfXsfCwvqoU4zRhRHzBiJxoyRSMyXOQktLN59910sWrQIH3/8MdasWYNgMIgzzzwTPp/POOf666/HK6+8gueffx7vvvsu9u3bhwsuuMC4PxwOY968eQgEAvjoo4/w1FNPYcWKFbjzzjuNc8rLyzFv3jycfvrp2LRpE5YuXYprrrkGq1ev7tf+9lY4HMb27dsRDofbDra7+nZuBtdYUO91mjGiOGLGSDRmjERivsyxJfLJX3/99ZjPV6xYgezsbGzcuBEzZ86E1+vF3/72NzzzzDP49re/DQB48sknMXHiRHz88cc46aST8MYbb+Drr7/Gm2++iZycHBx77LH41a9+hVtuuQV33XUX7HY7Hn/8cRQVFeGBBx4AAEycOBEffPABHnzwQcyZM6ff+90bHbY3azcVKs2uINOZgvqmINdYUK9wCz0SjRkj0ZgxEon56l5CC4v2vF4vAGDo0KEAgI0bNyIYDGL27NnGORMmTMDIkSOxbt06nHTSSVi3bh0mT56MnJwc45w5c+Zg4cKF2LJlC6ZOnYp169bFPEbknKVLl3baDr/fD7+/7VoQDQ0NAPRqNVKpSpIEWZahqmrM1mNdHZdlGZIkdXm8fQUcmcOnqirC4bDxr3E8bRiU1nPVwzWQAeRmpKK+KYiahhYEgyEoigxZlqFpWsw3Q0/bLqJPZo4ritJl29mn+Pap04xZvE9m2s4+9V+f2mcsGfqUjO+TlfvUWcas3qdkfJ+s3KdIvpKpT2aO92SL3QFTWKiqiqVLl+KUU07B0UcfDQCorq6G3W5HZmZmzLk5OTmorq42zokuKiL3R+470jkNDQ1obm5GWlpazH3Lly/HsmXLOrSxrKwMbrcbAODxeJCXl4eamhqjIAKArKwsZGVlYe/evTFTunJzc5GZmYldu3YhEAgYxwsKCuB2u1FWVhYThqKiIthsNpSWlkJVVdTV1WHHjh0YP348QqEQdlc3oKT13ObaCrgADHen4BsAwbCGDV99gxyPE2PGjIHX6zVeCwBwuVwoLCxEXV0damtrjeP92adoxcXFCIVCKC8vN47JsoySkhL4fD5UVlYax+12O/skoE+hUMjI2NixY5OiT8n4Plm5Ty0tLUbGRo4cmRR9Ssb3ycp9Onz4sJGx/Pz8pOhTMr5PVu3T/v37jXzJspwUfTL7PjmdTpglaQPkSh8LFy7Ea6+9hg8++AAFBQUAgGeeeQZXXXVVzOgBAEyfPh2nn3467rnnHlx77bXYvXt3zHqJpqYmuFwurFq1CnPnzkVJSQmuuuoq3HrrrcY5q1atwrx589DU1NShsOhsxCLyxmRkZADo36pc0zQEg0GkpKRAUfRxCjUchrw8D1I4AC3naEgLP8St//0S//p0DwDgxZ/OwJSCTP6lgX0y1adOM2bxPplpO/vUf31SVTUmY8nQp2R8n6zcp84yZvU+JeP7ZNU+hcNhBAIBpKSkQJKkpOiT2eONjY3IzMyE1+s1fg/uyoAYsVi8eDFeffVVvPfee0ZRAehVXyAQQH19fcyoRU1NDXJzc41zPv3005jHi+waFX1O+52kampqkJGR0aGoAACHwwGHw9HhuKIoxi9dEZE3vr2eHm//uNHHNU0z3mxJ0ncjUGw2fZ1Fw15IvgMAELMz1P7DAeO5JEnq9PHj1fbe9Mns8a7azj7Ft0+dZsziferrcfYpvn2K/h9xbzLW1XG+T+xT5HhnGbN6n3pynH0S3ye73R6Tr+7O72vbuzre3+9TdH+7k9BdoTRNw+LFi/HCCy/g7bffRlFRUcz906ZNQ0pKCt566y3j2LZt21BRUYEZM2YAAGbMmIHNmzdj//79xjlr1qxBRkYGJk2aZJwT/RiRcyKPMdCpqmpMiYrhytL/9dUCqhp7LYsGLuAm87rMGFGcMGMkGjNGIjFf5iR0xGLRokV45pln8NJLLyE9Pd2YF+bxeJCWlgaPx4Orr74aN9xwA4YOHYqMjAwsWbIEM2bMwEknnQQAOPPMMzFp0iRcfvnluPfee1FdXY3bb78dixYtMkYdrrvuOjz88MO4+eab8aMf/Qhvv/02nnvuOaxcuTJhfY+LyM5QWhhoqUd+1IgFd4YiIiIiov6U0BGLxx57DF6vF7NmzUJeXp7x8eyzzxrnPPjgg/jud7+L+fPnY+bMmcjNzcV///tf435FUfDqq69CURTMmDEDP/zhD7FgwQLcfffdxjlFRUVYuXIl1qxZg2OOOQYPPPAAnnjiCctsNdslV3bb7cb9vPo2ERERESVMQkcszKwbT01NxSOPPIJHHnmky3NGjRqFVatWHfFxZs2ahc8//7zHbRzQIlOhAP0iefnjjE+reJE8IiIiIupHCR2xIHNkWUZxcXHHRTbtrr7tdtiQnqrXihyxoJ7oMmNEccKMkWjMGInEfJnDV8ciQqFQx4PuqKlQxs5Q+nSoKm9Ljy5oQtRpxojiiBkj0ZgxEon56h4LCwtQVRXl5eVd7woFGIVFbusCbn9IRX1TsL+aSBbXZcaI4oQZI9GYMRKJ+TKHhYWVtZsKBQB5GW0LuLkzFBERERH1FxYWVhazK1RkxCL6WhZcwE1ERERE/YOFhUV0uljIOaztdrs1FgBHLKhnuCCNRGPGSDRmjERivrqX0O1myRxFUVBSUtLxDpsdSM0EWuqj1ljwWhbUc11mjChOmDESjRkjkZgvc1h6WYCmaWhsbOx8l6fIzlDGiEXb1bf31bOwIHOOmDGiOGDGSDRmjERivsxhYWEBqqqisrKy850IIgu4A41AoIlrLKhXjpgxojhgxkg0ZoxEYr7MYWFhddFbzjbVIiPVBqddAcA1FkRERETUf1hYWF27naEkSTJGLap5kTwiIiIi6icsLCxAkiTY7XZIktTxzs6uZdFaWDQFwmho4VUiqXtHzBhRHDBjJBozRiIxX+ZwVygLkGUZY8aM6fzOzq6+ndG2gLva2wJPWorI5lESOGLGiOKAGSPRmDESifkyhyMWFqBpGurr64+8KxQA+PYDaH8tCy7gpu4dMWNEccCMkWjMGInEfJnDwsICVFVFdXV15zsRpOe13fbuBcBrWVDPHTFjRHHAjJFozBiJxHyZw8LC6jyFbbe9ewDw6ttERERE1P9YWFidazigOPTb9XphwRELIiIiIupvLCwsQJIkuFyuzncikGXAU6Df9u4BNC3m6ttVDSwsqHtHzBhRHDBjJBozRiIxX+awsLAAWZZRWFgIWe7i7cpsnQ4VaASaD2GIMwUOm35uNRdvkwndZoyoj5gxEo0ZI5GYL3P46liAqqqora3tesFQzDqLSkiSZKyz4BoLMqPbjBH1ETNGojFjJBLzZQ4LCwvQNA21tbVdb3GWObLttjd2ncXhlhAa/bxIHh1Ztxkj6iNmjERjxkgk5sscFhbJIHrEoj6yM1TsRfKIiIiIiERiYZEMIou3gQ4jFgALCyIiIiISj4WFBUiSBI/H0/VOBJnRIxYVAHj1beqZbjNG1EfMGInGjJFIzJc5tkQ3gLonyzLy8vK6PiFjBCDJgKa2jVhkcMSCzOs2Y0R9xIyRaMwYicR8mcMRCwtQVRVVVVVd70SgpADprWHvZI3FPhYW1I1uM0bUR8wYicaMkUjMlzksLCxA0zR4vd4j70QQWcDdVAsEmtqtseBUKDoyUxkj6gNmjERjxkgk5sscFhbJIjP2WhbDXHakKPo8QF7LgoiIiIhEY2GRLGIuklcBWZaQ07rOorqBhQURERERicXCwgIkSUJWVtaRdyLI7OxaFnphUd8URHMgLLKJZHGmMkbUB8wYicaMkUjMlzm9Kiz27NmDyspK4/NPP/0US5cuxV/+8pe4NYzayLKMrKwsyPIR3i5PZ1ffjrpIHkct6AhMZYyoD5gxEo0ZI5GYL3N69er84Ac/wDvvvAMAqK6uxne+8x18+umnuO2223D33XfHtYGk70SwZ8+eI+9EcIQRC4DXsqAjM5Uxoj5gxkg0ZoxEYr7M6VVh8dVXX2H69OkAgOeeew5HH300PvroIzz99NNYsWJFPNtH0Hci8Pl83ewK1cnVt3ktCzLJVMaI+oAZI9GYMRKJ+TKnV4VFMBiEw+EAALz55ps499xzAQATJkxAVVVV/FpH5tldgHOYfturT1OLHbFgYUFERERE4vSqsDjqqKPw+OOP4/3338eaNWtw1llnAQD27duHYcOGxbWB1AORnaEa9gHhULtrWbCwICIiIiJxelVY3HPPPfjzn/+MWbNm4dJLL8UxxxwDAHj55ZeNKVIUP7IsIzc3t/sFQ5F1FloYOLwv5urbHLGgIzGdMaJeYsZINGaMRGK+zLH15otmzZqF2tpaNDQ0YMiQIcbxa6+9Fk6nM26NI50kScjMzOz+RE/sAu7hIwuhyBLCqobqBi7epq6ZzhhRLzFjJBozRiIxX+b0quxqbm6G3+83iordu3fjD3/4A7Zt24bs7Oy4NpD0nQh27tzZ/U4EMRfJ2wNFlpCdrq+F4VQoOhLTGSPqJWaMRGPGSCTmy5xeFRbnnXce/vGPfwAA6uvrceKJJ+KBBx7A+eefj8ceeyyuDSR9J4JAIND9TgSdbDkbWWdR2xiAP8SL5FHnTGeMqJeYMRKNGSORmC9zelVYfPbZZzjttNMAAP/5z3+Qk5OD3bt34x//+Af+9Kc/xbWB1AMxIxYVAGJ3hqrx+vu7RUREREQ0SPSqsGhqakJ6ejoA4I033sAFF1wAWZZx0kknYffu3XFtIPVAZtTVt1tHLPKjFnDvrec6CyIiIiISo1eFxbhx4/Diiy9iz549WL16Nc4880wAwP79+5GRkRHXBpK+E0FBQUH3OxGkDQFSXPrt1ovk5We2FRb7WFhQF0xnjKiXmDESjRkjkZgvc3r16tx555248cYbMXr0aEyfPh0zZswAoI9eTJ06Na4NJH0nArfbDUmSujuxbZ2FtxLQNBYWZIrpjBH1EjNGojFjJBLzZU6vCovvf//7qKiowIYNG7B69Wrj+BlnnIEHH3wwbo0jXTgcxvbt2xEOm1h8HVlnEWoBfAdQMCSqsPCysKDO9ShjRL3AjJFozBiJxHyZ06vrWABAbm4ucnNzUVlZCQAoKCjgxfEEMr29WbudofKHTDY+3VvPLWepa9xCj0Rjxkg0ZoxEYr6616sRC1VVcffdd8Pj8WDUqFEYNWoUMjMz8atf/YoveqK12xlqiDMFqSn627z3UFOCGkVEREREya5XIxa33XYb/va3v+F3v/sdTjnlFADABx98gLvuugstLS34zW9+E9dGUg9E7wzlrYQkScjPTMPOAz7sq2+BpmmcH0hEREREcderwuKpp57CE088gXPPPdc4NmXKFIwYMQI//elPWVjEmSzLKCoqMrcTgafjRfJGtBYWzcEw6puCGOKyC2opWVWPMkbUC8wYicaMkUjMlzm9enXq6uowYcKEDscnTJiAurq6PjeKOrLZTNaA0WssvG2FRQSvZUFdMZ0xol5ixkg0ZoxEYr6616vC4phjjsHDDz/c4fjDDz+MKVOm9LlRFEtVVZSWlppbv+LOBeQU/XY9r2VB5vQoY0S9wIyRaMwYicR8mdOr0uvee+/FvHnz8OabbxrXsFi3bh327NmDVatWxbWB1EOyDHhGAId2Ad4KALGFBUcsiIiIiEiEXo1YfOtb38L27dvxve99D/X19aivr8cFF1yALVu24J///Ge820g9FVln0eIFWhqQn5lq3MURCyIiIiISodeTxfLz8zss0v7iiy/wt7/9DX/5y1/63DDqA0/sOosRmaONT/fxWhZEREREJACXtluALMsoLi42vxNBu4vk5XpSEdlhllOhqDM9zhhRDzFjJBozRiIxX+bw1bGIUChk/uR2IxYOm4LhbgcAToWirvUoY0S9wIyRaMwYicR8dY+FhQWoqory8nLzOxHEjFjELuDef9gPfygc7yaSxfU4Y0Q9xIyRaMwYicR8mdOjNRYXXHDBEe+vr6/vS1soXjydX8ti0556AEC1twWjhrkS0DAiIiIiSlY9Kiw8Hk+39y9YsKBPDaI48BS03TauZdG2M9Te+mYWFkREREQUVz0qLJ588sm4Pvl7772H++67Dxs3bkRVVRVeeOEFnH/++cb9V155JZ566qmYr5kzZw5ef/114/O6ujosWbIEr7zyCmRZxvz58/HHP/4RbrfbOOfLL7/EokWLsH79egwfPhxLlizBzTffHNe+iNajxUI2h36hvMbqTq++zZ2hqDNckEaiMWMkGjNGIjFf3UvoK+Tz+XDMMcfgkUce6fKcs846C1VVVcbHv/71r5j7L7vsMmzZsgVr1qzBq6++ivfeew/XXnutcX9DQwPOPPNMjBo1Chs3bsR9992Hu+66y1Jb4iqKgpKSEiiKYv6LIussGmuAkJ9X36Yj6lXGiHqAGSPRmDESifkyp9fXsYiHuXPnYu7cuUc8x+FwIDc3t9P7tm7ditdffx3r16/H8ccfDwB46KGHcPbZZ+P+++9Hfn4+nn76aQQCAfz973+H3W7HUUcdhU2bNuH3v/99TAEykGmaBp/PB5fLBSmyb2x3PIVA5Xr9trcS+ZlZxl0sLKi9XmWMqAeYMRKNGSORmC9zBvyYztq1a5GdnY3x48dj4cKFOHjwoHHfunXrkJmZaRQVADB79mzIsoxPPvnEOGfmzJmw2+3GOXPmzMG2bdtw6NCh/utIH6iqisrKyp7tRJDZ/iJ5bSMWvJYFtderjBH1ADNGojFjJBLzZU5CRyy6c9ZZZ+GCCy5AUVERysrK8D//8z+YO3cu1q1bB0VRUF1djezs7JivsdlsGDp0KKqrqwEA1dXVKCoqijknJyfHuG/IkCEdntfv98Pv9xufNzQ0AADC4TDCYX2rVkmSIMsyVFWFpmnGuV0dl2UZkiR1eTzyuNHHAT3I4XDY+Df6eDRFUaBpmnFcyigwqkatvgLpI09FWoqC5mAYew81G49hpu0i+mTmePs+Rbelq+PsU+/6ZCZjVuuTmbazT/3Xp/YZS4Y+JeP7ZOU+dZYxq/cpGd8nK/cpkq9k6pOZ49G3uzOgC4tLLrnEuD158mRMmTIFY8eOxdq1a3HGGWcIe97ly5dj2bJlHY6XlZUZi8I9Hg/y8vJQU1MDr9drnJOVlYWsrCzs3bsXPp/POJ6bm4vMzEzs2rULgUDAOF5QUAC3242ysrKYMBQVFcFms6G0tBSqqqKurg47duzA+PHjEQqFUF5ebpwryzJKSkrg8/lQWVkJAHD5ZETGLPw1O7Brxw4Md8qo8Iaxt74Jmqahrq4OtbW1xuP0Z5+iFRcXm+oTANjtdowZMwZer9coHgHA5XKhsLCQfepln0KhkJGxsWPHJkWfkvF9snKfWlpajIyNHDkyKfqUjO+Tlft0+PBhI2P5+flJ0adkfJ+s2qf9+/cb+ZJlOSn6ZPZ9cjqdMEvSelKGCCRJUoddoTozfPhw/PrXv8ZPfvIT/P3vf8cvfvGLmClNoVAIqampeP755/G9730PCxYsQENDA1588UXjnHfeeQff/va3UVdXZ3rEIvLGZGRkGO3tr6pcVVXs3r0bo0aNgs1mM45H61DB7v8ayp9PBQBox1wK9dxHcOWKDXi/VA/WZ3d8B5lptkH1lwb2qes+mcmY1fpkpu3sU/+OWERnLBn6lIzvk5X71FnGrN6nZHyfrNqnUCiEXbt2YdSoUUb7rN4ns8cbGxuRmZkJr9dr/B7clQE9YtFeZWUlDh48iLy8PADAjBkzUF9fj40bN2LatGkAgLfffhuqquLEE080zrntttsQDAaRkpICAFizZg3Gjx/faVEB6AvGHQ5Hh+OKonTYDSDyxrfX0+Nd7TIQec5x48Z1e74kSW3Hh4xqO+6thKIoKBgSuzPUUFfn1yXpjz6ZPR7TJxPH49X2wdYnsxnr6vhA7FNfj7NP8e2TLMt9ylhXx/k+sU+R451lzOp96slx9klsn2w2W4d8Hel8K/TJ7HFJMr9YPaGLtxsbG7Fp0yZs2rQJAFBeXo5NmzahoqICjY2NuOmmm/Dxxx9j165deOutt3Deeedh3LhxmDNnDgBg4sSJOOuss/DjH/8Yn376KT788EMsXrwYl1xyCfLz8wEAP/jBD2C323H11Vdjy5YtePbZZ/HHP/4RN9xwQ6K63WOapqG+vr5Hc9yQmgGkthYO9RUAgHxPW2FReYgLuKlNrzJG1APMGInGjJFIzJc5CS0sNmzYgKlTp2Lq1KkAgBtuuAFTp07FnXfeCUVR8OWXX+Lcc89FSUkJrr76akybNg3vv/9+zGjC008/jQkTJuCMM87A2WefjVNPPTXmGhUejwdvvPEGysvLMW3aNPziF7/AnXfeaZmtZgF9GKy6urrDcFi3PCP1fxv2AmqY17KgLvU6Y0QmMWMkGjNGIjFf5iR0KtSsWbOOWPmtXr2628cYOnQonnnmmSOeM2XKFLz//vs9bp/lZRYCNZsBNQQcrsaIISwsiIiIiEiMAX8dC+oDT9fXstjnZWFBRERERPHDwsICJEnq3ZUeoy+SV78HORmpiDzE3vqW+DWQLK/XGSMyiRkj0ZgxEon5MoeFhQXIsozCwsIuV+93KWbEogJ2m4zsdH19yl4u3qYovc4YkUnMGInGjJFIzJc5fHUsQFVV1NbW9nzBUPSIhVe/kEpkAXdtox8twXBnX0WDUK8zRmQSM0aiMWMkEvNlDgsLC9A0DbW1tT3f4iyyKxQA1O8BgJh1FtVeTociXa8zRmQSM0aiMWMkEvNlDguLZObKAmythYS3Y2HBnaGIiIiIKF5YWCQzSQI8Bfrt+j2ApsVcy2IvCwsiIiIiihMWFhYgSRI8Hk/vdiKIrLMI+oDmQywsqFN9yhiRCcwYicaMkUjMlzkJvUAemSPLMvLy8nr3xdE7Q9VXID+zyPiUU6Eook8ZIzKBGSPRmDESifkyhyMWFqCqKqqqqnq3E0Fm9ALuinZrLLh4m3R9yhiRCcwYicaMkUjMlzksLCxA0zR4vd7e7USQOartdn0FPGkpcNkVAByxoDZ9yhiRCcwYicaMkUjMlzksLJJduxELSZKMdRZ765v5DUJEREREccHCItkNiR6x2A2g7SJ5/pCKOl8gEa0iIiIioiTDwsICJElCVlZW73YicGUDikO/XV8BANwZijroU8aITGDGSDRmjERivsxhYWEBsiwjKysLstyLt0uW27acra8ANA0jMlONu7nOgoA+ZozIBGaMRGPGSCTmyxy+Ohagqir27NnT+50IIussAo1A8yGMGBI9YsGdoSgOGSPqBjNGojFjJBLzZQ4LCwvQNA0+n6/3C61jFnDvRr4nestZjlhQHDJG1A1mjERjxkgk5sscFhaDQbudofIzWVgQERERUXyxsBgM2l3LIteTisjaIy7eJiIiIqJ4YGFhAbIsIzc3t/cLhtqNWKQoMnLS9QXcHLEgIA4ZI+oGM0aiMWMkEvNlDl8dC5AkCZmZmb3f4qxdYQHAWMBd2xhASzDc1yaSxfU5Y0TdYMZINGaMRGK+zGFhYQGqqmLnzp2934mgm2tZVHm5M9Rg1+eMEXWDGSPRmDESifkyh4WFBWiahkAg0PudCDq5lkU+r2VBUfqcMaJuMGMkGjNGIjFf5rCwGCzaX8si+urbh1hYEBEREVHfsLAYLI5wLQvuDEVEREREfcXCwgJkWUZBQUHfdiJot4A7+urbnApFcckY0REwYyQaM0YiMV/m8NWxAEmS4Ha7+7YTQbtrWcRcJM/LwmKwi0vGiI6AGSPRmDESifkyh4WFBYTDYWzfvh3hcB+2hW03YpGRaoPbYQMA7KvnrlCDXVwyRnQEzBiJxoyRSMyXOSwsLKLP25u1KywkSTJ2htpb3wxV5S4Hgx230CPRmDESjRkjkZiv7rGwGCyir2VxaDeAtmtZBEIqDvoCiWoZERERESUBFhaDRSfXsojecpYLuImIiIioL1hYWIAsyygqKur7TgSR6VBBH9BUF7uAm4XFoBa3jBF1gRkj0ZgxEon5MoevjkXYbLa+P0i7a1nEXCSPhcWgF5eMER0BM0aiMWMkEvPVPRYWFqCqKkpLS+O+gDufhQW1ilvGiLrAjJFozBiJxHyZw8JiMOlwLYtU41NOhSIiIiKivmBhMZi0G7HIzUiFIusXetlTx8KCiIiIiHqPhcVg0q6wsCkyCofo06F2HfRB03gtCyIiIiLqHRYWFiDLMoqLi/u+E0H0tSzqKwAAY4a7AQBNgTBqGvx9e3yyrLhljKgLzBiJxoyRSMyXOXx1LCIUCvX9QTq5lkVRlsu4e2dtY9+fgywrLhkjOgJmjERjxkgk5qt7LCwsQFVVlJeXx2cngsgC7tZrWcQUFgd8fX98sqS4ZoyoE8wYicaMkUjMlzksLAabdteyGDO8rbAor2VhQURERES9w8JisGm3gHtMltv4lIUFEREREfUWCwuLiNtioXaFRU6GA067AgDYeYBrLAYzLkgj0ZgxEo0ZI5GYr+7xFbIARVFQUlICRVH6/mDtLpInSZKxzmLPoWYEQpw7OBjFNWNEnWDGSDRmjERivsxhYWEBmqahsbExPteZaDdiAcAoLMKqhj2Hmvr+HGQ5cc0YUSeYMRKNGSORmC9zWFhYgKqqqKysjM9OBO5swJaq345cyyJqZ6hy7gw1KMU1Y0SdYMZINGaMRGK+zGFhMdhIEuBpdy2L4byWBRERERH1DQuLwSgyHSroA5oOcmcoIiIiIuozFhYWIEkS7HY7JEmKzwO2u5bFaF4kb9CLe8aI2mHGSDRmjERivsxhYWEBsixjzJgxwrac9aSlIMttBwDs5IjFoBT3jBG1w4yRaMwYicR8mcNXxwI0TUN9fX38diLoZGeoyHSoA4f9ONwSjM/zkGXEPWNE7TBjJBozRiIxX+awsLAAVVVRXV0dv50I2l3LAmjbchYAdtVyy9nBJu4ZI2qHGSPRmDESifkyh4XFYNTZtSy4MxQRERER9QELi8Gok2tZFHEBNxERERH1AQsLC5AkCS6XK347EXRyLYuxUSMW3HJ28Il7xojaYcZINGaMRGK+zGFhYQGyLKOwsDC+OxEY17JoApoOonCoE3Lr9woLi8FHSMaIojBjJBozRiIxX+bw1bEAVVVRW1sb3wVD7a5l4bApKBjiBADsPNDIXQ8GGSEZI4rCjJFozBiJxHyZw8LCAjRNQ21tbXx/2e9sy9nW6VC+QBgHDvvj91w04AnJGFEUZoxEY8ZIJObLHBYWg1VnO0NFL+DmdCgiIiIi6oGEFhbvvfcezjnnHOTn50OSJLz44osx92uahjvvvBN5eXlIS0vD7NmzUVpaGnNOXV0dLrvsMmRkZCAzMxNXX301Ghtjt0v98ssvcdpppyE1NRWFhYW49957RXdt4OvkWhZjuDMUEREREfVSQgsLn8+HY445Bo888kin9997773405/+hMcffxyffPIJXC4X5syZg5aWFuOcyy67DFu2bMGaNWvw6quv4r333sO1115r3N/Q0IAzzzwTo0aNwsaNG3Hffffhrrvuwl/+8hfh/YsXSZLg8XjiuxNBp1Oh3Mahcl7LYlARkjGiKMwYicaMkUjMlzmSNkAmi0mShBdeeAHnn38+AH20Ij8/H7/4xS9w4403AgC8Xi9ycnKwYsUKXHLJJdi6dSsmTZqE9evX4/jjjwcAvP766zj77LNRWVmJ/Px8PPbYY7jttttQXV0Nu90OAPjlL3+JF198Ed98842ptjU0NMDj8cDr9SIjIyP+nU8ETQN+kwuEWoDhE4BFn2BffTNO/t3bAIDZE7PxxBUnJLiRRERERJRIPfk92NZPbeqx8vJyVFdXY/bs2cYxj8eDE088EevWrcMll1yCdevWITMz0ygqAGD27NmQZRmffPIJvve972HdunWYOXOmUVQAwJw5c3DPPffg0KFDGDJkSIfn9vv98PvbFi83NDQAAMLhMMLhMAC9EJJlGaqqxizk6eq4LMuQJKnL45HHjT4O6LsQqKqK/fv3Izs7GzabzTgeTVEUaJoWczzSli6PewohHSyFVl8BNRRCttuO1BQZLUEVZQcau+1rX/pk5nhv+mT2/WCfYvtkJmNW65OZtrNP/dencDgck7Fk6FMyvk9W7lNnGbN6n5LxfbJqn0KhEGpqapCdnW20z+p9Mnu8J2MQA7awqK6uBgDk5OTEHM/JyTHuq66uRnZ2dsz9NpsNQ4cOjTmnqKiow2NE7uussFi+fDmWLVvW4XhZWRncbn26kMfjQV5eHmpqauD1eo1zsrKykJWVhb1798Lna1unkJubi8zMTOzatQuBQMA4XlBQALfbjbKyspgwFBUVwWazobS0FKqqoq6uDl6vF+PHj0coFEJ5eblxrizLKCkpgc/nQ2VlpXHcbrdjzJgx8Hq9xusBAC6XC4WFhQi6cmE/WAop2ISdX62HO2c0Rg9z4Zvqw6g42ISt27bDJktC+hStuLg4bn2qq6tDbW2tcbw/3ycr9ykUChkZGzt2bFL0KRnfJyv3qaWlxcjYyJEjk6JPyfg+WblPhw8fNjKWn5+fFH1KxvfJyn0qLy+H1+uFLMtJ0ycz75PT6YRZA3Yq1EcffYRTTjkF+/btQ15ennHeRRddBEmS8Oyzz+K3v/0tnnrqKWzbti3msbKzs7Fs2TIsXLgQZ555JoqKivDnP//ZuP/rr7/GUUcdha+//hoTJ07s0JbORiwib0xkCKg/q/JwOIwdO3Zg3LhxSElJMY5H69WIxSs/h7RxBQAg/KM1kAqOx5J/bcLKzVUAgDevPw1FWa5BVZUP1j6ZyZjV+mSm7exT//UpFArFZCwZ+pSM75OV+9RZxqzep2R8n6zap2AwiNLSUowbNw6KoiRFn8web2xsRGZmprWnQuXm5gIAampqYgqLmpoaHHvsscY5+/fvj/m6yF9eI1+fm5uLmpqamHMin0fOac/hcMDhcHQ4rigKFEWJORZ549vr6fH2j9v+uCzLRpC7Ol+SpJ4dj9oZSmmoBOTpMVvOVhxqxrictgDFu09mjve0T/F6PwZjn8xkrKvjA7VPfTnOPsW3T4qi9CljXR3n+8Q+RY53ljGr96knx9kn8X2K5Cv666zeJzPHI99PZgzY61gUFRUhNzcXb731lnGsoaEBn3zyCWbMmAEAmDFjBurr67Fx40bjnLfffhuqquLEE080znnvvfcQDAaNc9asWYPx48d3Og1qIJIkfTpST95YU4Z03HK2iFvODkrCMkbUihkj0ZgxEon5MiehhUVjYyM2bdqETZs2AdAXbG/atAkVFRWQJAlLly7Fr3/9a7z88svYvHkzFixYgPz8fGO61MSJE3HWWWfhxz/+MT799FN8+OGHWLx4MS655BLk5+cDAH7wgx/Abrfj6quvxpYtW/Dss8/ij3/8I2644YYE9brnZFmfy9dVZdlrnV3LYjgvkjcYCcsYUStmjERjxkgk5suchL46GzZswNSpUzF16lQAwA033ICpU6fizjvvBADcfPPNWLJkCa699lqccMIJaGxsxOuvv47U1FTjMZ5++mlMmDABZ5xxBs4++2yceuqpMdeo8Hg8eOONN1BeXo5p06bhF7/4Be68886Ya10MdKqqYs+ePR3m2fVZzLUsdgOIHbEo54jFoCEsY0StmDESjRkjkZgvcxK6xmLWrFlH3MJKkiTcfffduPvuu7s8Z+jQoXjmmWeO+DxTpkzB+++/3+t2JpqmafD5fD3a7ssU13DAng4EDgP79Wt6ZDrtGOqyo84XwE5eJG/QEJYxolbMGInGjJFIzJc5HM8ZzCQJyJui326oBHz6VmRjWkctahr88PlDiWodEREREVkIC4vBLu/Yttv7NgFoNx2K6yyIiIiIyAQWFhYgyzJyc3PFLBjKO6btdtXnAIAiLuAedIRmjAjMGInHjJFIzJc5A/Y6FtRGkiRkZmaKefD8Y9tut45YjOEC7kFHaMaIwIyReMwYicR8mcOyywJUVcXOnTvF7EQwbByQ0lpIVH0JABgz3G3cXc4F3IOC0IwRgRkj8ZgxEon5MoeFhQVomoZAICBmJwJZaVvA7a0AmuowcqgTkeu/cI3F4CA0Y0Rgxkg8ZoxEYr7MYWFB7RZwf47UFAUjMtMA6Fff5jcREREREXWHhQXFrrOo2gSgbTrUYX8ItY2B/m8TEREREVkKCwsLkGUZBQUF4nYi6GTL2THccnZQEZ4xGvSYMRKNGSORmC9z+OpYgCRJcLvdkCILH+ItqxhIceq3W0csoq9lsfMAF3AnO+EZo0GPGSPRmDESifkyh4WFBYTDYWzfvh3hcFjME8gKkDtZv12vL+AeM5wjFoOJ8IzRoMeMkWjMGInEfJnDwsIihG9vFj0dquqL2BELFhaDArfQI9GYMRKNGSORmK/usbAgXbsF3PmeNNhtejw4FYqIiIiIusPCgnTtFnDLsoSiYfqoRUVdE0JhVulERERE1DUWFhYgyzKKiorE7kSQVQLY9GtXtG05qxcWwbCGvfXN4p6bEq5fMkaDGjNGojFjJBLzZQ5fHYuw2Wxin0CxtS3gPrQLaD7EdRaDjPCM0aDHjJFozBiJxHx1j4WFBaiqitLSUvGLhmLWWXxhXCQPALZXHxb73JRQ/ZYxGrSYMRKNGSORmC9zWFhQm3brLCblZRifbtnX0P/tISIiIiLLYGFBbfKOabtd9QWKc9ywK3pEvtrnTVCjiIiIiMgKWFhQm+ETAFuqfrtqE1IUGRPy0gHoF8lr9IcS2DgiIiIiGshYWFiALMsoLi4WvxOBYgNyjtZv1+0EWrw4Kt8DANA0YGsVp0Mlq37LGA1azBiJxoyRSMyXOXx1LCIU6qfRgnYLuCeP8Bifbq7kdKhk1m8Zo0GLGSPRmDESifnqHgsLC1BVFeXl5f2zE0G7BdxHj2hbwM11FsmrXzNGgxIzRqIxYyQS82UOCwuKFTNisQklOemwyRIAYMteToUiIiIios6xsKBYwycAikO/vW8TUlMUFOfoC7hL9x9GcyCcwMYRERER0UDFwsIi+m2xkJIC5Byl364rA1oaMLl1OpSqAVurOWqRrLggjURjxkg0ZoxEYr66x1fIAhRFQUlJCRRF6Z8njJ4OVf0ljo5awL1lL9dZJKN+zxgNOswYicaMkUjMlzksLCxA0zQ0NjZC07T+ecJ2C7gjW84CwFdcZ5GU+j1jNOgwYyQaM0YiMV/msLCwAFVVUVlZ2X87EbRbwD0pLwOt67e5M1SS6veM0aDDjJFozBiJxHyZw8KCOho+EVDs+u19m5BmVzAu2w0A2F5zGP4QF3ATERERUSwWFtSRzd62gPvgDsB/GEe3TocKhjVsr25MYOOIiIiIaCBiYWEBkiTBbrdDkqT+e1JjnYUGVG/GUVELuDkdKvkkJGM0qDBjJBozRiIxX+awsLAAWZYxZsyY/t3mLHqdxb5NODo/6grc3Bkq6SQkYzSoMGMkGjNGIjFf5vDVsQBN01BfX9+/OxHkHdN2u2pT7IgFC4ukk5CM0aDCjJFozBiJxHyZw8LCAlRVRXV1df/uRJA9CZBT9Nv7NsHtsGFMlgsAsLX6MIJh7oqQTBKSMRpUmDESjRkjkZgvc1hYUOdsDiBnkn67djvgbzRGLQIhFTv2cwE3EREREbVhYUFda7eAm+ssiIiIiKgrLCwsQJIkuFyu/t+JIGYB9+eYzHUWSSthGaNBgxkj0ZgxEon5MoeFhQXIsozCwsL+34lgxPFtt/duwFH50VvONvRvW0iohGWMBg1mjERjxkgk5sscvjoWoKoqamtr+3/BUPYkIMWp365cD48zBYVD0wAAX+9rQFjlzgjJImEZo0GDGSPRmDESifkyh4WFBWiahtra2v7f4kyxAflT9dv1FcDhGuMK3M3BMMpruYA7WSQsYzRoMGMkGjNGIjFf5rCwoCMriJ0OdXTUOovNXGdBRERERK1YWNCRFZzQdrtyfUxh8dVerrMgIiIiIh0LCwuQJAkejycxOxFEL+Cu3ICjuOVsUkpoxmhQYMZINGaMRGK+zGFhYQGyLCMvLy8xOxFk5AGeQv323s+Q5bQhz5MKQF/ArXIBd1JIaMZoUGDGSDRmjERivszhq2MBqqqiqqoqcTsRjJim/xv0Afu3GtOhDvtD2F3XlJg2UVwlPGOU9JgxEo0ZI5GYL3NYWFiApmnwer2J24mg/TqLfF4oL9kkPGOU9JgxEo0ZI5GYL3NYWFD3YgqLDTh6RNQ6i30sLIiIiIiIhQWZkTcFkFP02+12htrCnaGIiIiICCwsLEGSJGRlZSVuJ4KUNCB3sn67dhuyU5qR5XYA0K9lwWFB60t4xijpMWMkGjNGIjFf5rCwsABZlpGVlZXYnQiiLpQn7fsck1unQ3mbg6g81JyoVlGcDIiMUVJjxkg0ZoxEYr7M4atjAaqqYs+ePYndiaDDOouo6VBcZ2F5AyJjlNSYMRKNGSORmC9zWFhYgKZp8Pl8iZ1yVBB9obz1OCqfV+BOJgMiY5TUmDESjRkjkZgvc1hYkDlDigDnMP125QYcnZ9u3MWdoYiIiIiIhQWZI0lt06Ga6zBCq8YQp75T1FdcwE1EREQ06LGwsABZlpGbm5v4BUMjohZwR62zqG0McAG3xQ2YjFHSYsZINGaMRGK+zOGrYwGSJCEzMzPxW5y1W2dxwuihxqeflNcloEEULwMmY5S0mDESjRkjkZgvc1hYWICqqti5c2fidyIYcRyA1m+oyvWYXtRWWHxafjAxbaK4GDAZo6TFjJFozBiJxHyZM6ALi7vuuguSJMV8TJgwwbi/paUFixYtwrBhw+B2uzF//nzU1NTEPEZFRQXmzZsHp9OJ7Oxs3HTTTQiFQv3dlT7RNA2BQCDx6xhSPcDw1te/5iscm+uAXdEj9ClHLCxtwGSMkhYzRqIxYyQS82XOgC4sAOCoo45CVVWV8fHBBx8Y911//fV45ZVX8Pzzz+Pdd9/Fvn37cMEFFxj3h8NhzJs3D4FAAB999BGeeuoprFixAnfeeWciupIcItOh1BBSa7/CMYX6OotdB5tQ09CSwIYRERERUSIN+MLCZrMhNzfX+MjKygIAeL1e/O1vf8Pvf/97fPvb38a0adPw5JNP4qOPPsLHH38MAHjjjTfw9ddf4//+7/9w7LHHYu7cufjVr36FRx55BIFAIJHdsq6YC+Wtx4lFw4xPuc6CiIiIaPCyJboB3SktLUV+fj5SU1MxY8YMLF++HCNHjsTGjRsRDAYxe/Zs49wJEyZg5MiRWLduHU466SSsW7cOkydPRk5OjnHOnDlzsHDhQmzZsgVTp07t9Dn9fj/8fr/xeUODfgG4cDiMcDgMQF/EI8syVFWNGRbr6rgsy5AkqcvjkceNPg7AOD8/Px+aphlf236On6Io0DQt5nikLV0dN9v2mON5x0FpPa5Vrsf0Yy8B3tE//6SsFvOOzjHVJzPH+61PcXqfrNwnMxmzWp/MtJ196t8+RWcsWfrU/jj7lNg+tc9YMvSpfdvZp8T0CYCRr3A4nBR9Mnu8J9O/BnRhceKJJ2LFihUYP348qqqqsGzZMpx22mn46quvUF1dDbvdjszMzJivycnJQXV1NQCguro6pqiI3B+5ryvLly/HsmXLOhwvKyuD2+0GAHg8HuTl5aGmpgZeb9sF4rKyspCVlYW9e/fC5/MZx3Nzc5GZmYldu3bFjJYUFBTA7XajrKwsJgxFRUWw2WwoLS2NaUNxcTFCoRDKy8uNY7Iso6SkBD6fD5WVlcZxu92OMWPGwOv1xvTX5XKhsLAQdXV1qK2tNY6b6tNhGcU2J5RQE7SKT3HceUMgS4CqAR9sr0Zpqd16fUrG94l9Yp/YJ/aJfWKf2Ke49Wn//v1J1yez75PT6YRZkmahVSj19fUYNWoUfv/73yMtLQ1XXXVVzMgCAEyfPh2nn3467rnnHlx77bXYvXs3Vq9ebdzf1NQEl8uFVatWYe7cuZ0+T2cjFpE3JiMjA0D/VuXhcBg7d+7EmDFjkJKSYhyP1p8VrPzP8yHtek+/84atOPcfO/FlpR7M9f/zbWSlp1q2Kk/GvzSY6ZOZjFmtT2bazj71X59CoVBMxpKhT8n4Plm5T51lzOp9Ssb3yap9CgaDKCsrw5gxY6AoSlL0yezxxsZGZGZmwuv1Gr8Hd2VAj1i0l5mZiZKSEuzYsQPf+c53EAgEUF9fHzNqUVNTg9zcXAB61fjpp5/GPEZk16jIOZ1xOBxwOBwdjiuKAkVRYo5F3vj2enq8/eN2djwS5K7OlySpR8d73fbCE4BIYVG5ASeNKTYKi40VXpx1dJrpPnV3vN/6ZPJ4svepu4x1dXwg96m3x9mn+PYp8vi9zVhXx/k+sU+R451lzOp96slx9ql/+tT+d8Fk6FN3xyPfT2YM+MXb0RobG1FWVoa8vDxMmzYNKSkpeOutt4z7t23bhoqKCsyYMQMAMGPGDGzevBn79+83zlmzZg0yMjIwadKkfm9/0mi3gHt61IXyuO0sERER0eA0oEcsbrzxRpxzzjkYNWoU9u3bh//93/+Foii49NJL4fF4cPXVV+OGG27A0KFDkZGRgSVLlmDGjBk46aSTAABnnnkmJk2ahMsvvxz33nsvqqurcfvtt2PRokWdjkiQSSOir8C9ASecOhSSBGga8OkuXiiPiIiIaDAa0IVFZWUlLr30Uhw8eBDDhw/Hqaeeio8//hjDhw8HADz44IOQZRnz58+H3+/HnDlz8OijjxpfrygKXn31VSxcuBAzZsyAy+XCFVdcgbvvvjtRXeoVWZZRVFTU5ZBVv3MPBzJHAfW7gX2fw+MAxuek45vqw/h6XwMaWoLISE1JdCupBwZcxijpMGMkGjNGIjFf5lhq8XaiNDQ0wOPxmFq0IkJkMU5k4c+A8J+rga/+o9++9l3ctSEFKz7aBQB48qoTcPr47MS1jXpsQGaMkgozRqIxYyTSYM5XT34PZtllAaqqorS0tMPOAAnVfp1FUds6i092cp2F1QzIjFFSYcZINGaMRGK+zGFhQb0TXVjs3YgTYhZwc50FERER0WDDwoJ6J/doQLHrtyvXY3i6A2OGuwAAX1Z60RwIH+GLiYiIiCjZsLCg3rE5gLxj9NsHdwCHa3Bi0TAAQEjV8HnFoQQ2joiIiIj6GwsLC5BlGcXFxQNvJ4KimW23y97GiVHrLD7m9SwsZcBmjJIGM0aiMWMkEvNlDl8diwiFQoluQkfjZrfd3vFmzAJurrOwngGZMUoqzBiJxoyRSMxX91hYWICqqigvLx94OxEUTAccHv122dvIz7CjcGgaAODzinr4Q1xnYRUDNmOUNJgxEo0ZI5GYL3NYWFDvKTZgzLf02811wL7PMX20vs7CH1KxudKbwMYRERERUX9iYUF90246VPQ6i0+4zoKIiIho0GBhYREDdrHQuDPabrdbZ8HCwloGbMYoaTBjJBozRiIxX93jK2QBiqKgpKQEiqIkuikdeQqA4RP123s3YlRaC3IyHACAjbvqEApzLqIVDOiMUVJgxkg0ZoxEYr7MYWFhAZqmobGxEZqmJbopnYuMWmgqpPK1mN56PQtfIIyvqxoS2DAya8BnjCyPGSPRmDESifkyh4WFBaiqisrKyoG7E0Hxd9pu73ir3baznA5lBQM+Y2R5zBiJxoyRSMyXOSwsqO9GzgBSnPrtHW/ixNGZxl0f72RhQURERDQYsLCgvrM52q7C3ViDYm0XhrrsAID1u+qgqhw2JCIiIkp2LCwsQJIk2O12SJKU6KZ0LWrbWWnHWzhh9BAAgLc5iO37DyeqVWSSJTJGlsaMkWjMGInEfJnDwsICZFnGmDFjBvY2ZzHbzr5lLOAGuM7CCiyRMbI0ZoxEY8ZIJObLHL46FqBpGurr6wf2TgRDxwBDx+q393yMGSNSjLveL61NUKPILEtkjCyNGSPRmDESifkyh4WFBaiqiurq6oG/E0FkOpQawvjmzzA8Xb+exbvbDsDbHExgw6g7lskYWRYzRqIxYyQS82UOCwuKn6h1FkrZWzhnSj4AIBBWsfqr6kS1ioiIiIj6AQsLip/RpwCKPkqBHW/hvGPyjLte+mJvghpFRERERP2BhYUFSJIEl8s18HcisLuAUSfrt717MCW1BqOH6de3+KjsIPY3tCSwcXQklskYWRYzRqIxYyQS82UOCwsLkGUZhYWF1tiJIHrb2bK3cO6xIwAAmga88mVVolpF3bBUxsiSmDESjRkjkZgvc/jqWICqqqitrbXGgqHi77TdLl2Dc4/JNz59eROnQw1UlsoYWRIzRqIxYyQS82UOCwsL0DQNtbW11tjiLKsE8BTqt3d/iHGZEo4ekQEA+KLSi/JaXwIbR12xVMbIkpgxEo0ZI5GYL3NYWFB8SVLbxfLCAWDXhzjvmBHG3S9v2peghhERERGRSCwsKP6i1llgx5v47jF5iKx1eumLvaz2iYiIiJIQCwsLkCQJHo/HOjsRFM0EZJt+e8ebyPOk4cSioQCAnQd82LKvIYGNo85YLmNkOcwYicaMkUjMlzksLCxAlmXk5eVZZyeCVA9QeKJ+u64MqNuJ845tmw71EhdxDziWyxhZDjNGojFjJBLzZQ5fHQtQVRVVVVXW2okgZjrUW5h7dC5SFL3Kf/mLfQirnA41kFgyY2QpzBiJxoyRSMyXOSwsLEDTNHi9XmutTYguLDb/B5lOO75Vkg0AqGnw49PyugQ1jDpjyYyRpTBjJBozRiIxX+awsCAxcicDWeP123s+BvZ8ivOOjbqmxRecDkVERESUTFhYkBiSBJzys7bPP/wjZk/MgdOuAABWba6GPxROUOOIiIiIKN5YWFiAJEnIysqy3k4Eky8E0vP029+sRFrDTsw5KhcA4G0O4r3ttQlsHEWzbMbIMpgxEo0ZI5GYL3NYWFiALMvIysqy3k4ENgdw0sLWTzTgo4dwbtR0KO4ONXBYNmNkGcwYicaMkUjMlzl8dSxAVVXs2bPHmjsRTLsSsKfrt7/4F07NDWOoyw4AeHNrDRr9ocS1jQyWzhhZAjNGojFjJBLzZQ4LCwvQNA0+n8+aOxGkeoDjr9JvhwNI2fBXzJusT49qCap4Y0t1AhtHEZbOGFkCM0aiMWMkEvNlDgsLEu+khYCcot9e/wS+d1SGcddLm/YlqFFEREREFE8sLEi8jHxgysX67RYvph54GQVD0gAAH+yoxYHD/gQ2joiIiIjigYWFBciyjNzcXGsvGDp5iXFT+vgxnD9Zv1heWNVw3+pvEtUqapUUGaMBjRkj0ZgxEon5MoevjgVIkoTMzExrb3GWPQEoOUu/3VCJHw/dhHSHDQDw3IZKvF96IIGNo6TIGA1ozBiJxoyRSMyXOSwsLEBVVezcudP6OxGc8nPjpuezR3Hr3AnG57f+dzN83CEqYZImYzRgMWMkGjNGIjFf5rCwsABN0xAIBKy/E8HIGcCI4/Xb+7fgkqHbcdKYoQCAykPNuP+NbQls3OCWNBmjAYsZI9GYMRKJ+TKHhQX1H0mKGbWQ1/0Jv7tgChw2PYYrPtqFjbsPJap1RERERNQHLCyof02YBwwdq98ufw+j/dvwizNLAACaBtzy/76EPxROYAOJiIiIqDdYWFiALMsoKChIjp0IZCVmhyh89Cf86JQiTCnwAAB27G/EI2/vSFDjBq+kyhgNSMwYicaMkUjMlzl8dSxAkiS43e7k2YngmEsB13D99tcvwVb1Ge6ZPwU2We/fo2vLsLWqIYENHHySLmM04DBjJBozRiIxX+awsLCAcDiM7du3IxxOkilCKanAidfptzUV+L/5mCjtxk9n6VOkQqqGW/7flwiFufNCf0m6jNGAw4yRaMwYicR8mcPCwiKSbnuzGYuBUafqt1vqgX+ch8WTQyjOdgMAvqz04u8flieufYNQ0mWMBhxmjERjxkgk5qt7LCwoMVJSgR/8GyiYrn/edBD2p7+HP3wnHZFRxgfe2I5dtb7EtZGIiIiITGNhQYnjSAcuex7IO0b/vLEGR625HEunpQIA/CEVP336M+yrb05gI4mIiIjIDEnjlT661dDQAI/HA6/Xi4yMjH5//shFWex2e3IuGmqqA1Z8F9i/BQCgZo7C91tux2f1LgDAMJcdD//gOMwYOyyRrUxqSZ8xSjhmjERjxkikwZyvnvwezBELi7DZbIlugjjOocCCF4Es/XoWcv1u/NuxHFMyWwAAB30B/PBvn+CJ93fyipcCJXXGaEBgxki0hGTMVwvs39r/z0v9jj/DusfCwgJUVUVpaWlyLxpyZwMLXgKGjAYA2L078YL7XswdkwIACKsafr1yK37+701oCoQS2NDkNCgyRgnFjJFoCclYzRbg4eOBR08C1v6u/56X+h1/hpnDwoIGjox84IpXAE8hAECp/QaPNi7Fo0dvgwT9G/nlL/bhgkc/wu6DXNRNREQJ5N0LPH0h0HxI/3ztcmDDk4ltE1GCsbCggSVzJHDFy0B6HgBAatiLs3csw6bc3+IMhz7U/E31YZzz0Ad455v9iWwpERENVi1evaho2Bt7fOUNwLbXE9MmogGAhQUNPEPHAD96HSieYxzy1H+Nv0m/wr9dD6BYqkRDSwhXrViPcx/+AI+8swM79jcmsMFERDRohALAsz80NhzBkNHA8T/Sb2sq8J+rgMqNCWseUSJxVygTBsKuUKqqQpblQbcTAXauBd64HajebBxSIePfoW/hwdD3cQCZAPTXZFy2G2cdlYs5R+Xi6BEZg++16oNBnTHqF8wYidYvGdM04L/XApuf0z9PGwpc8yYwpAj47zXAV/9PP+4cBly9Bhg2Vkw7qN8N5p9hPfk9eFAVFo888gjuu+8+VFdX45hjjsFDDz2E6dOnd/t1A6GwGKxbnAEAVFX/If7W3R2HnSOnaBJUSNCg/wtJRpPNA5+zEOHM0bBljUV6/jik5xZDGlqk70RFhkGfMRKOGSPR+iVjby4DPvi9ftuWClzxKlB4gv55yA/833xg1/v650OK9OLCPVxMW6hfDeafYSwsOvHss89iwYIFePzxx3HiiSfiD3/4A55//nls27YN2dnZR/zaRBcW4XAYpaWlKC4uhqIo/f78A0awGfj4MeD93wOBw316KJ+cjoNpRfBllkAbPhGOEZMxZPQxGJKVM+h+YADxz1gorGLzXi8+2VGFXaVbEAoGkFM0CcePHYFpo4cgIzUlDq0mK+HPMRJNeMbWPwGs/EXrJxJw8f8BE78be05zPfDkXGD/1/rn+ccBV74K2F3xbw/1q8H8M4yFRSdOPPFEnHDCCXj44YcB6NuGFRYWYsmSJfjlL395xK9lYTHA+GqBDx4EKtfr81k1DdBUBMNhHG4O4HBzAM3+ALKlQxgq9WztxX5tCHbbRqHRkYNUWUWqFIKj9cMuhZAC/QOyAlV2IKw4oCqpUBUHVNlu3IYtFZI9FbClQbanQk5Jg5ySCtnuhAINSqgRcrARSkD/Vw76IAcOQwr6EAoGEQiFEQyFEQirCIRUBENhhMIqgpqEkD0TmnMYFPcw2NOHw5k5HBlDc+AZlouUtAxAsQM2u/6v4gBkBeimWDIyVlQIxd8ANNfpO500H9IvYNh8SH8Muwuwu/V/U5zG7ZAmoWLnN6gu/wrN1aVIO7wLI1GFfByELOk/YlRNQoWWjVKtAPWuIqTkTUL22GMx8ehpGJKZqTdE0wA13Pq+hvXb0PTnknuYfU0DwgFADbU+ntqWl8hzQAMkGWgd5YIU+bf1w+YAlB4UQeEQ0FKvv2Zhv351eUeG/qF0s/+5qgJBH+A/rBfRKWltr3dP+97V44da9I9woPV2IObzcNAPyDYo7ix9iodzqP4axAF/jgkWCgCNNUBjDbSGfQj5DkJJGwI5Ixdw5+gfdmeiW9lz4RDQVAv4DgCN+/Xv21QPkJqp/5uWaWRUaMa+WQU8e1nrzw1g94nL8Mnw+Siv9WFXrQ+HW0IYnu5AdoYDY1Lqce6GK5DWUqO3a9yZUC79V/c/A2hAG8w/w1hYtBMIBOB0OvGf//wH559/vnH8iiuuQH19PV566aUjfj0LC+tpDoSxu86HvdU18O7djsCBnZDqy+Fq3IOhgb0YLVUhX6pLdDP7hQoJAdgQRAo0dF1gpCCENPj7sWVtgpoCRVIho+sfR37Y0SKlokVKa/1X/9AApGp+OLQWODQ/UrUWpGotcMAPBX3fbzyAFLTITjRLTjRLaWiR0+CXnfBLDqSqzXCpDXCrh+FWD8Oldb0NcjNS0SS70CQ50SS7IGthpKrNSNOa4NSa4ERLl1/bAntrv/XnD0l6sSNFvV6S8aNcQwqCSFEDsCMAu+aHXQvAjmCv+t+INHiRjgYpA4fldISlFMgSIEmyXodJEmRJgiRJ+uet7ZKgAZrW+p62fgRbkCaHkKL5kaLq7UrRIv8GEIaCkJSCoJSCUOQDNuO2BgmaJEOF3Dr1UYYmSVAhQ9JUKAjBpoUgayEoWgiKFoYC/V8tMlVS0vcs0dMmtX5PSJFJlK2PDONzCRrCkBGCDUHYENQfUf+e0hSENRl2KQS7FIYdQdgRgh1B/T1ACBKAkGRDGDaEJb0vYUlBGPq/gAZZUyEjpP+rhSFD/1eCFvUa2Nq9JjY4wk3whA9iiFoHj9b9KG4jnKiTh+CQPBQ+OQOQFWhGIa3o/8oKJEmGghBStABSVD9sWgApagA2Tc+TogWhQjH6o0pKa7/0fmqSrPdDC0NG679aCErr7cjX6h8pCEk2qJLeJwmAK1SP9HAd0kP1cKkNR/y5AAB+yYEmyYUm2Y0Wzab/gUeOPHYKwnIKVCkFqmyD/m2iIfKQ+q9AmvEMEgANUtvItaSnYOrhtbBr+s/Hx0Pn4HehS4/YpvFSBZ63L0OG1AwA2KiNxwE5C4osQ5EVyIoCRZagKAoUWY7644/+r9buX+Nurf1PcT3Fsqb/EUbWVEgIt/6rGj8XNCmSZv25tKi+tT0X2l6Xdv2RWl+j6J85xrlS63e8htbH1s+StRBS1EDU97s/KlNBhCUFIaQgKNlav1tsCEqR7xyl7Q88shz7Bx9ZMb5X9O/vUFvGWjMXORb5/tdv6+cAQBiKkb2QlBLzeRj694UqKfrPCUlBWFKgQUYgpMKWkgIp+l3o8Ic7/Wdh+3dKAmBT/bBrzUgJt8CutcCuRj6aoWghBCU7QpIdAcmOoOzQ/4UdAUm/nTHrZ5gwfXb7uAnXk9+DB0X5XFtbi3A4jJycnJjjOTk5+Oabbzqc7/f74fe3/YLV0NAAQP8FPxwOA2j9n6ksQ1XVmKtBd3U8stinq+ORx40+DugjK5H7wuFwzPFoiqIYC4vat6Wr42bbLqJPZo73pU92BSge7kJJ9ljIxxbHtDGsaqhp8GNj7QEcrvgCavVWpNZvw5DGHRgRKEeGif9BW4kMDakIIrWXv1j2VpPsRlP6aDiyx8FmS4G/aivSGsrgUJs7nJsihTt5hFgOBODQAvBoDSKa2yU7grCrXmTA26fHSUML0tQWDMNBoPvuxkhFAKlaANC8iEOt1CNuNMONZozQ9ve43T0lQ0WKFkSa1f7cZZH2utEEt9qEkWrna9WsyqH54dD8GKK2/rFIYE5fCp+Me0IXd3veNm0kfhK8AStS7oFDCmGatA3QtultCwP9/OOY4qnj/8L6zWcHLwCAfv99rydjEIOisOip5cuXY9myZR2Ol5WVwe12AwA8Hg/y8vJQU1MDr7ftF46srCxkZWVh79698Pna/nqZm5uLzMxM7Nq1C4FAwDheUFAAt9uNsrKymDAUFRXBZrOhtLTUOLZz504UFxcjFAqhvLzcOC7LMkpKSuDz+VBZWWkct9vtGDNmDLxeL6qrq43jLpcLhYWFqKurQ21trXE8EX0CkLA+TRs/GnucCnyF443jak4ONKUFZVu/QINfQ5NmQ1NIQeqQHIRsTmyv3A9fAFDVMBQ1gKxMF2xaCN4D+6BoQdjCfiiaHx5nKtRgM5obDkIKB6CE/VC0ANJSZATDYTQEU9AspaFJSkOL5ITqHIrDoRTUBxQ47DZ40mwYlp6G0XnDYVP9kINN8DgU2BUVUigARQvgwN5yNNdXAy1eyP56OEINcGh+yGpQ/wujFtL/aiqF9b8FqUHjr04AjL/IGQUXFHjhxmE5HQ1IxyHNBS/caIAbXrihKAqcaIYjrP91PRV+ONECl6z/Ndyfmg37sFHIyhuD4SNLUDRpGgJeL/ZG3qdjAJczDYXpQNXWj1G5bQO0up1I91VAifz1SpUQ0iT9L8SajJCm/10oTfIjDX640II0yQ8X/HBKsaMrLVoKmuBAM1LRAjuaNAda4EAI+l+dwq1/g1Y1/W/S4dbdtiN/X2v9+3frh/43cYcUhBstcKEZLqkFbjTDIcVe+T2sSaiHG/WaG164UK+lwws3AlJK6y/lTUhHk/6v1KzfllqgahJ8SIUPafBBz0KT5EQTUuGHHXYtgDS0wIkWpGktSEMznGiBGy1wSEGoWtvfDrV2f3UMwqaPcsAOv2aHH3b4pRT4YUcAKQhK+t/UQ1IKNFsqgkhBAApUKQU2hJChHUa61oh01YsMtQEZOAwPGrv9y7FZfi0FLUjR26i1thMpsEFt/Wu/PvWwbQRAn4IYmVbXnZAmIwQFQdgQgoJQ63sdeX+lmH/1kYnWv++2jWxE3Vagtk6DDHfbDr+WAj9sCCAFgdb/xabo4xXdPkZYk4y8hqC0fq3e/66e06/ZcABDcFAagjopE4fkIThsy4LfMRQpQS8yggeRqdZhqFqPYTiELByCsw8jk82aHUHY9CIQoQ7fD90Jaorew27ey2bNjlrNg4PIwAHNY9wOQ0EGfMiQfMhAEzwx//r0rJj4Q0VvvBeejPvsi3BctgtHjRyO/HQbXKoPIzLsSHfI8IVl2DOGo7y6DjurD6Ku6WT89eD1+In3QX0KLaG59fs9ABsUqMYI35Ey3lsBTdFH91q/n8JQEISijxxEfS/aEO5xjuOlWbO3/n/LgYBmg10K6X9IQgCO1v9/R6ttaAKAfv99z+k0P42SU6E6mQrV2YhF5I2JDAH151/3NU1DU1MTnE6nMRWKIxbsUzz7ZCZjieiTprX+8VHTp9Xo027055Q0DWqgEZqqGmsw+vo+RZ5blhWomqa/NgBUrXWCgiRDDbUg3NwALeCD5MiAzelBSkoKJE2LGRHvqk9Gx4z1HbHvR3fvU1jVP2Ql0vbYx5bl1qlJmgZZbmtQn98nNQwpcBgyNKhqGGo4jKCqIRQKI6RqCGtAIBjSC5zWtSqyrE9bUFUNqqahJajCmZ6JlBS73jZNhdz6ngISZFkvltSw2lY0aRokqXVCVWTNTet6GZssQQuHoIVDkGQFks0OW4odNpsNgKa/Bq1TtGRZNvrUvq+QZITDYaiaBlWL5EDvR1hVoUiATZaMqSuSpkINtkALBfS1QEoK5JRUSIodoXAYgbCGQEhFIKwiMrWlQ/agQQv7oWkSJMVmvF42RZ8epbVOeZFa/6NAgxZqgRYMQFIDQDiAlDQ3UtOz9NemJz8jWhqg+g4hpIYRDoWgairCYQ3BoB+hUOsx2QYpJQ2wpelTiWx6/9D6GEafNA3QwpDUoD59Sw1CDYUAxQZNtkGSbJBT7Pq0K60tj5KmQtKCQDgINehv7VMIkqRBcg0HUtKMaXZy62tvUxRomhozQiTLbX0Nh1X4mprgsKfo70soiFCwGWrADzUcgBoKQlFkKLIMTdWM11aWJH06EoCwqn+faa0/cyBJ+vcHZAwdUaw/dk+/nwKHIfsb9PejdX1XOBRCS0iFPxhGcyCg/xyLnA9AkiVoqqq/vK2TkOTWuYZqa9OgfzdAkRX9+8z43lMA2QZZsQGQoGqR59W/QgKgaar+oar6NMbW1xKt9xntaM2lpunfw1Lr96r+3sjQtHDra9k69VECZEnSj0spQEoqNFsqpJRUKClORN48qfU8myJDUWT9e1ULQ1IDUNQgZC2EsKoiEAgirIYRCoUQDqtQ1TDC4RDCmgTIKdBkRf/+U+yAbNOnF0o2Y12aositP3+iv//05428DrIktU7b1P8AoIZCUNUgJFWFpgb1rEKFGgqguakJDoej9eeD/hqpqmZ8n2maCqn157qmqVAjU+00DRIkSPY0hOVUaHYnpBQnIMl6WyT9/TZ+XkkSFEWfUohgE6SwH0q4BWmebKS5Pf3+e0RjYyMyMzO5xiLaiSeeiOnTp+Ohhx4CoP9AHDlyJBYvXszF2zToMWMkGjNGojFjJNJgzhfXWHTihhtuwBVXXIHjjz8e06dPxx/+8Af4fD5cddVViW4aEREREZHlDZrC4uKLL8aBAwdw5513orq6Gsceeyxef/31Dgu6iYiIiIio5wZNYQEAixcvxuLFixPdjB6TJGlQXumR+g8zRqIxYyQaM0YiMV/mDJo1Fn2R6DUWRET/v727j6my/OM4/jk8HR5ERJgHfCCxnODjVNQIt1a6lFzLh2pu5LD+YCQY6iodZdrMUFu1aYXlyv6QtGhZ6rJmWGw6EcSBOhHdctOFSKwQxMdxrt8frbPOz9Rjd7e3B96v7Wxw3ZfyPdtnk8/uc18CAOCEO/k9OOSWV3FPMMaora3tjs4RBu4EGYPdyBjsRsZgJ/IVGIpFEPB6vWpubr7huELgv0LGYDcyBruRMdiJfAWGYgEAAADAMooFAAAAAMsoFkHA5XIpJiaGkwhgGzIGu5Ex2I2MwU7kKzCcChUAToUCAABAT8SpUN2M1+tVa2srDwzBNmQMdiNjsBsZg53IV2AoFkHAGKPW1laOOINtyBjsRsZgNzIGO5GvwFAsAAAAAFhGsQAAAABgGcUiCLhcLsXFxXESAWxDxmA3Mga7kTHYiXwFhlOhAsCpUAAAAOiJOBWqm/F6vTp37hwnEcA2ZAx2I2OwGxmDnchXYCgWQcAYowsXLnASAWxDxmA3Mga7kTHYiXwFhmIBAAAAwLIwpwcIBn+10/b2dkd+fldXly5evKj29naFhoY6MgO6NzIGu5Ex2I2MwU49OV9//f4byN0aikUAOjo6JEmDBg1yeBIAAADg7uvo6FBcXNwt93AqVAC8Xq+ampoUGxvryDFj7e3tGjRokM6ePcupVLAFGYPdyBjsRsZgp56cL2OMOjo61L9/f4WE3PopCu5YBCAkJEQDBw50egz17t27x4UZdxcZg93IGOxGxmCnnpqv292p+AsPbwMAAACwjGIBAAAAwDKKRRBwu91asWKF3G6306OgmyJjsBsZg93IGOxEvgLDw9sAAAAALOOOBQAAAADLKBYAAAAALKNYAAAAALCMYhEEPvjgAw0ePFiRkZGaNGmSqqurnR4JQaikpEQTJkxQbGys+vXrp5kzZ6qxsdFvz5UrV1RQUKCEhAT16tVLc+bM0fnz5x2aGMFuzZo1crlcWrRokW+NjMGqX3/9Vc8++6wSEhIUFRWlUaNG6dChQ77rxhi9/vrrSk5OVlRUlKZOnapTp045ODGCSVdXl5YvX67U1FRFRUXp/vvv16pVq/T3R5LJ2M1RLO5xX3zxhZYsWaIVK1bo8OHDGjNmjKZNm6aWlhanR0OQqaysVEFBgaqqqrRnzx5dv35djz32mDo7O317Fi9erJ07d6q8vFyVlZVqamrS7NmzHZwawaqmpkYfffSRRo8e7bdOxmDFH3/8oaysLIWHh2v37t06fvy43nnnHcXHx/v2rFu3TuvXr9fGjRt18OBBxcTEaNq0abpy5YqDkyNYrF27VqWlpXr//ffV0NCgtWvXat26ddqwYYNvDxm7BYN72sSJE01BQYHv+66uLtO/f39TUlLi4FToDlpaWowkU1lZaYwxpq2tzYSHh5vy8nLfnoaGBiPJHDhwwKkxEYQ6OjrM0KFDzZ49e8zDDz9sioqKjDFkDNYtXbrUTJ48+abXvV6vSUpKMm+//bZvra2tzbjdbrN169a7MSKC3IwZM8zzzz/vtzZ79myTk5NjjCFjt8Mdi3vYtWvXVFtbq6lTp/rWQkJCNHXqVB04cMDBydAdXLhwQZLUt29fSVJtba2uX7/ul7e0tDSlpKSQN9yRgoICzZgxwy9LEhmDdTt27FBGRoaefvpp9evXT2PHjtWmTZt810+fPq3m5ma/jMXFxWnSpElkDAF56KGHVFFRoZMnT0qS6uvrtW/fPmVnZ0siY7cT5vQAuLnW1lZ1dXXJ4/H4rXs8Hp04ccKhqdAdeL1eLVq0SFlZWRo5cqQkqbm5WREREerTp4/fXo/Ho+bmZgemRDDatm2bDh8+rJqamhuukTFY9csvv6i0tFRLlixRcXGxampq9OKLLyoiIkK5ubm+HP3Tv5tkDIFYtmyZ2tvblZaWptDQUHV1dWn16tXKycmRJDJ2GxQLoAcqKCjQsWPHtG/fPqdHQTdy9uxZFRUVac+ePYqMjHR6HHRDXq9XGRkZeuuttyRJY8eO1bFjx7Rx40bl5uY6PB26gy+//FJlZWX6/PPPNWLECNXV1WnRokXq378/GQsAH4W6hyUmJio0NPSGE1POnz+vpKQkh6ZCsCssLNSuXbv0008/aeDAgb71pKQkXbt2TW1tbX77yRsCVVtbq5aWFo0bN05hYWEKCwtTZWWl1q9fr7CwMHk8HjIGS5KTkzV8+HC/tfT0dJ05c0aSfDni3038Wy+//LKWLVumuXPnatSoUZo3b54WL16skpISSWTsdigW97CIiAiNHz9eFRUVvjWv16uKigplZmY6OBmCkTFGhYWF2r59u/bu3avU1FS/6+PHj1d4eLhf3hobG3XmzBnyhoBMmTJFR48eVV1dne+VkZGhnJwc39dkDFZkZWXdcEz2yZMndd9990mSUlNTlZSU5Jex9vZ2HTx4kIwhIJcuXVJIiP+vx6GhofJ6vZLI2G05/fQ4bm3btm3G7Xabzz77zBw/ftzk5eWZPn36mObmZqdHQ5B54YUXTFxcnPn555/NuXPnfK9Lly759uTn55uUlBSzd+9ec+jQIZOZmWkyMzMdnBrB7u+nQhlDxmBNdXW1CQsLM6tXrzanTp0yZWVlJjo62mzZssW3Z82aNaZPnz7m22+/NUeOHDFPPvmkSU1NNZcvX3ZwcgSL3NxcM2DAALNr1y5z+vRp8/XXX5vExETzyiuv+PaQsZujWASBDRs2mJSUFBMREWEmTpxoqqqqnB4JQUjSP742b97s23P58mWzYMECEx8fb6Kjo82sWbPMuXPnnBsaQe//iwUZg1U7d+40I0eONG6326SlpZmPP/7Y77rX6zXLly83Ho/HuN1uM2XKFNPY2OjQtAg27e3tpqioyKSkpJjIyEgzZMgQ8+qrr5qrV6/69pCxm3MZ87f/ShAAAAAA/gWesQAAAABgGcUCAAAAgGUUCwAAAACWUSwAAAAAWEaxAAAAAGAZxQIAAACAZRQLAAAAAJZRLAAAAABYRrEAAHRLLpdL33zzjdNjAECPQbEAAPzn5s+fL5fLdcNr+vTpTo8GALBJmNMDAAC6p+nTp2vz5s1+a26326FpAAB2444FAMAWbrdbSUlJfq/4+HhJf35MqbS0VNnZ2YqKitKQIUP01Vdf+f35o0eP6tFHH1VUVJQSEhKUl5enixcv+u359NNPNWLECLndbiUnJ6uwsNDvemtrq2bNmqXo6GgNHTpUO3bssPdNA0APRrEAADhi+fLlmjNnjurr65WTk6O5c+eqoaFBktTZ2alp06YpPj5eNTU1Ki8v148//uhXHEpLS1VQUKC8vDwdPXpUO3bs0AMPPOD3M9544w0988wzOnLkiB5//HHl5OTo999/v6vvEwB6Cpcxxjg9BACge5k/f762bNmiyMhIv/Xi4mIVFxfL5XIpPz9fpaWlvmsPPvigxo0bpw8//FCbNm3S0qVLdfbsWcXExEiSvvvuOz3xxBNqamqSx+PRgAED9Nxzz+nNN9/8xxlcLpdee+01rVq1StKfZaVXr17avXs3z3oAgA14xgIAYItHHnnErzhIUt++fX1fZ2Zm+l3LzMxUXV2dJKmhoUFjxozxlQpJysrKktfrVWNjo1wul5qamjRlypRbzjB69Gjf1zExMerdu7daWlr+7VsCANwCxQIAYIuYmJgbPpr0X4mKigpoX3h4uN/3LpdLXq/XjpEAoMfjGQsAgCOqqqpu+D49PV2SlJ6ervr6enV2dvqu79+/XyEhIRo2bJhiY2M1ePBgVVRU3NWZAQA3xx0LAIAtrl69qubmZr+1sLAwJSYmSpLKy8uVkZGhyZMnq6ysTNXV1frkk08kSTk5OVqxYoVyc3O1cuVK/fbbb1q4cKHmzZsnj8cjSVq5cqXy8/PVr18/ZWdnq6OjQ/v379fChQvv7hsFAEiiWAAAbPL9998rOTnZb23YsGE6ceKEpD9PbNq2bZsWLFig5ORkbd26VcOHD5ckRUdH64cfflBRUZEmTJig6OhozZkzR++++67v78rNzdWVK1f03nvv6aWXXlJiYqKeeuqpu/cGAQB+OBUKAHDXuVwubd++XTNnznR6FADAf4RnLAAAAABYRrEAAAAAYBnPWAAA7jo+hQsA3Q93LAAAAABYRrEAAAAAYBnFAgAAAIBlFAsAAAAAllEsAAAAAFhGsQAAAABgGcUCAAAAgGUUCwAAAACWUSwAAAAAWPY/Mcq9lw64hmMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
