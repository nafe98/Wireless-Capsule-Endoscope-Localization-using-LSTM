{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_Reg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.362170</td>\n",
       "      <td>92.059179</td>\n",
       "      <td>81.639492</td>\n",
       "      <td>83.046279</td>\n",
       "      <td>87.557608</td>\n",
       "      <td>88.829065</td>\n",
       "      <td>86.304978</td>\n",
       "      <td>91.903081</td>\n",
       "      <td>90.974425</td>\n",
       "      <td>87.964761</td>\n",
       "      <td>...</td>\n",
       "      <td>80.716972</td>\n",
       "      <td>82.980276</td>\n",
       "      <td>91.631634</td>\n",
       "      <td>83.478056</td>\n",
       "      <td>87.786838</td>\n",
       "      <td>89.127069</td>\n",
       "      <td>80.956526</td>\n",
       "      <td>92.613983</td>\n",
       "      <td>77.410061</td>\n",
       "      <td>81.618024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.314653</td>\n",
       "      <td>91.868510</td>\n",
       "      <td>81.753510</td>\n",
       "      <td>83.102770</td>\n",
       "      <td>87.667622</td>\n",
       "      <td>89.014976</td>\n",
       "      <td>86.405655</td>\n",
       "      <td>91.761364</td>\n",
       "      <td>90.912489</td>\n",
       "      <td>88.054400</td>\n",
       "      <td>...</td>\n",
       "      <td>80.811359</td>\n",
       "      <td>82.990759</td>\n",
       "      <td>91.367214</td>\n",
       "      <td>83.430809</td>\n",
       "      <td>87.885892</td>\n",
       "      <td>88.975725</td>\n",
       "      <td>81.054012</td>\n",
       "      <td>92.393965</td>\n",
       "      <td>77.261457</td>\n",
       "      <td>81.568429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.265068</td>\n",
       "      <td>91.681240</td>\n",
       "      <td>81.863257</td>\n",
       "      <td>83.160659</td>\n",
       "      <td>87.776139</td>\n",
       "      <td>89.204368</td>\n",
       "      <td>86.502722</td>\n",
       "      <td>91.621445</td>\n",
       "      <td>90.850914</td>\n",
       "      <td>88.142340</td>\n",
       "      <td>...</td>\n",
       "      <td>80.909673</td>\n",
       "      <td>82.999959</td>\n",
       "      <td>91.107171</td>\n",
       "      <td>83.381318</td>\n",
       "      <td>87.981265</td>\n",
       "      <td>88.825590</td>\n",
       "      <td>81.151156</td>\n",
       "      <td>92.174215</td>\n",
       "      <td>77.116598</td>\n",
       "      <td>81.522525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.213900</td>\n",
       "      <td>91.497346</td>\n",
       "      <td>81.968488</td>\n",
       "      <td>83.220587</td>\n",
       "      <td>87.883528</td>\n",
       "      <td>89.397035</td>\n",
       "      <td>86.596130</td>\n",
       "      <td>91.482991</td>\n",
       "      <td>90.789162</td>\n",
       "      <td>88.228029</td>\n",
       "      <td>...</td>\n",
       "      <td>81.011722</td>\n",
       "      <td>83.008250</td>\n",
       "      <td>90.851560</td>\n",
       "      <td>83.329554</td>\n",
       "      <td>88.072869</td>\n",
       "      <td>88.677138</td>\n",
       "      <td>81.247568</td>\n",
       "      <td>91.955906</td>\n",
       "      <td>76.975967</td>\n",
       "      <td>81.480128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.161546</td>\n",
       "      <td>91.316921</td>\n",
       "      <td>82.068671</td>\n",
       "      <td>83.283040</td>\n",
       "      <td>87.990020</td>\n",
       "      <td>89.592487</td>\n",
       "      <td>86.686142</td>\n",
       "      <td>91.345397</td>\n",
       "      <td>90.726820</td>\n",
       "      <td>88.311044</td>\n",
       "      <td>...</td>\n",
       "      <td>81.117124</td>\n",
       "      <td>83.015760</td>\n",
       "      <td>90.600178</td>\n",
       "      <td>83.275815</td>\n",
       "      <td>88.160725</td>\n",
       "      <td>88.530710</td>\n",
       "      <td>81.342925</td>\n",
       "      <td>91.740266</td>\n",
       "      <td>76.839861</td>\n",
       "      <td>81.440740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>91.606026</td>\n",
       "      <td>88.069788</td>\n",
       "      <td>79.531699</td>\n",
       "      <td>85.419493</td>\n",
       "      <td>91.976891</td>\n",
       "      <td>92.243208</td>\n",
       "      <td>87.175784</td>\n",
       "      <td>88.590557</td>\n",
       "      <td>91.681913</td>\n",
       "      <td>90.119286</td>\n",
       "      <td>...</td>\n",
       "      <td>84.995612</td>\n",
       "      <td>87.574274</td>\n",
       "      <td>86.411497</td>\n",
       "      <td>79.268845</td>\n",
       "      <td>90.031115</td>\n",
       "      <td>86.998545</td>\n",
       "      <td>84.367165</td>\n",
       "      <td>86.580171</td>\n",
       "      <td>78.319799</td>\n",
       "      <td>79.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>91.746546</td>\n",
       "      <td>88.072322</td>\n",
       "      <td>79.441336</td>\n",
       "      <td>85.618475</td>\n",
       "      <td>91.938881</td>\n",
       "      <td>92.245196</td>\n",
       "      <td>87.057803</td>\n",
       "      <td>88.739174</td>\n",
       "      <td>91.794750</td>\n",
       "      <td>90.221004</td>\n",
       "      <td>...</td>\n",
       "      <td>85.070026</td>\n",
       "      <td>87.674484</td>\n",
       "      <td>86.261152</td>\n",
       "      <td>79.277755</td>\n",
       "      <td>89.975204</td>\n",
       "      <td>86.894393</td>\n",
       "      <td>84.210259</td>\n",
       "      <td>86.641084</td>\n",
       "      <td>78.327737</td>\n",
       "      <td>79.590486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>91.887422</td>\n",
       "      <td>88.071942</td>\n",
       "      <td>79.354928</td>\n",
       "      <td>85.820510</td>\n",
       "      <td>91.904210</td>\n",
       "      <td>92.245867</td>\n",
       "      <td>86.937141</td>\n",
       "      <td>88.888750</td>\n",
       "      <td>91.904207</td>\n",
       "      <td>90.319187</td>\n",
       "      <td>...</td>\n",
       "      <td>85.149368</td>\n",
       "      <td>87.775550</td>\n",
       "      <td>86.108531</td>\n",
       "      <td>79.287771</td>\n",
       "      <td>89.921318</td>\n",
       "      <td>86.790128</td>\n",
       "      <td>84.051367</td>\n",
       "      <td>86.698728</td>\n",
       "      <td>78.334857</td>\n",
       "      <td>79.743283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.028156</td>\n",
       "      <td>88.068195</td>\n",
       "      <td>79.272625</td>\n",
       "      <td>86.026023</td>\n",
       "      <td>91.872413</td>\n",
       "      <td>92.245207</td>\n",
       "      <td>86.814066</td>\n",
       "      <td>89.039560</td>\n",
       "      <td>92.010556</td>\n",
       "      <td>90.413832</td>\n",
       "      <td>...</td>\n",
       "      <td>85.234268</td>\n",
       "      <td>87.876485</td>\n",
       "      <td>85.954382</td>\n",
       "      <td>79.298734</td>\n",
       "      <td>89.869081</td>\n",
       "      <td>86.686079</td>\n",
       "      <td>83.891148</td>\n",
       "      <td>86.753257</td>\n",
       "      <td>78.341213</td>\n",
       "      <td>79.895198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.168162</td>\n",
       "      <td>88.060630</td>\n",
       "      <td>79.194408</td>\n",
       "      <td>86.235405</td>\n",
       "      <td>91.843085</td>\n",
       "      <td>92.243320</td>\n",
       "      <td>86.689144</td>\n",
       "      <td>89.192029</td>\n",
       "      <td>92.114117</td>\n",
       "      <td>90.504833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.324974</td>\n",
       "      <td>87.976567</td>\n",
       "      <td>85.799219</td>\n",
       "      <td>79.310773</td>\n",
       "      <td>89.818353</td>\n",
       "      <td>86.582394</td>\n",
       "      <td>83.730161</td>\n",
       "      <td>86.805090</td>\n",
       "      <td>78.347071</td>\n",
       "      <td>80.045589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     87.362170  92.059179  81.639492  83.046279  87.557608  88.829065   \n",
       "1     87.314653  91.868510  81.753510  83.102770  87.667622  89.014976   \n",
       "2     87.265068  91.681240  81.863257  83.160659  87.776139  89.204368   \n",
       "3     87.213900  91.497346  81.968488  83.220587  87.883528  89.397035   \n",
       "4     87.161546  91.316921  82.068671  83.283040  87.990020  89.592487   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  91.606026  88.069788  79.531699  85.419493  91.976891  92.243208   \n",
       "2439  91.746546  88.072322  79.441336  85.618475  91.938881  92.245196   \n",
       "2440  91.887422  88.071942  79.354928  85.820510  91.904210  92.245867   \n",
       "2441  92.028156  88.068195  79.272625  86.026023  91.872413  92.245207   \n",
       "2442  92.168162  88.060630  79.194408  86.235405  91.843085  92.243320   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     86.304978  91.903081  90.974425  87.964761  ...  80.716972  82.980276   \n",
       "1     86.405655  91.761364  90.912489  88.054400  ...  80.811359  82.990759   \n",
       "2     86.502722  91.621445  90.850914  88.142340  ...  80.909673  82.999959   \n",
       "3     86.596130  91.482991  90.789162  88.228029  ...  81.011722  83.008250   \n",
       "4     86.686142  91.345397  90.726820  88.311044  ...  81.117124  83.015760   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.175784  88.590557  91.681913  90.119286  ...  84.995612  87.574274   \n",
       "2439  87.057803  88.739174  91.794750  90.221004  ...  85.070026  87.674484   \n",
       "2440  86.937141  88.888750  91.904207  90.319187  ...  85.149368  87.775550   \n",
       "2441  86.814066  89.039560  92.010556  90.413832  ...  85.234268  87.876485   \n",
       "2442  86.689144  89.192029  92.114117  90.504833  ...  85.324974  87.976567   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     91.631634  83.478056  87.786838  89.127069  80.956526  92.613983   \n",
       "1     91.367214  83.430809  87.885892  88.975725  81.054012  92.393965   \n",
       "2     91.107171  83.381318  87.981265  88.825590  81.151156  92.174215   \n",
       "3     90.851560  83.329554  88.072869  88.677138  81.247568  91.955906   \n",
       "4     90.600178  83.275815  88.160725  88.530710  81.342925  91.740266   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  86.411497  79.268845  90.031115  86.998545  84.367165  86.580171   \n",
       "2439  86.261152  79.277755  89.975204  86.894393  84.210259  86.641084   \n",
       "2440  86.108531  79.287771  89.921318  86.790128  84.051367  86.698728   \n",
       "2441  85.954382  79.298734  89.869081  86.686079  83.891148  86.753257   \n",
       "2442  85.799219  79.310773  89.818353  86.582394  83.730161  86.805090   \n",
       "\n",
       "             46         47  \n",
       "0     77.410061  81.618024  \n",
       "1     77.261457  81.568429  \n",
       "2     77.116598  81.522525  \n",
       "3     76.975967  81.480128  \n",
       "4     76.839861  81.440740  \n",
       "...         ...        ...  \n",
       "2438  78.319799  79.437264  \n",
       "2439  78.327737  79.590486  \n",
       "2440  78.334857  79.743283  \n",
       "2441  78.341213  79.895198  \n",
       "2442  78.347071  80.045589  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.362170</td>\n",
       "      <td>92.059179</td>\n",
       "      <td>81.639492</td>\n",
       "      <td>83.046279</td>\n",
       "      <td>87.557608</td>\n",
       "      <td>88.829065</td>\n",
       "      <td>86.304978</td>\n",
       "      <td>91.903081</td>\n",
       "      <td>90.974425</td>\n",
       "      <td>87.964761</td>\n",
       "      <td>...</td>\n",
       "      <td>80.716972</td>\n",
       "      <td>82.980276</td>\n",
       "      <td>91.631634</td>\n",
       "      <td>83.478056</td>\n",
       "      <td>87.786838</td>\n",
       "      <td>89.127069</td>\n",
       "      <td>80.956526</td>\n",
       "      <td>92.613983</td>\n",
       "      <td>77.410061</td>\n",
       "      <td>81.618024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.314653</td>\n",
       "      <td>91.868510</td>\n",
       "      <td>81.753510</td>\n",
       "      <td>83.102770</td>\n",
       "      <td>87.667622</td>\n",
       "      <td>89.014976</td>\n",
       "      <td>86.405655</td>\n",
       "      <td>91.761364</td>\n",
       "      <td>90.912489</td>\n",
       "      <td>88.054400</td>\n",
       "      <td>...</td>\n",
       "      <td>80.811359</td>\n",
       "      <td>82.990759</td>\n",
       "      <td>91.367214</td>\n",
       "      <td>83.430809</td>\n",
       "      <td>87.885892</td>\n",
       "      <td>88.975725</td>\n",
       "      <td>81.054012</td>\n",
       "      <td>92.393965</td>\n",
       "      <td>77.261457</td>\n",
       "      <td>81.568429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.265068</td>\n",
       "      <td>91.681240</td>\n",
       "      <td>81.863257</td>\n",
       "      <td>83.160659</td>\n",
       "      <td>87.776139</td>\n",
       "      <td>89.204368</td>\n",
       "      <td>86.502722</td>\n",
       "      <td>91.621445</td>\n",
       "      <td>90.850914</td>\n",
       "      <td>88.142340</td>\n",
       "      <td>...</td>\n",
       "      <td>80.909673</td>\n",
       "      <td>82.999959</td>\n",
       "      <td>91.107171</td>\n",
       "      <td>83.381318</td>\n",
       "      <td>87.981265</td>\n",
       "      <td>88.825590</td>\n",
       "      <td>81.151156</td>\n",
       "      <td>92.174215</td>\n",
       "      <td>77.116598</td>\n",
       "      <td>81.522525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.213900</td>\n",
       "      <td>91.497346</td>\n",
       "      <td>81.968488</td>\n",
       "      <td>83.220587</td>\n",
       "      <td>87.883528</td>\n",
       "      <td>89.397035</td>\n",
       "      <td>86.596130</td>\n",
       "      <td>91.482991</td>\n",
       "      <td>90.789162</td>\n",
       "      <td>88.228029</td>\n",
       "      <td>...</td>\n",
       "      <td>81.011722</td>\n",
       "      <td>83.008250</td>\n",
       "      <td>90.851560</td>\n",
       "      <td>83.329554</td>\n",
       "      <td>88.072869</td>\n",
       "      <td>88.677138</td>\n",
       "      <td>81.247568</td>\n",
       "      <td>91.955906</td>\n",
       "      <td>76.975967</td>\n",
       "      <td>81.480128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.161546</td>\n",
       "      <td>91.316921</td>\n",
       "      <td>82.068671</td>\n",
       "      <td>83.283040</td>\n",
       "      <td>87.990020</td>\n",
       "      <td>89.592487</td>\n",
       "      <td>86.686142</td>\n",
       "      <td>91.345397</td>\n",
       "      <td>90.726820</td>\n",
       "      <td>88.311044</td>\n",
       "      <td>...</td>\n",
       "      <td>81.117124</td>\n",
       "      <td>83.015760</td>\n",
       "      <td>90.600178</td>\n",
       "      <td>83.275815</td>\n",
       "      <td>88.160725</td>\n",
       "      <td>88.530710</td>\n",
       "      <td>81.342925</td>\n",
       "      <td>91.740266</td>\n",
       "      <td>76.839861</td>\n",
       "      <td>81.440740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>91.606026</td>\n",
       "      <td>88.069788</td>\n",
       "      <td>79.531699</td>\n",
       "      <td>85.419493</td>\n",
       "      <td>91.976891</td>\n",
       "      <td>92.243208</td>\n",
       "      <td>87.175784</td>\n",
       "      <td>88.590557</td>\n",
       "      <td>91.681913</td>\n",
       "      <td>90.119286</td>\n",
       "      <td>...</td>\n",
       "      <td>84.995612</td>\n",
       "      <td>87.574274</td>\n",
       "      <td>86.411497</td>\n",
       "      <td>79.268845</td>\n",
       "      <td>90.031115</td>\n",
       "      <td>86.998545</td>\n",
       "      <td>84.367165</td>\n",
       "      <td>86.580171</td>\n",
       "      <td>78.319799</td>\n",
       "      <td>79.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>91.746546</td>\n",
       "      <td>88.072322</td>\n",
       "      <td>79.441336</td>\n",
       "      <td>85.618475</td>\n",
       "      <td>91.938881</td>\n",
       "      <td>92.245196</td>\n",
       "      <td>87.057803</td>\n",
       "      <td>88.739174</td>\n",
       "      <td>91.794750</td>\n",
       "      <td>90.221004</td>\n",
       "      <td>...</td>\n",
       "      <td>85.070026</td>\n",
       "      <td>87.674484</td>\n",
       "      <td>86.261152</td>\n",
       "      <td>79.277755</td>\n",
       "      <td>89.975204</td>\n",
       "      <td>86.894393</td>\n",
       "      <td>84.210259</td>\n",
       "      <td>86.641084</td>\n",
       "      <td>78.327737</td>\n",
       "      <td>79.590486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>91.887422</td>\n",
       "      <td>88.071942</td>\n",
       "      <td>79.354928</td>\n",
       "      <td>85.820510</td>\n",
       "      <td>91.904210</td>\n",
       "      <td>92.245867</td>\n",
       "      <td>86.937141</td>\n",
       "      <td>88.888750</td>\n",
       "      <td>91.904207</td>\n",
       "      <td>90.319187</td>\n",
       "      <td>...</td>\n",
       "      <td>85.149368</td>\n",
       "      <td>87.775550</td>\n",
       "      <td>86.108531</td>\n",
       "      <td>79.287771</td>\n",
       "      <td>89.921318</td>\n",
       "      <td>86.790128</td>\n",
       "      <td>84.051367</td>\n",
       "      <td>86.698728</td>\n",
       "      <td>78.334857</td>\n",
       "      <td>79.743283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.028156</td>\n",
       "      <td>88.068195</td>\n",
       "      <td>79.272625</td>\n",
       "      <td>86.026023</td>\n",
       "      <td>91.872413</td>\n",
       "      <td>92.245207</td>\n",
       "      <td>86.814066</td>\n",
       "      <td>89.039560</td>\n",
       "      <td>92.010556</td>\n",
       "      <td>90.413832</td>\n",
       "      <td>...</td>\n",
       "      <td>85.234268</td>\n",
       "      <td>87.876485</td>\n",
       "      <td>85.954382</td>\n",
       "      <td>79.298734</td>\n",
       "      <td>89.869081</td>\n",
       "      <td>86.686079</td>\n",
       "      <td>83.891148</td>\n",
       "      <td>86.753257</td>\n",
       "      <td>78.341213</td>\n",
       "      <td>79.895198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.168162</td>\n",
       "      <td>88.060630</td>\n",
       "      <td>79.194408</td>\n",
       "      <td>86.235405</td>\n",
       "      <td>91.843085</td>\n",
       "      <td>92.243320</td>\n",
       "      <td>86.689144</td>\n",
       "      <td>89.192029</td>\n",
       "      <td>92.114117</td>\n",
       "      <td>90.504833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.324974</td>\n",
       "      <td>87.976567</td>\n",
       "      <td>85.799219</td>\n",
       "      <td>79.310773</td>\n",
       "      <td>89.818353</td>\n",
       "      <td>86.582394</td>\n",
       "      <td>83.730161</td>\n",
       "      <td>86.805090</td>\n",
       "      <td>78.347071</td>\n",
       "      <td>80.045589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     87.362170  92.059179  81.639492  83.046279  87.557608  88.829065   \n",
       "1     87.314653  91.868510  81.753510  83.102770  87.667622  89.014976   \n",
       "2     87.265068  91.681240  81.863257  83.160659  87.776139  89.204368   \n",
       "3     87.213900  91.497346  81.968488  83.220587  87.883528  89.397035   \n",
       "4     87.161546  91.316921  82.068671  83.283040  87.990020  89.592487   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  91.606026  88.069788  79.531699  85.419493  91.976891  92.243208   \n",
       "2439  91.746546  88.072322  79.441336  85.618475  91.938881  92.245196   \n",
       "2440  91.887422  88.071942  79.354928  85.820510  91.904210  92.245867   \n",
       "2441  92.028156  88.068195  79.272625  86.026023  91.872413  92.245207   \n",
       "2442  92.168162  88.060630  79.194408  86.235405  91.843085  92.243320   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     86.304978  91.903081  90.974425  87.964761  ...  80.716972  82.980276   \n",
       "1     86.405655  91.761364  90.912489  88.054400  ...  80.811359  82.990759   \n",
       "2     86.502722  91.621445  90.850914  88.142340  ...  80.909673  82.999959   \n",
       "3     86.596130  91.482991  90.789162  88.228029  ...  81.011722  83.008250   \n",
       "4     86.686142  91.345397  90.726820  88.311044  ...  81.117124  83.015760   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  87.175784  88.590557  91.681913  90.119286  ...  84.995612  87.574274   \n",
       "2439  87.057803  88.739174  91.794750  90.221004  ...  85.070026  87.674484   \n",
       "2440  86.937141  88.888750  91.904207  90.319187  ...  85.149368  87.775550   \n",
       "2441  86.814066  89.039560  92.010556  90.413832  ...  85.234268  87.876485   \n",
       "2442  86.689144  89.192029  92.114117  90.504833  ...  85.324974  87.976567   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     91.631634  83.478056  87.786838  89.127069  80.956526  92.613983   \n",
       "1     91.367214  83.430809  87.885892  88.975725  81.054012  92.393965   \n",
       "2     91.107171  83.381318  87.981265  88.825590  81.151156  92.174215   \n",
       "3     90.851560  83.329554  88.072869  88.677138  81.247568  91.955906   \n",
       "4     90.600178  83.275815  88.160725  88.530710  81.342925  91.740266   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  86.411497  79.268845  90.031115  86.998545  84.367165  86.580171   \n",
       "2439  86.261152  79.277755  89.975204  86.894393  84.210259  86.641084   \n",
       "2440  86.108531  79.287771  89.921318  86.790128  84.051367  86.698728   \n",
       "2441  85.954382  79.298734  89.869081  86.686079  83.891148  86.753257   \n",
       "2442  85.799219  79.310773  89.818353  86.582394  83.730161  86.805090   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     77.410061  81.618024  \n",
       "1     77.261457  81.568429  \n",
       "2     77.116598  81.522525  \n",
       "3     76.975967  81.480128  \n",
       "4     76.839861  81.440740  \n",
       "...         ...        ...  \n",
       "2438  78.319799  79.437264  \n",
       "2439  78.327737  79.590486  \n",
       "2440  78.334857  79.743283  \n",
       "2441  78.341213  79.895198  \n",
       "2442  78.347071  80.045589  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.362170</td>\n",
       "      <td>92.059179</td>\n",
       "      <td>81.639492</td>\n",
       "      <td>83.046279</td>\n",
       "      <td>87.557608</td>\n",
       "      <td>88.829065</td>\n",
       "      <td>86.304978</td>\n",
       "      <td>91.903081</td>\n",
       "      <td>90.974425</td>\n",
       "      <td>87.964761</td>\n",
       "      <td>83.794925</td>\n",
       "      <td>83.903161</td>\n",
       "      <td>88.081739</td>\n",
       "      <td>92.605825</td>\n",
       "      <td>88.363182</td>\n",
       "      <td>89.926102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.314653</td>\n",
       "      <td>91.868510</td>\n",
       "      <td>81.753510</td>\n",
       "      <td>83.102770</td>\n",
       "      <td>87.667622</td>\n",
       "      <td>89.014976</td>\n",
       "      <td>86.405655</td>\n",
       "      <td>91.761364</td>\n",
       "      <td>90.912489</td>\n",
       "      <td>88.054400</td>\n",
       "      <td>83.728260</td>\n",
       "      <td>84.052630</td>\n",
       "      <td>88.169888</td>\n",
       "      <td>92.557122</td>\n",
       "      <td>88.327160</td>\n",
       "      <td>89.775790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.265068</td>\n",
       "      <td>91.681240</td>\n",
       "      <td>81.863257</td>\n",
       "      <td>83.160659</td>\n",
       "      <td>87.776139</td>\n",
       "      <td>89.204368</td>\n",
       "      <td>86.502722</td>\n",
       "      <td>91.621445</td>\n",
       "      <td>90.850914</td>\n",
       "      <td>88.142340</td>\n",
       "      <td>83.662607</td>\n",
       "      <td>84.200113</td>\n",
       "      <td>88.258172</td>\n",
       "      <td>92.508183</td>\n",
       "      <td>88.292243</td>\n",
       "      <td>89.627778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.213900</td>\n",
       "      <td>91.497346</td>\n",
       "      <td>81.968488</td>\n",
       "      <td>83.220587</td>\n",
       "      <td>87.883528</td>\n",
       "      <td>89.397035</td>\n",
       "      <td>86.596130</td>\n",
       "      <td>91.482991</td>\n",
       "      <td>90.789162</td>\n",
       "      <td>88.228029</td>\n",
       "      <td>83.598118</td>\n",
       "      <td>84.345347</td>\n",
       "      <td>88.346355</td>\n",
       "      <td>92.459445</td>\n",
       "      <td>88.258696</td>\n",
       "      <td>89.481986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.161546</td>\n",
       "      <td>91.316921</td>\n",
       "      <td>82.068671</td>\n",
       "      <td>83.283040</td>\n",
       "      <td>87.990020</td>\n",
       "      <td>89.592487</td>\n",
       "      <td>86.686142</td>\n",
       "      <td>91.345397</td>\n",
       "      <td>90.726820</td>\n",
       "      <td>88.311044</td>\n",
       "      <td>83.534657</td>\n",
       "      <td>84.488354</td>\n",
       "      <td>88.434525</td>\n",
       "      <td>92.411430</td>\n",
       "      <td>88.226767</td>\n",
       "      <td>89.338511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>91.606026</td>\n",
       "      <td>88.069788</td>\n",
       "      <td>79.531699</td>\n",
       "      <td>85.419493</td>\n",
       "      <td>91.976891</td>\n",
       "      <td>92.243208</td>\n",
       "      <td>87.175784</td>\n",
       "      <td>88.590557</td>\n",
       "      <td>91.681913</td>\n",
       "      <td>90.119286</td>\n",
       "      <td>83.322372</td>\n",
       "      <td>76.849833</td>\n",
       "      <td>93.175857</td>\n",
       "      <td>93.090935</td>\n",
       "      <td>90.310752</td>\n",
       "      <td>88.900240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>91.746546</td>\n",
       "      <td>88.072322</td>\n",
       "      <td>79.441336</td>\n",
       "      <td>85.618475</td>\n",
       "      <td>91.938881</td>\n",
       "      <td>92.245196</td>\n",
       "      <td>87.057803</td>\n",
       "      <td>88.739174</td>\n",
       "      <td>91.794750</td>\n",
       "      <td>90.221004</td>\n",
       "      <td>83.387865</td>\n",
       "      <td>76.721244</td>\n",
       "      <td>93.131220</td>\n",
       "      <td>93.244562</td>\n",
       "      <td>90.293016</td>\n",
       "      <td>88.995463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>91.887422</td>\n",
       "      <td>88.071942</td>\n",
       "      <td>79.354928</td>\n",
       "      <td>85.820510</td>\n",
       "      <td>91.904210</td>\n",
       "      <td>92.245867</td>\n",
       "      <td>86.937141</td>\n",
       "      <td>88.888750</td>\n",
       "      <td>91.904207</td>\n",
       "      <td>90.319187</td>\n",
       "      <td>83.453330</td>\n",
       "      <td>76.596511</td>\n",
       "      <td>93.083326</td>\n",
       "      <td>93.401602</td>\n",
       "      <td>90.272957</td>\n",
       "      <td>89.090579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>92.028156</td>\n",
       "      <td>88.068195</td>\n",
       "      <td>79.272625</td>\n",
       "      <td>86.026023</td>\n",
       "      <td>91.872413</td>\n",
       "      <td>92.245207</td>\n",
       "      <td>86.814066</td>\n",
       "      <td>89.039560</td>\n",
       "      <td>92.010556</td>\n",
       "      <td>90.413832</td>\n",
       "      <td>83.518531</td>\n",
       "      <td>76.475662</td>\n",
       "      <td>93.032209</td>\n",
       "      <td>93.562653</td>\n",
       "      <td>90.251211</td>\n",
       "      <td>89.185688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>92.168162</td>\n",
       "      <td>88.060630</td>\n",
       "      <td>79.194408</td>\n",
       "      <td>86.235405</td>\n",
       "      <td>91.843085</td>\n",
       "      <td>92.243320</td>\n",
       "      <td>86.689144</td>\n",
       "      <td>89.192029</td>\n",
       "      <td>92.114117</td>\n",
       "      <td>90.504833</td>\n",
       "      <td>83.583451</td>\n",
       "      <td>76.358871</td>\n",
       "      <td>92.977781</td>\n",
       "      <td>93.728058</td>\n",
       "      <td>90.228021</td>\n",
       "      <td>89.281172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     87.362170  92.059179  81.639492  83.046279  87.557608  88.829065   \n",
       "1     87.314653  91.868510  81.753510  83.102770  87.667622  89.014976   \n",
       "2     87.265068  91.681240  81.863257  83.160659  87.776139  89.204368   \n",
       "3     87.213900  91.497346  81.968488  83.220587  87.883528  89.397035   \n",
       "4     87.161546  91.316921  82.068671  83.283040  87.990020  89.592487   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  91.606026  88.069788  79.531699  85.419493  91.976891  92.243208   \n",
       "2439  91.746546  88.072322  79.441336  85.618475  91.938881  92.245196   \n",
       "2440  91.887422  88.071942  79.354928  85.820510  91.904210  92.245867   \n",
       "2441  92.028156  88.068195  79.272625  86.026023  91.872413  92.245207   \n",
       "2442  92.168162  88.060630  79.194408  86.235405  91.843085  92.243320   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     86.304978  91.903081  90.974425  87.964761  83.794925  83.903161   \n",
       "1     86.405655  91.761364  90.912489  88.054400  83.728260  84.052630   \n",
       "2     86.502722  91.621445  90.850914  88.142340  83.662607  84.200113   \n",
       "3     86.596130  91.482991  90.789162  88.228029  83.598118  84.345347   \n",
       "4     86.686142  91.345397  90.726820  88.311044  83.534657  84.488354   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  87.175784  88.590557  91.681913  90.119286  83.322372  76.849833   \n",
       "2439  87.057803  88.739174  91.794750  90.221004  83.387865  76.721244   \n",
       "2440  86.937141  88.888750  91.904207  90.319187  83.453330  76.596511   \n",
       "2441  86.814066  89.039560  92.010556  90.413832  83.518531  76.475662   \n",
       "2442  86.689144  89.192029  92.114117  90.504833  83.583451  76.358871   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     88.081739  92.605825  88.363182  89.926102  \n",
       "1     88.169888  92.557122  88.327160  89.775790  \n",
       "2     88.258172  92.508183  88.292243  89.627778  \n",
       "3     88.346355  92.459445  88.258696  89.481986  \n",
       "4     88.434525  92.411430  88.226767  89.338511  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  93.175857  93.090935  90.310752  88.900240  \n",
       "2439  93.131220  93.244562  90.293016  88.995463  \n",
       "2440  93.083326  93.401602  90.272957  89.090579  \n",
       "2441  93.032209  93.562653  90.251211  89.185688  \n",
       "2442  92.977781  93.728058  90.228021  89.281172  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 3456.8213 - val_loss: 2749.4385\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2575.7764 - val_loss: 2476.7822\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2358.4685 - val_loss: 2290.1230\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1985.8287 - val_loss: 2040.4192\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1531.2131 - val_loss: 1456.8801\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1193.9390 - val_loss: 1152.4357\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1021.3003 - val_loss: 1176.3729\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 948.4631 - val_loss: 1076.8896\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 911.1393 - val_loss: 1007.4346\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 889.2460 - val_loss: 1016.6093\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 870.5667 - val_loss: 950.6862\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 834.3514 - val_loss: 1118.9109\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 783.0733 - val_loss: 1084.2167\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 693.1215 - val_loss: 999.3896\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 556.3343 - val_loss: 857.2515\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 425.6434 - val_loss: 776.3922\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 329.0978 - val_loss: 426.1388\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 237.0319 - val_loss: 1183.0237\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 187.1649 - val_loss: 423.0863\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 152.1937 - val_loss: 354.5718\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 141.8578 - val_loss: 273.1420\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 101.4728 - val_loss: 231.1281\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 85.9139 - val_loss: 188.8445\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 66.0168 - val_loss: 537.3412\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 53.7695 - val_loss: 48.1573\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 40.6728 - val_loss: 118.5505\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 65.7077 - val_loss: 257.0629\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.9800 - val_loss: 323.1087\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.2497 - val_loss: 268.8620\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.5749 - val_loss: 16.8301\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 20.9681 - val_loss: 319.0441\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 19.4712 - val_loss: 41.7399\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.3251 - val_loss: 10.9983\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.3012 - val_loss: 2245.0068\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 83.7179 - val_loss: 12.2669\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.0059 - val_loss: 14.0121\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.5062 - val_loss: 6.5852\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.4334 - val_loss: 16.6150\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1029 - val_loss: 36.7196\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.2810 - val_loss: 46.8269\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.9428 - val_loss: 53.3043\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.0930 - val_loss: 8.1276\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2895 - val_loss: 12.6542\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 49.9027 - val_loss: 384.3416\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.5084 - val_loss: 112.7284\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.7768 - val_loss: 8.0203\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2483 - val_loss: 18.3378\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9942 - val_loss: 4.2164\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7511 - val_loss: 5.8291\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8099 - val_loss: 5.9453\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.5312 - val_loss: 7.7758\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 21.4593 - val_loss: 1161.9587\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 57.7198 - val_loss: 255.0125\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.8403 - val_loss: 11.5107\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4343 - val_loss: 5.1667\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0896 - val_loss: 2.2829\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.2187 - val_loss: 8.2568\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7168 - val_loss: 3.0063\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6469 - val_loss: 164.0555\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6930 - val_loss: 1.9347\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3159 - val_loss: 4.2949\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8325 - val_loss: 7.0147\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8986 - val_loss: 11.2422\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.1964 - val_loss: 1345.7198\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 44.4532 - val_loss: 23.8912\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5292 - val_loss: 7.7286\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6799 - val_loss: 1.4446\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0169 - val_loss: 6.9261\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3857 - val_loss: 2.9384\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3743 - val_loss: 3.9755\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1290 - val_loss: 5.7949\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1661 - val_loss: 1.1422\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2561 - val_loss: 3.4094\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5698 - val_loss: 1017.4014\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 64.5415 - val_loss: 12.1177\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6022 - val_loss: 1.8393\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6344 - val_loss: 1.5504\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2376 - val_loss: 1.4597\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0828 - val_loss: 2.0360\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9392 - val_loss: 1.1006\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9522 - val_loss: 1.0311\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8637 - val_loss: 1.4108\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0852 - val_loss: 0.9118\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9439 - val_loss: 2.1164\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0819 - val_loss: 16.4514\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0089 - val_loss: 10.5540\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4383 - val_loss: 4.2855\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1448 - val_loss: 0.8465\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.8349 - val_loss: 1743.9832\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 32.2913 - val_loss: 3.8471\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2220 - val_loss: 0.9252\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9061 - val_loss: 0.7726\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.9754 - val_loss: 0.9784\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8401 - val_loss: 0.9920\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8569 - val_loss: 1.2329\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6954 - val_loss: 1.5065\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6446 - val_loss: 1.8766\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6282 - val_loss: 1.7034\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7870 - val_loss: 97.8986\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1001 - val_loss: 2.3978\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9971 - val_loss: 2.6069\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 26.1217 - val_loss: 1155.7300\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6015 - val_loss: 1.2269\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9998 - val_loss: 2.4093\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6654 - val_loss: 0.6757\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5776 - val_loss: 0.6152\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6898 - val_loss: 0.8868\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6683 - val_loss: 66.2265\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0758 - val_loss: 1.0433\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5332 - val_loss: 1.4483\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5052 - val_loss: 0.9359\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7006 - val_loss: 1.9985\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3660 - val_loss: 2.4087\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8443 - val_loss: 0.9300\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1574 - val_loss: 582.8312\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4545 - val_loss: 2.7091\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6209 - val_loss: 1.2644\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6025 - val_loss: 0.6168\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4953 - val_loss: 1.0668\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4840 - val_loss: 0.5205\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4523 - val_loss: 0.6518\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4581 - val_loss: 0.7612\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5229 - val_loss: 0.9998\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5827 - val_loss: 1.0062\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 36.1191 - val_loss: 455.6096\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0082 - val_loss: 1.5372\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8884 - val_loss: 0.6121\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6398 - val_loss: 0.5126\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4781 - val_loss: 0.9670\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4711 - val_loss: 0.7234\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4508 - val_loss: 0.6052\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3965 - val_loss: 0.3852\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5383 - val_loss: 1.5834\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5980 - val_loss: 0.8273\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4865 - val_loss: 0.6579\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4109 - val_loss: 0.6369\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4083 - val_loss: 22.8451\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6204 - val_loss: 2.3263\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6778 - val_loss: 0.9339\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6344 - val_loss: 2.4350\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6045 - val_loss: 0.8645\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 15.2357 - val_loss: 220.3539\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.0721 - val_loss: 0.8210\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.3752 - val_loss: 51.3426\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1249 - val_loss: 0.6893\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7299 - val_loss: 0.4954\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4340 - val_loss: 0.3898\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3589 - val_loss: 0.4553\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3651 - val_loss: 0.3870\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3121 - val_loss: 0.2725\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3902 - val_loss: 0.3318\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3120 - val_loss: 0.4686\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3967 - val_loss: 0.6765\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3570 - val_loss: 0.4422\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3481 - val_loss: 0.6187\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4528 - val_loss: 1.0947\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6038 - val_loss: 0.8155\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4890 - val_loss: 1.1016\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4973 - val_loss: 2.1459\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9498 - val_loss: 94.2792\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6802 - val_loss: 2.9570\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5730 - val_loss: 3.7819\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.5690 - val_loss: 303.8238\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7575 - val_loss: 0.5838\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4019 - val_loss: 0.3693\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3794 - val_loss: 0.2804\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2677 - val_loss: 0.3361\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2464 - val_loss: 0.2602\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2491 - val_loss: 0.3503\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2425 - val_loss: 0.3457\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2590 - val_loss: 0.3617\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2487 - val_loss: 0.3955\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3475 - val_loss: 0.4633\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.0384 - val_loss: 11.4563\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3937 - val_loss: 0.5567\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3434 - val_loss: 0.6029\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5828 - val_loss: 1.1303\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4520 - val_loss: 2.0355\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5488 - val_loss: 1.6295\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7761 - val_loss: 1809.7316\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0064 - val_loss: 1193.8363\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8390 - val_loss: 0.5897\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3817 - val_loss: 0.3222\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3758 - val_loss: 0.4841\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2306 - val_loss: 0.2877\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2169 - val_loss: 0.2498\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2242 - val_loss: 0.2209\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2118 - val_loss: 0.2225\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2142 - val_loss: 0.3092\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2672 - val_loss: 0.5626\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2679 - val_loss: 0.2835\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2591 - val_loss: 0.5998\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3160 - val_loss: 0.9444\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4589 - val_loss: 0.9109\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3607 - val_loss: 20.9799\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6747 - val_loss: 7.1784\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4242 - val_loss: 2.4969\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3663 - val_loss: 1.4939\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5713 - val_loss: 7.1034\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6041 - val_loss: 1.8258\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 1.825744842840724\n",
      "Mean Absolute Error (MAE): 0.9639880277153164\n",
      "Root Mean Squared Error (RMSE): 1.3512012591915106\n",
      "Time taken: 629.657665014267\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 3486.5535 - val_loss: 2811.7927\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2587.9038 - val_loss: 2564.0471\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2432.9788 - val_loss: 2375.1404\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2116.4634 - val_loss: 1967.2694\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1693.1659 - val_loss: 1558.9829\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1310.7054 - val_loss: 1260.5897\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1082.0817 - val_loss: 1119.8512\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 970.2674 - val_loss: 1209.6465\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 920.0992 - val_loss: 1052.8979\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 908.3740 - val_loss: 1134.5199\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 891.3000 - val_loss: 952.2717\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 874.7045 - val_loss: 1037.5007\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 860.3362 - val_loss: 1014.1817\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 836.3817 - val_loss: 1031.4500\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 793.1911 - val_loss: 1082.1993\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 711.6744 - val_loss: 937.9539\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 595.9572 - val_loss: 680.2078\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 482.0275 - val_loss: 2685.8359\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 367.7769 - val_loss: 838.1439\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 290.0196 - val_loss: 339.9229\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 251.6832 - val_loss: 576.1143\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 196.2820 - val_loss: 1036.4822\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 155.8002 - val_loss: 186.8328\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 124.6065 - val_loss: 608.1489\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 120.7810 - val_loss: 438.0555\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 92.5573 - val_loss: 188.0060\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 69.3820 - val_loss: 107.6327\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 67.6698 - val_loss: 318.3083\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 48.8400 - val_loss: 638.2827\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 33.1372 - val_loss: 131.9965\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 41.3042 - val_loss: 260.0097\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.5335 - val_loss: 30.4674\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.4049 - val_loss: 237.5681\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 23.4096 - val_loss: 182.2247\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 50.6829 - val_loss: 114.7846\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.6755 - val_loss: 40.0271\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.3605 - val_loss: 15.0570\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.9467 - val_loss: 21.6509\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.9140 - val_loss: 47.2406\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.2293 - val_loss: 13.0271\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.6463 - val_loss: 9.1546\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.7680 - val_loss: 54.3115\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.2330 - val_loss: 154.6713\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1813 - val_loss: 328.1542\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.7812 - val_loss: 8.2903\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8584 - val_loss: 533.2357\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.9756 - val_loss: 6.5471\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.9214 - val_loss: 3.5816\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.3376 - val_loss: 40.4364\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.0800 - val_loss: 9.1708\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4610 - val_loss: 4.4377\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9454 - val_loss: 51.8246\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 47.5213 - val_loss: 11.2047\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.1791 - val_loss: 204.8609\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0291 - val_loss: 8.0545\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3288 - val_loss: 10.3242\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.1285 - val_loss: 314.9982\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.3443 - val_loss: 1.8639\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2.4124 - val_loss: 3.2802\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6569 - val_loss: 4.8421\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0418 - val_loss: 5.1837\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7771 - val_loss: 18.9878\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0619 - val_loss: 13.3192\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1388 - val_loss: 4.5458\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9603 - val_loss: 19.9098\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.9416 - val_loss: 835.2758\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 60.0108 - val_loss: 127.9343\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6401 - val_loss: 5.2588\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2020 - val_loss: 5.1501\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.6527 - val_loss: 4.0010\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3968 - val_loss: 2.5163\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2821 - val_loss: 1.2498\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1297 - val_loss: 1.2120\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4150 - val_loss: 1.1880\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5266 - val_loss: 6.5440\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6005 - val_loss: 2.7709\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3744 - val_loss: 9.1972\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2484 - val_loss: 1.0948\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5986 - val_loss: 3.7387\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5780 - val_loss: 35.6356\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 61.2406 - val_loss: 1333.3434\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.5546 - val_loss: 17.7641\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.1258 - val_loss: 3.5288\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6666 - val_loss: 1.9456\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3570 - val_loss: 1.6377\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3342 - val_loss: 1.2137\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0006 - val_loss: 2.2045\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4568 - val_loss: 200.0656\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0188 - val_loss: 1.1201\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9868 - val_loss: 3.1271\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0240 - val_loss: 2.2675\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9126 - val_loss: 1.4442\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2014 - val_loss: 6.1751\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.5554 - val_loss: 269.7422\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.2643 - val_loss: 2.9743\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2319 - val_loss: 1.0349\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9264 - val_loss: 0.8909\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8108 - val_loss: 1.6204\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9133 - val_loss: 1.1189\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7305 - val_loss: 0.8489\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7316 - val_loss: 2.1516\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7324 - val_loss: 2.1892\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7452 - val_loss: 2.3815\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8298 - val_loss: 4.6994\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9878 - val_loss: 5.3915\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8850 - val_loss: 75.4338\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 28.8651 - val_loss: 19.2097\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3396 - val_loss: 2.3590\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9915 - val_loss: 1.2468\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8618 - val_loss: 2.7047\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6469 - val_loss: 0.9275\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6171 - val_loss: 24.0881\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8837 - val_loss: 1.0433\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7670 - val_loss: 3.0066\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5361 - val_loss: 0.7944\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6519 - val_loss: 0.9383\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7449 - val_loss: 5.5596\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8068 - val_loss: 1.7062\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9235 - val_loss: 30.3989\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 21.5266 - val_loss: 24.1033\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0382 - val_loss: 0.8117\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5997 - val_loss: 0.6351\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5764 - val_loss: 0.7346\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4886 - val_loss: 0.4483\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6965 - val_loss: 2.7875\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0879 - val_loss: 0.6650\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4407 - val_loss: 0.8471\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4543 - val_loss: 0.6413\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5122 - val_loss: 1.4043\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5024 - val_loss: 1.1583\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6426 - val_loss: 1.2605\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6419 - val_loss: 1.6775\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 28.3162 - val_loss: 55.9289\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8430 - val_loss: 1.2690\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8072 - val_loss: 1.0021\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6095 - val_loss: 1.3124\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4491 - val_loss: 0.9083\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4331 - val_loss: 0.5332\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4177 - val_loss: 0.6729\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4247 - val_loss: 0.5179\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3702 - val_loss: 0.6114\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4222 - val_loss: 0.7110\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4907 - val_loss: 1.2841\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4535 - val_loss: 0.6880\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5600 - val_loss: 10.1854\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.3982 - val_loss: 1435.3336\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3400 - val_loss: 1.2523\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7726 - val_loss: 0.6782\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5130 - val_loss: 0.8246\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4169 - val_loss: 0.5540\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3889 - val_loss: 0.4955\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3537 - val_loss: 0.3622\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3720 - val_loss: 0.5564\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3061 - val_loss: 0.3942\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3486 - val_loss: 1.6597\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4724 - val_loss: 0.5538\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3568 - val_loss: 0.5290\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4706 - val_loss: 0.6128\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5072 - val_loss: 0.7832\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4850 - val_loss: 0.9308\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.5070 - val_loss: 17.8779\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7228 - val_loss: 0.5364\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3777 - val_loss: 0.7502\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3380 - val_loss: 0.5432\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3945 - val_loss: 0.8466\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3886 - val_loss: 0.6286\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4643 - val_loss: 1.2323\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2035 - val_loss: 2.8322\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4721 - val_loss: 1.3952\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.1488 - val_loss: 726.1534\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4145 - val_loss: 0.8740\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4817 - val_loss: 0.4330\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3458 - val_loss: 0.4402\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2928 - val_loss: 0.3734\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2638 - val_loss: 0.4943\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2632 - val_loss: 0.3399\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2931 - val_loss: 0.4468\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2737 - val_loss: 0.3037\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3592 - val_loss: 0.6620\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3089 - val_loss: 1.0801\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3666 - val_loss: 0.5481\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3815 - val_loss: 1.2423\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5439 - val_loss: 0.7756\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3947 - val_loss: 0.5327\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5063 - val_loss: 1.0381\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5347 - val_loss: 1.7220\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5145 - val_loss: 1.1602\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8663 - val_loss: 32.1315\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6607 - val_loss: 1.3051\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6506 - val_loss: 7.6096\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5772 - val_loss: 206.1659\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 25.1445 - val_loss: 5.6500\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7561 - val_loss: 0.6445\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5395 - val_loss: 0.4754\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3428 - val_loss: 0.3819\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2868 - val_loss: 0.3206\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2381 - val_loss: 0.2570\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2180 - val_loss: 0.2699\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1970 - val_loss: 0.2597\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4003 - val_loss: 0.5234\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.5233499752014771\n",
      "Mean Absolute Error (MAE): 0.5437779011059193\n",
      "Root Mean Squared Error (RMSE): 0.7234293159676881\n",
      "Time taken: 620.48140001297\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 3416.9216 - val_loss: 2589.3616\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 2555.9678 - val_loss: 2302.2754\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2312.3989 - val_loss: 1990.6803\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1878.0171 - val_loss: 1521.2494\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1454.3168 - val_loss: 1196.2301\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1174.2205 - val_loss: 1073.2565\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1027.9740 - val_loss: 994.3788\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 963.7084 - val_loss: 922.8339\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 938.9693 - val_loss: 847.9285\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 919.0689 - val_loss: 801.6703\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 896.1486 - val_loss: 1005.7280\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 863.5411 - val_loss: 783.2835\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 818.5122 - val_loss: 894.0994\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 735.0151 - val_loss: 789.3160\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 622.3709 - val_loss: 532.6183\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 475.7275 - val_loss: 652.8346\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 360.9887 - val_loss: 665.2714\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 309.2294 - val_loss: 371.4349\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 233.0782 - val_loss: 744.3078\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 201.2125 - val_loss: 434.9149\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 186.4851 - val_loss: 204.8794\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 146.1816 - val_loss: 399.0971\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 133.1398 - val_loss: 130.1078\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 115.3967 - val_loss: 131.3640\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 115.3214 - val_loss: 370.3833\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 75.2513 - val_loss: 129.1116\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 58.4381 - val_loss: 262.8483\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 79.9831 - val_loss: 229.4664\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 47.5058 - val_loss: 427.6690\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 32.8499 - val_loss: 53.9272\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 49.8886 - val_loss: 351.2008\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 28.8230 - val_loss: 71.6765\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 19.9397 - val_loss: 35.8347\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.3006 - val_loss: 137.7809\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 48.7563 - val_loss: 288.9714\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 60.3899 - val_loss: 37.7923\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.3804 - val_loss: 26.2287\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8517 - val_loss: 11.0302\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.7092 - val_loss: 23.6227\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 8.8103 - val_loss: 15.4959\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.9337 - val_loss: 271.3522\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 35.7631 - val_loss: 1591.4130\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 12.3993 - val_loss: 8.2347\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.7210 - val_loss: 12.8521\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.7182 - val_loss: 6.5596\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5448 - val_loss: 14.4131\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.1677 - val_loss: 613.0789\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 71.8721 - val_loss: 19.7759\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.4997 - val_loss: 8.7007\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7909 - val_loss: 18.2366\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.6944 - val_loss: 5.2944\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.3565 - val_loss: 7.5302\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 39.5498 - val_loss: 69.9809\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.5171 - val_loss: 13.1707\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9322 - val_loss: 6.6300\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8417 - val_loss: 2.1165\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0395 - val_loss: 4.7337\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0972 - val_loss: 4.0564\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3836 - val_loss: 5.8911\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4388 - val_loss: 1097.1819\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 55.3629 - val_loss: 15.3301\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2078 - val_loss: 4.3659\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2013 - val_loss: 2.3345\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.5173 - val_loss: 185.2130\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3761 - val_loss: 1.8084\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6586 - val_loss: 2.7252\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7424 - val_loss: 2.4853\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6912 - val_loss: 2.1325\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6056 - val_loss: 2.4760\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 32.0692 - val_loss: 880.8801\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.1350 - val_loss: 2.9210\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9381 - val_loss: 1.7090\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5843 - val_loss: 1.2338\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5879 - val_loss: 1.9851\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3963 - val_loss: 2.1524\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2225 - val_loss: 2.2376\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6459 - val_loss: 4.4200\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.9482 - val_loss: 1.6236\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4956 - val_loss: 2.6762\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5440 - val_loss: 3.2199\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6244 - val_loss: 3.2284\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5357 - val_loss: 2.1064\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9473 - val_loss: 8.0078\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1396 - val_loss: 425.9941\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 25.9628 - val_loss: 2.8544\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5593 - val_loss: 1.0757\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9823 - val_loss: 1.4180\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.8553 - val_loss: 0.9766\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 4s 13ms/step - loss: 0.7700 - val_loss: 1.3148\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9995 - val_loss: 2.2656\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0677 - val_loss: 8.4402\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8412 - val_loss: 2.0454\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 20.4522 - val_loss: 173.0648\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3337 - val_loss: 2.0090\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9220 - val_loss: 1.0342\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8409 - val_loss: 0.7589\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7848 - val_loss: 0.7652\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.1215 - val_loss: 578.9752\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 23.1649 - val_loss: 1.5828\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1224 - val_loss: 1.9096\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8254 - val_loss: 1.2511\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1865 - val_loss: 1.8406\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7151 - val_loss: 1.0797\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7093 - val_loss: 0.7582\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7615 - val_loss: 0.4945\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7327 - val_loss: 1.1271\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7193 - val_loss: 1.6858\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7624 - val_loss: 1.0388\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7778 - val_loss: 1.4906\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2591 - val_loss: 150.0316\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 46.1311 - val_loss: 382.6143\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.5929 - val_loss: 1.9017\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2488 - val_loss: 0.9660\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8814 - val_loss: 0.6962\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7510 - val_loss: 1.0351\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6420 - val_loss: 0.5851\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5292 - val_loss: 0.5644\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5539 - val_loss: 1.0327\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5521 - val_loss: 1.2349\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5221 - val_loss: 0.7905\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5824 - val_loss: 0.8238\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6924 - val_loss: 77.6261\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8024 - val_loss: 2.3352\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7820 - val_loss: 1.6874\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0603 - val_loss: 277.2588\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.9626 - val_loss: 2.3459\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8148 - val_loss: 0.6886\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5431 - val_loss: 0.9689\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4721 - val_loss: 0.6122\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4649 - val_loss: 0.5781\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4247 - val_loss: 0.5408\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4487 - val_loss: 0.5830\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6178 - val_loss: 1.4094\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5644 - val_loss: 1.0285\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6835 - val_loss: 1.1582\n",
      "16/16 [==============================] - 1s 15ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.49449936572287845\n",
      "Mean Absolute Error (MAE): 0.5212154843984039\n",
      "Root Mean Squared Error (RMSE): 0.7032064886808699\n",
      "Time taken: 428.364045381546\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 3380.5073 - val_loss: 2709.7676\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2574.8289 - val_loss: 2509.7429\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2513.6738 - val_loss: 2506.0386\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2437.4153 - val_loss: 2297.1477\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1939.2855 - val_loss: 1776.4966\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1433.9890 - val_loss: 1724.6222\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1145.1469 - val_loss: 1172.7167\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1003.9883 - val_loss: 1026.7491\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 947.6650 - val_loss: 1058.4485\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 923.6945 - val_loss: 888.7661\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 903.6069 - val_loss: 837.9838\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 879.4279 - val_loss: 849.3782\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 861.0952 - val_loss: 893.8339\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 823.7842 - val_loss: 787.1799\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 769.9229 - val_loss: 911.4511\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 669.5641 - val_loss: 904.1490\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 512.8848 - val_loss: 805.9864\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 383.9459 - val_loss: 738.8298\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 320.3300 - val_loss: 710.1453\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 223.8415 - val_loss: 410.1515\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 192.1898 - val_loss: 1462.7354\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 164.8380 - val_loss: 203.3497\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 147.1646 - val_loss: 539.5464\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 127.1232 - val_loss: 519.6252\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 104.0612 - val_loss: 242.8328\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 93.1487 - val_loss: 153.4570\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 72.1549 - val_loss: 92.9297\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 56.7642 - val_loss: 91.0788\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 92.6361 - val_loss: 283.5210\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.9034 - val_loss: 152.7136\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 51.7476 - val_loss: 115.0879\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 29.2598 - val_loss: 100.8117\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 27.1749 - val_loss: 42.2997\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 17.7848 - val_loss: 230.8621\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 18.6728 - val_loss: 24.8550\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 43.9077 - val_loss: 387.7231\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.4867 - val_loss: 106.5675\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 13.6470 - val_loss: 29.3344\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.4374 - val_loss: 26.7610\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.9039 - val_loss: 780.2743\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.9572 - val_loss: 19.6458\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.0924 - val_loss: 49.4544\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.1172 - val_loss: 17.4145\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.4160 - val_loss: 77.7937\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7762 - val_loss: 8.4761\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 12.1976 - val_loss: 384.4467\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 10.8034 - val_loss: 259.0707\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.9454 - val_loss: 10.1577\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 25.8355 - val_loss: 3552.1831\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 30.0822 - val_loss: 13.6417\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.7789 - val_loss: 3.8695\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1262 - val_loss: 5.8038\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4723 - val_loss: 2.5934\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.3445 - val_loss: 364.1651\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.9701 - val_loss: 3.1023\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7548 - val_loss: 2.6452\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3051 - val_loss: 193.2401\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8269 - val_loss: 4.4956\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1709 - val_loss: 2.8237\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9450 - val_loss: 3548.0891\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 37.0920 - val_loss: 4.2495\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7550 - val_loss: 9.2411\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2421 - val_loss: 2.2670\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7643 - val_loss: 1.7374\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.3313 - val_loss: 1.9307\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.6448 - val_loss: 3.0671\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.5124 - val_loss: 1.4202\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6229 - val_loss: 2.3280\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.9711 - val_loss: 19.0454\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9625 - val_loss: 4.5211\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3995 - val_loss: 1.5867\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4026 - val_loss: 4.6590\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 30.8357 - val_loss: 3931.2561\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 43.5771 - val_loss: 7.1922\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1398 - val_loss: 2.7812\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1952 - val_loss: 1.4859\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5352 - val_loss: 2.9720\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2849 - val_loss: 1.7945\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2866 - val_loss: 3.4272\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2797 - val_loss: 4.6225\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7965 - val_loss: 4.2758\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5774 - val_loss: 1.4702\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.2011 - val_loss: 2.2151\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 7.2933 - val_loss: 157.1172\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2.7741 - val_loss: 3.2271\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8652 - val_loss: 120.1564\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.4931 - val_loss: 2.2835\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0668 - val_loss: 5.8509\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9058 - val_loss: 1.3064\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9072 - val_loss: 1.9654\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9124 - val_loss: 1.5487\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 29.2405 - val_loss: 60.3562\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 12.8087 - val_loss: 3.9571\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2618 - val_loss: 1.1789\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9115 - val_loss: 1.2925\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8067 - val_loss: 0.9151\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7930 - val_loss: 0.9416\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1977 - val_loss: 283.6669\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9292 - val_loss: 1.3085\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7513 - val_loss: 1.2141\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7777 - val_loss: 3.4336\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7895 - val_loss: 1.3143\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9915 - val_loss: 3.0619\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8221 - val_loss: 2.3018\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0718 - val_loss: 2.0890\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0772 - val_loss: 10.6059\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3227 - val_loss: 43.9790\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4394 - val_loss: 2.1454\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.0098 - val_loss: 20.8284\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4171 - val_loss: 1.0277\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6857 - val_loss: 1.5447\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6379 - val_loss: 2.2185\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5796 - val_loss: 0.5583\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 33.1466 - val_loss: 1002.8563\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.3485 - val_loss: 1.6582\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2952 - val_loss: 1.2327\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7533 - val_loss: 0.6561\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7028 - val_loss: 1.1988\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5952 - val_loss: 0.8717\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5121 - val_loss: 0.6479\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5109 - val_loss: 0.6231\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4403 - val_loss: 0.6487\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4414 - val_loss: 0.4783\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5924 - val_loss: 0.8015\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 28.2960 - val_loss: 8.0216\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3078 - val_loss: 1.1087\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7223 - val_loss: 1.0110\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5386 - val_loss: 0.7777\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5553 - val_loss: 0.4940\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4823 - val_loss: 0.7149\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4080 - val_loss: 0.4694\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4507 - val_loss: 0.7753\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4690 - val_loss: 0.8325\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4977 - val_loss: 0.8534\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5193 - val_loss: 1.0773\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5757 - val_loss: 1.2797\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8118 - val_loss: 1.1781\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6336 - val_loss: 1.9061\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8332 - val_loss: 1.8830\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7842 - val_loss: 1.7523\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4393 - val_loss: 1.9887\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7270 - val_loss: 2.4683\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2794 - val_loss: 728.2836\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 16.1962 - val_loss: 1.7205\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7153 - val_loss: 1.3225\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1449 - val_loss: 0.8765\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4079 - val_loss: 0.5343\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3885 - val_loss: 0.5382\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3355 - val_loss: 0.3880\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3886 - val_loss: 0.7610\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3880 - val_loss: 0.4712\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4128 - val_loss: 21.2581\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4012 - val_loss: 0.7602\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6314 - val_loss: 18.7467\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6040 - val_loss: 1.0996\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4914 - val_loss: 0.6500\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9678 - val_loss: 6.7067\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 20.2617 - val_loss: 14.6376\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8604 - val_loss: 0.7695\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4240 - val_loss: 0.3314\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3205 - val_loss: 0.3578\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2792 - val_loss: 0.2679\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2956 - val_loss: 0.3103\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2614 - val_loss: 0.4065\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2675 - val_loss: 0.7388\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2847 - val_loss: 0.3392\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3508 - val_loss: 25.0851\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4538 - val_loss: 0.5439\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3864 - val_loss: 0.8282\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5436 - val_loss: 1.5805\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4998 - val_loss: 1.1402\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6268 - val_loss: 1.0180\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7202 - val_loss: 3.5566\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5903 - val_loss: 3.4368\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6595 - val_loss: 1.0427\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7142 - val_loss: 7.4303\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7482 - val_loss: 49.6614\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.6664 - val_loss: 132.2566\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5852 - val_loss: 0.6018\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3747 - val_loss: 0.3887\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3217 - val_loss: 0.4764\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2620 - val_loss: 0.2765\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2325 - val_loss: 0.3215\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2743 - val_loss: 0.2561\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2127 - val_loss: 0.3042\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2073 - val_loss: 0.3177\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2239 - val_loss: 0.4533\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2476 - val_loss: 0.3027\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2924 - val_loss: 1.1779\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3090 - val_loss: 0.9063\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2925 - val_loss: 0.4555\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3726 - val_loss: 1.2273\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3720 - val_loss: 1.4750\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5672 - val_loss: 1.2981\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4793 - val_loss: 1.1699\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 14.8378 - val_loss: 688.8801\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0055 - val_loss: 0.6672\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4612 - val_loss: 0.4048\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3188 - val_loss: 0.3285\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2461 - val_loss: 0.2457\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.24586530401939558\n",
      "Mean Absolute Error (MAE): 0.37318458997325404\n",
      "Root Mean Squared Error (RMSE): 0.49584806545896254\n",
      "Time taken: 627.6662895679474\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 15ms/step - loss: 3415.2039 - val_loss: 2868.8611\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 2562.9116 - val_loss: 2614.7849\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2490.5115 - val_loss: 2601.9878\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2479.9363 - val_loss: 2580.1331\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2177.7708 - val_loss: 2005.5325\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1630.6649 - val_loss: 2359.8752\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1228.9797 - val_loss: 1242.8402\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1020.9184 - val_loss: 1307.4745\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 930.1287 - val_loss: 1019.9939\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 894.6420 - val_loss: 972.1177\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 860.7786 - val_loss: 1018.0520\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 838.4785 - val_loss: 991.7186\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 797.9507 - val_loss: 944.7786\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 730.1592 - val_loss: 825.8683\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 633.3293 - val_loss: 1154.8278\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 538.6636 - val_loss: 581.9672\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 388.0342 - val_loss: 438.7804\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 297.9792 - val_loss: 734.9822\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 224.4658 - val_loss: 381.7891\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 175.8615 - val_loss: 852.6252\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 142.2862 - val_loss: 357.2239\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 131.3954 - val_loss: 157.3158\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 86.9973 - val_loss: 135.8070\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 71.3198 - val_loss: 83.4068\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 55.0270 - val_loss: 71.8708\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 42.2426 - val_loss: 74.4191\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 51.2479 - val_loss: 72.9398\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 42.7570 - val_loss: 150.2582\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.2404 - val_loss: 26.7123\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.1569 - val_loss: 61.9635\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.6308 - val_loss: 53.0721\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.4455 - val_loss: 13.9398\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.9351 - val_loss: 128.1442\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 16.3265 - val_loss: 17.1136\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 17.9977 - val_loss: 82.7638\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.0931 - val_loss: 27.8639\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8652 - val_loss: 9.7457\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.6729 - val_loss: 31.7851\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.8728 - val_loss: 46.2632\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.0067 - val_loss: 24.4696\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 43.0549 - val_loss: 88.9384\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.9711 - val_loss: 6.8119\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.6765 - val_loss: 5.6362\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7591 - val_loss: 4.9677\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7070 - val_loss: 4.2534\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.2603 - val_loss: 3.1534\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.7275 - val_loss: 1785.7336\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.8666 - val_loss: 49.3074\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1052 - val_loss: 30.8852\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1361 - val_loss: 2.8375\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0110 - val_loss: 4.8845\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7320 - val_loss: 4.0234\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0636 - val_loss: 1.9579\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2735 - val_loss: 2.1307\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6627 - val_loss: 7.7162\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 97.8996 - val_loss: 96.6922\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8211 - val_loss: 6.7376\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3531 - val_loss: 4.5026\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4472 - val_loss: 2.0469\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8055 - val_loss: 3.0562\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6994 - val_loss: 1.8239\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7355 - val_loss: 3.1265\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9975 - val_loss: 12.9824\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.5253 - val_loss: 21.3352\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5550 - val_loss: 3.9297\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4324 - val_loss: 2.3468\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3285 - val_loss: 1.8867\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1776 - val_loss: 10.5270\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4049 - val_loss: 1.8828\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8169 - val_loss: 31.7893\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3137 - val_loss: 2.8006\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 47.0696 - val_loss: 23.6373\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1175 - val_loss: 1.9800\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4918 - val_loss: 1.3698\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0059 - val_loss: 0.7209\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8619 - val_loss: 1.1378\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9663 - val_loss: 1.1536\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8903 - val_loss: 1.2720\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9455 - val_loss: 1.3160\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8865 - val_loss: 1.7109\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8606 - val_loss: 1.1160\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 14.2297 - val_loss: 11.3653\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2422 - val_loss: 2.1698\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.1057 - val_loss: 56.1598\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2806 - val_loss: 4.0959\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8387 - val_loss: 0.5783\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6365 - val_loss: 0.5097\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7299 - val_loss: 0.9102\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6344 - val_loss: 0.9184\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7861 - val_loss: 2.3144\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8880 - val_loss: 2.7798\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8835 - val_loss: 0.8121\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1259 - val_loss: 1.9894\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3503 - val_loss: 6.3545\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2156 - val_loss: 1.9156\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8493 - val_loss: 2.4861\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 28.8482 - val_loss: 230.7186\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0828 - val_loss: 0.9811\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8979 - val_loss: 0.5596\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6466 - val_loss: 0.5551\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5164 - val_loss: 0.5810\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4594 - val_loss: 0.5244\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4667 - val_loss: 0.6812\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4355 - val_loss: 0.4595\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0451 - val_loss: 0.6657\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4531 - val_loss: 0.4207\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5098 - val_loss: 1.3981\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8535 - val_loss: 1.3240\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7515 - val_loss: 1.2196\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8335 - val_loss: 1.1124\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6769 - val_loss: 2.3292\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7021 - val_loss: 1.2986\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3699 - val_loss: 8.6420\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9751 - val_loss: 1.8596\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7971 - val_loss: 2.3090\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.7100 - val_loss: 435.0617\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.2614 - val_loss: 0.7588\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5041 - val_loss: 1.8294\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4326 - val_loss: 0.6816\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3420 - val_loss: 0.6011\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3555 - val_loss: 0.6498\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7393 - val_loss: 342.5297\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 30.1534 - val_loss: 7.0109\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8982 - val_loss: 0.5516\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6523 - val_loss: 0.7067\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4709 - val_loss: 0.3439\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4217 - val_loss: 0.4354\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3570 - val_loss: 0.3967\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3361 - val_loss: 0.2874\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3291 - val_loss: 0.2709\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3238 - val_loss: 0.4745\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3445 - val_loss: 0.4563\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3710 - val_loss: 0.5313\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3325 - val_loss: 0.4002\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4457 - val_loss: 0.6888\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4765 - val_loss: 0.9558\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6246 - val_loss: 1.2527\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6133 - val_loss: 1.1520\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9532 - val_loss: 15.1025\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5744 - val_loss: 4.4640\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8504 - val_loss: 9.0026\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6854 - val_loss: 0.8166\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4926 - val_loss: 1.6787\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7163 - val_loss: 13.0178\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9119 - val_loss: 15.0811\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.4518 - val_loss: 388.9960\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0965 - val_loss: 0.5665\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3966 - val_loss: 0.3043\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2933 - val_loss: 0.2290\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2332 - val_loss: 0.2089\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2290 - val_loss: 0.2361\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2633 - val_loss: 0.2835\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2720 - val_loss: 0.2301\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2210 - val_loss: 0.2091\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2314 - val_loss: 0.3211\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2316 - val_loss: 0.2525\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2986 - val_loss: 0.4994\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3342 - val_loss: 0.6600\n",
      "Epoch 159/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3352 - val_loss: 0.9815\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4671 - val_loss: 1.2888\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5677 - val_loss: 0.6831\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4130 - val_loss: 1.5352\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7009 - val_loss: 1.4959\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7164 - val_loss: 0.8324\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4422 - val_loss: 0.4298\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5868 - val_loss: 4.0006\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6851 - val_loss: 1.6766\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4772 - val_loss: 0.9912\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4219 - val_loss: 2.5514\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2237 - val_loss: 882.5427\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.1679 - val_loss: 0.5986\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3533 - val_loss: 0.3389\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2474 - val_loss: 0.3114\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1976 - val_loss: 0.2996\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1748 - val_loss: 0.1394\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1661 - val_loss: 0.1687\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1524 - val_loss: 0.2151\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1606 - val_loss: 0.2120\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1707 - val_loss: 0.1909\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1932 - val_loss: 0.3101\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2554 - val_loss: 1.7964\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2557 - val_loss: 0.3958\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2518 - val_loss: 0.4945\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4815 - val_loss: 0.9638\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3123 - val_loss: 1.6699\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3777 - val_loss: 0.6095\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2381 - val_loss: 0.3118\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2150 - val_loss: 0.3437\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3954 - val_loss: 2.0945\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3329 - val_loss: 1.1494\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5400 - val_loss: 1.0111\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4598 - val_loss: 3.7408\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4969 - val_loss: 21.4480\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 25.4789 - val_loss: 9.5826\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7028 - val_loss: 0.3968\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3072 - val_loss: 0.2524\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2142 - val_loss: 0.1785\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1862 - val_loss: 0.1721\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1530 - val_loss: 0.1795\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1397 - val_loss: 0.1555\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.15543278945693528\n",
      "Mean Absolute Error (MAE): 0.2964135553071918\n",
      "Root Mean Squared Error (RMSE): 0.39424965371821863\n",
      "Time taken: 629.4827275276184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_3740\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  1.825745  0.963988  1.351201  629.657665\n",
      "1        2  0.523350  0.543778  0.723429  620.481400\n",
      "2        3  0.494499  0.521215  0.703206  428.364045\n",
      "3        4  0.245865  0.373185  0.495848  627.666290\n",
      "4        5  0.155433  0.296414  0.394250  629.482728\n",
      "5  Average  0.648978  0.539716  0.733587  587.130426\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_Reg3.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_Reg3.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_Reg3.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
