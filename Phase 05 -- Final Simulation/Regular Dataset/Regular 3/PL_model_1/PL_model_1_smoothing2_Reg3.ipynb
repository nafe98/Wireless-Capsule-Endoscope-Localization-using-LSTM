{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217.774599</td>\n",
       "      <td>242.930668</td>\n",
       "      <td>180.930066</td>\n",
       "      <td>209.165430</td>\n",
       "      <td>263.740760</td>\n",
       "      <td>282.804639</td>\n",
       "      <td>234.185665</td>\n",
       "      <td>251.670782</td>\n",
       "      <td>225.920056</td>\n",
       "      <td>236.535232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.275029</td>\n",
       "      <td>198.288288</td>\n",
       "      <td>226.977928</td>\n",
       "      <td>188.625021</td>\n",
       "      <td>269.387212</td>\n",
       "      <td>240.345304</td>\n",
       "      <td>199.864666</td>\n",
       "      <td>222.641881</td>\n",
       "      <td>152.935187</td>\n",
       "      <td>181.772174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.170207</td>\n",
       "      <td>243.162352</td>\n",
       "      <td>181.188283</td>\n",
       "      <td>209.229425</td>\n",
       "      <td>263.645224</td>\n",
       "      <td>282.566208</td>\n",
       "      <td>233.849222</td>\n",
       "      <td>251.399776</td>\n",
       "      <td>226.070043</td>\n",
       "      <td>236.605439</td>\n",
       "      <td>...</td>\n",
       "      <td>206.327307</td>\n",
       "      <td>198.312659</td>\n",
       "      <td>227.204584</td>\n",
       "      <td>188.755642</td>\n",
       "      <td>269.186951</td>\n",
       "      <td>239.957409</td>\n",
       "      <td>199.826290</td>\n",
       "      <td>222.569317</td>\n",
       "      <td>152.659342</td>\n",
       "      <td>181.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.566640</td>\n",
       "      <td>243.391827</td>\n",
       "      <td>181.448150</td>\n",
       "      <td>209.293887</td>\n",
       "      <td>263.546981</td>\n",
       "      <td>282.329663</td>\n",
       "      <td>233.513263</td>\n",
       "      <td>251.127383</td>\n",
       "      <td>226.217667</td>\n",
       "      <td>236.678232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.381960</td>\n",
       "      <td>198.338475</td>\n",
       "      <td>227.430371</td>\n",
       "      <td>188.887730</td>\n",
       "      <td>268.984770</td>\n",
       "      <td>239.570115</td>\n",
       "      <td>199.788536</td>\n",
       "      <td>222.498237</td>\n",
       "      <td>152.383094</td>\n",
       "      <td>181.396746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.963998</td>\n",
       "      <td>243.619173</td>\n",
       "      <td>181.709370</td>\n",
       "      <td>209.359166</td>\n",
       "      <td>263.446095</td>\n",
       "      <td>282.095104</td>\n",
       "      <td>233.177442</td>\n",
       "      <td>250.853437</td>\n",
       "      <td>226.363208</td>\n",
       "      <td>236.753423</td>\n",
       "      <td>...</td>\n",
       "      <td>206.438990</td>\n",
       "      <td>198.365888</td>\n",
       "      <td>227.655468</td>\n",
       "      <td>189.021122</td>\n",
       "      <td>268.780495</td>\n",
       "      <td>239.183495</td>\n",
       "      <td>199.751706</td>\n",
       "      <td>222.428767</td>\n",
       "      <td>152.106454</td>\n",
       "      <td>181.212740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.362447</td>\n",
       "      <td>243.844582</td>\n",
       "      <td>181.971595</td>\n",
       "      <td>209.425648</td>\n",
       "      <td>263.342829</td>\n",
       "      <td>281.862540</td>\n",
       "      <td>232.841279</td>\n",
       "      <td>250.577746</td>\n",
       "      <td>226.507030</td>\n",
       "      <td>236.830717</td>\n",
       "      <td>...</td>\n",
       "      <td>206.498603</td>\n",
       "      <td>198.394939</td>\n",
       "      <td>227.880017</td>\n",
       "      <td>189.155630</td>\n",
       "      <td>268.573997</td>\n",
       "      <td>238.797611</td>\n",
       "      <td>199.715905</td>\n",
       "      <td>222.361175</td>\n",
       "      <td>151.829327</td>\n",
       "      <td>181.031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.842192</td>\n",
       "      <td>225.493872</td>\n",
       "      <td>186.161293</td>\n",
       "      <td>168.746854</td>\n",
       "      <td>290.125966</td>\n",
       "      <td>278.359253</td>\n",
       "      <td>251.284803</td>\n",
       "      <td>238.057134</td>\n",
       "      <td>234.084081</td>\n",
       "      <td>227.072033</td>\n",
       "      <td>...</td>\n",
       "      <td>208.063014</td>\n",
       "      <td>218.150333</td>\n",
       "      <td>227.500042</td>\n",
       "      <td>171.740167</td>\n",
       "      <td>279.291011</td>\n",
       "      <td>241.091078</td>\n",
       "      <td>226.577232</td>\n",
       "      <td>211.259677</td>\n",
       "      <td>167.182356</td>\n",
       "      <td>144.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.838339</td>\n",
       "      <td>225.193644</td>\n",
       "      <td>186.158236</td>\n",
       "      <td>168.801166</td>\n",
       "      <td>290.099684</td>\n",
       "      <td>278.240476</td>\n",
       "      <td>251.416575</td>\n",
       "      <td>238.186267</td>\n",
       "      <td>233.905906</td>\n",
       "      <td>227.057967</td>\n",
       "      <td>...</td>\n",
       "      <td>207.847027</td>\n",
       "      <td>218.029800</td>\n",
       "      <td>227.372573</td>\n",
       "      <td>171.813916</td>\n",
       "      <td>279.132031</td>\n",
       "      <td>241.322583</td>\n",
       "      <td>226.512762</td>\n",
       "      <td>211.122695</td>\n",
       "      <td>167.361938</td>\n",
       "      <td>144.194156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.835359</td>\n",
       "      <td>224.890461</td>\n",
       "      <td>186.152840</td>\n",
       "      <td>168.856330</td>\n",
       "      <td>290.072086</td>\n",
       "      <td>278.120520</td>\n",
       "      <td>251.546598</td>\n",
       "      <td>238.314937</td>\n",
       "      <td>233.725835</td>\n",
       "      <td>227.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>207.630922</td>\n",
       "      <td>217.908241</td>\n",
       "      <td>227.248109</td>\n",
       "      <td>171.889770</td>\n",
       "      <td>278.973606</td>\n",
       "      <td>241.555246</td>\n",
       "      <td>226.448731</td>\n",
       "      <td>210.986402</td>\n",
       "      <td>167.540383</td>\n",
       "      <td>144.375144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.833573</td>\n",
       "      <td>224.584238</td>\n",
       "      <td>186.145056</td>\n",
       "      <td>168.912353</td>\n",
       "      <td>290.043592</td>\n",
       "      <td>277.999301</td>\n",
       "      <td>251.674989</td>\n",
       "      <td>238.443174</td>\n",
       "      <td>233.543693</td>\n",
       "      <td>227.032526</td>\n",
       "      <td>...</td>\n",
       "      <td>207.414766</td>\n",
       "      <td>217.785837</td>\n",
       "      <td>227.126460</td>\n",
       "      <td>171.968173</td>\n",
       "      <td>278.815774</td>\n",
       "      <td>241.789214</td>\n",
       "      <td>226.385133</td>\n",
       "      <td>210.850667</td>\n",
       "      <td>167.717528</td>\n",
       "      <td>144.556286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.833291</td>\n",
       "      <td>224.274990</td>\n",
       "      <td>186.134915</td>\n",
       "      <td>168.969321</td>\n",
       "      <td>290.014623</td>\n",
       "      <td>277.876785</td>\n",
       "      <td>251.801863</td>\n",
       "      <td>238.570923</td>\n",
       "      <td>233.359245</td>\n",
       "      <td>227.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>207.198785</td>\n",
       "      <td>217.662767</td>\n",
       "      <td>227.007411</td>\n",
       "      <td>172.049382</td>\n",
       "      <td>278.658692</td>\n",
       "      <td>242.024537</td>\n",
       "      <td>226.321951</td>\n",
       "      <td>210.715291</td>\n",
       "      <td>167.893351</td>\n",
       "      <td>144.737845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     217.774599  242.930668  180.930066  209.165430  263.740760  282.804639   \n",
       "1     218.170207  243.162352  181.188283  209.229425  263.645224  282.566208   \n",
       "2     218.566640  243.391827  181.448150  209.293887  263.546981  282.329663   \n",
       "3     218.963998  243.619173  181.709370  209.359166  263.446095  282.095104   \n",
       "4     219.362447  243.844582  181.971595  209.425648  263.342829  281.862540   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.842192  225.493872  186.161293  168.746854  290.125966  278.359253   \n",
       "2439  240.838339  225.193644  186.158236  168.801166  290.099684  278.240476   \n",
       "2440  240.835359  224.890461  186.152840  168.856330  290.072086  278.120520   \n",
       "2441  240.833573  224.584238  186.145056  168.912353  290.043592  277.999301   \n",
       "2442  240.833291  224.274990  186.134915  168.969321  290.014623  277.876785   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     234.185665  251.670782  225.920056  236.535232  ...  206.275029   \n",
       "1     233.849222  251.399776  226.070043  236.605439  ...  206.327307   \n",
       "2     233.513263  251.127383  226.217667  236.678232  ...  206.381960   \n",
       "3     233.177442  250.853437  226.363208  236.753423  ...  206.438990   \n",
       "4     232.841279  250.577746  226.507030  236.830717  ...  206.498603   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  251.284803  238.057134  234.084081  227.072033  ...  208.063014   \n",
       "2439  251.416575  238.186267  233.905906  227.057967  ...  207.847027   \n",
       "2440  251.546598  238.314937  233.725835  227.044989  ...  207.630922   \n",
       "2441  251.674989  238.443174  233.543693  227.032526  ...  207.414766   \n",
       "2442  251.801863  238.570923  233.359245  227.020120  ...  207.198785   \n",
       "\n",
       "              39          40          41          42          43          44  \\\n",
       "0     198.288288  226.977928  188.625021  269.387212  240.345304  199.864666   \n",
       "1     198.312659  227.204584  188.755642  269.186951  239.957409  199.826290   \n",
       "2     198.338475  227.430371  188.887730  268.984770  239.570115  199.788536   \n",
       "3     198.365888  227.655468  189.021122  268.780495  239.183495  199.751706   \n",
       "4     198.394939  227.880017  189.155630  268.573997  238.797611  199.715905   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  218.150333  227.500042  171.740167  279.291011  241.091078  226.577232   \n",
       "2439  218.029800  227.372573  171.813916  279.132031  241.322583  226.512762   \n",
       "2440  217.908241  227.248109  171.889770  278.973606  241.555246  226.448731   \n",
       "2441  217.785837  227.126460  171.968173  278.815774  241.789214  226.385133   \n",
       "2442  217.662767  227.007411  172.049382  278.658692  242.024537  226.321951   \n",
       "\n",
       "              45          46          47  \n",
       "0     222.641881  152.935187  181.772174  \n",
       "1     222.569317  152.659342  181.583242  \n",
       "2     222.498237  152.383094  181.396746  \n",
       "3     222.428767  152.106454  181.212740  \n",
       "4     222.361175  151.829327  181.031371  \n",
       "...          ...         ...         ...  \n",
       "2438  211.259677  167.182356  144.012906  \n",
       "2439  211.122695  167.361938  144.194156  \n",
       "2440  210.986402  167.540383  144.375144  \n",
       "2441  210.850667  167.717528  144.556286  \n",
       "2442  210.715291  167.893351  144.737845  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg2_3.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1       2\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217.774599</td>\n",
       "      <td>242.930668</td>\n",
       "      <td>180.930066</td>\n",
       "      <td>209.165430</td>\n",
       "      <td>263.740760</td>\n",
       "      <td>282.804639</td>\n",
       "      <td>234.185665</td>\n",
       "      <td>251.670782</td>\n",
       "      <td>225.920056</td>\n",
       "      <td>236.535232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.275029</td>\n",
       "      <td>198.288288</td>\n",
       "      <td>226.977928</td>\n",
       "      <td>188.625021</td>\n",
       "      <td>269.387212</td>\n",
       "      <td>240.345304</td>\n",
       "      <td>199.864666</td>\n",
       "      <td>222.641881</td>\n",
       "      <td>152.935187</td>\n",
       "      <td>181.772174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.170207</td>\n",
       "      <td>243.162352</td>\n",
       "      <td>181.188283</td>\n",
       "      <td>209.229425</td>\n",
       "      <td>263.645224</td>\n",
       "      <td>282.566208</td>\n",
       "      <td>233.849222</td>\n",
       "      <td>251.399776</td>\n",
       "      <td>226.070043</td>\n",
       "      <td>236.605439</td>\n",
       "      <td>...</td>\n",
       "      <td>206.327307</td>\n",
       "      <td>198.312659</td>\n",
       "      <td>227.204584</td>\n",
       "      <td>188.755642</td>\n",
       "      <td>269.186951</td>\n",
       "      <td>239.957409</td>\n",
       "      <td>199.826290</td>\n",
       "      <td>222.569317</td>\n",
       "      <td>152.659342</td>\n",
       "      <td>181.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.566640</td>\n",
       "      <td>243.391827</td>\n",
       "      <td>181.448150</td>\n",
       "      <td>209.293887</td>\n",
       "      <td>263.546981</td>\n",
       "      <td>282.329663</td>\n",
       "      <td>233.513263</td>\n",
       "      <td>251.127383</td>\n",
       "      <td>226.217667</td>\n",
       "      <td>236.678232</td>\n",
       "      <td>...</td>\n",
       "      <td>206.381960</td>\n",
       "      <td>198.338475</td>\n",
       "      <td>227.430371</td>\n",
       "      <td>188.887730</td>\n",
       "      <td>268.984770</td>\n",
       "      <td>239.570115</td>\n",
       "      <td>199.788536</td>\n",
       "      <td>222.498237</td>\n",
       "      <td>152.383094</td>\n",
       "      <td>181.396746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.963998</td>\n",
       "      <td>243.619173</td>\n",
       "      <td>181.709370</td>\n",
       "      <td>209.359166</td>\n",
       "      <td>263.446095</td>\n",
       "      <td>282.095104</td>\n",
       "      <td>233.177442</td>\n",
       "      <td>250.853437</td>\n",
       "      <td>226.363208</td>\n",
       "      <td>236.753423</td>\n",
       "      <td>...</td>\n",
       "      <td>206.438990</td>\n",
       "      <td>198.365888</td>\n",
       "      <td>227.655468</td>\n",
       "      <td>189.021122</td>\n",
       "      <td>268.780495</td>\n",
       "      <td>239.183495</td>\n",
       "      <td>199.751706</td>\n",
       "      <td>222.428767</td>\n",
       "      <td>152.106454</td>\n",
       "      <td>181.212740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.362447</td>\n",
       "      <td>243.844582</td>\n",
       "      <td>181.971595</td>\n",
       "      <td>209.425648</td>\n",
       "      <td>263.342829</td>\n",
       "      <td>281.862540</td>\n",
       "      <td>232.841279</td>\n",
       "      <td>250.577746</td>\n",
       "      <td>226.507030</td>\n",
       "      <td>236.830717</td>\n",
       "      <td>...</td>\n",
       "      <td>206.498603</td>\n",
       "      <td>198.394939</td>\n",
       "      <td>227.880017</td>\n",
       "      <td>189.155630</td>\n",
       "      <td>268.573997</td>\n",
       "      <td>238.797611</td>\n",
       "      <td>199.715905</td>\n",
       "      <td>222.361175</td>\n",
       "      <td>151.829327</td>\n",
       "      <td>181.031371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.842192</td>\n",
       "      <td>225.493872</td>\n",
       "      <td>186.161293</td>\n",
       "      <td>168.746854</td>\n",
       "      <td>290.125966</td>\n",
       "      <td>278.359253</td>\n",
       "      <td>251.284803</td>\n",
       "      <td>238.057134</td>\n",
       "      <td>234.084081</td>\n",
       "      <td>227.072033</td>\n",
       "      <td>...</td>\n",
       "      <td>208.063014</td>\n",
       "      <td>218.150333</td>\n",
       "      <td>227.500042</td>\n",
       "      <td>171.740167</td>\n",
       "      <td>279.291011</td>\n",
       "      <td>241.091078</td>\n",
       "      <td>226.577232</td>\n",
       "      <td>211.259677</td>\n",
       "      <td>167.182356</td>\n",
       "      <td>144.012906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.838339</td>\n",
       "      <td>225.193644</td>\n",
       "      <td>186.158236</td>\n",
       "      <td>168.801166</td>\n",
       "      <td>290.099684</td>\n",
       "      <td>278.240476</td>\n",
       "      <td>251.416575</td>\n",
       "      <td>238.186267</td>\n",
       "      <td>233.905906</td>\n",
       "      <td>227.057967</td>\n",
       "      <td>...</td>\n",
       "      <td>207.847027</td>\n",
       "      <td>218.029800</td>\n",
       "      <td>227.372573</td>\n",
       "      <td>171.813916</td>\n",
       "      <td>279.132031</td>\n",
       "      <td>241.322583</td>\n",
       "      <td>226.512762</td>\n",
       "      <td>211.122695</td>\n",
       "      <td>167.361938</td>\n",
       "      <td>144.194156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.835359</td>\n",
       "      <td>224.890461</td>\n",
       "      <td>186.152840</td>\n",
       "      <td>168.856330</td>\n",
       "      <td>290.072086</td>\n",
       "      <td>278.120520</td>\n",
       "      <td>251.546598</td>\n",
       "      <td>238.314937</td>\n",
       "      <td>233.725835</td>\n",
       "      <td>227.044989</td>\n",
       "      <td>...</td>\n",
       "      <td>207.630922</td>\n",
       "      <td>217.908241</td>\n",
       "      <td>227.248109</td>\n",
       "      <td>171.889770</td>\n",
       "      <td>278.973606</td>\n",
       "      <td>241.555246</td>\n",
       "      <td>226.448731</td>\n",
       "      <td>210.986402</td>\n",
       "      <td>167.540383</td>\n",
       "      <td>144.375144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.833573</td>\n",
       "      <td>224.584238</td>\n",
       "      <td>186.145056</td>\n",
       "      <td>168.912353</td>\n",
       "      <td>290.043592</td>\n",
       "      <td>277.999301</td>\n",
       "      <td>251.674989</td>\n",
       "      <td>238.443174</td>\n",
       "      <td>233.543693</td>\n",
       "      <td>227.032526</td>\n",
       "      <td>...</td>\n",
       "      <td>207.414766</td>\n",
       "      <td>217.785837</td>\n",
       "      <td>227.126460</td>\n",
       "      <td>171.968173</td>\n",
       "      <td>278.815774</td>\n",
       "      <td>241.789214</td>\n",
       "      <td>226.385133</td>\n",
       "      <td>210.850667</td>\n",
       "      <td>167.717528</td>\n",
       "      <td>144.556286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.833291</td>\n",
       "      <td>224.274990</td>\n",
       "      <td>186.134915</td>\n",
       "      <td>168.969321</td>\n",
       "      <td>290.014623</td>\n",
       "      <td>277.876785</td>\n",
       "      <td>251.801863</td>\n",
       "      <td>238.570923</td>\n",
       "      <td>233.359245</td>\n",
       "      <td>227.020120</td>\n",
       "      <td>...</td>\n",
       "      <td>207.198785</td>\n",
       "      <td>217.662767</td>\n",
       "      <td>227.007411</td>\n",
       "      <td>172.049382</td>\n",
       "      <td>278.658692</td>\n",
       "      <td>242.024537</td>\n",
       "      <td>226.321951</td>\n",
       "      <td>210.715291</td>\n",
       "      <td>167.893351</td>\n",
       "      <td>144.737845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     217.774599  242.930668  180.930066  209.165430  263.740760  282.804639   \n",
       "1     218.170207  243.162352  181.188283  209.229425  263.645224  282.566208   \n",
       "2     218.566640  243.391827  181.448150  209.293887  263.546981  282.329663   \n",
       "3     218.963998  243.619173  181.709370  209.359166  263.446095  282.095104   \n",
       "4     219.362447  243.844582  181.971595  209.425648  263.342829  281.862540   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.842192  225.493872  186.161293  168.746854  290.125966  278.359253   \n",
       "2439  240.838339  225.193644  186.158236  168.801166  290.099684  278.240476   \n",
       "2440  240.835359  224.890461  186.152840  168.856330  290.072086  278.120520   \n",
       "2441  240.833573  224.584238  186.145056  168.912353  290.043592  277.999301   \n",
       "2442  240.833291  224.274990  186.134915  168.969321  290.014623  277.876785   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     234.185665  251.670782  225.920056  236.535232  ...  206.275029   \n",
       "1     233.849222  251.399776  226.070043  236.605439  ...  206.327307   \n",
       "2     233.513263  251.127383  226.217667  236.678232  ...  206.381960   \n",
       "3     233.177442  250.853437  226.363208  236.753423  ...  206.438990   \n",
       "4     232.841279  250.577746  226.507030  236.830717  ...  206.498603   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  251.284803  238.057134  234.084081  227.072033  ...  208.063014   \n",
       "2439  251.416575  238.186267  233.905906  227.057967  ...  207.847027   \n",
       "2440  251.546598  238.314937  233.725835  227.044989  ...  207.630922   \n",
       "2441  251.674989  238.443174  233.543693  227.032526  ...  207.414766   \n",
       "2442  251.801863  238.570923  233.359245  227.020120  ...  207.198785   \n",
       "\n",
       "        sensor40    sensor41    sensor42    sensor43    sensor44    sensor45  \\\n",
       "0     198.288288  226.977928  188.625021  269.387212  240.345304  199.864666   \n",
       "1     198.312659  227.204584  188.755642  269.186951  239.957409  199.826290   \n",
       "2     198.338475  227.430371  188.887730  268.984770  239.570115  199.788536   \n",
       "3     198.365888  227.655468  189.021122  268.780495  239.183495  199.751706   \n",
       "4     198.394939  227.880017  189.155630  268.573997  238.797611  199.715905   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  218.150333  227.500042  171.740167  279.291011  241.091078  226.577232   \n",
       "2439  218.029800  227.372573  171.813916  279.132031  241.322583  226.512762   \n",
       "2440  217.908241  227.248109  171.889770  278.973606  241.555246  226.448731   \n",
       "2441  217.785837  227.126460  171.968173  278.815774  241.789214  226.385133   \n",
       "2442  217.662767  227.007411  172.049382  278.658692  242.024537  226.321951   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     222.641881  152.935187  181.772174  \n",
       "1     222.569317  152.659342  181.583242  \n",
       "2     222.498237  152.383094  181.396746  \n",
       "3     222.428767  152.106454  181.212740  \n",
       "4     222.361175  151.829327  181.031371  \n",
       "...          ...         ...         ...  \n",
       "2438  211.259677  167.182356  144.012906  \n",
       "2439  211.122695  167.361938  144.194156  \n",
       "2440  210.986402  167.540383  144.375144  \n",
       "2441  210.850667  167.717528  144.556286  \n",
       "2442  210.715291  167.893351  144.737845  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217.774599</td>\n",
       "      <td>242.930668</td>\n",
       "      <td>180.930066</td>\n",
       "      <td>209.165430</td>\n",
       "      <td>263.740760</td>\n",
       "      <td>282.804639</td>\n",
       "      <td>234.185665</td>\n",
       "      <td>251.670782</td>\n",
       "      <td>225.920056</td>\n",
       "      <td>236.535232</td>\n",
       "      <td>183.564676</td>\n",
       "      <td>195.973873</td>\n",
       "      <td>264.912572</td>\n",
       "      <td>276.966525</td>\n",
       "      <td>233.044251</td>\n",
       "      <td>242.307870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218.170207</td>\n",
       "      <td>243.162352</td>\n",
       "      <td>181.188283</td>\n",
       "      <td>209.229425</td>\n",
       "      <td>263.645224</td>\n",
       "      <td>282.566208</td>\n",
       "      <td>233.849222</td>\n",
       "      <td>251.399776</td>\n",
       "      <td>226.070043</td>\n",
       "      <td>236.605439</td>\n",
       "      <td>183.798584</td>\n",
       "      <td>196.168549</td>\n",
       "      <td>264.811639</td>\n",
       "      <td>276.635184</td>\n",
       "      <td>232.858123</td>\n",
       "      <td>242.065580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.566640</td>\n",
       "      <td>243.391827</td>\n",
       "      <td>181.448150</td>\n",
       "      <td>209.293887</td>\n",
       "      <td>263.546981</td>\n",
       "      <td>282.329663</td>\n",
       "      <td>233.513263</td>\n",
       "      <td>251.127383</td>\n",
       "      <td>226.217667</td>\n",
       "      <td>236.678232</td>\n",
       "      <td>184.030027</td>\n",
       "      <td>196.366635</td>\n",
       "      <td>264.711879</td>\n",
       "      <td>276.306067</td>\n",
       "      <td>232.673976</td>\n",
       "      <td>241.824102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218.963998</td>\n",
       "      <td>243.619173</td>\n",
       "      <td>181.709370</td>\n",
       "      <td>209.359166</td>\n",
       "      <td>263.446095</td>\n",
       "      <td>282.095104</td>\n",
       "      <td>233.177442</td>\n",
       "      <td>250.853437</td>\n",
       "      <td>226.363208</td>\n",
       "      <td>236.753423</td>\n",
       "      <td>184.258905</td>\n",
       "      <td>196.568125</td>\n",
       "      <td>264.613093</td>\n",
       "      <td>275.979488</td>\n",
       "      <td>232.491537</td>\n",
       "      <td>241.583727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>219.362447</td>\n",
       "      <td>243.844582</td>\n",
       "      <td>181.971595</td>\n",
       "      <td>209.425648</td>\n",
       "      <td>263.342829</td>\n",
       "      <td>281.862540</td>\n",
       "      <td>232.841279</td>\n",
       "      <td>250.577746</td>\n",
       "      <td>226.507030</td>\n",
       "      <td>236.830717</td>\n",
       "      <td>184.485218</td>\n",
       "      <td>196.772793</td>\n",
       "      <td>264.514959</td>\n",
       "      <td>275.655679</td>\n",
       "      <td>232.310236</td>\n",
       "      <td>241.344612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>240.842192</td>\n",
       "      <td>225.493872</td>\n",
       "      <td>186.161293</td>\n",
       "      <td>168.746854</td>\n",
       "      <td>290.125966</td>\n",
       "      <td>278.359253</td>\n",
       "      <td>251.284803</td>\n",
       "      <td>238.057134</td>\n",
       "      <td>234.084081</td>\n",
       "      <td>227.072033</td>\n",
       "      <td>179.059737</td>\n",
       "      <td>171.963089</td>\n",
       "      <td>286.548777</td>\n",
       "      <td>279.005660</td>\n",
       "      <td>243.505529</td>\n",
       "      <td>237.338586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>240.838339</td>\n",
       "      <td>225.193644</td>\n",
       "      <td>186.158236</td>\n",
       "      <td>168.801166</td>\n",
       "      <td>290.099684</td>\n",
       "      <td>278.240476</td>\n",
       "      <td>251.416575</td>\n",
       "      <td>238.186267</td>\n",
       "      <td>233.905906</td>\n",
       "      <td>227.057967</td>\n",
       "      <td>179.261030</td>\n",
       "      <td>172.125902</td>\n",
       "      <td>286.510363</td>\n",
       "      <td>278.936680</td>\n",
       "      <td>243.703772</td>\n",
       "      <td>237.472438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>240.835359</td>\n",
       "      <td>224.890461</td>\n",
       "      <td>186.152840</td>\n",
       "      <td>168.856330</td>\n",
       "      <td>290.072086</td>\n",
       "      <td>278.120520</td>\n",
       "      <td>251.546598</td>\n",
       "      <td>238.314937</td>\n",
       "      <td>233.725835</td>\n",
       "      <td>227.044989</td>\n",
       "      <td>179.461892</td>\n",
       "      <td>172.285884</td>\n",
       "      <td>286.470168</td>\n",
       "      <td>278.865475</td>\n",
       "      <td>243.903421</td>\n",
       "      <td>237.603481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>240.833573</td>\n",
       "      <td>224.584238</td>\n",
       "      <td>186.145056</td>\n",
       "      <td>168.912353</td>\n",
       "      <td>290.043592</td>\n",
       "      <td>277.999301</td>\n",
       "      <td>251.674989</td>\n",
       "      <td>238.443174</td>\n",
       "      <td>233.543693</td>\n",
       "      <td>227.032526</td>\n",
       "      <td>179.662373</td>\n",
       "      <td>172.443341</td>\n",
       "      <td>286.428153</td>\n",
       "      <td>278.792015</td>\n",
       "      <td>244.104302</td>\n",
       "      <td>237.731490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>240.833291</td>\n",
       "      <td>224.274990</td>\n",
       "      <td>186.134915</td>\n",
       "      <td>168.969321</td>\n",
       "      <td>290.014623</td>\n",
       "      <td>277.876785</td>\n",
       "      <td>251.801863</td>\n",
       "      <td>238.570923</td>\n",
       "      <td>233.359245</td>\n",
       "      <td>227.020120</td>\n",
       "      <td>179.862610</td>\n",
       "      <td>172.598410</td>\n",
       "      <td>286.384354</td>\n",
       "      <td>278.716168</td>\n",
       "      <td>244.306320</td>\n",
       "      <td>237.856379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     217.774599  242.930668  180.930066  209.165430  263.740760  282.804639   \n",
       "1     218.170207  243.162352  181.188283  209.229425  263.645224  282.566208   \n",
       "2     218.566640  243.391827  181.448150  209.293887  263.546981  282.329663   \n",
       "3     218.963998  243.619173  181.709370  209.359166  263.446095  282.095104   \n",
       "4     219.362447  243.844582  181.971595  209.425648  263.342829  281.862540   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  240.842192  225.493872  186.161293  168.746854  290.125966  278.359253   \n",
       "2439  240.838339  225.193644  186.158236  168.801166  290.099684  278.240476   \n",
       "2440  240.835359  224.890461  186.152840  168.856330  290.072086  278.120520   \n",
       "2441  240.833573  224.584238  186.145056  168.912353  290.043592  277.999301   \n",
       "2442  240.833291  224.274990  186.134915  168.969321  290.014623  277.876785   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10    sensor11    sensor12  \\\n",
       "0     234.185665  251.670782  225.920056  236.535232  183.564676  195.973873   \n",
       "1     233.849222  251.399776  226.070043  236.605439  183.798584  196.168549   \n",
       "2     233.513263  251.127383  226.217667  236.678232  184.030027  196.366635   \n",
       "3     233.177442  250.853437  226.363208  236.753423  184.258905  196.568125   \n",
       "4     232.841279  250.577746  226.507030  236.830717  184.485218  196.772793   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  251.284803  238.057134  234.084081  227.072033  179.059737  171.963089   \n",
       "2439  251.416575  238.186267  233.905906  227.057967  179.261030  172.125902   \n",
       "2440  251.546598  238.314937  233.725835  227.044989  179.461892  172.285884   \n",
       "2441  251.674989  238.443174  233.543693  227.032526  179.662373  172.443341   \n",
       "2442  251.801863  238.570923  233.359245  227.020120  179.862610  172.598410   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     264.912572  276.966525  233.044251  242.307870  \n",
       "1     264.811639  276.635184  232.858123  242.065580  \n",
       "2     264.711879  276.306067  232.673976  241.824102  \n",
       "3     264.613093  275.979488  232.491537  241.583727  \n",
       "4     264.514959  275.655679  232.310236  241.344612  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  286.548777  279.005660  243.505529  237.338586  \n",
       "2439  286.510363  278.936680  243.703772  237.472438  \n",
       "2440  286.470168  278.865475  243.903421  237.603481  \n",
       "2441  286.428153  278.792015  244.104302  237.731490  \n",
       "2442  286.384354  278.716168  244.306320  237.856379  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-75.968791</td>\n",
       "      <td>60.239368</td>\n",
       "      <td>-105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-75.314716</td>\n",
       "      <td>60.181623</td>\n",
       "      <td>-104.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.653109</td>\n",
       "      <td>60.131806</td>\n",
       "      <td>-104.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.984037</td>\n",
       "      <td>60.089935</td>\n",
       "      <td>-104.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.307567</td>\n",
       "      <td>60.056029</td>\n",
       "      <td>-104.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-99.899763</td>\n",
       "      <td>81.788725</td>\n",
       "      <td>65.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-99.939531</td>\n",
       "      <td>81.389997</td>\n",
       "      <td>65.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-99.969304</td>\n",
       "      <td>80.990713</td>\n",
       "      <td>65.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-99.989081</td>\n",
       "      <td>80.591032</td>\n",
       "      <td>65.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-99.998859</td>\n",
       "      <td>80.191116</td>\n",
       "      <td>65.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y   Pos Z\n",
       "0    -75.968791  60.239368 -105.00\n",
       "1    -75.314716  60.181623 -104.93\n",
       "2    -74.653109  60.131806 -104.86\n",
       "3    -73.984037  60.089935 -104.79\n",
       "4    -73.307567  60.056029 -104.72\n",
       "...         ...        ...     ...\n",
       "2438 -99.899763  81.788725   65.66\n",
       "2439 -99.939531  81.389997   65.73\n",
       "2440 -99.969304  80.990713   65.80\n",
       "2441 -99.989081  80.591032   65.87\n",
       "2442 -99.998859  80.191116   65.94\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 6s 10ms/step - loss: 3309.7883 - val_loss: 2610.6086\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2347.9849 - val_loss: 2144.0994\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1863.2850 - val_loss: 1636.1495\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1393.4233 - val_loss: 1185.7643\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1074.1671 - val_loss: 1011.7547\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 858.1964 - val_loss: 858.2172\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 718.4500 - val_loss: 743.8112\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 516.6716 - val_loss: 444.2455\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 295.2759 - val_loss: 229.1581\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 151.2160 - val_loss: 110.5671\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 77.0489 - val_loss: 55.3643\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 37.0681 - val_loss: 105.5999\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 24.7798 - val_loss: 31.4046\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.0876 - val_loss: 101.7055\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 11.2515 - val_loss: 13.2411\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.1517 - val_loss: 16.9371\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 4.9999 - val_loss: 5.5235\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.7319 - val_loss: 10.5540\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.7310 - val_loss: 3.9417\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.8314 - val_loss: 3.9465\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.5176 - val_loss: 2.8890\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.0012 - val_loss: 5.5936\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9289 - val_loss: 1.7859\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.8085 - val_loss: 18.0720\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.5486 - val_loss: 1.7228\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.4888 - val_loss: 14.9888\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 25.9350 - val_loss: 24.3171\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4079 - val_loss: 2.6410\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9534 - val_loss: 1.1610\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6489 - val_loss: 1.6028\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.0782 - val_loss: 96.2975\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6235 - val_loss: 1.4964\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9632 - val_loss: 1.3427\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4656 - val_loss: 1.2371\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4472 - val_loss: 4.2954\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4924 - val_loss: 1.7207\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7084 - val_loss: 4.7068\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.6400 - val_loss: 1.9297\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.6793 - val_loss: 41.5389\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3540 - val_loss: 1.8242\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1069 - val_loss: 1.1075\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9633 - val_loss: 0.9163\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1110 - val_loss: 3.0211\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1269 - val_loss: 1.1663\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9473 - val_loss: 2.1536\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1565 - val_loss: 2.0428\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9720 - val_loss: 1.4291\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9341 - val_loss: 1.4679\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.1622 - val_loss: 2.5171\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4385 - val_loss: 8.0299\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0792 - val_loss: 3.5159\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1988 - val_loss: 0.8129\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6295 - val_loss: 2.0582\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7004 - val_loss: 0.9565\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4411 - val_loss: 6.6824\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1627 - val_loss: 0.8075\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.8135 - val_loss: 1.5580\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6532 - val_loss: 1.9688\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3849 - val_loss: 7.1128\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1138 - val_loss: 0.9140\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5272 - val_loss: 0.4531\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4261 - val_loss: 0.6772\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6686 - val_loss: 0.8080\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4115 - val_loss: 1.8491\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7079 - val_loss: 0.6261\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8636 - val_loss: 1.7501\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9537 - val_loss: 0.5030\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7058 - val_loss: 1.9125\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8961 - val_loss: 0.9556\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.0015 - val_loss: 431.8249\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 5.7511 - val_loss: 0.3705\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4114 - val_loss: 0.4636\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2809 - val_loss: 0.3200\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3218 - val_loss: 0.3853\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3569 - val_loss: 0.9840\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4548 - val_loss: 0.2912\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4775 - val_loss: 1.9514\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5725 - val_loss: 1.8296\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5519 - val_loss: 0.3396\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6313 - val_loss: 1.2794\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5206 - val_loss: 0.6904\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.2853 - val_loss: 29.5178\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9544 - val_loss: 1.1158\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3750 - val_loss: 0.8115\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2765 - val_loss: 0.4427\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2805 - val_loss: 0.3704\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4351 - val_loss: 2.3311\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3166 - val_loss: 0.7264\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5575 - val_loss: 7.6867\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8331 - val_loss: 0.5742\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3700 - val_loss: 0.4270\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5077 - val_loss: 0.7535\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4612 - val_loss: 1.0319\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5342 - val_loss: 1.5633\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4169 - val_loss: 0.7756\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7410 - val_loss: 1.8114\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.6200 - val_loss: 8.1754\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4693 - val_loss: 0.7477\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2585 - val_loss: 0.5031\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1922 - val_loss: 0.3201\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2344 - val_loss: 0.4881\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2957 - val_loss: 0.7389\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2164 - val_loss: 0.2089\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2277 - val_loss: 0.7001\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3304 - val_loss: 0.5097\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3970 - val_loss: 1.3354\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4877 - val_loss: 1.0238\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4975 - val_loss: 0.8418\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4401 - val_loss: 2.3345\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4870 - val_loss: 0.5692\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6101 - val_loss: 2.1252\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.6825 - val_loss: 1.2457\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2499 - val_loss: 0.8974\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2568 - val_loss: 0.2086\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1681 - val_loss: 0.1445\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1586 - val_loss: 0.1767\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2012 - val_loss: 0.4718\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2262 - val_loss: 0.8382\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3556 - val_loss: 1.2617\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3706 - val_loss: 0.3986\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4237 - val_loss: 0.8084\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9936 - val_loss: 0.3950\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3260 - val_loss: 0.6020\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2510 - val_loss: 0.3871\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2250 - val_loss: 0.3357\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3535 - val_loss: 2.0846\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5302 - val_loss: 2.3629\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6800 - val_loss: 0.2535\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3579 - val_loss: 0.9329\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2772 - val_loss: 1.0248\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5952 - val_loss: 1.9666\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.4806 - val_loss: 11.3811\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8973 - val_loss: 3.3769\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2667 - val_loss: 0.5473\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2068 - val_loss: 0.2111\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1776 - val_loss: 0.1792\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4464 - val_loss: 29.9870\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3210 - val_loss: 0.4349\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1908 - val_loss: 0.2660\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1271 - val_loss: 0.1182\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2671 - val_loss: 0.7229\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3339 - val_loss: 0.5420\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2337 - val_loss: 0.4397\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2177 - val_loss: 0.5074\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2403 - val_loss: 0.5994\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3249 - val_loss: 1.1138\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4417 - val_loss: 0.4642\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2930 - val_loss: 0.3674\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2615 - val_loss: 0.5843\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3446 - val_loss: 0.8949\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3625 - val_loss: 0.3944\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4614 - val_loss: 1.1961\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3883 - val_loss: 1.0073\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2631 - val_loss: 0.3478\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3317 - val_loss: 2.2938\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3941 - val_loss: 1.0708\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2134 - val_loss: 0.3792\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2788 - val_loss: 1.2507\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3052 - val_loss: 0.6289\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3354 - val_loss: 1.9039\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2569 - val_loss: 0.7900\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5540 - val_loss: 278.3115\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 5.2343 - val_loss: 0.7644\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2026 - val_loss: 0.3228\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1115 - val_loss: 0.1714\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1117 - val_loss: 0.0780\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0926 - val_loss: 0.1438\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1104 - val_loss: 0.1258\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1015 - val_loss: 0.2639\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1300 - val_loss: 0.1588\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2116 - val_loss: 0.4002\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2805 - val_loss: 1.5603\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2399 - val_loss: 0.2689\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1780 - val_loss: 0.1959\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2008 - val_loss: 1.5169\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3606 - val_loss: 0.9316\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3228 - val_loss: 1.3896\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2677 - val_loss: 3.9419\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.0181 - val_loss: 0.4809\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1848 - val_loss: 0.2124\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1056 - val_loss: 0.1869\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.0858 - val_loss: 0.1075\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0826 - val_loss: 0.1196\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.0744 - val_loss: 0.1458\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1034 - val_loss: 0.4214\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1305 - val_loss: 0.3926\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1900 - val_loss: 0.2361\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2125 - val_loss: 0.1336\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1820 - val_loss: 0.3724\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1580 - val_loss: 0.4521\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3584 - val_loss: 0.1506\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2137 - val_loss: 0.4343\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2746 - val_loss: 0.5191\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4438 - val_loss: 0.3108\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1123 - val_loss: 0.1471\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1884 - val_loss: 1.3914\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.07789890775021836\n",
      "Mean Absolute Error (MAE): 0.1870803401448169\n",
      "Root Mean Squared Error (RMSE): 0.27910375803671716\n",
      "Time taken: 542.979799747467\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 3398.9368 - val_loss: 2642.3879\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2505.2119 - val_loss: 2227.3821\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2069.6362 - val_loss: 1754.3362\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1561.1528 - val_loss: 1322.6693\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1181.8048 - val_loss: 1022.9736\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 971.0473 - val_loss: 979.9822\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 820.0311 - val_loss: 768.9102\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 666.2809 - val_loss: 536.4147\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 434.4893 - val_loss: 292.3916\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 228.0733 - val_loss: 171.6145\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 115.4487 - val_loss: 105.5457\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 58.2054 - val_loss: 36.4708\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 28.3707 - val_loss: 26.7417\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 20.2944 - val_loss: 12.6646\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 10.5593 - val_loss: 6.3601\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.9906 - val_loss: 19.2117\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 8.3132 - val_loss: 6.5623\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.8994 - val_loss: 7.8318\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.2212 - val_loss: 25.1915\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.0945 - val_loss: 4.9531\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8341 - val_loss: 2.5304\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.6431 - val_loss: 6.0040\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2413 - val_loss: 3.2794\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.3104 - val_loss: 45.8031\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.0386 - val_loss: 3.6603\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.6181 - val_loss: 4.0975\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7295 - val_loss: 1.6764\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 12.7618 - val_loss: 4.6950\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.2132 - val_loss: 4.5805\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9287 - val_loss: 2.2837\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4768 - val_loss: 1.6560\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.4723 - val_loss: 7.9649\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4005 - val_loss: 1.8081\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9476 - val_loss: 2.4607\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.0786 - val_loss: 25.7024\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.9601 - val_loss: 0.9222\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9921 - val_loss: 1.0489\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.3090 - val_loss: 1.4552\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1870 - val_loss: 1.7023\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9983 - val_loss: 0.7887\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1681 - val_loss: 1.2660\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4111 - val_loss: 1.2744\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.4344 - val_loss: 3.6792\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0623 - val_loss: 1.6678\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9753 - val_loss: 1.7945\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.9117 - val_loss: 66.9083\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8439 - val_loss: 1.0382\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9075 - val_loss: 0.9852\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8887 - val_loss: 0.8630\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8083 - val_loss: 1.4092\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7391 - val_loss: 1.2620\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9373 - val_loss: 1.2979\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9707 - val_loss: 1.1445\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7534 - val_loss: 16.2958\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3014 - val_loss: 2.0829\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1261 - val_loss: 0.7308\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5282 - val_loss: 1.0390\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6016 - val_loss: 1.2372\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7619 - val_loss: 0.7243\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8473 - val_loss: 0.9148\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.9977 - val_loss: 268.4420\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.5432 - val_loss: 1.4653\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5049 - val_loss: 0.4436\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5781 - val_loss: 0.8724\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7388 - val_loss: 3.6214\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 13.2594 - val_loss: 5.9209\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5199 - val_loss: 0.5972\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8740 - val_loss: 3.2929\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6702 - val_loss: 0.7255\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5737 - val_loss: 0.9197\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6256 - val_loss: 1.5347\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8382 - val_loss: 8.3069\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9731 - val_loss: 1.5431\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5894 - val_loss: 0.9740\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4050 - val_loss: 0.6736\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5736 - val_loss: 0.8433\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6010 - val_loss: 0.6449\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.5364 - val_loss: 1.9451\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6291 - val_loss: 1.2035\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5057 - val_loss: 0.7683\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9776 - val_loss: 3.0830\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9228 - val_loss: 1.5243\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9420 - val_loss: 1.1264\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6543 - val_loss: 2.1215\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8543 - val_loss: 0.5620\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.1455 - val_loss: 1.2550\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3570 - val_loss: 0.4470\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2752 - val_loss: 0.5372\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2994 - val_loss: 0.3673\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3991 - val_loss: 1.6104\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3709 - val_loss: 0.3470\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5505 - val_loss: 0.5541\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4031 - val_loss: 2.1487\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5751 - val_loss: 0.8115\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.8645 - val_loss: 17.7890\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7785 - val_loss: 1.1720\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3376 - val_loss: 0.4170\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2564 - val_loss: 0.2791\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3005 - val_loss: 0.4698\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2518 - val_loss: 0.6351\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3435 - val_loss: 0.4358\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3341 - val_loss: 0.6125\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6891 - val_loss: 9.8869\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.6991 - val_loss: 2.1294\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2642 - val_loss: 0.2228\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2335 - val_loss: 0.1863\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1913 - val_loss: 0.6767\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2087 - val_loss: 0.3636\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2982 - val_loss: 0.3238\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4123 - val_loss: 0.9162\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4381 - val_loss: 0.6816\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2642 - val_loss: 0.4144\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4884 - val_loss: 0.4117\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5991 - val_loss: 1.6884\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7865 - val_loss: 1.3607\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5267 - val_loss: 0.9212\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5881 - val_loss: 0.7142\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2918 - val_loss: 0.3153\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 1.0463\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3261 - val_loss: 0.6859\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7650 - val_loss: 0.7025\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.9724 - val_loss: 15.8954\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2509 - val_loss: 0.3911\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1771 - val_loss: 0.3848\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1707 - val_loss: 0.4462\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2572 - val_loss: 0.6230\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2722 - val_loss: 0.3037\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3639 - val_loss: 0.2695\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3008 - val_loss: 0.6565\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5342 - val_loss: 1.7013\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 6ms/step - loss: 0.4371 - val_loss: 0.3683\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 8.0319 - val_loss: 20.3307\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2530 - val_loss: 0.5341\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2095 - val_loss: 0.2871\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1735 - val_loss: 0.2836\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1577 - val_loss: 0.2222\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.18629393452922757\n",
      "Mean Absolute Error (MAE): 0.3286902081226993\n",
      "Root Mean Squared Error (RMSE): 0.4316178107182645\n",
      "Time taken: 366.61473298072815\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 3464.2852 - val_loss: 2794.9316\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2487.5095 - val_loss: 2333.3127\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2057.2078 - val_loss: 1896.9738\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1615.7548 - val_loss: 1485.2587\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1256.2616 - val_loss: 1194.0409\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1032.3944 - val_loss: 1008.6855\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 853.0137 - val_loss: 1004.4636\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 674.7071 - val_loss: 629.9448\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 443.3773 - val_loss: 352.6905\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 240.0589 - val_loss: 172.3735\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 112.9479 - val_loss: 76.1522\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 53.4275 - val_loss: 38.8908\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 24.9371 - val_loss: 21.7797\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.4438 - val_loss: 14.1137\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.1604 - val_loss: 21.5852\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.5198 - val_loss: 5.7057\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 12.4818 - val_loss: 5.2456\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.6961 - val_loss: 10.2607\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.4590 - val_loss: 3.1051\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3332 - val_loss: 13.2469\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6491 - val_loss: 3.9634\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8119 - val_loss: 4.7682\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.2330 - val_loss: 1.9639\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4694 - val_loss: 33.1106\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6027 - val_loss: 2.4270\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6144 - val_loss: 4.5924\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.9909 - val_loss: 10.0828\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7765 - val_loss: 1.3893\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.7207 - val_loss: 9.6295\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3033 - val_loss: 2.8970\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3542 - val_loss: 0.9042\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2597 - val_loss: 7.0413\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0469 - val_loss: 1.6037\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3303 - val_loss: 2.5719\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.2066 - val_loss: 0.9960\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0609 - val_loss: 1.6942\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1108 - val_loss: 3.8727\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.1422 - val_loss: 5.9858\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6291 - val_loss: 1.7653\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.1915 - val_loss: 1.3601\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8752 - val_loss: 0.8199\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.0073 - val_loss: 0.6943\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8206 - val_loss: 1.4846\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0218 - val_loss: 1.9775\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8074 - val_loss: 1.1038\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2626 - val_loss: 2.3364\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8180 - val_loss: 2.6915\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.4286 - val_loss: 3.6282\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0786 - val_loss: 1.7443\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1709 - val_loss: 5.4041\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.4452 - val_loss: 2.8692\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0844 - val_loss: 2.1531\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7875 - val_loss: 1.1793\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7896 - val_loss: 0.9007\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5793 - val_loss: 0.4318\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6658 - val_loss: 0.9315\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9383 - val_loss: 1.8467\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8806 - val_loss: 1.8366\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9670 - val_loss: 166.8486\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 11.6452 - val_loss: 4.4304\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9038 - val_loss: 0.9255\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6249 - val_loss: 1.6947\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5218 - val_loss: 0.5039\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5295 - val_loss: 1.4949\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5716 - val_loss: 1.3205\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6125 - val_loss: 0.6546\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7186 - val_loss: 1.5663\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6434 - val_loss: 1.5313\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8430 - val_loss: 0.5504\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8351 - val_loss: 2.0626\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1167 - val_loss: 1.8075\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7118 - val_loss: 1.8460\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8707 - val_loss: 0.9461\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6005 - val_loss: 0.7159\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6144 - val_loss: 1.4189\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.0231 - val_loss: 1.6228\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4448 - val_loss: 0.3734\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2986 - val_loss: 0.2749\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3436 - val_loss: 0.5567\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3302 - val_loss: 0.5201\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2559 - val_loss: 0.4548\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4905 - val_loss: 1.8906\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4402 - val_loss: 0.9440\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4923 - val_loss: 1.5037\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8613 - val_loss: 0.6550\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4220 - val_loss: 1.0230\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7546 - val_loss: 1.7527\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0568 - val_loss: 10.1031\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0627 - val_loss: 0.3209\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3767 - val_loss: 0.5484\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5030 - val_loss: 1.6662\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6365 - val_loss: 0.4301\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5569 - val_loss: 0.8407\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0157 - val_loss: 1.1966\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3261 - val_loss: 0.4121\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4141 - val_loss: 2.2348\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.8683 - val_loss: 1.9071\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4905 - val_loss: 0.8009\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3117 - val_loss: 0.3448\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5734 - val_loss: 0.4540\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2386 - val_loss: 0.5114\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2761 - val_loss: 0.5340\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3474 - val_loss: 0.2525\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2118 - val_loss: 0.2736\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5222 - val_loss: 0.3736\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2353 - val_loss: 1.1298\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4001 - val_loss: 0.8581\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3894 - val_loss: 1.1171\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3537 - val_loss: 0.4247\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3166 - val_loss: 0.6191\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6173 - val_loss: 2.3132\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5546 - val_loss: 0.6839\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4626 - val_loss: 2.9341\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6098 - val_loss: 1.2716\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3468 - val_loss: 0.5500\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6290 - val_loss: 0.7223\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4307 - val_loss: 5.3443\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0823 - val_loss: 66.5588\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6444 - val_loss: 1.8279\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2390 - val_loss: 0.2770\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1797 - val_loss: 0.3131\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1675 - val_loss: 0.4217\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1639 - val_loss: 0.1884\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1775 - val_loss: 0.3777\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3216 - val_loss: 0.3185\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2069 - val_loss: 0.3688\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2395 - val_loss: 0.2555\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4468 - val_loss: 0.4194\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2540 - val_loss: 1.1424\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2417 - val_loss: 0.1580\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2836 - val_loss: 0.5706\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4624 - val_loss: 0.9125\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2657 - val_loss: 0.3319\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5508 - val_loss: 1.3347\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4037 - val_loss: 39.5928\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5872 - val_loss: 0.7735\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2439 - val_loss: 0.2755\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1872 - val_loss: 0.1970\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1802 - val_loss: 0.1759\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1657 - val_loss: 0.3175\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1530 - val_loss: 0.1128\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1452 - val_loss: 0.2250\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1766 - val_loss: 0.1267\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2405 - val_loss: 0.4679\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3012 - val_loss: 0.4982\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2866 - val_loss: 0.6580\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2455 - val_loss: 0.8490\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2516 - val_loss: 0.7984\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2665 - val_loss: 0.4682\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4859 - val_loss: 0.6152\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2466 - val_loss: 0.4267\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3458 - val_loss: 0.4480\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2640 - val_loss: 7.8675\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.4999 - val_loss: 276.8976\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.0673 - val_loss: 0.7474\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2459 - val_loss: 2.6647\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2149 - val_loss: 0.1568\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1215 - val_loss: 0.1616\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1316 - val_loss: 0.2136\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1118 - val_loss: 0.1410\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1471 - val_loss: 0.3987\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1654 - val_loss: 0.5483\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2031 - val_loss: 0.3276\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1721 - val_loss: 0.6651\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3162 - val_loss: 0.4719\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2155 - val_loss: 0.1690\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3427 - val_loss: 0.9053\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3557 - val_loss: 1.4293\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1922 - val_loss: 0.2062\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3598 - val_loss: 2.0399\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3220 - val_loss: 0.4322\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.11280124147732688\n",
      "Mean Absolute Error (MAE): 0.24754234082652204\n",
      "Root Mean Squared Error (RMSE): 0.3358589606923223\n",
      "Time taken: 468.40825629234314\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 3379.1658 - val_loss: 2593.4241\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2360.8411 - val_loss: 2028.5422\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1834.0995 - val_loss: 1644.9800\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1371.6472 - val_loss: 1181.2882\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1039.1062 - val_loss: 891.2119\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 782.8007 - val_loss: 696.3810\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 589.2924 - val_loss: 519.0991\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 378.4548 - val_loss: 293.2635\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 207.1985 - val_loss: 184.3586\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 106.6134 - val_loss: 203.9794\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 56.4396 - val_loss: 51.1627\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 26.4332 - val_loss: 38.2556\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 23.6203 - val_loss: 18.9991\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 11.6790 - val_loss: 20.4569\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.8693 - val_loss: 14.1406\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.6113 - val_loss: 12.2286\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.0081 - val_loss: 9.8161\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8245 - val_loss: 6.0771\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.0061 - val_loss: 10.4236\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.3028 - val_loss: 3.9398\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7369 - val_loss: 6.1051\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2972 - val_loss: 6.5869\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.5222 - val_loss: 3.3573\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2131 - val_loss: 24.9172\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2263 - val_loss: 6.7561\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9399 - val_loss: 5.4583\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 8.4053 - val_loss: 78.2324\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 7.1657 - val_loss: 2.0424\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.1530 - val_loss: 2.3323\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5008 - val_loss: 0.9049\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.0055 - val_loss: 115.7660\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7328 - val_loss: 5.8417\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5579 - val_loss: 4.8785\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4450 - val_loss: 2.7717\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.9601 - val_loss: 69.6861\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 13.7325 - val_loss: 4.2056\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.5048 - val_loss: 1.1688\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.2875 - val_loss: 1.5496\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7460 - val_loss: 65.8193\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.5002 - val_loss: 15.9402\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1597 - val_loss: 2.0763\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9685 - val_loss: 3.0108\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.0149 - val_loss: 1.9120\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.8810 - val_loss: 9.2393\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.3758 - val_loss: 3.0227\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1125 - val_loss: 5.2572\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4994 - val_loss: 6.1490\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9706 - val_loss: 1.8386\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0443 - val_loss: 3.4877\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4485 - val_loss: 12.9673\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.1234 - val_loss: 3.3357\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6816 - val_loss: 2.1441\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8293 - val_loss: 1.9272\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0411 - val_loss: 1.8596\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6895 - val_loss: 0.6868\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 8.6288 - val_loss: 48.9446\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6024 - val_loss: 3.1146\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6635 - val_loss: 1.1142\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7308 - val_loss: 3.5517\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7850 - val_loss: 1.1118\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8161 - val_loss: 1.5450\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.3401 - val_loss: 11.8682\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.8789 - val_loss: 15.8331\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 2.5624 - val_loss: 1.1960\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4248 - val_loss: 0.6182\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3670 - val_loss: 0.8671\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3437 - val_loss: 0.3150\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4618 - val_loss: 1.5176\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6570 - val_loss: 1.2669\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5596 - val_loss: 0.5814\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6808 - val_loss: 1.6411\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9891 - val_loss: 1.3674\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5902 - val_loss: 1.3620\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4211 - val_loss: 28.7809\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.3291 - val_loss: 0.8397\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3377 - val_loss: 0.9494\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3601 - val_loss: 0.4197\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7597 - val_loss: 0.6458\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 9.9172 - val_loss: 9.4594\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 2s 7ms/step - loss: 0.8957 - val_loss: 0.7737\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3627 - val_loss: 0.6517\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2790 - val_loss: 0.7826\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2643 - val_loss: 0.2785\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2412 - val_loss: 0.5988\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2759 - val_loss: 0.6367\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3060 - val_loss: 0.6993\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5858 - val_loss: 0.4357\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4179 - val_loss: 0.8320\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5258 - val_loss: 0.6703\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4106 - val_loss: 1.4446\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6181 - val_loss: 0.6480\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.7041 - val_loss: 8.3716\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5620 - val_loss: 0.7388\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2295 - val_loss: 0.2730\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2274 - val_loss: 1.5233\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2530 - val_loss: 0.3811\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2301 - val_loss: 0.2713\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2625 - val_loss: 0.9169\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3984 - val_loss: 0.9674\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4321 - val_loss: 0.6941\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.3208 - val_loss: 206.7599\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4504 - val_loss: 1.3903\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6952 - val_loss: 0.5100\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2829 - val_loss: 0.3154\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3065 - val_loss: 0.3658\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3289 - val_loss: 0.5395\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2595 - val_loss: 0.2940\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3276 - val_loss: 0.7586\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4550 - val_loss: 0.5983\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3871 - val_loss: 1.1966\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.6722 - val_loss: 2.3438\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 3.8951 - val_loss: 19.6834\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9281 - val_loss: 0.4928\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2352 - val_loss: 0.4028\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1847 - val_loss: 0.1961\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1833 - val_loss: 0.3105\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1582 - val_loss: 0.3637\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2748 - val_loss: 0.3853\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3126 - val_loss: 1.1867\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3221 - val_loss: 4.7913\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.6472 - val_loss: 1.1920\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3072 - val_loss: 0.7716\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2237 - val_loss: 0.2344\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2271 - val_loss: 0.2075\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2373 - val_loss: 0.3051\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2370 - val_loss: 0.5295\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2707 - val_loss: 0.5556\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3407 - val_loss: 0.5380\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2516 - val_loss: 0.4164\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4509 - val_loss: 4.3657\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6747 - val_loss: 1.2803\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8597 - val_loss: 78.0917\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6959 - val_loss: 0.4154\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3410 - val_loss: 0.3223\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1917 - val_loss: 0.6405\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1841 - val_loss: 0.3747\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2054 - val_loss: 0.6727\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1544 - val_loss: 0.1720\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2945 - val_loss: 1.5751\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.1276 - val_loss: 6.6419\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3834 - val_loss: 0.8079\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1896 - val_loss: 0.2094\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1181 - val_loss: 0.2671\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1107 - val_loss: 0.1766\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1407 - val_loss: 0.3284\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1715 - val_loss: 0.6914\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3393 - val_loss: 0.4754\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2008 - val_loss: 0.4052\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2425 - val_loss: 0.5001\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3310 - val_loss: 0.2539\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2603 - val_loss: 0.5488\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4449 - val_loss: 0.5051\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5003 - val_loss: 1.4995\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2836 - val_loss: 1.1984\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8887 - val_loss: 476.1490\n",
      "Epoch 156/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.3357 - val_loss: 1.0822\n",
      "Epoch 157/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2531 - val_loss: 0.1987\n",
      "Epoch 158/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1631 - val_loss: 0.1389\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1137 - val_loss: 0.1471\n",
      "Epoch 160/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1228 - val_loss: 0.2936\n",
      "Epoch 161/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1274 - val_loss: 0.2611\n",
      "Epoch 162/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1258 - val_loss: 0.4200\n",
      "Epoch 163/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1372 - val_loss: 0.1101\n",
      "Epoch 164/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1256 - val_loss: 0.3050\n",
      "Epoch 165/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2410 - val_loss: 0.6587\n",
      "Epoch 166/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3372 - val_loss: 0.5638\n",
      "Epoch 167/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4465 - val_loss: 0.3906\n",
      "Epoch 168/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.1832 - val_loss: 0.3209\n",
      "Epoch 169/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2175 - val_loss: 0.4063\n",
      "Epoch 170/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2482 - val_loss: 0.4105\n",
      "Epoch 171/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3670 - val_loss: 1.6868\n",
      "Epoch 172/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9716 - val_loss: 8.0516\n",
      "Epoch 173/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 1.4411 - val_loss: 2.1019\n",
      "Epoch 174/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1407 - val_loss: 0.1931\n",
      "Epoch 175/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1048 - val_loss: 0.6782\n",
      "Epoch 176/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1244 - val_loss: 0.0798\n",
      "Epoch 177/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1409 - val_loss: 0.3492\n",
      "Epoch 178/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1332 - val_loss: 0.4875\n",
      "Epoch 179/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5631 - val_loss: 9.3517\n",
      "Epoch 180/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 6.1262 - val_loss: 0.4654\n",
      "Epoch 181/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2217 - val_loss: 0.2932\n",
      "Epoch 182/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1824 - val_loss: 0.1277\n",
      "Epoch 183/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1363 - val_loss: 0.1119\n",
      "Epoch 184/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1186 - val_loss: 0.3624\n",
      "Epoch 185/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1182 - val_loss: 0.1460\n",
      "Epoch 186/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1912 - val_loss: 1.1086\n",
      "Epoch 187/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3098 - val_loss: 0.5760\n",
      "Epoch 188/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1920 - val_loss: 0.2392\n",
      "Epoch 189/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3766 - val_loss: 0.4274\n",
      "Epoch 190/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2962 - val_loss: 0.1997\n",
      "Epoch 191/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1461 - val_loss: 1.1206\n",
      "Epoch 192/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3966 - val_loss: 2.6517\n",
      "Epoch 193/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2888 - val_loss: 0.4515\n",
      "Epoch 194/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1612 - val_loss: 0.1427\n",
      "Epoch 195/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1786 - val_loss: 1.7256\n",
      "Epoch 196/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.3659 - val_loss: 0.5838\n",
      "Epoch 197/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3301 - val_loss: 0.8748\n",
      "Epoch 198/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.2786 - val_loss: 1.6816\n",
      "Epoch 199/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2733 - val_loss: 3.1099\n",
      "Epoch 200/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1715 - val_loss: 0.1925\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.19243892431598594\n",
      "Mean Absolute Error (MAE): 0.3147270105180672\n",
      "Root Mean Squared Error (RMSE): 0.4386786116463691\n",
      "Time taken: 544.8066201210022\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 11ms/step - loss: 3285.5088 - val_loss: 2810.3438\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2518.9065 - val_loss: 2622.1226\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2365.6670 - val_loss: 2347.2305\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1977.0510 - val_loss: 1923.1852\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1499.3406 - val_loss: 1367.5402\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1100.9991 - val_loss: 980.8036\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 866.9749 - val_loss: 830.6243\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 693.4879 - val_loss: 620.9085\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 475.4430 - val_loss: 348.2176\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 268.7703 - val_loss: 269.9445\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 146.5766 - val_loss: 89.2026\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 66.6485 - val_loss: 53.6661\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 33.9782 - val_loss: 32.1493\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 22.2258 - val_loss: 11.3411\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 11.3315 - val_loss: 19.8938\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 7.5593 - val_loss: 12.6985\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 5.6182 - val_loss: 7.5397\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 6.0582 - val_loss: 6.5130\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8003 - val_loss: 5.8185\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.1708 - val_loss: 3.3125\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7161 - val_loss: 2.3901\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.4009 - val_loss: 4.0108\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.5475 - val_loss: 1.6030\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 2.0603 - val_loss: 4.5912\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8951 - val_loss: 3.6677\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0164 - val_loss: 6.2864\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8132 - val_loss: 9.1213\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.6997 - val_loss: 48.1667\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.3525 - val_loss: 2.8453\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5490 - val_loss: 3.2756\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0974 - val_loss: 1.3639\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0477 - val_loss: 0.6487\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4174 - val_loss: 1.4857\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1878 - val_loss: 1.1746\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0229 - val_loss: 1.4803\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8420 - val_loss: 1.1703\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7791 - val_loss: 9.0917\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1063 - val_loss: 4.4690\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2737 - val_loss: 0.8069\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.7705 - val_loss: 1.3283\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2745 - val_loss: 1.1099\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9529 - val_loss: 3.1220\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2865 - val_loss: 1.5442\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.5239 - val_loss: 5.3983\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 10.2147 - val_loss: 117.1347\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.1829 - val_loss: 2.0805\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.0791 - val_loss: 0.8433\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7117 - val_loss: 3.2328\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7739 - val_loss: 1.5681\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7430 - val_loss: 0.7462\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5730 - val_loss: 0.6606\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7324 - val_loss: 2.1765\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.9373 - val_loss: 1.1686\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1393 - val_loss: 1.5594\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.7923 - val_loss: 0.9250\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9089 - val_loss: 0.4429\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7576 - val_loss: 0.7292\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.6331 - val_loss: 21.2186\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 10.6618 - val_loss: 3.1649\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.6981 - val_loss: 0.8232\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5227 - val_loss: 0.7027\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4996 - val_loss: 0.8603\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4574 - val_loss: 0.3537\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4822 - val_loss: 1.1772\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5955 - val_loss: 0.2715\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6376 - val_loss: 0.6769\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4536 - val_loss: 0.5428\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 1.0128 - val_loss: 0.6007\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6748 - val_loss: 0.5985\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5918 - val_loss: 1.1922\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7419 - val_loss: 0.8091\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4643 - val_loss: 2.3748\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6851 - val_loss: 0.7612\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4184 - val_loss: 0.3824\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5754 - val_loss: 0.9755\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6354 - val_loss: 4.5607\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2894 - val_loss: 3.8807\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9205 - val_loss: 0.8353\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6745 - val_loss: 0.6699\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4701 - val_loss: 1.1985\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5078 - val_loss: 2.4290\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.1190 - val_loss: 0.6220\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4762 - val_loss: 0.2790\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3448 - val_loss: 0.2181\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3326 - val_loss: 0.4046\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3509 - val_loss: 0.2061\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3229 - val_loss: 0.5223\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2411 - val_loss: 0.8153\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4040 - val_loss: 1.2947\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.3450 - val_loss: 0.2412\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7113 - val_loss: 0.5613\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4430 - val_loss: 0.3400\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.2652 - val_loss: 1.7264\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3681 - val_loss: 0.3121\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1931 - val_loss: 0.1202\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2721 - val_loss: 0.3582\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1814 - val_loss: 0.2366\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3021 - val_loss: 0.3187\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3636 - val_loss: 0.6907\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3760 - val_loss: 0.2158\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3849 - val_loss: 0.9080\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3534 - val_loss: 0.2129\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.4186 - val_loss: 0.4211\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.5926 - val_loss: 0.8868\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 3.3856 - val_loss: 2.7627\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2107 - val_loss: 0.1542\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2108 - val_loss: 0.1705\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2102 - val_loss: 0.3135\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3070 - val_loss: 0.3428\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4002 - val_loss: 0.5736\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3306 - val_loss: 0.3539\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2312 - val_loss: 0.3691\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3889 - val_loss: 0.4711\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3926 - val_loss: 0.8403\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8585 - val_loss: 0.5438\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3286 - val_loss: 1.0518\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3865 - val_loss: 1.8154\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4361 - val_loss: 1.3241\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3994 - val_loss: 0.2705\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3736 - val_loss: 0.7803\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4660 - val_loss: 15.6606\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 7.6101 - val_loss: 0.4473\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2196 - val_loss: 0.3276\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1504 - val_loss: 0.2448\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1399 - val_loss: 0.1249\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.12026049325832477\n",
      "Mean Absolute Error (MAE): 0.2671825250341229\n",
      "Root Mean Squared Error (RMSE): 0.3467859473195602\n",
      "Time taken: 342.3406090736389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_16796\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.077899  0.187080  0.279104  542.979800\n",
      "1        2  0.186294  0.328690  0.431618  366.614733\n",
      "2        3  0.112801  0.247542  0.335859  468.408256\n",
      "3        4  0.192439  0.314727  0.438679  544.806620\n",
      "4        5  0.120260  0.267183  0.346786  342.340609\n",
      "5  Average  0.137939  0.269044  0.366409  453.030004\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_Reg3.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_Reg3.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_Reg3.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
