{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_1_smoothing2_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.569633</td>\n",
       "      <td>132.669774</td>\n",
       "      <td>83.130710</td>\n",
       "      <td>119.261204</td>\n",
       "      <td>126.578979</td>\n",
       "      <td>148.830041</td>\n",
       "      <td>111.311166</td>\n",
       "      <td>139.430392</td>\n",
       "      <td>99.615316</td>\n",
       "      <td>118.557826</td>\n",
       "      <td>...</td>\n",
       "      <td>101.044016</td>\n",
       "      <td>97.658267</td>\n",
       "      <td>102.844237</td>\n",
       "      <td>86.402391</td>\n",
       "      <td>128.906224</td>\n",
       "      <td>113.014998</td>\n",
       "      <td>103.942203</td>\n",
       "      <td>128.786522</td>\n",
       "      <td>77.145139</td>\n",
       "      <td>118.042869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.590690</td>\n",
       "      <td>132.661834</td>\n",
       "      <td>83.202932</td>\n",
       "      <td>119.275839</td>\n",
       "      <td>126.462565</td>\n",
       "      <td>148.697946</td>\n",
       "      <td>111.148479</td>\n",
       "      <td>139.223949</td>\n",
       "      <td>99.741752</td>\n",
       "      <td>118.556519</td>\n",
       "      <td>...</td>\n",
       "      <td>101.107786</td>\n",
       "      <td>97.777283</td>\n",
       "      <td>103.059338</td>\n",
       "      <td>86.470380</td>\n",
       "      <td>128.767509</td>\n",
       "      <td>112.867564</td>\n",
       "      <td>103.812490</td>\n",
       "      <td>128.886979</td>\n",
       "      <td>77.151769</td>\n",
       "      <td>117.929142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.614362</td>\n",
       "      <td>132.654377</td>\n",
       "      <td>83.275524</td>\n",
       "      <td>119.290404</td>\n",
       "      <td>126.346475</td>\n",
       "      <td>148.563611</td>\n",
       "      <td>110.985467</td>\n",
       "      <td>139.017736</td>\n",
       "      <td>99.869282</td>\n",
       "      <td>118.556061</td>\n",
       "      <td>...</td>\n",
       "      <td>101.172404</td>\n",
       "      <td>97.896426</td>\n",
       "      <td>103.277594</td>\n",
       "      <td>86.537182</td>\n",
       "      <td>128.629069</td>\n",
       "      <td>112.718192</td>\n",
       "      <td>103.682163</td>\n",
       "      <td>128.986052</td>\n",
       "      <td>77.158113</td>\n",
       "      <td>117.815084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.641009</td>\n",
       "      <td>132.647443</td>\n",
       "      <td>83.348540</td>\n",
       "      <td>119.304798</td>\n",
       "      <td>126.230663</td>\n",
       "      <td>148.427340</td>\n",
       "      <td>110.822296</td>\n",
       "      <td>138.811784</td>\n",
       "      <td>99.997960</td>\n",
       "      <td>118.556686</td>\n",
       "      <td>...</td>\n",
       "      <td>101.238191</td>\n",
       "      <td>98.016139</td>\n",
       "      <td>103.498832</td>\n",
       "      <td>86.603111</td>\n",
       "      <td>128.491033</td>\n",
       "      <td>112.567106</td>\n",
       "      <td>103.551552</td>\n",
       "      <td>129.083881</td>\n",
       "      <td>77.163684</td>\n",
       "      <td>117.700897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.670973</td>\n",
       "      <td>132.641270</td>\n",
       "      <td>83.422108</td>\n",
       "      <td>119.319043</td>\n",
       "      <td>126.115230</td>\n",
       "      <td>148.289453</td>\n",
       "      <td>110.658971</td>\n",
       "      <td>138.605917</td>\n",
       "      <td>100.127885</td>\n",
       "      <td>118.558828</td>\n",
       "      <td>...</td>\n",
       "      <td>101.305368</td>\n",
       "      <td>98.136740</td>\n",
       "      <td>103.722692</td>\n",
       "      <td>86.668617</td>\n",
       "      <td>128.353449</td>\n",
       "      <td>112.414828</td>\n",
       "      <td>103.421125</td>\n",
       "      <td>129.180734</td>\n",
       "      <td>77.168086</td>\n",
       "      <td>117.586815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.182404</td>\n",
       "      <td>113.040767</td>\n",
       "      <td>105.847805</td>\n",
       "      <td>78.217642</td>\n",
       "      <td>150.269492</td>\n",
       "      <td>137.927741</td>\n",
       "      <td>134.314210</td>\n",
       "      <td>115.883798</td>\n",
       "      <td>116.732585</td>\n",
       "      <td>107.086022</td>\n",
       "      <td>...</td>\n",
       "      <td>108.782340</td>\n",
       "      <td>119.546095</td>\n",
       "      <td>108.046566</td>\n",
       "      <td>68.424907</td>\n",
       "      <td>137.006882</td>\n",
       "      <td>114.335066</td>\n",
       "      <td>132.446443</td>\n",
       "      <td>116.008277</td>\n",
       "      <td>108.614011</td>\n",
       "      <td>80.271199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>127.864707</td>\n",
       "      <td>112.881785</td>\n",
       "      <td>105.963448</td>\n",
       "      <td>78.318383</td>\n",
       "      <td>150.043134</td>\n",
       "      <td>137.881055</td>\n",
       "      <td>134.455501</td>\n",
       "      <td>115.928398</td>\n",
       "      <td>116.563981</td>\n",
       "      <td>106.879909</td>\n",
       "      <td>...</td>\n",
       "      <td>108.681141</td>\n",
       "      <td>119.427729</td>\n",
       "      <td>107.891232</td>\n",
       "      <td>68.365363</td>\n",
       "      <td>136.918609</td>\n",
       "      <td>114.504131</td>\n",
       "      <td>132.374344</td>\n",
       "      <td>115.948280</td>\n",
       "      <td>108.700997</td>\n",
       "      <td>80.242280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>127.541664</td>\n",
       "      <td>112.723090</td>\n",
       "      <td>106.081989</td>\n",
       "      <td>78.418798</td>\n",
       "      <td>149.813444</td>\n",
       "      <td>137.835463</td>\n",
       "      <td>134.599156</td>\n",
       "      <td>115.974725</td>\n",
       "      <td>116.396053</td>\n",
       "      <td>106.673908</td>\n",
       "      <td>...</td>\n",
       "      <td>108.578553</td>\n",
       "      <td>119.309889</td>\n",
       "      <td>107.736048</td>\n",
       "      <td>68.304497</td>\n",
       "      <td>136.832191</td>\n",
       "      <td>114.676755</td>\n",
       "      <td>132.301638</td>\n",
       "      <td>115.889961</td>\n",
       "      <td>108.788631</td>\n",
       "      <td>80.215094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>127.213582</td>\n",
       "      <td>112.564486</td>\n",
       "      <td>106.203485</td>\n",
       "      <td>78.518849</td>\n",
       "      <td>149.580484</td>\n",
       "      <td>137.791058</td>\n",
       "      <td>134.745220</td>\n",
       "      <td>116.022311</td>\n",
       "      <td>116.228730</td>\n",
       "      <td>106.467758</td>\n",
       "      <td>...</td>\n",
       "      <td>108.474328</td>\n",
       "      <td>119.192398</td>\n",
       "      <td>107.581271</td>\n",
       "      <td>68.242740</td>\n",
       "      <td>136.747871</td>\n",
       "      <td>114.853115</td>\n",
       "      <td>132.228809</td>\n",
       "      <td>115.833382</td>\n",
       "      <td>108.877154</td>\n",
       "      <td>80.190119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>126.880923</td>\n",
       "      <td>112.405914</td>\n",
       "      <td>106.327742</td>\n",
       "      <td>78.618462</td>\n",
       "      <td>149.344480</td>\n",
       "      <td>137.747874</td>\n",
       "      <td>134.893804</td>\n",
       "      <td>116.070826</td>\n",
       "      <td>116.062017</td>\n",
       "      <td>106.261338</td>\n",
       "      <td>...</td>\n",
       "      <td>108.368265</td>\n",
       "      <td>119.075008</td>\n",
       "      <td>107.426963</td>\n",
       "      <td>68.180452</td>\n",
       "      <td>136.665841</td>\n",
       "      <td>115.033293</td>\n",
       "      <td>132.156320</td>\n",
       "      <td>115.778564</td>\n",
       "      <td>108.966672</td>\n",
       "      <td>80.167640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5   \\\n",
       "0     102.569633  132.669774   83.130710  119.261204  126.578979  148.830041   \n",
       "1     102.590690  132.661834   83.202932  119.275839  126.462565  148.697946   \n",
       "2     102.614362  132.654377   83.275524  119.290404  126.346475  148.563611   \n",
       "3     102.641009  132.647443   83.348540  119.304798  126.230663  148.427340   \n",
       "4     102.670973  132.641270   83.422108  119.319043  126.115230  148.289453   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.182404  113.040767  105.847805   78.217642  150.269492  137.927741   \n",
       "2439  127.864707  112.881785  105.963448   78.318383  150.043134  137.881055   \n",
       "2440  127.541664  112.723090  106.081989   78.418798  149.813444  137.835463   \n",
       "2441  127.213582  112.564486  106.203485   78.518849  149.580484  137.791058   \n",
       "2442  126.880923  112.405914  106.327742   78.618462  149.344480  137.747874   \n",
       "\n",
       "              6           7           8           9   ...          38  \\\n",
       "0     111.311166  139.430392   99.615316  118.557826  ...  101.044016   \n",
       "1     111.148479  139.223949   99.741752  118.556519  ...  101.107786   \n",
       "2     110.985467  139.017736   99.869282  118.556061  ...  101.172404   \n",
       "3     110.822296  138.811784   99.997960  118.556686  ...  101.238191   \n",
       "4     110.658971  138.605917  100.127885  118.558828  ...  101.305368   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  134.314210  115.883798  116.732585  107.086022  ...  108.782340   \n",
       "2439  134.455501  115.928398  116.563981  106.879909  ...  108.681141   \n",
       "2440  134.599156  115.974725  116.396053  106.673908  ...  108.578553   \n",
       "2441  134.745220  116.022311  116.228730  106.467758  ...  108.474328   \n",
       "2442  134.893804  116.070826  116.062017  106.261338  ...  108.368265   \n",
       "\n",
       "              39          40         41          42          43          44  \\\n",
       "0      97.658267  102.844237  86.402391  128.906224  113.014998  103.942203   \n",
       "1      97.777283  103.059338  86.470380  128.767509  112.867564  103.812490   \n",
       "2      97.896426  103.277594  86.537182  128.629069  112.718192  103.682163   \n",
       "3      98.016139  103.498832  86.603111  128.491033  112.567106  103.551552   \n",
       "4      98.136740  103.722692  86.668617  128.353449  112.414828  103.421125   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  119.546095  108.046566  68.424907  137.006882  114.335066  132.446443   \n",
       "2439  119.427729  107.891232  68.365363  136.918609  114.504131  132.374344   \n",
       "2440  119.309889  107.736048  68.304497  136.832191  114.676755  132.301638   \n",
       "2441  119.192398  107.581271  68.242740  136.747871  114.853115  132.228809   \n",
       "2442  119.075008  107.426963  68.180452  136.665841  115.033293  132.156320   \n",
       "\n",
       "              45          46          47  \n",
       "0     128.786522   77.145139  118.042869  \n",
       "1     128.886979   77.151769  117.929142  \n",
       "2     128.986052   77.158113  117.815084  \n",
       "3     129.083881   77.163684  117.700897  \n",
       "4     129.180734   77.168086  117.586815  \n",
       "...          ...         ...         ...  \n",
       "2438  116.008277  108.614011   80.271199  \n",
       "2439  115.948280  108.700997   80.242280  \n",
       "2440  115.889961  108.788631   80.215094  \n",
       "2441  115.833382  108.877154   80.190119  \n",
       "2442  115.778564  108.966672   80.167640  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.569633</td>\n",
       "      <td>132.669774</td>\n",
       "      <td>83.130710</td>\n",
       "      <td>119.261204</td>\n",
       "      <td>126.578979</td>\n",
       "      <td>148.830041</td>\n",
       "      <td>111.311166</td>\n",
       "      <td>139.430392</td>\n",
       "      <td>99.615316</td>\n",
       "      <td>118.557826</td>\n",
       "      <td>...</td>\n",
       "      <td>101.044016</td>\n",
       "      <td>97.658267</td>\n",
       "      <td>102.844237</td>\n",
       "      <td>86.402391</td>\n",
       "      <td>128.906224</td>\n",
       "      <td>113.014998</td>\n",
       "      <td>103.942203</td>\n",
       "      <td>128.786522</td>\n",
       "      <td>77.145139</td>\n",
       "      <td>118.042869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.590690</td>\n",
       "      <td>132.661834</td>\n",
       "      <td>83.202932</td>\n",
       "      <td>119.275839</td>\n",
       "      <td>126.462565</td>\n",
       "      <td>148.697946</td>\n",
       "      <td>111.148479</td>\n",
       "      <td>139.223949</td>\n",
       "      <td>99.741752</td>\n",
       "      <td>118.556519</td>\n",
       "      <td>...</td>\n",
       "      <td>101.107786</td>\n",
       "      <td>97.777283</td>\n",
       "      <td>103.059338</td>\n",
       "      <td>86.470380</td>\n",
       "      <td>128.767509</td>\n",
       "      <td>112.867564</td>\n",
       "      <td>103.812490</td>\n",
       "      <td>128.886979</td>\n",
       "      <td>77.151769</td>\n",
       "      <td>117.929142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.614362</td>\n",
       "      <td>132.654377</td>\n",
       "      <td>83.275524</td>\n",
       "      <td>119.290404</td>\n",
       "      <td>126.346475</td>\n",
       "      <td>148.563611</td>\n",
       "      <td>110.985467</td>\n",
       "      <td>139.017736</td>\n",
       "      <td>99.869282</td>\n",
       "      <td>118.556061</td>\n",
       "      <td>...</td>\n",
       "      <td>101.172404</td>\n",
       "      <td>97.896426</td>\n",
       "      <td>103.277594</td>\n",
       "      <td>86.537182</td>\n",
       "      <td>128.629069</td>\n",
       "      <td>112.718192</td>\n",
       "      <td>103.682163</td>\n",
       "      <td>128.986052</td>\n",
       "      <td>77.158113</td>\n",
       "      <td>117.815084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.641009</td>\n",
       "      <td>132.647443</td>\n",
       "      <td>83.348540</td>\n",
       "      <td>119.304798</td>\n",
       "      <td>126.230663</td>\n",
       "      <td>148.427340</td>\n",
       "      <td>110.822296</td>\n",
       "      <td>138.811784</td>\n",
       "      <td>99.997960</td>\n",
       "      <td>118.556686</td>\n",
       "      <td>...</td>\n",
       "      <td>101.238191</td>\n",
       "      <td>98.016139</td>\n",
       "      <td>103.498832</td>\n",
       "      <td>86.603111</td>\n",
       "      <td>128.491033</td>\n",
       "      <td>112.567106</td>\n",
       "      <td>103.551552</td>\n",
       "      <td>129.083881</td>\n",
       "      <td>77.163684</td>\n",
       "      <td>117.700897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.670973</td>\n",
       "      <td>132.641270</td>\n",
       "      <td>83.422108</td>\n",
       "      <td>119.319043</td>\n",
       "      <td>126.115230</td>\n",
       "      <td>148.289453</td>\n",
       "      <td>110.658971</td>\n",
       "      <td>138.605917</td>\n",
       "      <td>100.127885</td>\n",
       "      <td>118.558828</td>\n",
       "      <td>...</td>\n",
       "      <td>101.305368</td>\n",
       "      <td>98.136740</td>\n",
       "      <td>103.722692</td>\n",
       "      <td>86.668617</td>\n",
       "      <td>128.353449</td>\n",
       "      <td>112.414828</td>\n",
       "      <td>103.421125</td>\n",
       "      <td>129.180734</td>\n",
       "      <td>77.168086</td>\n",
       "      <td>117.586815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.182404</td>\n",
       "      <td>113.040767</td>\n",
       "      <td>105.847805</td>\n",
       "      <td>78.217642</td>\n",
       "      <td>150.269492</td>\n",
       "      <td>137.927741</td>\n",
       "      <td>134.314210</td>\n",
       "      <td>115.883798</td>\n",
       "      <td>116.732585</td>\n",
       "      <td>107.086022</td>\n",
       "      <td>...</td>\n",
       "      <td>108.782340</td>\n",
       "      <td>119.546095</td>\n",
       "      <td>108.046566</td>\n",
       "      <td>68.424907</td>\n",
       "      <td>137.006882</td>\n",
       "      <td>114.335066</td>\n",
       "      <td>132.446443</td>\n",
       "      <td>116.008277</td>\n",
       "      <td>108.614011</td>\n",
       "      <td>80.271199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>127.864707</td>\n",
       "      <td>112.881785</td>\n",
       "      <td>105.963448</td>\n",
       "      <td>78.318383</td>\n",
       "      <td>150.043134</td>\n",
       "      <td>137.881055</td>\n",
       "      <td>134.455501</td>\n",
       "      <td>115.928398</td>\n",
       "      <td>116.563981</td>\n",
       "      <td>106.879909</td>\n",
       "      <td>...</td>\n",
       "      <td>108.681141</td>\n",
       "      <td>119.427729</td>\n",
       "      <td>107.891232</td>\n",
       "      <td>68.365363</td>\n",
       "      <td>136.918609</td>\n",
       "      <td>114.504131</td>\n",
       "      <td>132.374344</td>\n",
       "      <td>115.948280</td>\n",
       "      <td>108.700997</td>\n",
       "      <td>80.242280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>127.541664</td>\n",
       "      <td>112.723090</td>\n",
       "      <td>106.081989</td>\n",
       "      <td>78.418798</td>\n",
       "      <td>149.813444</td>\n",
       "      <td>137.835463</td>\n",
       "      <td>134.599156</td>\n",
       "      <td>115.974725</td>\n",
       "      <td>116.396053</td>\n",
       "      <td>106.673908</td>\n",
       "      <td>...</td>\n",
       "      <td>108.578553</td>\n",
       "      <td>119.309889</td>\n",
       "      <td>107.736048</td>\n",
       "      <td>68.304497</td>\n",
       "      <td>136.832191</td>\n",
       "      <td>114.676755</td>\n",
       "      <td>132.301638</td>\n",
       "      <td>115.889961</td>\n",
       "      <td>108.788631</td>\n",
       "      <td>80.215094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>127.213582</td>\n",
       "      <td>112.564486</td>\n",
       "      <td>106.203485</td>\n",
       "      <td>78.518849</td>\n",
       "      <td>149.580484</td>\n",
       "      <td>137.791058</td>\n",
       "      <td>134.745220</td>\n",
       "      <td>116.022311</td>\n",
       "      <td>116.228730</td>\n",
       "      <td>106.467758</td>\n",
       "      <td>...</td>\n",
       "      <td>108.474328</td>\n",
       "      <td>119.192398</td>\n",
       "      <td>107.581271</td>\n",
       "      <td>68.242740</td>\n",
       "      <td>136.747871</td>\n",
       "      <td>114.853115</td>\n",
       "      <td>132.228809</td>\n",
       "      <td>115.833382</td>\n",
       "      <td>108.877154</td>\n",
       "      <td>80.190119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>126.880923</td>\n",
       "      <td>112.405914</td>\n",
       "      <td>106.327742</td>\n",
       "      <td>78.618462</td>\n",
       "      <td>149.344480</td>\n",
       "      <td>137.747874</td>\n",
       "      <td>134.893804</td>\n",
       "      <td>116.070826</td>\n",
       "      <td>116.062017</td>\n",
       "      <td>106.261338</td>\n",
       "      <td>...</td>\n",
       "      <td>108.368265</td>\n",
       "      <td>119.075008</td>\n",
       "      <td>107.426963</td>\n",
       "      <td>68.180452</td>\n",
       "      <td>136.665841</td>\n",
       "      <td>115.033293</td>\n",
       "      <td>132.156320</td>\n",
       "      <td>115.778564</td>\n",
       "      <td>108.966672</td>\n",
       "      <td>80.167640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     102.569633  132.669774   83.130710  119.261204  126.578979  148.830041   \n",
       "1     102.590690  132.661834   83.202932  119.275839  126.462565  148.697946   \n",
       "2     102.614362  132.654377   83.275524  119.290404  126.346475  148.563611   \n",
       "3     102.641009  132.647443   83.348540  119.304798  126.230663  148.427340   \n",
       "4     102.670973  132.641270   83.422108  119.319043  126.115230  148.289453   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.182404  113.040767  105.847805   78.217642  150.269492  137.927741   \n",
       "2439  127.864707  112.881785  105.963448   78.318383  150.043134  137.881055   \n",
       "2440  127.541664  112.723090  106.081989   78.418798  149.813444  137.835463   \n",
       "2441  127.213582  112.564486  106.203485   78.518849  149.580484  137.791058   \n",
       "2442  126.880923  112.405914  106.327742   78.618462  149.344480  137.747874   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10  ...    sensor39  \\\n",
       "0     111.311166  139.430392   99.615316  118.557826  ...  101.044016   \n",
       "1     111.148479  139.223949   99.741752  118.556519  ...  101.107786   \n",
       "2     110.985467  139.017736   99.869282  118.556061  ...  101.172404   \n",
       "3     110.822296  138.811784   99.997960  118.556686  ...  101.238191   \n",
       "4     110.658971  138.605917  100.127885  118.558828  ...  101.305368   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2438  134.314210  115.883798  116.732585  107.086022  ...  108.782340   \n",
       "2439  134.455501  115.928398  116.563981  106.879909  ...  108.681141   \n",
       "2440  134.599156  115.974725  116.396053  106.673908  ...  108.578553   \n",
       "2441  134.745220  116.022311  116.228730  106.467758  ...  108.474328   \n",
       "2442  134.893804  116.070826  116.062017  106.261338  ...  108.368265   \n",
       "\n",
       "        sensor40    sensor41   sensor42    sensor43    sensor44    sensor45  \\\n",
       "0      97.658267  102.844237  86.402391  128.906224  113.014998  103.942203   \n",
       "1      97.777283  103.059338  86.470380  128.767509  112.867564  103.812490   \n",
       "2      97.896426  103.277594  86.537182  128.629069  112.718192  103.682163   \n",
       "3      98.016139  103.498832  86.603111  128.491033  112.567106  103.551552   \n",
       "4      98.136740  103.722692  86.668617  128.353449  112.414828  103.421125   \n",
       "...          ...         ...        ...         ...         ...         ...   \n",
       "2438  119.546095  108.046566  68.424907  137.006882  114.335066  132.446443   \n",
       "2439  119.427729  107.891232  68.365363  136.918609  114.504131  132.374344   \n",
       "2440  119.309889  107.736048  68.304497  136.832191  114.676755  132.301638   \n",
       "2441  119.192398  107.581271  68.242740  136.747871  114.853115  132.228809   \n",
       "2442  119.075008  107.426963  68.180452  136.665841  115.033293  132.156320   \n",
       "\n",
       "        sensor46    sensor47    sensor48  \n",
       "0     128.786522   77.145139  118.042869  \n",
       "1     128.886979   77.151769  117.929142  \n",
       "2     128.986052   77.158113  117.815084  \n",
       "3     129.083881   77.163684  117.700897  \n",
       "4     129.180734   77.168086  117.586815  \n",
       "...          ...         ...         ...  \n",
       "2438  116.008277  108.614011   80.271199  \n",
       "2439  115.948280  108.700997   80.242280  \n",
       "2440  115.889961  108.788631   80.215094  \n",
       "2441  115.833382  108.877154   80.190119  \n",
       "2442  115.778564  108.966672   80.167640  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.569633</td>\n",
       "      <td>132.669774</td>\n",
       "      <td>83.130710</td>\n",
       "      <td>119.261204</td>\n",
       "      <td>126.578979</td>\n",
       "      <td>148.830041</td>\n",
       "      <td>111.311166</td>\n",
       "      <td>139.430392</td>\n",
       "      <td>99.615316</td>\n",
       "      <td>118.557826</td>\n",
       "      <td>80.112906</td>\n",
       "      <td>101.006811</td>\n",
       "      <td>125.762083</td>\n",
       "      <td>137.059833</td>\n",
       "      <td>109.985530</td>\n",
       "      <td>122.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.590690</td>\n",
       "      <td>132.661834</td>\n",
       "      <td>83.202932</td>\n",
       "      <td>119.275839</td>\n",
       "      <td>126.462565</td>\n",
       "      <td>148.697946</td>\n",
       "      <td>111.148479</td>\n",
       "      <td>139.223949</td>\n",
       "      <td>99.741752</td>\n",
       "      <td>118.556519</td>\n",
       "      <td>80.086282</td>\n",
       "      <td>101.103795</td>\n",
       "      <td>125.602765</td>\n",
       "      <td>136.903049</td>\n",
       "      <td>109.790512</td>\n",
       "      <td>122.456806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.614362</td>\n",
       "      <td>132.654377</td>\n",
       "      <td>83.275524</td>\n",
       "      <td>119.290404</td>\n",
       "      <td>126.346475</td>\n",
       "      <td>148.563611</td>\n",
       "      <td>110.985467</td>\n",
       "      <td>139.017736</td>\n",
       "      <td>99.869282</td>\n",
       "      <td>118.556061</td>\n",
       "      <td>80.064809</td>\n",
       "      <td>101.195995</td>\n",
       "      <td>125.444053</td>\n",
       "      <td>136.746996</td>\n",
       "      <td>109.592455</td>\n",
       "      <td>122.417392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102.641009</td>\n",
       "      <td>132.647443</td>\n",
       "      <td>83.348540</td>\n",
       "      <td>119.304798</td>\n",
       "      <td>126.230663</td>\n",
       "      <td>148.427340</td>\n",
       "      <td>110.822296</td>\n",
       "      <td>138.811784</td>\n",
       "      <td>99.997960</td>\n",
       "      <td>118.556686</td>\n",
       "      <td>80.048614</td>\n",
       "      <td>101.283715</td>\n",
       "      <td>125.286030</td>\n",
       "      <td>136.591765</td>\n",
       "      <td>109.391553</td>\n",
       "      <td>122.375185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.670973</td>\n",
       "      <td>132.641270</td>\n",
       "      <td>83.422108</td>\n",
       "      <td>119.319043</td>\n",
       "      <td>126.115230</td>\n",
       "      <td>148.289453</td>\n",
       "      <td>110.658971</td>\n",
       "      <td>138.605917</td>\n",
       "      <td>100.127885</td>\n",
       "      <td>118.558828</td>\n",
       "      <td>80.037614</td>\n",
       "      <td>101.367491</td>\n",
       "      <td>125.128632</td>\n",
       "      <td>136.437373</td>\n",
       "      <td>109.188220</td>\n",
       "      <td>122.330185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>128.182404</td>\n",
       "      <td>113.040767</td>\n",
       "      <td>105.847805</td>\n",
       "      <td>78.217642</td>\n",
       "      <td>150.269492</td>\n",
       "      <td>137.927741</td>\n",
       "      <td>134.314210</td>\n",
       "      <td>115.883798</td>\n",
       "      <td>116.732585</td>\n",
       "      <td>107.086022</td>\n",
       "      <td>84.632752</td>\n",
       "      <td>67.063674</td>\n",
       "      <td>141.723353</td>\n",
       "      <td>134.911413</td>\n",
       "      <td>121.716794</td>\n",
       "      <td>111.989042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>127.864707</td>\n",
       "      <td>112.881785</td>\n",
       "      <td>105.963448</td>\n",
       "      <td>78.318383</td>\n",
       "      <td>150.043134</td>\n",
       "      <td>137.881055</td>\n",
       "      <td>134.455501</td>\n",
       "      <td>115.928398</td>\n",
       "      <td>116.563981</td>\n",
       "      <td>106.879909</td>\n",
       "      <td>84.557746</td>\n",
       "      <td>67.178331</td>\n",
       "      <td>141.651369</td>\n",
       "      <td>134.820616</td>\n",
       "      <td>121.795183</td>\n",
       "      <td>112.011359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>127.541664</td>\n",
       "      <td>112.723090</td>\n",
       "      <td>106.081989</td>\n",
       "      <td>78.418798</td>\n",
       "      <td>149.813444</td>\n",
       "      <td>137.835463</td>\n",
       "      <td>134.599156</td>\n",
       "      <td>115.974725</td>\n",
       "      <td>116.396053</td>\n",
       "      <td>106.673908</td>\n",
       "      <td>84.481153</td>\n",
       "      <td>67.291987</td>\n",
       "      <td>141.580708</td>\n",
       "      <td>134.728295</td>\n",
       "      <td>121.873632</td>\n",
       "      <td>112.034771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>127.213582</td>\n",
       "      <td>112.564486</td>\n",
       "      <td>106.203485</td>\n",
       "      <td>78.518849</td>\n",
       "      <td>149.580484</td>\n",
       "      <td>137.791058</td>\n",
       "      <td>134.745220</td>\n",
       "      <td>116.022311</td>\n",
       "      <td>116.228730</td>\n",
       "      <td>106.467758</td>\n",
       "      <td>84.402598</td>\n",
       "      <td>67.404851</td>\n",
       "      <td>141.511331</td>\n",
       "      <td>134.634398</td>\n",
       "      <td>121.951891</td>\n",
       "      <td>112.059291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>126.880923</td>\n",
       "      <td>112.405914</td>\n",
       "      <td>106.327742</td>\n",
       "      <td>78.618462</td>\n",
       "      <td>149.344480</td>\n",
       "      <td>137.747874</td>\n",
       "      <td>134.893804</td>\n",
       "      <td>116.070826</td>\n",
       "      <td>116.062017</td>\n",
       "      <td>106.261338</td>\n",
       "      <td>84.321800</td>\n",
       "      <td>67.516960</td>\n",
       "      <td>141.443364</td>\n",
       "      <td>134.538968</td>\n",
       "      <td>122.029925</td>\n",
       "      <td>112.084752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor1     sensor2     sensor3     sensor4     sensor5     sensor6  \\\n",
       "0     102.569633  132.669774   83.130710  119.261204  126.578979  148.830041   \n",
       "1     102.590690  132.661834   83.202932  119.275839  126.462565  148.697946   \n",
       "2     102.614362  132.654377   83.275524  119.290404  126.346475  148.563611   \n",
       "3     102.641009  132.647443   83.348540  119.304798  126.230663  148.427340   \n",
       "4     102.670973  132.641270   83.422108  119.319043  126.115230  148.289453   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2438  128.182404  113.040767  105.847805   78.217642  150.269492  137.927741   \n",
       "2439  127.864707  112.881785  105.963448   78.318383  150.043134  137.881055   \n",
       "2440  127.541664  112.723090  106.081989   78.418798  149.813444  137.835463   \n",
       "2441  127.213582  112.564486  106.203485   78.518849  149.580484  137.791058   \n",
       "2442  126.880923  112.405914  106.327742   78.618462  149.344480  137.747874   \n",
       "\n",
       "         sensor7     sensor8     sensor9    sensor10   sensor11    sensor12  \\\n",
       "0     111.311166  139.430392   99.615316  118.557826  80.112906  101.006811   \n",
       "1     111.148479  139.223949   99.741752  118.556519  80.086282  101.103795   \n",
       "2     110.985467  139.017736   99.869282  118.556061  80.064809  101.195995   \n",
       "3     110.822296  138.811784   99.997960  118.556686  80.048614  101.283715   \n",
       "4     110.658971  138.605917  100.127885  118.558828  80.037614  101.367491   \n",
       "...          ...         ...         ...         ...        ...         ...   \n",
       "2438  134.314210  115.883798  116.732585  107.086022  84.632752   67.063674   \n",
       "2439  134.455501  115.928398  116.563981  106.879909  84.557746   67.178331   \n",
       "2440  134.599156  115.974725  116.396053  106.673908  84.481153   67.291987   \n",
       "2441  134.745220  116.022311  116.228730  106.467758  84.402598   67.404851   \n",
       "2442  134.893804  116.070826  116.062017  106.261338  84.321800   67.516960   \n",
       "\n",
       "        sensor13    sensor14    sensor15    sensor16  \n",
       "0     125.762083  137.059833  109.985530  122.493300  \n",
       "1     125.602765  136.903049  109.790512  122.456806  \n",
       "2     125.444053  136.746996  109.592455  122.417392  \n",
       "3     125.286030  136.591765  109.391553  122.375185  \n",
       "4     125.128632  136.437373  109.188220  122.330185  \n",
       "...          ...         ...         ...         ...  \n",
       "2438  141.723353  134.911413  121.716794  111.989042  \n",
       "2439  141.651369  134.820616  121.795183  112.011359  \n",
       "2440  141.580708  134.728295  121.873632  112.034771  \n",
       "2441  141.511331  134.634398  121.951891  112.059291  \n",
       "2442  141.443364  134.538968  122.029925  112.084752  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 13ms/step - loss: 1056.1469 - val_loss: 781.1294\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 623.2094 - val_loss: 549.9550\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 366.5468 - val_loss: 288.9173\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 199.6530 - val_loss: 148.5897\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 90.0267 - val_loss: 58.1545\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 35.8071 - val_loss: 21.0297\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 18.3103 - val_loss: 12.8688\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.8059 - val_loss: 6.4659\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.1651 - val_loss: 17.2634\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.7858 - val_loss: 1.7582\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4053 - val_loss: 3.9022\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8797 - val_loss: 1.0139\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1050 - val_loss: 2.6421\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6625 - val_loss: 1.6540\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.4862 - val_loss: 1.4914\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4050 - val_loss: 1.4538\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2950 - val_loss: 1.4871\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0338 - val_loss: 1.9726\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8114 - val_loss: 0.7817\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8911 - val_loss: 1.8735\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0531 - val_loss: 1.0428\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8833 - val_loss: 0.7752\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9687 - val_loss: 1.5610\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.5796 - val_loss: 1.0376\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0342 - val_loss: 0.8252\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7991 - val_loss: 0.5186\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6809 - val_loss: 2.1979\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6773 - val_loss: 0.9379\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9620 - val_loss: 0.6202\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7643 - val_loss: 0.8322\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8342 - val_loss: 0.5556\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8283 - val_loss: 0.8042\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6949 - val_loss: 1.5612\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3745 - val_loss: 3.1494\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9696 - val_loss: 0.5266\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4982 - val_loss: 0.4448\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5117 - val_loss: 0.6497\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5398 - val_loss: 0.1966\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4950 - val_loss: 0.5462\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5218 - val_loss: 1.0619\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5561 - val_loss: 1.1783\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5768 - val_loss: 0.8500\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8311 - val_loss: 1.1900\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4302 - val_loss: 0.4723\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7061 - val_loss: 0.8920\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4875 - val_loss: 1.5029\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7887 - val_loss: 0.3469\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3820 - val_loss: 0.6941\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4063 - val_loss: 1.1478\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.7515 - val_loss: 0.3580\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6432 - val_loss: 0.5826\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3041 - val_loss: 0.5335\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3907 - val_loss: 0.9770\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6927 - val_loss: 0.5957\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4446 - val_loss: 2.5509\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4319 - val_loss: 0.6618\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3281 - val_loss: 0.3863\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4469 - val_loss: 0.4749\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3619 - val_loss: 0.6669\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3160 - val_loss: 0.8342\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3693 - val_loss: 0.8837\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4548 - val_loss: 0.4481\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0240 - val_loss: 11.2116\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7272 - val_loss: 0.1640\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1711 - val_loss: 0.1414\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1903 - val_loss: 0.4422\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2346 - val_loss: 0.1669\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2032 - val_loss: 0.3220\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2513 - val_loss: 0.7312\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3245 - val_loss: 0.2207\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2298 - val_loss: 0.2026\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1906 - val_loss: 0.2061\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.6158 - val_loss: 0.3779\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2172 - val_loss: 0.3658\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2114 - val_loss: 0.3244\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2706 - val_loss: 0.3136\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2076 - val_loss: 0.2134\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3336 - val_loss: 0.2142\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2476 - val_loss: 0.6435\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2245 - val_loss: 0.6906\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3053 - val_loss: 1.3719\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3047 - val_loss: 0.3494\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2861 - val_loss: 0.2072\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2604 - val_loss: 0.1612\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2360 - val_loss: 0.3493\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1743 - val_loss: 0.4780\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4892 - val_loss: 0.3769\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2704 - val_loss: 0.6563\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3276 - val_loss: 0.4291\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1808 - val_loss: 0.1506\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3454 - val_loss: 0.7796\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1682 - val_loss: 0.1888\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1708 - val_loss: 0.2394\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1724 - val_loss: 0.3807\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2856 - val_loss: 0.5551\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.14133619686646404\n",
      "Mean Absolute Error (MAE): 0.2843503523855295\n",
      "Root Mean Squared Error (RMSE): 0.37594706657515503\n",
      "Time taken: 308.51984906196594\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1022.4423 - val_loss: 747.1257\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 543.7584 - val_loss: 512.6935\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 348.7208 - val_loss: 366.2929\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 213.2974 - val_loss: 154.3685\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 89.2151 - val_loss: 73.3081\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 36.3462 - val_loss: 21.8486\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.8859 - val_loss: 12.3051\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.4213 - val_loss: 6.9425\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.2125 - val_loss: 4.3792\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8399 - val_loss: 5.1207\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.9514 - val_loss: 4.5632\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5088 - val_loss: 2.7578\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.4966 - val_loss: 2.6497\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.0369 - val_loss: 9.7565\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.6125 - val_loss: 1.5288\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.8098 - val_loss: 4.2145\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5124 - val_loss: 0.9549\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7044 - val_loss: 2.3404\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.3292 - val_loss: 1.9869\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.3121 - val_loss: 1.5782\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3223 - val_loss: 0.8322\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8902 - val_loss: 3.7198\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7994 - val_loss: 2.7986\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0764 - val_loss: 2.6135\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9455 - val_loss: 0.5554\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7280 - val_loss: 0.8632\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1541 - val_loss: 1.9219\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8813 - val_loss: 0.4401\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.8612 - val_loss: 0.9860\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7004 - val_loss: 0.9110\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8727 - val_loss: 1.2981\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1018 - val_loss: 1.4238\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9971 - val_loss: 1.6905\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7164 - val_loss: 0.5812\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7938 - val_loss: 0.3592\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6665 - val_loss: 2.5301\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.0365 - val_loss: 3.1257\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8649 - val_loss: 0.4918\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5179 - val_loss: 0.7849\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5058 - val_loss: 1.4052\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5191 - val_loss: 0.6798\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5128 - val_loss: 0.7218\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6162 - val_loss: 0.4057\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4487 - val_loss: 0.3923\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5086 - val_loss: 1.1231\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8932 - val_loss: 0.9279\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4864 - val_loss: 0.2928\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6849 - val_loss: 0.8227\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5359 - val_loss: 0.6787\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5941 - val_loss: 0.9921\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5100 - val_loss: 1.2767\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4942 - val_loss: 0.4874\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3488 - val_loss: 0.3947\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4631 - val_loss: 0.7841\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5620 - val_loss: 0.3211\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7759 - val_loss: 0.4236\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3697 - val_loss: 0.2409\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4026 - val_loss: 0.8097\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3358 - val_loss: 0.4565\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3787 - val_loss: 0.4502\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3313 - val_loss: 0.7040\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8135 - val_loss: 0.3289\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2354 - val_loss: 0.2277\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1968 - val_loss: 0.1690\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3740 - val_loss: 0.2233\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2278 - val_loss: 0.2886\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3959 - val_loss: 0.3488\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2688 - val_loss: 0.4443\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2407 - val_loss: 0.5940\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3615 - val_loss: 0.3610\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3001 - val_loss: 0.4723\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4688 - val_loss: 0.3053\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1885 - val_loss: 0.2774\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3207 - val_loss: 0.7204\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8519 - val_loss: 0.2653\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2270 - val_loss: 0.2283\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1828 - val_loss: 0.4501\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2821 - val_loss: 0.3029\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1795 - val_loss: 0.1421\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3119 - val_loss: 3.5779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4800 - val_loss: 0.2427\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2153 - val_loss: 0.3019\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2096 - val_loss: 0.3564\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1736 - val_loss: 0.2612\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2277 - val_loss: 0.5341\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3859 - val_loss: 0.6905\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3726 - val_loss: 0.5008\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2872 - val_loss: 0.2252\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1620 - val_loss: 0.2553\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1816 - val_loss: 0.2799\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3483 - val_loss: 0.1836\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2385 - val_loss: 0.7137\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2161 - val_loss: 0.7576\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2727 - val_loss: 0.3232\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2486 - val_loss: 1.0782\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2262 - val_loss: 0.2575\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1245 - val_loss: 0.4049\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3800 - val_loss: 0.3530\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1333 - val_loss: 0.3216\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2102 - val_loss: 0.9748\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3185 - val_loss: 0.1535\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1544 - val_loss: 0.1345\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2638 - val_loss: 0.4317\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2107 - val_loss: 0.3650\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1582 - val_loss: 0.1393\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1493 - val_loss: 0.4063\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2488 - val_loss: 0.2686\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1751 - val_loss: 0.8827\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2789 - val_loss: 0.2367\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2098 - val_loss: 0.2712\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1556 - val_loss: 0.5278\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2071 - val_loss: 0.4917\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1740 - val_loss: 0.2517\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3258 - val_loss: 0.1127\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5381 - val_loss: 0.7461\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1021 - val_loss: 0.0622\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0912 - val_loss: 0.5244\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1211 - val_loss: 0.1832\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0905 - val_loss: 0.0835\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0977 - val_loss: 0.1893\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1468 - val_loss: 0.3857\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1247 - val_loss: 0.1941\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1822 - val_loss: 0.1461\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2414 - val_loss: 0.3423\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1308 - val_loss: 0.1282\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1336 - val_loss: 0.1368\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1047 - val_loss: 0.4380\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2324 - val_loss: 0.4876\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2138 - val_loss: 0.3107\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1480 - val_loss: 0.0880\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1036 - val_loss: 0.0958\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1157 - val_loss: 0.3720\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1561 - val_loss: 0.4254\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1272 - val_loss: 0.4780\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1786 - val_loss: 1.0760\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7719 - val_loss: 0.1272\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0507 - val_loss: 0.0624\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0584 - val_loss: 0.0976\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0684 - val_loss: 0.1713\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0799 - val_loss: 0.1403\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1034 - val_loss: 0.1400\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0937 - val_loss: 0.1827\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1122 - val_loss: 0.6715\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1628 - val_loss: 0.1910\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0758 - val_loss: 0.1570\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1337 - val_loss: 0.1541\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.06219757870875537\n",
      "Mean Absolute Error (MAE): 0.1743677115829644\n",
      "Root Mean Squared Error (RMSE): 0.24939442397286146\n",
      "Time taken: 473.8651988506317\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1077.7096 - val_loss: 855.4512\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 638.9323 - val_loss: 470.3846\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 384.0997 - val_loss: 308.8430\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 202.2100 - val_loss: 131.7700\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 79.1628 - val_loss: 40.6211\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 30.8753 - val_loss: 16.6159\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.0715 - val_loss: 7.9756\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 7.4312 - val_loss: 3.5412\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4728 - val_loss: 2.6688\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0970 - val_loss: 1.6063\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.9043 - val_loss: 1.4697\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3143 - val_loss: 2.4843\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7221 - val_loss: 0.9903\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4495 - val_loss: 3.1295\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1402 - val_loss: 1.3663\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.5344 - val_loss: 1.8069\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4487 - val_loss: 6.4053\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1839 - val_loss: 1.4729\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5655 - val_loss: 3.8755\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.9874 - val_loss: 2.2279\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7363 - val_loss: 1.8075\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9850 - val_loss: 1.0914\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8275 - val_loss: 0.6522\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7341 - val_loss: 0.3957\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8091 - val_loss: 2.3885\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6805 - val_loss: 0.7551\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3207 - val_loss: 1.8621\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7842 - val_loss: 0.6582\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4920 - val_loss: 0.7569\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4986 - val_loss: 0.6179\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5642 - val_loss: 0.9329\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7683 - val_loss: 1.4016\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4706 - val_loss: 0.5494\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5570 - val_loss: 0.4609\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6801 - val_loss: 0.5412\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7715 - val_loss: 0.6884\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5664 - val_loss: 1.3150\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5428 - val_loss: 1.3877\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6186 - val_loss: 1.2553\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4582 - val_loss: 0.5297\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3454 - val_loss: 0.2337\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3614 - val_loss: 0.5642\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3905 - val_loss: 0.4201\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4793 - val_loss: 0.7118\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4737 - val_loss: 0.4120\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5421 - val_loss: 0.7040\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4182 - val_loss: 0.6467\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4431 - val_loss: 1.3746\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4784 - val_loss: 0.4204\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3756 - val_loss: 0.2495\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4591 - val_loss: 0.5776\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5043 - val_loss: 0.7840\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5425 - val_loss: 0.3813\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4814 - val_loss: 0.2572\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3308 - val_loss: 0.4535\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.1130 - val_loss: 1.1457\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2545 - val_loss: 0.2923\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2473 - val_loss: 0.2807\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3598 - val_loss: 0.2362\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5870 - val_loss: 0.3071\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2858 - val_loss: 0.2881\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3898 - val_loss: 0.8777\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2640 - val_loss: 0.1746\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2675 - val_loss: 0.7549\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3227 - val_loss: 0.2778\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2125 - val_loss: 0.2539\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3813 - val_loss: 0.4002\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3496 - val_loss: 0.2268\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2987 - val_loss: 0.2847\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2459 - val_loss: 0.3462\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3171 - val_loss: 0.4686\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2771 - val_loss: 0.9808\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4495 - val_loss: 0.5102\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1774 - val_loss: 0.2894\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3248 - val_loss: 0.1858\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2922 - val_loss: 0.4445\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2831 - val_loss: 0.3224\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2055 - val_loss: 0.3121\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2166 - val_loss: 0.7991\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9295 - val_loss: 0.2506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1842 - val_loss: 0.2816\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2208 - val_loss: 0.3382\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1861 - val_loss: 0.1402\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1388 - val_loss: 0.2228\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3414 - val_loss: 0.1450\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1572 - val_loss: 0.1639\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1546 - val_loss: 0.2042\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3552 - val_loss: 1.0800\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6793 - val_loss: 0.2669\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1002 - val_loss: 0.0875\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1346 - val_loss: 0.1072\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2031 - val_loss: 0.2805\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1811 - val_loss: 0.2033\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1853 - val_loss: 0.1372\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2027 - val_loss: 0.1547\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2583 - val_loss: 0.6504\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2315 - val_loss: 0.2863\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1604 - val_loss: 0.2138\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1617 - val_loss: 0.1348\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1975 - val_loss: 0.1945\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1618 - val_loss: 0.3677\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1905 - val_loss: 0.1841\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2144 - val_loss: 0.1906\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2308 - val_loss: 0.1137\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1382 - val_loss: 0.3616\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1802 - val_loss: 0.0910\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2955 - val_loss: 21.4743\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2408 - val_loss: 0.0553\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0600 - val_loss: 0.0582\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0808 - val_loss: 0.1297\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1006 - val_loss: 0.2141\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1128 - val_loss: 0.1195\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1203 - val_loss: 0.1356\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1050 - val_loss: 0.0862\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2248 - val_loss: 0.4133\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1575 - val_loss: 0.2369\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1420 - val_loss: 0.2664\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1561 - val_loss: 0.1860\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1487 - val_loss: 0.0771\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1202 - val_loss: 0.1661\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1359 - val_loss: 0.1258\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1844 - val_loss: 0.5207\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1411 - val_loss: 0.2088\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2068 - val_loss: 0.3400\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1061 - val_loss: 0.2129\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1544 - val_loss: 0.4957\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1745 - val_loss: 0.8868\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1186 - val_loss: 0.4855\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1989 - val_loss: 0.2114\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1206 - val_loss: 0.2922\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1531 - val_loss: 0.4181\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1639 - val_loss: 0.2444\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2220 - val_loss: 0.6942\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1883 - val_loss: 0.1489\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1170 - val_loss: 0.2663\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0942 - val_loss: 0.1340\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0931 - val_loss: 0.3267\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1159 - val_loss: 0.1436\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.05517967888350175\n",
      "Mean Absolute Error (MAE): 0.17117020809190361\n",
      "Root Mean Squared Error (RMSE): 0.2349035523007299\n",
      "Time taken: 443.2807331085205\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1063.6368 - val_loss: 858.8666\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 647.2478 - val_loss: 613.4735\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 373.1615 - val_loss: 277.1743\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 182.6647 - val_loss: 125.6536\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 71.6431 - val_loss: 42.3718\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 28.8525 - val_loss: 35.2448\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 15.1790 - val_loss: 11.4052\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7464 - val_loss: 5.8109\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9463 - val_loss: 3.0882\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6494 - val_loss: 3.7143\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.3329 - val_loss: 2.4371\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6050 - val_loss: 0.8733\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8785 - val_loss: 2.4090\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3387 - val_loss: 1.4695\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.8032 - val_loss: 1.3013\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1136 - val_loss: 3.1273\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3925 - val_loss: 0.9100\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4524 - val_loss: 1.9323\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8591 - val_loss: 0.5005\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8814 - val_loss: 0.9583\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0061 - val_loss: 0.9142\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9615 - val_loss: 2.7795\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.6638 - val_loss: 1.2754\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7467 - val_loss: 0.3730\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9262 - val_loss: 0.4791\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6642 - val_loss: 0.9981\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5999 - val_loss: 0.8584\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.9630 - val_loss: 1.2542\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5394 - val_loss: 1.1485\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0949 - val_loss: 1.1293\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6041 - val_loss: 0.7136\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9316 - val_loss: 0.6866\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5011 - val_loss: 1.3218\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5601 - val_loss: 1.6906\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8342 - val_loss: 0.7281\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5052 - val_loss: 0.7163\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5814 - val_loss: 1.1126\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2058 - val_loss: 3.1295\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5545 - val_loss: 0.2200\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2998 - val_loss: 0.4095\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3484 - val_loss: 0.3712\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4005 - val_loss: 0.7158\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5352 - val_loss: 0.7695\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5091 - val_loss: 0.6113\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4893 - val_loss: 0.2660\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2627 - val_loss: 0.3753\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9602 - val_loss: 0.6325\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3191 - val_loss: 0.4008\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4674 - val_loss: 0.7587\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4099 - val_loss: 0.5272\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4720 - val_loss: 0.2731\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4951 - val_loss: 1.0480\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5898 - val_loss: 0.3288\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4399 - val_loss: 1.6232\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4741 - val_loss: 0.4009\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0700 - val_loss: 5.6821\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5887 - val_loss: 0.2278\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1828 - val_loss: 0.1399\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1517 - val_loss: 0.1934\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2091 - val_loss: 0.8920\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1927 - val_loss: 0.2139\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2362 - val_loss: 0.2618\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3242 - val_loss: 0.5193\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5053 - val_loss: 0.4889\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1904 - val_loss: 0.1462\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8078 - val_loss: 0.5110\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1758 - val_loss: 0.1836\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2166 - val_loss: 0.1707\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1982 - val_loss: 0.1197\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3465 - val_loss: 0.5732\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3907 - val_loss: 0.3074\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3034 - val_loss: 3.0624\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3459 - val_loss: 0.3031\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3037 - val_loss: 0.3521\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9884 - val_loss: 0.1477\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1395 - val_loss: 0.1295\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1357 - val_loss: 0.5375\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1877 - val_loss: 0.3757\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1546 - val_loss: 0.2175\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3490 - val_loss: 0.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2850 - val_loss: 0.1553\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1455 - val_loss: 0.1531\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2083 - val_loss: 0.8165\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5215 - val_loss: 0.1700\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1415 - val_loss: 0.2037\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1764 - val_loss: 0.1909\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2455 - val_loss: 0.1680\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3745 - val_loss: 0.7277\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1790 - val_loss: 0.1067\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0917 - val_loss: 0.0836\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0960 - val_loss: 0.1499\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1744 - val_loss: 0.1587\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1298 - val_loss: 0.1206\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1951 - val_loss: 0.9329\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2548 - val_loss: 0.2538\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1829 - val_loss: 0.4623\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2135 - val_loss: 0.1920\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1453 - val_loss: 0.5186\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1946 - val_loss: 0.3962\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1969 - val_loss: 0.1134\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2259 - val_loss: 0.2441\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1422 - val_loss: 0.2328\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3369 - val_loss: 0.8885\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1762 - val_loss: 0.2486\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1579 - val_loss: 0.2504\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1417 - val_loss: 0.6732\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1895 - val_loss: 0.4606\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1762 - val_loss: 2.0501\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8761 - val_loss: 0.5615\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1319 - val_loss: 0.1003\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0895 - val_loss: 0.0692\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0898 - val_loss: 0.1505\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0912 - val_loss: 0.1264\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1701 - val_loss: 0.5048\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1453 - val_loss: 0.5106\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1730 - val_loss: 0.2255\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1255 - val_loss: 0.1224\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2378 - val_loss: 0.1261\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2286 - val_loss: 0.3737\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1452 - val_loss: 0.1406\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1548 - val_loss: 0.3246\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2019 - val_loss: 0.1415\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1136 - val_loss: 0.1922\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1030 - val_loss: 0.1569\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2254 - val_loss: 0.2625\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1720 - val_loss: 0.5658\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1360 - val_loss: 0.1225\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0946 - val_loss: 0.3436\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1420 - val_loss: 0.3966\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1555 - val_loss: 0.3818\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2096 - val_loss: 0.4158\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1475 - val_loss: 0.1575\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1856 - val_loss: 0.1087\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1728 - val_loss: 0.4327\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1228 - val_loss: 0.0965\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1112 - val_loss: 0.1978\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0940 - val_loss: 0.2833\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2235 - val_loss: 0.2070\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1512 - val_loss: 0.1020\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1306 - val_loss: 1.2641\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1275 - val_loss: 0.1334\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.06924965039263763\n",
      "Mean Absolute Error (MAE): 0.192546184771455\n",
      "Root Mean Squared Error (RMSE): 0.2631532830740244\n",
      "Time taken: 447.88270926475525\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 13ms/step - loss: 1068.8199 - val_loss: 847.4100\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 683.4905 - val_loss: 541.1461\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 404.0965 - val_loss: 298.8670\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 222.4692 - val_loss: 163.7322\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 107.1787 - val_loss: 72.9812\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 43.5698 - val_loss: 24.2137\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 17.4567 - val_loss: 10.8869\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.9353 - val_loss: 6.4967\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3510 - val_loss: 3.4663\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.1526 - val_loss: 6.2650\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4670 - val_loss: 1.6533\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.7583 - val_loss: 1.4499\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5807 - val_loss: 1.6263\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.6192 - val_loss: 0.7082\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.1599 - val_loss: 1.5113\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2859 - val_loss: 1.7596\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9683 - val_loss: 1.1133\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.2992 - val_loss: 0.8316\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7270 - val_loss: 1.5013\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1521 - val_loss: 1.2070\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9403 - val_loss: 3.5221\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.0486 - val_loss: 1.5083\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7745 - val_loss: 0.7085\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6249 - val_loss: 0.3651\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4980 - val_loss: 0.8296\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7393 - val_loss: 2.1461\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6491 - val_loss: 0.3767\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6253 - val_loss: 0.4362\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8758 - val_loss: 1.0917\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5871 - val_loss: 0.6352\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8208 - val_loss: 1.3005\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7642 - val_loss: 0.9305\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5364 - val_loss: 0.2590\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9561 - val_loss: 3.4197\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.5770 - val_loss: 0.5154\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7443 - val_loss: 0.4696\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5555 - val_loss: 0.3932\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.4444 - val_loss: 0.6998\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5031 - val_loss: 0.5698\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9402 - val_loss: 1.0340\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4989 - val_loss: 0.8757\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4006 - val_loss: 0.7214\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7225 - val_loss: 0.2923\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7482 - val_loss: 0.5890\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4405 - val_loss: 1.5934\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5639 - val_loss: 0.5357\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3357 - val_loss: 0.4199\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4245 - val_loss: 0.6177\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0117 - val_loss: 0.6043\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4878 - val_loss: 1.2734\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2487 - val_loss: 0.1222\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2465 - val_loss: 1.0583\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5444 - val_loss: 0.7676\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3276 - val_loss: 0.7952\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5553 - val_loss: 0.3436\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3448 - val_loss: 0.3707\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2901 - val_loss: 0.1900\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3927 - val_loss: 0.6287\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3662 - val_loss: 1.0636\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4860 - val_loss: 0.4484\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3918 - val_loss: 0.7599\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4329 - val_loss: 0.7861\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3500 - val_loss: 0.4462\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4170 - val_loss: 43.8172\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0461 - val_loss: 0.7377\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1775 - val_loss: 0.1840\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1739 - val_loss: 0.1144\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1446 - val_loss: 0.1888\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2005 - val_loss: 0.0970\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1553 - val_loss: 0.2682\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2536 - val_loss: 0.2045\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1680 - val_loss: 0.1795\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1647 - val_loss: 0.1618\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2205 - val_loss: 0.2778\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2048 - val_loss: 0.2305\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2828 - val_loss: 0.5424\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2603 - val_loss: 0.3493\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2554 - val_loss: 0.9215\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3805 - val_loss: 0.2651\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1660 - val_loss: 0.3059\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2256 - val_loss: 0.3094\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3017 - val_loss: 0.2887\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2870 - val_loss: 0.2427\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3023 - val_loss: 0.1840\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2402 - val_loss: 0.5321\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1803 - val_loss: 0.2874\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4227 - val_loss: 0.1975\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1517 - val_loss: 0.2477\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2784 - val_loss: 0.3897\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1803 - val_loss: 0.3832\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2264 - val_loss: 0.4795\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3137 - val_loss: 0.3312\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1608 - val_loss: 0.2068\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1535 - val_loss: 0.5630\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1955 - val_loss: 0.2772\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5636 - val_loss: 0.4294\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1371 - val_loss: 0.4751\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.0968 - val_loss: 0.1420\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2342 - val_loss: 0.6861\n",
      "16/16 [==============================] - 1s 4ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.09697852335206403\n",
      "Mean Absolute Error (MAE): 0.2355422763040277\n",
      "Root Mean Squared Error (RMSE): 0.31141374945892164\n",
      "Time taken: 322.4002125263214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_14688\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.141336  0.284350  0.375947  308.519849\n",
      "1        2  0.062198  0.174368  0.249394  473.865199\n",
      "2        3  0.055180  0.171170  0.234904  443.280733\n",
      "3        4  0.069250  0.192546  0.263153  447.882709\n",
      "4        5  0.096979  0.235542  0.311414  322.400213\n",
      "5  Average  0.084988  0.211595  0.286962  399.189741\n",
      "Results saved to 'LSTM Results PL_model_1_smoothing2_Reg1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_1_smoothing2_Reg1.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_1_smoothing2_Reg1.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
