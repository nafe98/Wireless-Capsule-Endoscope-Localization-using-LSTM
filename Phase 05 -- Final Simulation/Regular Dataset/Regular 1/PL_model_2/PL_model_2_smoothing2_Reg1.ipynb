{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4e64b",
   "metadata": {},
   "source": [
    "# Importing Libraries for Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a085cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014d550",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a290f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.read_excel('PL_model_2_smoothing2_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb43499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.255262</td>\n",
       "      <td>71.971142</td>\n",
       "      <td>59.297033</td>\n",
       "      <td>70.168324</td>\n",
       "      <td>72.223257</td>\n",
       "      <td>78.156174</td>\n",
       "      <td>70.720922</td>\n",
       "      <td>71.671559</td>\n",
       "      <td>66.034745</td>\n",
       "      <td>68.503224</td>\n",
       "      <td>...</td>\n",
       "      <td>68.551835</td>\n",
       "      <td>66.226435</td>\n",
       "      <td>61.979756</td>\n",
       "      <td>61.124430</td>\n",
       "      <td>73.919407</td>\n",
       "      <td>69.627583</td>\n",
       "      <td>69.312761</td>\n",
       "      <td>70.642658</td>\n",
       "      <td>60.969641</td>\n",
       "      <td>71.643253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.029172</td>\n",
       "      <td>72.095736</td>\n",
       "      <td>59.195081</td>\n",
       "      <td>70.251247</td>\n",
       "      <td>72.172055</td>\n",
       "      <td>78.132133</td>\n",
       "      <td>70.750461</td>\n",
       "      <td>71.889199</td>\n",
       "      <td>66.181062</td>\n",
       "      <td>68.578317</td>\n",
       "      <td>...</td>\n",
       "      <td>68.400324</td>\n",
       "      <td>66.206345</td>\n",
       "      <td>62.317160</td>\n",
       "      <td>61.257023</td>\n",
       "      <td>73.842502</td>\n",
       "      <td>69.644318</td>\n",
       "      <td>69.143555</td>\n",
       "      <td>70.733665</td>\n",
       "      <td>60.837893</td>\n",
       "      <td>71.609484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.807358</td>\n",
       "      <td>72.215594</td>\n",
       "      <td>59.102536</td>\n",
       "      <td>70.334800</td>\n",
       "      <td>72.118766</td>\n",
       "      <td>78.106993</td>\n",
       "      <td>70.778609</td>\n",
       "      <td>72.106459</td>\n",
       "      <td>66.327732</td>\n",
       "      <td>68.653060</td>\n",
       "      <td>...</td>\n",
       "      <td>68.255380</td>\n",
       "      <td>66.185249</td>\n",
       "      <td>62.651930</td>\n",
       "      <td>61.392031</td>\n",
       "      <td>73.768352</td>\n",
       "      <td>69.654955</td>\n",
       "      <td>68.974929</td>\n",
       "      <td>70.827779</td>\n",
       "      <td>60.708759</td>\n",
       "      <td>71.577138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.590422</td>\n",
       "      <td>72.330512</td>\n",
       "      <td>59.019202</td>\n",
       "      <td>70.419252</td>\n",
       "      <td>72.063870</td>\n",
       "      <td>78.081097</td>\n",
       "      <td>70.804871</td>\n",
       "      <td>72.323391</td>\n",
       "      <td>66.474047</td>\n",
       "      <td>68.727520</td>\n",
       "      <td>...</td>\n",
       "      <td>68.117018</td>\n",
       "      <td>66.163235</td>\n",
       "      <td>62.984053</td>\n",
       "      <td>61.529815</td>\n",
       "      <td>73.696280</td>\n",
       "      <td>69.659422</td>\n",
       "      <td>68.807551</td>\n",
       "      <td>70.924403</td>\n",
       "      <td>60.581964</td>\n",
       "      <td>71.546492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.378722</td>\n",
       "      <td>72.440584</td>\n",
       "      <td>58.944518</td>\n",
       "      <td>70.504615</td>\n",
       "      <td>72.007938</td>\n",
       "      <td>78.054497</td>\n",
       "      <td>70.828614</td>\n",
       "      <td>72.539718</td>\n",
       "      <td>66.619117</td>\n",
       "      <td>68.801921</td>\n",
       "      <td>...</td>\n",
       "      <td>67.985182</td>\n",
       "      <td>66.140380</td>\n",
       "      <td>63.313694</td>\n",
       "      <td>61.670413</td>\n",
       "      <td>73.625580</td>\n",
       "      <td>69.657956</td>\n",
       "      <td>68.641912</td>\n",
       "      <td>71.022581</td>\n",
       "      <td>60.456878</td>\n",
       "      <td>71.517763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>73.098106</td>\n",
       "      <td>72.579984</td>\n",
       "      <td>70.393482</td>\n",
       "      <td>57.539933</td>\n",
       "      <td>76.493660</td>\n",
       "      <td>77.698430</td>\n",
       "      <td>73.453426</td>\n",
       "      <td>68.863653</td>\n",
       "      <td>71.662133</td>\n",
       "      <td>68.616475</td>\n",
       "      <td>...</td>\n",
       "      <td>66.113821</td>\n",
       "      <td>69.106403</td>\n",
       "      <td>65.986037</td>\n",
       "      <td>52.906771</td>\n",
       "      <td>72.026893</td>\n",
       "      <td>70.616229</td>\n",
       "      <td>71.559465</td>\n",
       "      <td>75.327743</td>\n",
       "      <td>65.497743</td>\n",
       "      <td>61.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>73.069455</td>\n",
       "      <td>72.686191</td>\n",
       "      <td>70.593724</td>\n",
       "      <td>57.422708</td>\n",
       "      <td>76.310721</td>\n",
       "      <td>77.797643</td>\n",
       "      <td>73.449306</td>\n",
       "      <td>68.746579</td>\n",
       "      <td>71.740129</td>\n",
       "      <td>68.624573</td>\n",
       "      <td>...</td>\n",
       "      <td>65.951813</td>\n",
       "      <td>69.014658</td>\n",
       "      <td>65.824532</td>\n",
       "      <td>52.921059</td>\n",
       "      <td>71.964983</td>\n",
       "      <td>70.654036</td>\n",
       "      <td>71.570827</td>\n",
       "      <td>75.516862</td>\n",
       "      <td>65.328859</td>\n",
       "      <td>61.156410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>73.044731</td>\n",
       "      <td>72.796004</td>\n",
       "      <td>70.796814</td>\n",
       "      <td>57.304143</td>\n",
       "      <td>76.124633</td>\n",
       "      <td>77.899235</td>\n",
       "      <td>73.448736</td>\n",
       "      <td>68.627163</td>\n",
       "      <td>71.822711</td>\n",
       "      <td>68.629821</td>\n",
       "      <td>...</td>\n",
       "      <td>65.790041</td>\n",
       "      <td>68.920597</td>\n",
       "      <td>65.658370</td>\n",
       "      <td>52.938117</td>\n",
       "      <td>71.897161</td>\n",
       "      <td>70.690490</td>\n",
       "      <td>71.585216</td>\n",
       "      <td>75.707701</td>\n",
       "      <td>65.159436</td>\n",
       "      <td>61.274501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>73.024322</td>\n",
       "      <td>72.909059</td>\n",
       "      <td>71.003291</td>\n",
       "      <td>57.183712</td>\n",
       "      <td>75.934742</td>\n",
       "      <td>78.003086</td>\n",
       "      <td>73.451758</td>\n",
       "      <td>68.505253</td>\n",
       "      <td>71.909019</td>\n",
       "      <td>68.631663</td>\n",
       "      <td>...</td>\n",
       "      <td>65.628825</td>\n",
       "      <td>68.823687</td>\n",
       "      <td>65.488250</td>\n",
       "      <td>52.958021</td>\n",
       "      <td>71.823691</td>\n",
       "      <td>70.725859</td>\n",
       "      <td>71.602832</td>\n",
       "      <td>75.900074</td>\n",
       "      <td>64.989387</td>\n",
       "      <td>61.396096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>73.008414</td>\n",
       "      <td>73.024663</td>\n",
       "      <td>71.213720</td>\n",
       "      <td>57.061155</td>\n",
       "      <td>75.740776</td>\n",
       "      <td>78.108850</td>\n",
       "      <td>73.458031</td>\n",
       "      <td>68.380598</td>\n",
       "      <td>71.998276</td>\n",
       "      <td>68.629823</td>\n",
       "      <td>...</td>\n",
       "      <td>65.468769</td>\n",
       "      <td>68.723471</td>\n",
       "      <td>65.315066</td>\n",
       "      <td>52.980600</td>\n",
       "      <td>71.744712</td>\n",
       "      <td>70.760411</td>\n",
       "      <td>71.623714</td>\n",
       "      <td>76.093722</td>\n",
       "      <td>64.818402</td>\n",
       "      <td>61.520882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5   \\\n",
       "0     72.255262  71.971142  59.297033  70.168324  72.223257  78.156174   \n",
       "1     72.029172  72.095736  59.195081  70.251247  72.172055  78.132133   \n",
       "2     71.807358  72.215594  59.102536  70.334800  72.118766  78.106993   \n",
       "3     71.590422  72.330512  59.019202  70.419252  72.063870  78.081097   \n",
       "4     71.378722  72.440584  58.944518  70.504615  72.007938  78.054497   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.098106  72.579984  70.393482  57.539933  76.493660  77.698430   \n",
       "2439  73.069455  72.686191  70.593724  57.422708  76.310721  77.797643   \n",
       "2440  73.044731  72.796004  70.796814  57.304143  76.124633  77.899235   \n",
       "2441  73.024322  72.909059  71.003291  57.183712  75.934742  78.003086   \n",
       "2442  73.008414  73.024663  71.213720  57.061155  75.740776  78.108850   \n",
       "\n",
       "             6          7          8          9   ...         38         39  \\\n",
       "0     70.720922  71.671559  66.034745  68.503224  ...  68.551835  66.226435   \n",
       "1     70.750461  71.889199  66.181062  68.578317  ...  68.400324  66.206345   \n",
       "2     70.778609  72.106459  66.327732  68.653060  ...  68.255380  66.185249   \n",
       "3     70.804871  72.323391  66.474047  68.727520  ...  68.117018  66.163235   \n",
       "4     70.828614  72.539718  66.619117  68.801921  ...  67.985182  66.140380   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  73.453426  68.863653  71.662133  68.616475  ...  66.113821  69.106403   \n",
       "2439  73.449306  68.746579  71.740129  68.624573  ...  65.951813  69.014658   \n",
       "2440  73.448736  68.627163  71.822711  68.629821  ...  65.790041  68.920597   \n",
       "2441  73.451758  68.505253  71.909019  68.631663  ...  65.628825  68.823687   \n",
       "2442  73.458031  68.380598  71.998276  68.629823  ...  65.468769  68.723471   \n",
       "\n",
       "             40         41         42         43         44         45  \\\n",
       "0     61.979756  61.124430  73.919407  69.627583  69.312761  70.642658   \n",
       "1     62.317160  61.257023  73.842502  69.644318  69.143555  70.733665   \n",
       "2     62.651930  61.392031  73.768352  69.654955  68.974929  70.827779   \n",
       "3     62.984053  61.529815  73.696280  69.659422  68.807551  70.924403   \n",
       "4     63.313694  61.670413  73.625580  69.657956  68.641912  71.022581   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  65.986037  52.906771  72.026893  70.616229  71.559465  75.327743   \n",
       "2439  65.824532  52.921059  71.964983  70.654036  71.570827  75.516862   \n",
       "2440  65.658370  52.938117  71.897161  70.690490  71.585216  75.707701   \n",
       "2441  65.488250  52.958021  71.823691  70.725859  71.602832  75.900074   \n",
       "2442  65.315066  52.980600  71.744712  70.760411  71.623714  76.093722   \n",
       "\n",
       "             46         47  \n",
       "0     60.969641  71.643253  \n",
       "1     60.837893  71.609484  \n",
       "2     60.708759  71.577138  \n",
       "3     60.581964  71.546492  \n",
       "4     60.456878  71.517763  \n",
       "...         ...        ...  \n",
       "2438  65.497743  61.041995  \n",
       "2439  65.328859  61.156410  \n",
       "2440  65.159436  61.274501  \n",
       "2441  64.989387  61.396096  \n",
       "2442  64.818402  61.520882  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7103d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.read_excel('real_positions_Reg1.xlsx', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1      2\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ead48",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b3cbc",
   "metadata": {},
   "source": [
    "### Sensors Data -- Labeling Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Number_of_Columns = sensors_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_columns = [\"sensor\" + str(x) for x in range(1,Sensors_Number_of_Columns + 1)]\n",
    "sensors_data.columns = (Sensors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb9c359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor39</th>\n",
       "      <th>sensor40</th>\n",
       "      <th>sensor41</th>\n",
       "      <th>sensor42</th>\n",
       "      <th>sensor43</th>\n",
       "      <th>sensor44</th>\n",
       "      <th>sensor45</th>\n",
       "      <th>sensor46</th>\n",
       "      <th>sensor47</th>\n",
       "      <th>sensor48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.255262</td>\n",
       "      <td>71.971142</td>\n",
       "      <td>59.297033</td>\n",
       "      <td>70.168324</td>\n",
       "      <td>72.223257</td>\n",
       "      <td>78.156174</td>\n",
       "      <td>70.720922</td>\n",
       "      <td>71.671559</td>\n",
       "      <td>66.034745</td>\n",
       "      <td>68.503224</td>\n",
       "      <td>...</td>\n",
       "      <td>68.551835</td>\n",
       "      <td>66.226435</td>\n",
       "      <td>61.979756</td>\n",
       "      <td>61.124430</td>\n",
       "      <td>73.919407</td>\n",
       "      <td>69.627583</td>\n",
       "      <td>69.312761</td>\n",
       "      <td>70.642658</td>\n",
       "      <td>60.969641</td>\n",
       "      <td>71.643253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.029172</td>\n",
       "      <td>72.095736</td>\n",
       "      <td>59.195081</td>\n",
       "      <td>70.251247</td>\n",
       "      <td>72.172055</td>\n",
       "      <td>78.132133</td>\n",
       "      <td>70.750461</td>\n",
       "      <td>71.889199</td>\n",
       "      <td>66.181062</td>\n",
       "      <td>68.578317</td>\n",
       "      <td>...</td>\n",
       "      <td>68.400324</td>\n",
       "      <td>66.206345</td>\n",
       "      <td>62.317160</td>\n",
       "      <td>61.257023</td>\n",
       "      <td>73.842502</td>\n",
       "      <td>69.644318</td>\n",
       "      <td>69.143555</td>\n",
       "      <td>70.733665</td>\n",
       "      <td>60.837893</td>\n",
       "      <td>71.609484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.807358</td>\n",
       "      <td>72.215594</td>\n",
       "      <td>59.102536</td>\n",
       "      <td>70.334800</td>\n",
       "      <td>72.118766</td>\n",
       "      <td>78.106993</td>\n",
       "      <td>70.778609</td>\n",
       "      <td>72.106459</td>\n",
       "      <td>66.327732</td>\n",
       "      <td>68.653060</td>\n",
       "      <td>...</td>\n",
       "      <td>68.255380</td>\n",
       "      <td>66.185249</td>\n",
       "      <td>62.651930</td>\n",
       "      <td>61.392031</td>\n",
       "      <td>73.768352</td>\n",
       "      <td>69.654955</td>\n",
       "      <td>68.974929</td>\n",
       "      <td>70.827779</td>\n",
       "      <td>60.708759</td>\n",
       "      <td>71.577138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.590422</td>\n",
       "      <td>72.330512</td>\n",
       "      <td>59.019202</td>\n",
       "      <td>70.419252</td>\n",
       "      <td>72.063870</td>\n",
       "      <td>78.081097</td>\n",
       "      <td>70.804871</td>\n",
       "      <td>72.323391</td>\n",
       "      <td>66.474047</td>\n",
       "      <td>68.727520</td>\n",
       "      <td>...</td>\n",
       "      <td>68.117018</td>\n",
       "      <td>66.163235</td>\n",
       "      <td>62.984053</td>\n",
       "      <td>61.529815</td>\n",
       "      <td>73.696280</td>\n",
       "      <td>69.659422</td>\n",
       "      <td>68.807551</td>\n",
       "      <td>70.924403</td>\n",
       "      <td>60.581964</td>\n",
       "      <td>71.546492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.378722</td>\n",
       "      <td>72.440584</td>\n",
       "      <td>58.944518</td>\n",
       "      <td>70.504615</td>\n",
       "      <td>72.007938</td>\n",
       "      <td>78.054497</td>\n",
       "      <td>70.828614</td>\n",
       "      <td>72.539718</td>\n",
       "      <td>66.619117</td>\n",
       "      <td>68.801921</td>\n",
       "      <td>...</td>\n",
       "      <td>67.985182</td>\n",
       "      <td>66.140380</td>\n",
       "      <td>63.313694</td>\n",
       "      <td>61.670413</td>\n",
       "      <td>73.625580</td>\n",
       "      <td>69.657956</td>\n",
       "      <td>68.641912</td>\n",
       "      <td>71.022581</td>\n",
       "      <td>60.456878</td>\n",
       "      <td>71.517763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>73.098106</td>\n",
       "      <td>72.579984</td>\n",
       "      <td>70.393482</td>\n",
       "      <td>57.539933</td>\n",
       "      <td>76.493660</td>\n",
       "      <td>77.698430</td>\n",
       "      <td>73.453426</td>\n",
       "      <td>68.863653</td>\n",
       "      <td>71.662133</td>\n",
       "      <td>68.616475</td>\n",
       "      <td>...</td>\n",
       "      <td>66.113821</td>\n",
       "      <td>69.106403</td>\n",
       "      <td>65.986037</td>\n",
       "      <td>52.906771</td>\n",
       "      <td>72.026893</td>\n",
       "      <td>70.616229</td>\n",
       "      <td>71.559465</td>\n",
       "      <td>75.327743</td>\n",
       "      <td>65.497743</td>\n",
       "      <td>61.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>73.069455</td>\n",
       "      <td>72.686191</td>\n",
       "      <td>70.593724</td>\n",
       "      <td>57.422708</td>\n",
       "      <td>76.310721</td>\n",
       "      <td>77.797643</td>\n",
       "      <td>73.449306</td>\n",
       "      <td>68.746579</td>\n",
       "      <td>71.740129</td>\n",
       "      <td>68.624573</td>\n",
       "      <td>...</td>\n",
       "      <td>65.951813</td>\n",
       "      <td>69.014658</td>\n",
       "      <td>65.824532</td>\n",
       "      <td>52.921059</td>\n",
       "      <td>71.964983</td>\n",
       "      <td>70.654036</td>\n",
       "      <td>71.570827</td>\n",
       "      <td>75.516862</td>\n",
       "      <td>65.328859</td>\n",
       "      <td>61.156410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>73.044731</td>\n",
       "      <td>72.796004</td>\n",
       "      <td>70.796814</td>\n",
       "      <td>57.304143</td>\n",
       "      <td>76.124633</td>\n",
       "      <td>77.899235</td>\n",
       "      <td>73.448736</td>\n",
       "      <td>68.627163</td>\n",
       "      <td>71.822711</td>\n",
       "      <td>68.629821</td>\n",
       "      <td>...</td>\n",
       "      <td>65.790041</td>\n",
       "      <td>68.920597</td>\n",
       "      <td>65.658370</td>\n",
       "      <td>52.938117</td>\n",
       "      <td>71.897161</td>\n",
       "      <td>70.690490</td>\n",
       "      <td>71.585216</td>\n",
       "      <td>75.707701</td>\n",
       "      <td>65.159436</td>\n",
       "      <td>61.274501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>73.024322</td>\n",
       "      <td>72.909059</td>\n",
       "      <td>71.003291</td>\n",
       "      <td>57.183712</td>\n",
       "      <td>75.934742</td>\n",
       "      <td>78.003086</td>\n",
       "      <td>73.451758</td>\n",
       "      <td>68.505253</td>\n",
       "      <td>71.909019</td>\n",
       "      <td>68.631663</td>\n",
       "      <td>...</td>\n",
       "      <td>65.628825</td>\n",
       "      <td>68.823687</td>\n",
       "      <td>65.488250</td>\n",
       "      <td>52.958021</td>\n",
       "      <td>71.823691</td>\n",
       "      <td>70.725859</td>\n",
       "      <td>71.602832</td>\n",
       "      <td>75.900074</td>\n",
       "      <td>64.989387</td>\n",
       "      <td>61.396096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>73.008414</td>\n",
       "      <td>73.024663</td>\n",
       "      <td>71.213720</td>\n",
       "      <td>57.061155</td>\n",
       "      <td>75.740776</td>\n",
       "      <td>78.108850</td>\n",
       "      <td>73.458031</td>\n",
       "      <td>68.380598</td>\n",
       "      <td>71.998276</td>\n",
       "      <td>68.629823</td>\n",
       "      <td>...</td>\n",
       "      <td>65.468769</td>\n",
       "      <td>68.723471</td>\n",
       "      <td>65.315066</td>\n",
       "      <td>52.980600</td>\n",
       "      <td>71.744712</td>\n",
       "      <td>70.760411</td>\n",
       "      <td>71.623714</td>\n",
       "      <td>76.093722</td>\n",
       "      <td>64.818402</td>\n",
       "      <td>61.520882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     72.255262  71.971142  59.297033  70.168324  72.223257  78.156174   \n",
       "1     72.029172  72.095736  59.195081  70.251247  72.172055  78.132133   \n",
       "2     71.807358  72.215594  59.102536  70.334800  72.118766  78.106993   \n",
       "3     71.590422  72.330512  59.019202  70.419252  72.063870  78.081097   \n",
       "4     71.378722  72.440584  58.944518  70.504615  72.007938  78.054497   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.098106  72.579984  70.393482  57.539933  76.493660  77.698430   \n",
       "2439  73.069455  72.686191  70.593724  57.422708  76.310721  77.797643   \n",
       "2440  73.044731  72.796004  70.796814  57.304143  76.124633  77.899235   \n",
       "2441  73.024322  72.909059  71.003291  57.183712  75.934742  78.003086   \n",
       "2442  73.008414  73.024663  71.213720  57.061155  75.740776  78.108850   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10  ...   sensor39   sensor40  \\\n",
       "0     70.720922  71.671559  66.034745  68.503224  ...  68.551835  66.226435   \n",
       "1     70.750461  71.889199  66.181062  68.578317  ...  68.400324  66.206345   \n",
       "2     70.778609  72.106459  66.327732  68.653060  ...  68.255380  66.185249   \n",
       "3     70.804871  72.323391  66.474047  68.727520  ...  68.117018  66.163235   \n",
       "4     70.828614  72.539718  66.619117  68.801921  ...  67.985182  66.140380   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "2438  73.453426  68.863653  71.662133  68.616475  ...  66.113821  69.106403   \n",
       "2439  73.449306  68.746579  71.740129  68.624573  ...  65.951813  69.014658   \n",
       "2440  73.448736  68.627163  71.822711  68.629821  ...  65.790041  68.920597   \n",
       "2441  73.451758  68.505253  71.909019  68.631663  ...  65.628825  68.823687   \n",
       "2442  73.458031  68.380598  71.998276  68.629823  ...  65.468769  68.723471   \n",
       "\n",
       "       sensor41   sensor42   sensor43   sensor44   sensor45   sensor46  \\\n",
       "0     61.979756  61.124430  73.919407  69.627583  69.312761  70.642658   \n",
       "1     62.317160  61.257023  73.842502  69.644318  69.143555  70.733665   \n",
       "2     62.651930  61.392031  73.768352  69.654955  68.974929  70.827779   \n",
       "3     62.984053  61.529815  73.696280  69.659422  68.807551  70.924403   \n",
       "4     63.313694  61.670413  73.625580  69.657956  68.641912  71.022581   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  65.986037  52.906771  72.026893  70.616229  71.559465  75.327743   \n",
       "2439  65.824532  52.921059  71.964983  70.654036  71.570827  75.516862   \n",
       "2440  65.658370  52.938117  71.897161  70.690490  71.585216  75.707701   \n",
       "2441  65.488250  52.958021  71.823691  70.725859  71.602832  75.900074   \n",
       "2442  65.315066  52.980600  71.744712  70.760411  71.623714  76.093722   \n",
       "\n",
       "       sensor47   sensor48  \n",
       "0     60.969641  71.643253  \n",
       "1     60.837893  71.609484  \n",
       "2     60.708759  71.577138  \n",
       "3     60.581964  71.546492  \n",
       "4     60.456878  71.517763  \n",
       "...         ...        ...  \n",
       "2438  65.497743  61.041995  \n",
       "2439  65.328859  61.156410  \n",
       "2440  65.159436  61.274501  \n",
       "2441  64.989387  61.396096  \n",
       "2442  64.818402  61.520882  \n",
       "\n",
       "[2443 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88f5b",
   "metadata": {},
   "source": [
    "# Taking Sensor 01 - Sensor 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fad6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor1</th>\n",
       "      <th>sensor2</th>\n",
       "      <th>sensor3</th>\n",
       "      <th>sensor4</th>\n",
       "      <th>sensor5</th>\n",
       "      <th>sensor6</th>\n",
       "      <th>sensor7</th>\n",
       "      <th>sensor8</th>\n",
       "      <th>sensor9</th>\n",
       "      <th>sensor10</th>\n",
       "      <th>sensor11</th>\n",
       "      <th>sensor12</th>\n",
       "      <th>sensor13</th>\n",
       "      <th>sensor14</th>\n",
       "      <th>sensor15</th>\n",
       "      <th>sensor16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.255262</td>\n",
       "      <td>71.971142</td>\n",
       "      <td>59.297033</td>\n",
       "      <td>70.168324</td>\n",
       "      <td>72.223257</td>\n",
       "      <td>78.156174</td>\n",
       "      <td>70.720922</td>\n",
       "      <td>71.671559</td>\n",
       "      <td>66.034745</td>\n",
       "      <td>68.503224</td>\n",
       "      <td>56.242402</td>\n",
       "      <td>64.012601</td>\n",
       "      <td>74.662423</td>\n",
       "      <td>68.449226</td>\n",
       "      <td>73.662662</td>\n",
       "      <td>69.929853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.029172</td>\n",
       "      <td>72.095736</td>\n",
       "      <td>59.195081</td>\n",
       "      <td>70.251247</td>\n",
       "      <td>72.172055</td>\n",
       "      <td>78.132133</td>\n",
       "      <td>70.750461</td>\n",
       "      <td>71.889199</td>\n",
       "      <td>66.181062</td>\n",
       "      <td>68.578317</td>\n",
       "      <td>56.599534</td>\n",
       "      <td>64.139680</td>\n",
       "      <td>74.514127</td>\n",
       "      <td>68.663798</td>\n",
       "      <td>73.424355</td>\n",
       "      <td>70.094454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.807358</td>\n",
       "      <td>72.215594</td>\n",
       "      <td>59.102536</td>\n",
       "      <td>70.334800</td>\n",
       "      <td>72.118766</td>\n",
       "      <td>78.106993</td>\n",
       "      <td>70.778609</td>\n",
       "      <td>72.106459</td>\n",
       "      <td>66.327732</td>\n",
       "      <td>68.653060</td>\n",
       "      <td>56.948999</td>\n",
       "      <td>64.268428</td>\n",
       "      <td>74.363041</td>\n",
       "      <td>68.883591</td>\n",
       "      <td>73.184962</td>\n",
       "      <td>70.253904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.590422</td>\n",
       "      <td>72.330512</td>\n",
       "      <td>59.019202</td>\n",
       "      <td>70.419252</td>\n",
       "      <td>72.063870</td>\n",
       "      <td>78.081097</td>\n",
       "      <td>70.804871</td>\n",
       "      <td>72.323391</td>\n",
       "      <td>66.474047</td>\n",
       "      <td>68.727520</td>\n",
       "      <td>57.290441</td>\n",
       "      <td>64.398580</td>\n",
       "      <td>74.209019</td>\n",
       "      <td>69.108973</td>\n",
       "      <td>72.944439</td>\n",
       "      <td>70.408272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.378722</td>\n",
       "      <td>72.440584</td>\n",
       "      <td>58.944518</td>\n",
       "      <td>70.504615</td>\n",
       "      <td>72.007938</td>\n",
       "      <td>78.054497</td>\n",
       "      <td>70.828614</td>\n",
       "      <td>72.539718</td>\n",
       "      <td>66.619117</td>\n",
       "      <td>68.801921</td>\n",
       "      <td>57.623636</td>\n",
       "      <td>64.530016</td>\n",
       "      <td>74.051985</td>\n",
       "      <td>69.340003</td>\n",
       "      <td>72.702958</td>\n",
       "      <td>70.557406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>73.098106</td>\n",
       "      <td>72.579984</td>\n",
       "      <td>70.393482</td>\n",
       "      <td>57.539933</td>\n",
       "      <td>76.493660</td>\n",
       "      <td>77.698430</td>\n",
       "      <td>73.453426</td>\n",
       "      <td>68.863653</td>\n",
       "      <td>71.662133</td>\n",
       "      <td>68.616475</td>\n",
       "      <td>61.212405</td>\n",
       "      <td>50.312288</td>\n",
       "      <td>74.940493</td>\n",
       "      <td>77.409982</td>\n",
       "      <td>73.606235</td>\n",
       "      <td>73.706958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>73.069455</td>\n",
       "      <td>72.686191</td>\n",
       "      <td>70.593724</td>\n",
       "      <td>57.422708</td>\n",
       "      <td>76.310721</td>\n",
       "      <td>77.797643</td>\n",
       "      <td>73.449306</td>\n",
       "      <td>68.746579</td>\n",
       "      <td>71.740129</td>\n",
       "      <td>68.624573</td>\n",
       "      <td>61.116749</td>\n",
       "      <td>50.351219</td>\n",
       "      <td>74.962081</td>\n",
       "      <td>77.545358</td>\n",
       "      <td>73.740910</td>\n",
       "      <td>73.980327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>73.044731</td>\n",
       "      <td>72.796004</td>\n",
       "      <td>70.796814</td>\n",
       "      <td>57.304143</td>\n",
       "      <td>76.124633</td>\n",
       "      <td>77.899235</td>\n",
       "      <td>73.448736</td>\n",
       "      <td>68.627163</td>\n",
       "      <td>71.822711</td>\n",
       "      <td>68.629821</td>\n",
       "      <td>61.020777</td>\n",
       "      <td>50.392217</td>\n",
       "      <td>74.982791</td>\n",
       "      <td>77.682613</td>\n",
       "      <td>73.877073</td>\n",
       "      <td>74.254373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>73.024322</td>\n",
       "      <td>72.909059</td>\n",
       "      <td>71.003291</td>\n",
       "      <td>57.183712</td>\n",
       "      <td>75.934742</td>\n",
       "      <td>78.003086</td>\n",
       "      <td>73.451758</td>\n",
       "      <td>68.505253</td>\n",
       "      <td>71.909019</td>\n",
       "      <td>68.631663</td>\n",
       "      <td>60.924331</td>\n",
       "      <td>50.435097</td>\n",
       "      <td>75.002745</td>\n",
       "      <td>77.821806</td>\n",
       "      <td>74.014988</td>\n",
       "      <td>74.529051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>73.008414</td>\n",
       "      <td>73.024663</td>\n",
       "      <td>71.213720</td>\n",
       "      <td>57.061155</td>\n",
       "      <td>75.740776</td>\n",
       "      <td>78.108850</td>\n",
       "      <td>73.458031</td>\n",
       "      <td>68.380598</td>\n",
       "      <td>71.998276</td>\n",
       "      <td>68.629823</td>\n",
       "      <td>60.827240</td>\n",
       "      <td>50.479714</td>\n",
       "      <td>75.022181</td>\n",
       "      <td>77.963016</td>\n",
       "      <td>74.155124</td>\n",
       "      <td>74.804433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sensor1    sensor2    sensor3    sensor4    sensor5    sensor6  \\\n",
       "0     72.255262  71.971142  59.297033  70.168324  72.223257  78.156174   \n",
       "1     72.029172  72.095736  59.195081  70.251247  72.172055  78.132133   \n",
       "2     71.807358  72.215594  59.102536  70.334800  72.118766  78.106993   \n",
       "3     71.590422  72.330512  59.019202  70.419252  72.063870  78.081097   \n",
       "4     71.378722  72.440584  58.944518  70.504615  72.007938  78.054497   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.098106  72.579984  70.393482  57.539933  76.493660  77.698430   \n",
       "2439  73.069455  72.686191  70.593724  57.422708  76.310721  77.797643   \n",
       "2440  73.044731  72.796004  70.796814  57.304143  76.124633  77.899235   \n",
       "2441  73.024322  72.909059  71.003291  57.183712  75.934742  78.003086   \n",
       "2442  73.008414  73.024663  71.213720  57.061155  75.740776  78.108850   \n",
       "\n",
       "        sensor7    sensor8    sensor9   sensor10   sensor11   sensor12  \\\n",
       "0     70.720922  71.671559  66.034745  68.503224  56.242402  64.012601   \n",
       "1     70.750461  71.889199  66.181062  68.578317  56.599534  64.139680   \n",
       "2     70.778609  72.106459  66.327732  68.653060  56.948999  64.268428   \n",
       "3     70.804871  72.323391  66.474047  68.727520  57.290441  64.398580   \n",
       "4     70.828614  72.539718  66.619117  68.801921  57.623636  64.530016   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2438  73.453426  68.863653  71.662133  68.616475  61.212405  50.312288   \n",
       "2439  73.449306  68.746579  71.740129  68.624573  61.116749  50.351219   \n",
       "2440  73.448736  68.627163  71.822711  68.629821  61.020777  50.392217   \n",
       "2441  73.451758  68.505253  71.909019  68.631663  60.924331  50.435097   \n",
       "2442  73.458031  68.380598  71.998276  68.629823  60.827240  50.479714   \n",
       "\n",
       "       sensor13   sensor14   sensor15   sensor16  \n",
       "0     74.662423  68.449226  73.662662  69.929853  \n",
       "1     74.514127  68.663798  73.424355  70.094454  \n",
       "2     74.363041  68.883591  73.184962  70.253904  \n",
       "3     74.209019  69.108973  72.944439  70.408272  \n",
       "4     74.051985  69.340003  72.702958  70.557406  \n",
       "...         ...        ...        ...        ...  \n",
       "2438  74.940493  77.409982  73.606235  73.706958  \n",
       "2439  74.962081  77.545358  73.740910  73.980327  \n",
       "2440  74.982791  77.682613  73.877073  74.254373  \n",
       "2441  75.002745  77.821806  74.014988  74.529051  \n",
       "2442  75.022181  77.963016  74.155124  74.804433  \n",
       "\n",
       "[2443 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensors_data = pd.concat([sensors_data.iloc[:,:16]], axis=1)\n",
    "sensors_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6e98b",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bef9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_data = pd.DataFrame(sensors_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6e6ea",
   "metadata": {},
   "source": [
    "### Position Data -- Labeling Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ee3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data.columns = ['Pos X', 'Pos Y', 'Pos Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0422e1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos X</th>\n",
       "      <th>Pos Y</th>\n",
       "      <th>Pos Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-45.581275</td>\n",
       "      <td>30.239368</td>\n",
       "      <td>-60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-45.188830</td>\n",
       "      <td>30.181623</td>\n",
       "      <td>-59.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-44.791865</td>\n",
       "      <td>30.131806</td>\n",
       "      <td>-59.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-44.390422</td>\n",
       "      <td>30.089935</td>\n",
       "      <td>-59.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-43.984540</td>\n",
       "      <td>30.056029</td>\n",
       "      <td>-59.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>-59.939858</td>\n",
       "      <td>51.788725</td>\n",
       "      <td>37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>-59.963718</td>\n",
       "      <td>51.389997</td>\n",
       "      <td>37.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>-59.981583</td>\n",
       "      <td>50.990713</td>\n",
       "      <td>37.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>-59.993448</td>\n",
       "      <td>50.591032</td>\n",
       "      <td>37.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>-59.999315</td>\n",
       "      <td>50.191116</td>\n",
       "      <td>37.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2443 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pos X      Pos Y  Pos Z\n",
       "0    -45.581275  30.239368 -60.00\n",
       "1    -45.188830  30.181623 -59.96\n",
       "2    -44.791865  30.131806 -59.92\n",
       "3    -44.390422  30.089935 -59.88\n",
       "4    -43.984540  30.056029 -59.84\n",
       "...         ...        ...    ...\n",
       "2438 -59.939858  51.788725  37.52\n",
       "2439 -59.963718  51.389997  37.56\n",
       "2440 -59.981583  50.990713  37.60\n",
       "2441 -59.993448  50.591032  37.64\n",
       "2442 -59.999315  50.191116  37.68\n",
       "\n",
       "[2443 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b88633",
   "metadata": {},
   "source": [
    "### Converting to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_data = pd.DataFrame(position_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742983ae",
   "metadata": {},
   "source": [
    "# Converting Numpy Arrays to Pandas DataFrames for Sensor and Position Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343d8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sensors_data\n",
    "y = position_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee6c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy array into pandas dataframe\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83978bb",
   "metadata": {},
   "source": [
    "# 1. Importing Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5e2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.layers import LSTM, BatchNormalization, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b46",
   "metadata": {},
   "source": [
    "# 2. Define Network with KFold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a45037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "time_taken = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b10dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1084.3760 - val_loss: 865.1582\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 692.5358 - val_loss: 595.0288\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 484.9948 - val_loss: 489.9920\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 373.2188 - val_loss: 466.5726\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 306.3549 - val_loss: 290.3843\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 271.0459 - val_loss: 276.4287\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 208.8212 - val_loss: 187.6402\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 115.2799 - val_loss: 92.8863\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 64.6367 - val_loss: 50.9532\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 36.6960 - val_loss: 32.0060\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 25.0533 - val_loss: 25.0662\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 17.0485 - val_loss: 34.1449\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 13.2068 - val_loss: 17.8825\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.5508 - val_loss: 24.9153\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 9.3244 - val_loss: 22.3644\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 7.2830 - val_loss: 41.9032\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.9834 - val_loss: 8.4317\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4933 - val_loss: 10.3683\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.8645 - val_loss: 31.3234\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.4140 - val_loss: 4.7394\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 3.8044 - val_loss: 5.4429\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.9175 - val_loss: 5.3401\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.0112 - val_loss: 7.9450\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1084 - val_loss: 11.8106\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8466 - val_loss: 5.2493\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.2938 - val_loss: 40.2895\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 2.6047 - val_loss: 12.3467\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2092 - val_loss: 3.3698\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3061 - val_loss: 39.2926\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.7662 - val_loss: 5.3073\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4402 - val_loss: 12.4653\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.8393 - val_loss: 4.3713\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1573 - val_loss: 2.3959\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9736 - val_loss: 1.2452\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9717 - val_loss: 1.5853\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7915 - val_loss: 2.1165\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0570 - val_loss: 3.0766\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5959 - val_loss: 9.7544\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.4792 - val_loss: 152.8758\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1406 - val_loss: 1.0389\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8469 - val_loss: 1.0506\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6510 - val_loss: 0.8348\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8298 - val_loss: 1.8819\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7775 - val_loss: 0.8627\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.7501 - val_loss: 113.6525\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.3078 - val_loss: 1.5215\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5968 - val_loss: 1.0277\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4600 - val_loss: 0.9565\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5235 - val_loss: 1.4357\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6730 - val_loss: 0.6658\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5300 - val_loss: 3.3287\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5045 - val_loss: 1.3217\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.7307 - val_loss: 141.4503\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3411 - val_loss: 0.7826\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3711 - val_loss: 0.6951\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3863 - val_loss: 0.5786\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4474 - val_loss: 1.2666\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2424 - val_loss: 12.9748\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.0276 - val_loss: 0.4695\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3165 - val_loss: 0.6924\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2903 - val_loss: 0.6059\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3546 - val_loss: 1.9419\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3931 - val_loss: 1.6987\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4080 - val_loss: 0.6718\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5670 - val_loss: 2.4632\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5306 - val_loss: 2.2560\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5175 - val_loss: 1.3122\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5345 - val_loss: 1.9652\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8764 - val_loss: 148.5860\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0336 - val_loss: 0.4761\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3463 - val_loss: 0.9630\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2830 - val_loss: 0.3821\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2410 - val_loss: 0.6160\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2847 - val_loss: 0.8709\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3839 - val_loss: 1.2416\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4691 - val_loss: 0.4510\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3504 - val_loss: 1.0304\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6967 - val_loss: 3.7053\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5030 - val_loss: 1.0423\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3453 - val_loss: 1.0490\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4103 - val_loss: 3.9835\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5938 - val_loss: 1.4323\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5869 - val_loss: 2.0231\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3705 - val_loss: 0.4107\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2817 - val_loss: 0.8297\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3341 - val_loss: 1.3799\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.7433 - val_loss: 1.5613\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3111 - val_loss: 0.3205\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1664 - val_loss: 0.2950\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1771 - val_loss: 0.2505\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1867 - val_loss: 0.3831\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2301 - val_loss: 0.3820\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2546 - val_loss: 1.3828\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3771 - val_loss: 1.0648\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8545 - val_loss: 0.2588\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1736 - val_loss: 0.2345\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1360 - val_loss: 0.1884\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1279 - val_loss: 0.1778\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1537 - val_loss: 0.2409\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.1774 - val_loss: 0.3839\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1798 - val_loss: 0.4279\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3207 - val_loss: 1.7339\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2187 - val_loss: 0.7944\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2007 - val_loss: 0.8038\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3433 - val_loss: 0.7981\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3149 - val_loss: 0.5227\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2440 - val_loss: 0.4409\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2594 - val_loss: 0.6579\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2963 - val_loss: 0.5646\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2651 - val_loss: 1.0810\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.4090 - val_loss: 0.5964\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1840 - val_loss: 0.1269\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1175 - val_loss: 0.2802\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1057 - val_loss: 0.1714\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1074 - val_loss: 0.1738\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1289 - val_loss: 0.2672\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1346 - val_loss: 0.1811\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1687 - val_loss: 0.3388\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2194 - val_loss: 0.2644\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2106 - val_loss: 0.5628\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2161 - val_loss: 0.2999\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2970 - val_loss: 1.2622\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3790 - val_loss: 1.9173\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2213 - val_loss: 0.4849\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2184 - val_loss: 0.7589\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2164 - val_loss: 0.5875\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2532 - val_loss: 1.8374\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4575 - val_loss: 0.3454\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1649 - val_loss: 0.7584\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1775 - val_loss: 0.7892\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2734 - val_loss: 0.6896\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1908 - val_loss: 0.4146\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1865 - val_loss: 0.3449\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2818 - val_loss: 0.8164\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2356 - val_loss: 1.1933\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1763 - val_loss: 0.3623\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2540 - val_loss: 1.6283\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2152 - val_loss: 0.2873\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2249 - val_loss: 0.6476\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1497 - val_loss: 0.2575\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1908 - val_loss: 1.8632\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.6840 - val_loss: 0.2657\n",
      "16/16 [==============================] - 1s 3ms/step\n",
      "Evaluation Results for Fold 1\n",
      "Mean Squared Error (MSE): 0.12690440739394243\n",
      "Mean Absolute Error (MAE): 0.2603339855614412\n",
      "Root Mean Squared Error (RMSE): 0.35623644871621774\n",
      "Time taken: 437.7806921005249\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1129.0469 - val_loss: 936.5422\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 807.0286 - val_loss: 707.1292\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 578.1280 - val_loss: 496.5530\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 416.8592 - val_loss: 477.4327\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 312.8789 - val_loss: 296.4335\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 239.2424 - val_loss: 216.6880\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 140.8896 - val_loss: 132.3705\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.7586 - val_loss: 97.8951\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 40.2764 - val_loss: 168.8838\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.7885 - val_loss: 58.8188\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 19.8876 - val_loss: 24.2969\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 15.0549 - val_loss: 30.2980\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 10.0705 - val_loss: 10.1330\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.1258 - val_loss: 33.5933\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.3728 - val_loss: 33.2118\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 9.5259 - val_loss: 11.4165\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.1709 - val_loss: 7.6527\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3265 - val_loss: 20.6213\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.5594 - val_loss: 197.7951\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.1174 - val_loss: 10.7445\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2378 - val_loss: 9.9261\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1382 - val_loss: 8.8209\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.8964 - val_loss: 9.9888\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.2577 - val_loss: 8.4766\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.5156 - val_loss: 6.7151\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7151 - val_loss: 18.2065\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.9730 - val_loss: 3.3921\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.6747 - val_loss: 3.4732\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2690 - val_loss: 4.5046\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3551 - val_loss: 1.4976\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1990 - val_loss: 1.9778\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.5447 - val_loss: 2.4038\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 6.0083 - val_loss: 14.3125\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.2519 - val_loss: 1.6698\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8670 - val_loss: 2.3212\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8556 - val_loss: 0.8383\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7386 - val_loss: 1.0632\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 1.1036 - val_loss: 1.7265\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9119 - val_loss: 3.1354\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.7845 - val_loss: 1.3952\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7318 - val_loss: 0.8931\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7914 - val_loss: 1.5651\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7346 - val_loss: 1.2419\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7419 - val_loss: 3.3003\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9174 - val_loss: 4.4866\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8233 - val_loss: 38.5158\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.8073 - val_loss: 1.4259\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5128 - val_loss: 0.8102\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4057 - val_loss: 0.8188\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4834 - val_loss: 0.6520\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6058 - val_loss: 2.3143\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.6063 - val_loss: 4.0683\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4991 - val_loss: 0.7300\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6046 - val_loss: 3.7822\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5539 - val_loss: 1.4510\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6342 - val_loss: 5.4398\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8448 - val_loss: 3.5519\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.8684 - val_loss: 0.9805\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3878 - val_loss: 0.6397\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3180 - val_loss: 0.6324\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3255 - val_loss: 0.3642\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3274 - val_loss: 0.7608\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4917 - val_loss: 2.1984\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4773 - val_loss: 1.4375\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2553 - val_loss: 115.7010\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6064 - val_loss: 0.8946\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3132 - val_loss: 0.3407\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2681 - val_loss: 0.2511\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2474 - val_loss: 1.8317\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3762 - val_loss: 0.3015\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5160 - val_loss: 8.6504\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8640 - val_loss: 0.5406\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2660 - val_loss: 1.2512\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4696 - val_loss: 3.8226\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4934 - val_loss: 0.9409\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3812 - val_loss: 0.5342\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3786 - val_loss: 1.9280\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4595 - val_loss: 6.7838\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5321 - val_loss: 4.6505\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4001 - val_loss: 0.5721\n",
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4955 - val_loss: 3.3101\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3309 - val_loss: 1.2389\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5465 - val_loss: 0.7207\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3194 - val_loss: 2.4785\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3001 - val_loss: 1.1484\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3958 - val_loss: 2.3618\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4820 - val_loss: 23.3501\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5926 - val_loss: 1.6980\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2364 - val_loss: 0.2164\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1607 - val_loss: 0.2833\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1580 - val_loss: 0.5226\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1599 - val_loss: 0.3503\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1705 - val_loss: 0.2720\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1840 - val_loss: 0.2685\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2167 - val_loss: 2.8670\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2819 - val_loss: 0.4982\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2734 - val_loss: 2.4436\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3921 - val_loss: 0.6549\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3265 - val_loss: 1.1349\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2505 - val_loss: 0.4535\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2668 - val_loss: 0.5047\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3504 - val_loss: 0.9809\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.0976 - val_loss: 43.8375\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1251 - val_loss: 0.6470\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2982 - val_loss: 0.4090\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1199 - val_loss: 0.1626\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1154 - val_loss: 0.2456\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1214 - val_loss: 0.6225\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1131 - val_loss: 0.1862\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1336 - val_loss: 0.2864\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.1721 - val_loss: 0.2939\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2932 - val_loss: 0.4590\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2355 - val_loss: 0.2779\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1852 - val_loss: 0.5374\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2764 - val_loss: 0.5149\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3422 - val_loss: 1.0140\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2218 - val_loss: 0.6356\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2397 - val_loss: 0.3602\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2535 - val_loss: 0.9251\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1903 - val_loss: 1.4611\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4496 - val_loss: 24.6562\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.8660 - val_loss: 0.5730\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1384 - val_loss: 0.3850\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.0995 - val_loss: 0.1988\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1069 - val_loss: 0.1386\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1069 - val_loss: 0.1661\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1149 - val_loss: 0.1837\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1031 - val_loss: 0.1737\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1463 - val_loss: 0.6833\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1659 - val_loss: 1.5367\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2246 - val_loss: 0.6312\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2051 - val_loss: 0.3373\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1896 - val_loss: 0.8427\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2544 - val_loss: 0.2871\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1598 - val_loss: 0.8889\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2641 - val_loss: 0.6409\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2690 - val_loss: 1.1978\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1652 - val_loss: 0.4052\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1613 - val_loss: 1.8753\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1790 - val_loss: 0.6500\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2488 - val_loss: 1.6203\n",
      "Epoch 142/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1826 - val_loss: 0.7813\n",
      "Epoch 143/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3168 - val_loss: 0.4749\n",
      "Epoch 144/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2031 - val_loss: 0.4099\n",
      "Epoch 145/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1544 - val_loss: 0.4485\n",
      "Epoch 146/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1305 - val_loss: 0.6172\n",
      "Epoch 147/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2417 - val_loss: 4.3068\n",
      "Epoch 148/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1462 - val_loss: 0.3303\n",
      "Epoch 149/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3315 - val_loss: 8.0354\n",
      "Epoch 150/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3909 - val_loss: 0.4185\n",
      "Epoch 151/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1023 - val_loss: 0.2015\n",
      "Epoch 152/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1125 - val_loss: 0.3123\n",
      "Epoch 153/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.0996 - val_loss: 0.2013\n",
      "Epoch 154/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1247 - val_loss: 0.3477\n",
      "Epoch 155/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2336 - val_loss: 0.6524\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 2\n",
      "Mean Squared Error (MSE): 0.13865322122812956\n",
      "Mean Absolute Error (MAE): 0.26645378785173796\n",
      "Root Mean Squared Error (RMSE): 0.3723616806656259\n",
      "Time taken: 479.85332322120667\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1127.3746 - val_loss: 897.3284\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 743.7472 - val_loss: 627.4429\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 504.2708 - val_loss: 519.8615\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 345.2198 - val_loss: 337.5310\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 222.6311 - val_loss: 221.5943\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 124.0952 - val_loss: 112.8481\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 72.0360 - val_loss: 69.2048\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 39.4399 - val_loss: 35.8483\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 27.3563 - val_loss: 36.6549\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 20.2579 - val_loss: 37.1405\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.1703 - val_loss: 37.5164\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.5730 - val_loss: 25.0504\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.5744 - val_loss: 21.7016\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 6.7136 - val_loss: 12.1146\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.6274 - val_loss: 6.2563\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.5216 - val_loss: 21.1553\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.8533 - val_loss: 15.6173\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.3636 - val_loss: 25.6699\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 4.9243 - val_loss: 8.7176\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6442 - val_loss: 4.0349\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6902 - val_loss: 3.7330\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.8651 - val_loss: 2.9092\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4421 - val_loss: 2.0050\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8028 - val_loss: 1.8247\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4668 - val_loss: 2.3393\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1093 - val_loss: 7.4022\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.6289 - val_loss: 8.7567\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.4132 - val_loss: 2.7145\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.1863 - val_loss: 2.7254\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.2717 - val_loss: 2.0326\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9966 - val_loss: 1.1443\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8197 - val_loss: 1.6283\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.9076 - val_loss: 2.7672\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7768 - val_loss: 0.7380\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.2554 - val_loss: 29.8854\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5820 - val_loss: 3.3869\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8415 - val_loss: 1.4723\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.6618 - val_loss: 1.6744\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7454 - val_loss: 2.1245\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.8606 - val_loss: 27.3388\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3580 - val_loss: 5.8585\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5350 - val_loss: 0.6111\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4821 - val_loss: 0.7451\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.1840 - val_loss: 0.9745\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6690 - val_loss: 1.4746\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.5508 - val_loss: 12.2988\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2857 - val_loss: 0.7751\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4815 - val_loss: 1.2244\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4471 - val_loss: 0.6945\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4472 - val_loss: 1.1435\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5351 - val_loss: 0.6100\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.4407 - val_loss: 13.2506\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.9824 - val_loss: 0.7205\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5576 - val_loss: 1.2907\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4746 - val_loss: 0.9784\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4139 - val_loss: 0.9299\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5430 - val_loss: 0.5459\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3659 - val_loss: 0.9684\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5090 - val_loss: 1.9984\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5058 - val_loss: 1.1559\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6660 - val_loss: 0.9224\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.7544 - val_loss: 0.9828\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5491 - val_loss: 1.3423\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.6639 - val_loss: 2.1632\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5800 - val_loss: 0.7296\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3176 - val_loss: 0.5710\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2921 - val_loss: 0.3034\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2754 - val_loss: 0.2953\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4515 - val_loss: 0.6006\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3419 - val_loss: 0.7918\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4129 - val_loss: 1.5231\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3774 - val_loss: 1.7577\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3527 - val_loss: 1.2802\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.5909 - val_loss: 0.9331\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3695 - val_loss: 0.5556\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.5239 - val_loss: 0.6323\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.8793 - val_loss: 30.8530\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2044 - val_loss: 0.5296\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3541 - val_loss: 0.3669\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2468 - val_loss: 0.5067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2724 - val_loss: 0.3554\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2215 - val_loss: 0.3896\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2581 - val_loss: 0.5671\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2837 - val_loss: 0.3327\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2376 - val_loss: 0.5132\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.4987 - val_loss: 0.8285\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 2s 7ms/step - loss: 0.2880 - val_loss: 0.6827\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3032 - val_loss: 0.5433\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4507 - val_loss: 1.8612\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3683 - val_loss: 0.6000\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3565 - val_loss: 0.5513\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3226 - val_loss: 0.6696\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3876 - val_loss: 0.5107\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4624 - val_loss: 1.5068\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5550 - val_loss: 1.1520\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2571 - val_loss: 0.4838\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2416 - val_loss: 0.6837\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1618 - val_loss: 4.0488\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 3\n",
      "Mean Squared Error (MSE): 0.29521592112645345\n",
      "Mean Absolute Error (MAE): 0.4024428417893276\n",
      "Root Mean Squared Error (RMSE): 0.5433377597097899\n",
      "Time taken: 296.40315079689026\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 7s 12ms/step - loss: 1088.2112 - val_loss: 924.8461\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 704.0260 - val_loss: 646.8217\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 490.2298 - val_loss: 492.0724\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 360.0445 - val_loss: 329.5519\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 284.4749 - val_loss: 292.7912\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 203.0092 - val_loss: 196.4765\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 119.5187 - val_loss: 84.1297\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 64.5744 - val_loss: 72.6932\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 36.4656 - val_loss: 76.2189\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 23.0168 - val_loss: 100.4223\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 15.2606 - val_loss: 25.1350\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 12.3883 - val_loss: 26.6725\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 8.5844 - val_loss: 12.1039\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 6.7902 - val_loss: 16.1064\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 7.4666 - val_loss: 19.3299\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.7816 - val_loss: 7.8340\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 6.7387 - val_loss: 31.1871\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.3286 - val_loss: 4.3683\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.4245 - val_loss: 7.7201\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 4.3084 - val_loss: 5.7556\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 4.4535 - val_loss: 5.4626\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.8665 - val_loss: 3.7933\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.6110 - val_loss: 3.3834\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.3603 - val_loss: 11.7817\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2064 - val_loss: 3.7094\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.5429 - val_loss: 21.0664\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 2.4325 - val_loss: 3.3656\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4373 - val_loss: 4.5215\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.5220 - val_loss: 5.2910\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.6726 - val_loss: 2.8513\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.3486 - val_loss: 13.8786\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.0866 - val_loss: 5.2379\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.5991 - val_loss: 3.7664\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0143 - val_loss: 2.0929\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3964 - val_loss: 1.8330\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.5044 - val_loss: 142.0382\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8471 - val_loss: 1.7293\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.3520 - val_loss: 3.2178\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7585 - val_loss: 0.8507\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6319 - val_loss: 1.0506\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6423 - val_loss: 1.5790\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.8469 - val_loss: 2.8504\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7530 - val_loss: 1.1139\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5717 - val_loss: 2.2107\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.1780 - val_loss: 1.6481\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5630 - val_loss: 0.9346\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4441 - val_loss: 2.4722\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7319 - val_loss: 3.8702\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.9764 - val_loss: 1.5288\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.6476 - val_loss: 1.5002\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5883 - val_loss: 0.9203\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.4988 - val_loss: 10.4685\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.8529 - val_loss: 0.5794\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4178 - val_loss: 0.8337\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3646 - val_loss: 0.3541\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3165 - val_loss: 0.9877\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.3991 - val_loss: 0.6955\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4072 - val_loss: 2.0416\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5109 - val_loss: 1.1470\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5682 - val_loss: 0.5748\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6084 - val_loss: 1.9764\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6347 - val_loss: 2.1109\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4844 - val_loss: 1.9821\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5349 - val_loss: 0.6089\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.9362 - val_loss: 30.5515\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4876 - val_loss: 0.5087\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2535 - val_loss: 0.6283\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2333 - val_loss: 0.2797\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2272 - val_loss: 0.6681\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2863 - val_loss: 0.5577\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2865 - val_loss: 0.4437\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3300 - val_loss: 1.4493\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4055 - val_loss: 0.6864\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3353 - val_loss: 0.5064\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.7441 - val_loss: 68.9652\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8682 - val_loss: 0.3191\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2145 - val_loss: 0.2471\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1936 - val_loss: 0.5896\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1758 - val_loss: 0.2395\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1673 - val_loss: 0.2397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2210 - val_loss: 0.4877\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2513 - val_loss: 0.5167\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2826 - val_loss: 1.3519\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3246 - val_loss: 0.5405\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3836 - val_loss: 0.5265\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3621 - val_loss: 0.8489\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4563 - val_loss: 4.0076\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2980 - val_loss: 0.8961\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2810 - val_loss: 0.8879\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.4361 - val_loss: 0.5821\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2839 - val_loss: 0.7715\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.3837 - val_loss: 2.8334\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.7385 - val_loss: 1.3154\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2466 - val_loss: 0.2661\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1543 - val_loss: 0.4177\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1321 - val_loss: 0.2143\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1429 - val_loss: 0.5591\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1408 - val_loss: 0.2932\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2077 - val_loss: 0.3331\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2135 - val_loss: 0.5089\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2845 - val_loss: 0.5068\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2541 - val_loss: 1.3595\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2560 - val_loss: 2.2041\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3509 - val_loss: 0.6917\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2635 - val_loss: 0.4190\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2331 - val_loss: 0.4244\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2893 - val_loss: 0.8200\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1700 - val_loss: 11.6909\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2771 - val_loss: 0.1808\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1193 - val_loss: 0.1598\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1088 - val_loss: 0.1089\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1063 - val_loss: 0.2112\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1059 - val_loss: 0.1499\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1067 - val_loss: 0.4257\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1373 - val_loss: 0.1791\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1560 - val_loss: 0.2547\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1755 - val_loss: 0.5460\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2531 - val_loss: 0.3916\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2417 - val_loss: 0.3083\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1791 - val_loss: 0.3642\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2451 - val_loss: 0.4219\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.4954 - val_loss: 1.1167\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3305 - val_loss: 0.2442\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1966 - val_loss: 0.1497\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1609 - val_loss: 0.2306\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1340 - val_loss: 0.1599\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1269 - val_loss: 0.1258\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1278 - val_loss: 0.5625\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1681 - val_loss: 0.2874\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1977 - val_loss: 0.2561\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1792 - val_loss: 0.2368\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1831 - val_loss: 1.0876\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2563 - val_loss: 0.7650\n",
      "Epoch 134/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2777 - val_loss: 0.7100\n",
      "Epoch 135/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2013 - val_loss: 0.3074\n",
      "Epoch 136/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2300 - val_loss: 0.5601\n",
      "Epoch 137/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.1889 - val_loss: 0.2866\n",
      "Epoch 138/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2642 - val_loss: 3.1810\n",
      "Epoch 139/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.3778 - val_loss: 1.0616\n",
      "Epoch 140/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2064 - val_loss: 0.4150\n",
      "Epoch 141/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1636 - val_loss: 2.0626\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 4\n",
      "Mean Squared Error (MSE): 0.10891969692307728\n",
      "Mean Absolute Error (MAE): 0.23801219985377656\n",
      "Root Mean Squared Error (RMSE): 0.330029842473491\n",
      "Time taken: 430.2645227909088\n",
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 8s 14ms/step - loss: 1120.7667 - val_loss: 893.5078\n",
      "Epoch 2/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 742.3145 - val_loss: 639.5802\n",
      "Epoch 3/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 518.9920 - val_loss: 480.5882\n",
      "Epoch 4/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 363.0362 - val_loss: 345.7305\n",
      "Epoch 5/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 256.2154 - val_loss: 191.9564\n",
      "Epoch 6/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 149.8849 - val_loss: 110.7594\n",
      "Epoch 7/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 74.5788 - val_loss: 96.9761\n",
      "Epoch 8/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 39.2088 - val_loss: 106.1634\n",
      "Epoch 9/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 22.1672 - val_loss: 21.7739\n",
      "Epoch 10/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 14.5607 - val_loss: 34.6981\n",
      "Epoch 11/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 11.3337 - val_loss: 10.6971\n",
      "Epoch 12/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 9.3127 - val_loss: 15.8948\n",
      "Epoch 13/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 8.0230 - val_loss: 11.9994\n",
      "Epoch 14/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.8582 - val_loss: 8.3089\n",
      "Epoch 15/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 5.6828 - val_loss: 23.5293\n",
      "Epoch 16/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.2019 - val_loss: 10.6354\n",
      "Epoch 17/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.1673 - val_loss: 11.5571\n",
      "Epoch 18/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 5.0038 - val_loss: 4.3863\n",
      "Epoch 19/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1255 - val_loss: 5.1831\n",
      "Epoch 20/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.2937 - val_loss: 3.0156\n",
      "Epoch 21/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 2.3264 - val_loss: 17.6824\n",
      "Epoch 22/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.5545 - val_loss: 3.0955\n",
      "Epoch 23/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4757 - val_loss: 1.7570\n",
      "Epoch 24/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.4539 - val_loss: 2.6010\n",
      "Epoch 25/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.1032 - val_loss: 3.3088\n",
      "Epoch 26/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.1036 - val_loss: 3.8935\n",
      "Epoch 27/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 1.2781 - val_loss: 23.3709\n",
      "Epoch 28/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 1.3592 - val_loss: 2.3721\n",
      "Epoch 29/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 1.0910 - val_loss: 3.5686\n",
      "Epoch 30/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.4266 - val_loss: 4.4074\n",
      "Epoch 31/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 1.4276 - val_loss: 3.2499\n",
      "Epoch 32/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 2.0170 - val_loss: 33.8327\n",
      "Epoch 33/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 3.2300 - val_loss: 1.1758\n",
      "Epoch 34/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.7347 - val_loss: 2.8444\n",
      "Epoch 35/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.8098 - val_loss: 1.6464\n",
      "Epoch 36/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.7583 - val_loss: 1.3939\n",
      "Epoch 37/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.8650 - val_loss: 1.8022\n",
      "Epoch 38/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.9509 - val_loss: 2.8108\n",
      "Epoch 39/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.1173 - val_loss: 47.5478\n",
      "Epoch 40/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 3.0520 - val_loss: 0.8848\n",
      "Epoch 41/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7931 - val_loss: 10.6006\n",
      "Epoch 42/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 1.0665 - val_loss: 0.7088\n",
      "Epoch 43/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4992 - val_loss: 1.3036\n",
      "Epoch 44/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5628 - val_loss: 0.3849\n",
      "Epoch 45/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.5334 - val_loss: 2.0725\n",
      "Epoch 46/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6901 - val_loss: 17.1438\n",
      "Epoch 47/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 5.3969 - val_loss: 1.1309\n",
      "Epoch 48/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4233 - val_loss: 0.4560\n",
      "Epoch 49/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4315 - val_loss: 0.7765\n",
      "Epoch 50/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3751 - val_loss: 0.3581\n",
      "Epoch 51/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4504 - val_loss: 3.0654\n",
      "Epoch 52/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4882 - val_loss: 0.4386\n",
      "Epoch 53/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4368 - val_loss: 0.4413\n",
      "Epoch 54/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5634 - val_loss: 0.7513\n",
      "Epoch 55/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.5579 - val_loss: 7.7442\n",
      "Epoch 56/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.7396 - val_loss: 1.6202\n",
      "Epoch 57/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5978 - val_loss: 0.5810\n",
      "Epoch 58/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4107 - val_loss: 1.2574\n",
      "Epoch 59/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4092 - val_loss: 1.3500\n",
      "Epoch 60/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5979 - val_loss: 2.9644\n",
      "Epoch 61/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.6155 - val_loss: 1.3824\n",
      "Epoch 62/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.5815 - val_loss: 0.6085\n",
      "Epoch 63/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 3.4693 - val_loss: 1.5414\n",
      "Epoch 64/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3126 - val_loss: 0.2944\n",
      "Epoch 65/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2357 - val_loss: 0.4215\n",
      "Epoch 66/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2162 - val_loss: 0.6833\n",
      "Epoch 67/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2180 - val_loss: 0.6179\n",
      "Epoch 68/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2477 - val_loss: 0.3790\n",
      "Epoch 69/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2320 - val_loss: 0.4669\n",
      "Epoch 70/200\n",
      "326/326 [==============================] - 4s 12ms/step - loss: 0.2884 - val_loss: 0.7336\n",
      "Epoch 71/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4139 - val_loss: 1.2844\n",
      "Epoch 72/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4350 - val_loss: 1.8016\n",
      "Epoch 73/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.4606 - val_loss: 0.4026\n",
      "Epoch 74/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4397 - val_loss: 0.6239\n",
      "Epoch 75/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.4208 - val_loss: 0.8275\n",
      "Epoch 76/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3478 - val_loss: 0.7823\n",
      "Epoch 77/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4199 - val_loss: 0.8504\n",
      "Epoch 78/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 3.4671 - val_loss: 1.5249\n",
      "Epoch 79/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2874 - val_loss: 0.2224\n",
      "Epoch 80/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1883 - val_loss: 0.2009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1436 - val_loss: 0.1830\n",
      "Epoch 82/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1509 - val_loss: 0.1381\n",
      "Epoch 83/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1566 - val_loss: 0.1702\n",
      "Epoch 84/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1946 - val_loss: 0.2408\n",
      "Epoch 85/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1837 - val_loss: 0.2974\n",
      "Epoch 86/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2402 - val_loss: 0.3307\n",
      "Epoch 87/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2354 - val_loss: 0.6822\n",
      "Epoch 88/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3155 - val_loss: 0.5912\n",
      "Epoch 89/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.4538 - val_loss: 0.6567\n",
      "Epoch 90/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2610 - val_loss: 0.6750\n",
      "Epoch 91/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2414 - val_loss: 0.4440\n",
      "Epoch 92/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2699 - val_loss: 1.2332\n",
      "Epoch 93/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3337 - val_loss: 1.1885\n",
      "Epoch 94/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2779 - val_loss: 2.5518\n",
      "Epoch 95/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3067 - val_loss: 0.7978\n",
      "Epoch 96/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3442 - val_loss: 1.0538\n",
      "Epoch 97/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.3753 - val_loss: 0.2912\n",
      "Epoch 98/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.2701 - val_loss: 1.1716\n",
      "Epoch 99/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.3479 - val_loss: 0.7322\n",
      "Epoch 100/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 4.0627 - val_loss: 11.1506\n",
      "Epoch 101/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.4221 - val_loss: 0.3637\n",
      "Epoch 102/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1553 - val_loss: 0.1170\n",
      "Epoch 103/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1155 - val_loss: 0.0942\n",
      "Epoch 104/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1103 - val_loss: 0.1124\n",
      "Epoch 105/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1108 - val_loss: 0.1313\n",
      "Epoch 106/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1135 - val_loss: 0.2253\n",
      "Epoch 107/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1273 - val_loss: 0.2819\n",
      "Epoch 108/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1542 - val_loss: 0.1902\n",
      "Epoch 109/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2411 - val_loss: 0.9484\n",
      "Epoch 110/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2296 - val_loss: 0.1732\n",
      "Epoch 111/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1839 - val_loss: 0.2783\n",
      "Epoch 112/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2182 - val_loss: 0.2975\n",
      "Epoch 113/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2431 - val_loss: 3.3573\n",
      "Epoch 114/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.3106 - val_loss: 0.3180\n",
      "Epoch 115/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1891 - val_loss: 0.3844\n",
      "Epoch 116/200\n",
      "326/326 [==============================] - 4s 11ms/step - loss: 0.2276 - val_loss: 0.4134\n",
      "Epoch 117/200\n",
      "326/326 [==============================] - 3s 11ms/step - loss: 0.2011 - val_loss: 0.3924\n",
      "Epoch 118/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.2015 - val_loss: 0.4490\n",
      "Epoch 119/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2408 - val_loss: 1.1485\n",
      "Epoch 120/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.6275 - val_loss: 1.0777\n",
      "Epoch 121/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1936 - val_loss: 0.4013\n",
      "Epoch 122/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1943 - val_loss: 0.6756\n",
      "Epoch 123/200\n",
      "326/326 [==============================] - 2s 8ms/step - loss: 0.1807 - val_loss: 0.7506\n",
      "Epoch 124/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1738 - val_loss: 0.6485\n",
      "Epoch 125/200\n",
      "326/326 [==============================] - 3s 10ms/step - loss: 0.1985 - val_loss: 0.4734\n",
      "Epoch 126/200\n",
      "326/326 [==============================] - 3s 8ms/step - loss: 0.1794 - val_loss: 0.4602\n",
      "Epoch 127/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2148 - val_loss: 0.5509\n",
      "Epoch 128/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2437 - val_loss: 0.5239\n",
      "Epoch 129/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2272 - val_loss: 0.2271\n",
      "Epoch 130/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1907 - val_loss: 1.2074\n",
      "Epoch 131/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.2419 - val_loss: 0.3103\n",
      "Epoch 132/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1372 - val_loss: 0.3363\n",
      "Epoch 133/200\n",
      "326/326 [==============================] - 3s 9ms/step - loss: 0.1912 - val_loss: 0.3359\n",
      "16/16 [==============================] - 1s 5ms/step\n",
      "Evaluation Results for Fold 5\n",
      "Mean Squared Error (MSE): 0.09421519090016656\n",
      "Mean Absolute Error (MAE): 0.23290468951455442\n",
      "Root Mean Squared Error (RMSE): 0.30694493138047835\n",
      "Time taken: 403.6537148952484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(128))\n",
    "    \n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(3))\n",
    "    \n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200, batch_size=6,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    rmse_scores.append(rmse)\n",
    "    time_taken.append(end_time - start_time)\n",
    "\n",
    "    print(\"Evaluation Results for Fold\", fold)\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Time taken:\", end_time - start_time)\n",
    "    print()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f170f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 16, 512)           1052672   \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 512)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16, 256)           787456    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,047,747\n",
      "Trainable params: 2,046,723\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52768fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model,'LSTM.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683875b",
   "metadata": {},
   "source": [
    "# 3. Evaluate Network: Calculate average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a23a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = np.mean(mse_scores)\n",
    "avg_mae = np.mean(mae_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "avg_time_taken = np.mean(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d528",
   "metadata": {},
   "source": [
    "# 4. Create a DataFrame for all fold results and average results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6a3e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafem\\AppData\\Local\\Temp\\ipykernel_10324\\3174907360.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append(avg_results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': list(range(1, kfold.n_splits + 1)),\n",
    "    'MSE': mse_scores,\n",
    "    'MAE': mae_scores,\n",
    "    'RMSE': rmse_scores,\n",
    "    'Time taken': time_taken\n",
    "})\n",
    "\n",
    "# Append average results to the DataFrame\n",
    "avg_results = {'Fold': 'Average', 'MSE': avg_mse, 'MAE': avg_mae, 'RMSE': avg_rmse, 'Time taken': avg_time_taken}\n",
    "results_df = results_df.append(avg_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80271b",
   "metadata": {},
   "source": [
    "# 5. Save the results to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12fa5708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all Folds and Average Results:\n",
      "      Fold       MSE       MAE      RMSE  Time taken\n",
      "0        1  0.126904  0.260334  0.356236  437.780692\n",
      "1        2  0.138653  0.266454  0.372362  479.853323\n",
      "2        3  0.295216  0.402443  0.543338  296.403151\n",
      "3        4  0.108920  0.238012  0.330030  430.264523\n",
      "4        5  0.094215  0.232905  0.306945  403.653715\n",
      "5  Average  0.152782  0.280030  0.381782  409.591081\n",
      "Results saved to 'LSTM Results PL_model_2_smoothing2_Reg1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "results_df.to_excel('LSTM Results PL_model_2_smoothing2_Reg1.xlsx', index=False)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Results for all Folds and Average Results:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Results saved to 'LSTM Results PL_model_2_smoothing2_Reg1.xlsx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa6e",
   "metadata": {},
   "source": [
    "# 6. Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04ced195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a493e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ddfe36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
